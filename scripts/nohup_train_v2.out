nohup: ignoring input
Loading weights from /autofs/space/nicc_005/users/olchanyi/DiffSR/models_fused/model_fused_v1/checkpoint_1100.pth
Epoch 1101 of 2000
Found 300 cases for training
   Iteration 1 of 100, tot loss = 4.8497419357299805, l1: 0.00010021883645094931, l2: 0.00038475534529425204   Iteration 2 of 100, tot loss = 3.958925724029541, l1: 9.450610377825797e-05, l2: 0.00030138645524857566   Iteration 3 of 100, tot loss = 4.553200085957845, l1: 0.00010600775325049956, l2: 0.0003493122397533928   Iteration 4 of 100, tot loss = 4.5902276039123535, l1: 0.00010446665692143142, l2: 0.00035455609395285137   Iteration 5 of 100, tot loss = 4.507484722137451, l1: 0.00010158693912671879, l2: 0.0003491615265375003   Iteration 6 of 100, tot loss = 4.55262819925944, l1: 0.00010399507300462574, l2: 0.0003512677382483768   Iteration 7 of 100, tot loss = 4.2942871025630405, l1: 9.896178276643955e-05, l2: 0.0003304669205265652   Iteration 8 of 100, tot loss = 4.240352272987366, l1: 9.703571686259238e-05, l2: 0.0003269995049777208   Iteration 9 of 100, tot loss = 4.307052294413249, l1: 9.971538060603457e-05, l2: 0.0003309898456791416   Iteration 10 of 100, tot loss = 4.201846170425415, l1: 9.573788956913631e-05, l2: 0.00032444672397105025   Iteration 11 of 100, tot loss = 4.17252685806968, l1: 9.620734206030399e-05, l2: 0.00032104533932976085   Iteration 12 of 100, tot loss = 4.35585230588913, l1: 0.00010049778241712677, l2: 0.00033508744309074245   Iteration 13 of 100, tot loss = 4.423883016292866, l1: 0.00010246485096733802, l2: 0.0003399234500158435   Iteration 14 of 100, tot loss = 4.399600727217538, l1: 0.00010254417247779202, l2: 0.00033741589972383475   Iteration 15 of 100, tot loss = 4.455956443150838, l1: 0.00010154316720824378, l2: 0.00034405247521741936   Iteration 16 of 100, tot loss = 4.367108643054962, l1: 0.00010036819753622694, l2: 0.00033634266492299503   Iteration 17 of 100, tot loss = 4.492345417247099, l1: 0.00010247808227979797, l2: 0.000346756457207341   Iteration 18 of 100, tot loss = 4.5958943367004395, l1: 0.00010351917505128465, l2: 0.00035607025670793117   Iteration 19 of 100, tot loss = 4.664778784701698, l1: 0.00010326754483302418, l2: 0.0003632103318140205   Iteration 20 of 100, tot loss = 4.664935040473938, l1: 0.00010295782649336616, l2: 0.0003635356762970332   Iteration 21 of 100, tot loss = 4.670659428551083, l1: 0.00010437958674932209, l2: 0.00036268635345290283   Iteration 22 of 100, tot loss = 4.66308745470914, l1: 0.000105122781778019, l2: 0.00036118596207498655   Iteration 23 of 100, tot loss = 4.605884842250658, l1: 0.0001041178147715982, l2: 0.0003564706680860937   Iteration 24 of 100, tot loss = 4.588436345259349, l1: 0.00010444322060720879, l2: 0.0003544004121067701   Iteration 25 of 100, tot loss = 4.5043270301818845, l1: 0.00010320737914298661, l2: 0.0003472253220388666   Iteration 26 of 100, tot loss = 4.464879714525663, l1: 0.00010327648669772316, l2: 0.0003432114826864563   Iteration 27 of 100, tot loss = 4.609458976321751, l1: 0.00010486189569713099, l2: 0.0003560839982970652   Iteration 28 of 100, tot loss = 4.534262418746948, l1: 0.00010282969144879774, l2: 0.0003505965464033319   Iteration 29 of 100, tot loss = 4.567152089086072, l1: 0.0001035404598826125, l2: 0.0003531747457474984   Iteration 30 of 100, tot loss = 4.56587913831075, l1: 0.00010401865899135979, l2: 0.000352569252330189   Iteration 31 of 100, tot loss = 4.607324446401289, l1: 0.00010438293623249797, l2: 0.0003563495054315295   Iteration 32 of 100, tot loss = 4.622992470860481, l1: 0.00010459452823852189, l2: 0.00035770471640717005   Iteration 33 of 100, tot loss = 4.589975516001384, l1: 0.00010371508643870722, l2: 0.00035528246239251035   Iteration 34 of 100, tot loss = 4.618425818050609, l1: 0.00010464830312925829, l2: 0.00035719427636221926   Iteration 35 of 100, tot loss = 4.625258432115827, l1: 0.00010539108084880613, l2: 0.00035713475995830126   Iteration 36 of 100, tot loss = 4.631501661406623, l1: 0.00010528654396087707, l2: 0.00035786361938032013   Iteration 37 of 100, tot loss = 4.670525035342655, l1: 0.00010566170818197566, l2: 0.00036139079256335626   Iteration 38 of 100, tot loss = 4.706201779214959, l1: 0.00010616481564571394, l2: 0.00036445536074767774   Iteration 39 of 100, tot loss = 4.746138156988682, l1: 0.00010686848261871208, l2: 0.0003677453317392904   Iteration 40 of 100, tot loss = 4.754070389270782, l1: 0.00010730675712693483, l2: 0.00036810028104810043   Iteration 41 of 100, tot loss = 4.746254769767203, l1: 0.00010764507737249227, l2: 0.0003669803987666056   Iteration 42 of 100, tot loss = 4.833064249583653, l1: 0.00010906874965820905, l2: 0.0003742376727695089   Iteration 43 of 100, tot loss = 4.828803993934809, l1: 0.0001086412462321933, l2: 0.0003742391506188311   Iteration 44 of 100, tot loss = 4.928576079281894, l1: 0.00011049664929239292, l2: 0.00038236095596485853   Iteration 45 of 100, tot loss = 4.914039368099637, l1: 0.00011067103833839711, l2: 0.00038073289647905364   Iteration 46 of 100, tot loss = 4.927423404610676, l1: 0.00011131805671459955, l2: 0.00038142428219662577   Iteration 47 of 100, tot loss = 4.935621342760451, l1: 0.00011103636163480065, l2: 0.0003825257714352313   Iteration 48 of 100, tot loss = 4.990001519521077, l1: 0.00011155485647880899, l2: 0.0003874452941090567   Iteration 49 of 100, tot loss = 4.969110352652414, l1: 0.0001113062279895233, l2: 0.0003856048059212614   Iteration 50 of 100, tot loss = 5.046779499053955, l1: 0.00011243134547839873, l2: 0.00039224660256877543   Iteration 51 of 100, tot loss = 4.996796682769177, l1: 0.00011155134644450656, l2: 0.0003881283197437833   Iteration 52 of 100, tot loss = 4.958558064240676, l1: 0.0001108983959476553, l2: 0.00038495740843175625   Iteration 53 of 100, tot loss = 4.9492107247406585, l1: 0.00011123187759262949, l2: 0.00038368919263549923   Iteration 54 of 100, tot loss = 4.920588881881149, l1: 0.00011026393938798854, l2: 0.00038179494650749903   Iteration 55 of 100, tot loss = 4.907564059170809, l1: 0.00010990879878094843, l2: 0.00038084760440555823   Iteration 56 of 100, tot loss = 4.861876700605665, l1: 0.00010911079107245314, l2: 0.0003770768762478838   Iteration 57 of 100, tot loss = 4.876778870298152, l1: 0.0001096323208721036, l2: 0.0003780455631306068   Iteration 58 of 100, tot loss = 4.843299010704303, l1: 0.0001091051620058916, l2: 0.0003752247360133534   Iteration 59 of 100, tot loss = 4.861525947764768, l1: 0.00010964764664156789, l2: 0.00037650494506756256   Iteration 60 of 100, tot loss = 4.86832267443339, l1: 0.00011005446097745638, l2: 0.000376777803467121   Iteration 61 of 100, tot loss = 4.879320941987585, l1: 0.0001102541576327611, l2: 0.0003776779335343325   Iteration 62 of 100, tot loss = 4.8574709546181465, l1: 0.00010995822189978489, l2: 0.0003757888705642413   Iteration 63 of 100, tot loss = 4.829217698838976, l1: 0.00010954875896069487, l2: 0.00037337300812988173   Iteration 64 of 100, tot loss = 4.801548048853874, l1: 0.00010943083947267951, l2: 0.00037072396253279294   Iteration 65 of 100, tot loss = 4.782845508135282, l1: 0.000108966202186331, l2: 0.00036931834586609444   Iteration 66 of 100, tot loss = 4.802770798856562, l1: 0.00010947673223254793, l2: 0.0003708003447779144   Iteration 67 of 100, tot loss = 4.7646009637348685, l1: 0.0001085439011447632, l2: 0.0003679161923920124   Iteration 68 of 100, tot loss = 4.758009072612314, l1: 0.00010824992440414943, l2: 0.00036755097993163336   Iteration 69 of 100, tot loss = 4.770846591479536, l1: 0.00010839674117354055, l2: 0.0003686879149493019   Iteration 70 of 100, tot loss = 4.796669942992074, l1: 0.00010839069883721615, l2: 0.0003712762930912764   Iteration 71 of 100, tot loss = 4.809972820147662, l1: 0.00010856322447759507, l2: 0.00037243405482436623   Iteration 72 of 100, tot loss = 4.810224821170171, l1: 0.00010828877475432819, l2: 0.00037273370516130753   Iteration 73 of 100, tot loss = 4.804716329052024, l1: 0.00010794206133290921, l2: 0.0003725295694631027   Iteration 74 of 100, tot loss = 4.805178761482239, l1: 0.00010800152223374746, l2: 0.00037251635139172494   Iteration 75 of 100, tot loss = 4.791829163233439, l1: 0.00010764110416251545, l2: 0.00037154180948467306   Iteration 76 of 100, tot loss = 4.780226936465816, l1: 0.00010762114116483932, l2: 0.0003704015499878503   Iteration 77 of 100, tot loss = 4.776220238053954, l1: 0.00010724805122356281, l2: 0.00037037397038596745   Iteration 78 of 100, tot loss = 4.78420043297303, l1: 0.00010749191931562308, l2: 0.00037092812234708946   Iteration 79 of 100, tot loss = 4.776871672159509, l1: 0.00010745176976204767, l2: 0.00037023539546600105   Iteration 80 of 100, tot loss = 4.789832016825676, l1: 0.00010726914342740201, l2: 0.0003717140558364918   Iteration 81 of 100, tot loss = 4.771661240377544, l1: 0.00010667778001839502, l2: 0.00037048834165483483   Iteration 82 of 100, tot loss = 4.742754587313024, l1: 0.00010597634585481988, l2: 0.00036829911053845116   Iteration 83 of 100, tot loss = 4.746744902737169, l1: 0.00010607288553378074, l2: 0.0003686016023997778   Iteration 84 of 100, tot loss = 4.777014726684207, l1: 0.00010644614957570975, l2: 0.00037125532062851736   Iteration 85 of 100, tot loss = 4.773814577214858, l1: 0.00010608882452395526, l2: 0.0003712926308199873   Iteration 86 of 100, tot loss = 4.763974417087644, l1: 0.00010606538485579983, l2: 0.00037033205461379706   Iteration 87 of 100, tot loss = 4.804680117245378, l1: 0.00010666809528671345, l2: 0.0003737999136685447   Iteration 88 of 100, tot loss = 4.830705772746693, l1: 0.00010729533679213174, l2: 0.0003757752378574382   Iteration 89 of 100, tot loss = 4.825663121898523, l1: 0.00010742041493440308, l2: 0.00037514589438502667   Iteration 90 of 100, tot loss = 4.807756498124864, l1: 0.00010729295128031582, l2: 0.0003734826955931365   Iteration 91 of 100, tot loss = 4.813686328929859, l1: 0.00010758957456686063, l2: 0.0003737790555153349   Iteration 92 of 100, tot loss = 4.816012087075607, l1: 0.00010778474307185009, l2: 0.0003738164626196047   Iteration 93 of 100, tot loss = 4.820646973066433, l1: 0.00010778718393750398, l2: 0.0003742775102133452   Iteration 94 of 100, tot loss = 4.818680981372265, l1: 0.00010780859601583599, l2: 0.00037405949889092726   Iteration 95 of 100, tot loss = 4.815375232696534, l1: 0.0001075760835527418, l2: 0.0003739614362865196   Iteration 96 of 100, tot loss = 4.817096134026845, l1: 0.00010742576989741792, l2: 0.0003742838396950295   Iteration 97 of 100, tot loss = 4.816125053720376, l1: 0.0001073443645790829, l2: 0.0003742681368861405   Iteration 98 of 100, tot loss = 4.853996948320038, l1: 0.00010787077438607765, l2: 0.00037752891660843766   Iteration 99 of 100, tot loss = 4.838778276636143, l1: 0.00010759116070623736, l2: 0.0003762866631726443   Iteration 100 of 100, tot loss = 4.84699189901352, l1: 0.0001078060247891699, l2: 0.00037689316101022994
   End of epoch 1101; saving model... 

Epoch 1102 of 2000
   Iteration 1 of 100, tot loss = 5.964718341827393, l1: 9.766277798917145e-05, l2: 0.0004988090367987752   Iteration 2 of 100, tot loss = 4.807004928588867, l1: 9.846341708907858e-05, l2: 0.000382237063604407   Iteration 3 of 100, tot loss = 4.6702812512715655, l1: 0.00010728159880576034, l2: 0.00035974651109427214   Iteration 4 of 100, tot loss = 4.913951277732849, l1: 0.00010847742123587523, l2: 0.0003829177003353834   Iteration 5 of 100, tot loss = 5.870949077606201, l1: 0.00012519023875938727, l2: 0.0004619046696461737   Iteration 6 of 100, tot loss = 6.438526391983032, l1: 0.00013374958386217864, l2: 0.0005101030595445385   Iteration 7 of 100, tot loss = 6.112910747528076, l1: 0.00012999978831170926, l2: 0.0004812912874123348   Iteration 8 of 100, tot loss = 6.064864873886108, l1: 0.00012976358630112372, l2: 0.00047672290020273067   Iteration 9 of 100, tot loss = 6.202629195319282, l1: 0.00013278550533060398, l2: 0.00048747741190406185   Iteration 10 of 100, tot loss = 6.208768701553344, l1: 0.00013252928765723483, l2: 0.000488347580539994   Iteration 11 of 100, tot loss = 6.124831503087824, l1: 0.000132023444166407, l2: 0.0004804597040426663   Iteration 12 of 100, tot loss = 6.071285565694173, l1: 0.00013118500889201337, l2: 0.0004759435444915046   Iteration 13 of 100, tot loss = 5.972839575547439, l1: 0.00012907496140822052, l2: 0.00046820899185079796   Iteration 14 of 100, tot loss = 5.816187722342355, l1: 0.0001260848720059065, l2: 0.0004555338964564726   Iteration 15 of 100, tot loss = 5.601155567169189, l1: 0.00012252741823128114, l2: 0.00043758813505216193   Iteration 16 of 100, tot loss = 5.458106949925423, l1: 0.00012046716574332095, l2: 0.0004253435263308347   Iteration 17 of 100, tot loss = 5.502977890126846, l1: 0.00012260630447759896, l2: 0.00042769148079541456   Iteration 18 of 100, tot loss = 5.421864681773716, l1: 0.00012091559741141585, l2: 0.00042127086797134124   Iteration 19 of 100, tot loss = 5.431555584857338, l1: 0.00012102075140175752, l2: 0.0004221348047538317   Iteration 20 of 100, tot loss = 5.475184047222138, l1: 0.00012104303314117715, l2: 0.00042647536829463204   Iteration 21 of 100, tot loss = 5.3895558175586515, l1: 0.00012010586284477973, l2: 0.00041884971558049856   Iteration 22 of 100, tot loss = 5.401346054944125, l1: 0.00012018728540913963, l2: 0.0004199473180712878   Iteration 23 of 100, tot loss = 5.444100732388704, l1: 0.00012046648265348504, l2: 0.0004239435878369714   Iteration 24 of 100, tot loss = 5.35509838660558, l1: 0.00011900990678744468, l2: 0.0004164999293910417   Iteration 25 of 100, tot loss = 5.341763343811035, l1: 0.00011886220891028642, l2: 0.00041531412221957   Iteration 26 of 100, tot loss = 5.38736394735483, l1: 0.00011929323283346513, l2: 0.00041944315811493795   Iteration 27 of 100, tot loss = 5.441902160644531, l1: 0.00012055774654813663, l2: 0.00042363246568668356   Iteration 28 of 100, tot loss = 5.438242520604815, l1: 0.00012100493820201206, l2: 0.00042281931122748313   Iteration 29 of 100, tot loss = 5.448546343836291, l1: 0.0001212783561264775, l2: 0.0004235762758541788   Iteration 30 of 100, tot loss = 5.43598108291626, l1: 0.00012065774765990985, l2: 0.00042294035811210053   Iteration 31 of 100, tot loss = 5.494266033172607, l1: 0.00012165410120242967, l2: 0.000427772500833887   Iteration 32 of 100, tot loss = 5.52421897649765, l1: 0.0001216502621446125, l2: 0.000430771634910343   Iteration 33 of 100, tot loss = 5.555703683332964, l1: 0.00012173010746306373, l2: 0.0004338402592993313   Iteration 34 of 100, tot loss = 5.514387453303618, l1: 0.00012005002795109142, l2: 0.0004313887154293137   Iteration 35 of 100, tot loss = 5.470774275915963, l1: 0.00011964036717212625, l2: 0.00042743705853354186   Iteration 36 of 100, tot loss = 5.375237418545617, l1: 0.00011776330908711922, l2: 0.0004197604309107798   Iteration 37 of 100, tot loss = 5.342677522350002, l1: 0.0001167702919062592, l2: 0.00041749745896841223   Iteration 38 of 100, tot loss = 5.320863202998512, l1: 0.00011629270537875862, l2: 0.00041579361392283127   Iteration 39 of 100, tot loss = 5.258929283190996, l1: 0.00011461851495252445, l2: 0.00041127441270360484   Iteration 40 of 100, tot loss = 5.232354801893234, l1: 0.00011370725533197402, l2: 0.00040952822419058064   Iteration 41 of 100, tot loss = 5.139774284711698, l1: 0.00011190936306886171, l2: 0.0004020680647865846   Iteration 42 of 100, tot loss = 5.125091300124214, l1: 0.00011175581667408725, l2: 0.0004007533127497438   Iteration 43 of 100, tot loss = 5.117247700691223, l1: 0.00011149000995426959, l2: 0.00040023475966611226   Iteration 44 of 100, tot loss = 5.102881905707446, l1: 0.00011129356004279212, l2: 0.00039899462982165545   Iteration 45 of 100, tot loss = 5.060810335477194, l1: 0.00011048920787288807, l2: 0.0003955918247811496   Iteration 46 of 100, tot loss = 5.057143224322277, l1: 0.00011032956354029011, l2: 0.00039538475887283033   Iteration 47 of 100, tot loss = 5.0431565046310425, l1: 0.00011012434194352419, l2: 0.00039419130808574724   Iteration 48 of 100, tot loss = 5.088184756537278, l1: 0.00011022761062425464, l2: 0.00039859086367262836   Iteration 49 of 100, tot loss = 5.018580981663296, l1: 0.00010884474598854895, l2: 0.00039301335078673625   Iteration 50 of 100, tot loss = 5.0555008602142335, l1: 0.00010984614709741436, l2: 0.0003957039382657967   Iteration 51 of 100, tot loss = 5.023625715106141, l1: 0.00010925839459934436, l2: 0.00039310417595712064   Iteration 52 of 100, tot loss = 5.021559563966898, l1: 0.00010950768522049587, l2: 0.00039264827025293087   Iteration 53 of 100, tot loss = 5.05826927131077, l1: 0.00010992014447868503, l2: 0.00039590678132745387   Iteration 54 of 100, tot loss = 5.049308587003638, l1: 0.00010966817623099175, l2: 0.00039526268099305325   Iteration 55 of 100, tot loss = 5.019124113429676, l1: 0.00010890193675136702, l2: 0.0003930104735032232   Iteration 56 of 100, tot loss = 5.033979147672653, l1: 0.00010895767406639476, l2: 0.00039444023968826514   Iteration 57 of 100, tot loss = 4.995047155179475, l1: 0.00010847336007077901, l2: 0.0003910313542674933   Iteration 58 of 100, tot loss = 5.062078151209601, l1: 0.00010900609476234892, l2: 0.0003972017190696126   Iteration 59 of 100, tot loss = 5.136889558727458, l1: 0.00011038852355371908, l2: 0.00040330043110909665   Iteration 60 of 100, tot loss = 5.131304570039114, l1: 0.00011072396203720321, l2: 0.0004024064937160195   Iteration 61 of 100, tot loss = 5.133326464012021, l1: 0.00011066767910961062, l2: 0.00040266496634118443   Iteration 62 of 100, tot loss = 5.159910428908564, l1: 0.00011104500111785808, l2: 0.000404946040596828   Iteration 63 of 100, tot loss = 5.140746445882888, l1: 0.00011103288575430357, l2: 0.00040304175756376474   Iteration 64 of 100, tot loss = 5.141575429588556, l1: 0.00011093429350239603, l2: 0.000403223248440554   Iteration 65 of 100, tot loss = 5.171345310944777, l1: 0.00011153658213935649, l2: 0.00040559794847817664   Iteration 66 of 100, tot loss = 5.187685970104102, l1: 0.00011159769388596584, l2: 0.0004071709023044219   Iteration 67 of 100, tot loss = 5.226109885457737, l1: 0.00011212341155102755, l2: 0.0004104875758335467   Iteration 68 of 100, tot loss = 5.189704625045552, l1: 0.00011143927258672193, l2: 0.00040753118853043653   Iteration 69 of 100, tot loss = 5.150744109913923, l1: 0.00011047701891452627, l2: 0.0004045973906033686   Iteration 70 of 100, tot loss = 5.16743825844356, l1: 0.00011090059919557201, l2: 0.00040584322559880095   Iteration 71 of 100, tot loss = 5.176886528310641, l1: 0.00011078370281070484, l2: 0.0004069049497098911   Iteration 72 of 100, tot loss = 5.18471947312355, l1: 0.00011079959237273822, l2: 0.00040767235481729987   Iteration 73 of 100, tot loss = 5.177593907264814, l1: 0.0001109230813186788, l2: 0.0004068363088339389   Iteration 74 of 100, tot loss = 5.192002376994571, l1: 0.00011101988457085099, l2: 0.00040818035238378406   Iteration 75 of 100, tot loss = 5.181812143325805, l1: 0.00011067965325006904, l2: 0.0004075015602090086   Iteration 76 of 100, tot loss = 5.173600752102701, l1: 0.00011029235194751221, l2: 0.0004070677228140228   Iteration 77 of 100, tot loss = 5.176027307262668, l1: 0.00011015611479789104, l2: 0.00040744661520592044   Iteration 78 of 100, tot loss = 5.1664905823194065, l1: 0.00011004037398192733, l2: 0.00040660868380188895   Iteration 79 of 100, tot loss = 5.133885090864157, l1: 0.00010970337696769864, l2: 0.0004036851318278998   Iteration 80 of 100, tot loss = 5.130349257588387, l1: 0.000109453990125985, l2: 0.0004035809351989883   Iteration 81 of 100, tot loss = 5.124100646854918, l1: 0.00010977962319660116, l2: 0.0004026304409813343   Iteration 82 of 100, tot loss = 5.1443969360212, l1: 0.00010993850965616463, l2: 0.00040450118316717943   Iteration 83 of 100, tot loss = 5.178038996386241, l1: 0.00011041800475557312, l2: 0.000407385894241855   Iteration 84 of 100, tot loss = 5.193987054484231, l1: 0.0001105277960093753, l2: 0.00040887090861880485   Iteration 85 of 100, tot loss = 5.1976918641258685, l1: 0.00011042179338640862, l2: 0.0004093473919270122   Iteration 86 of 100, tot loss = 5.254038336665132, l1: 0.0001110437006414137, l2: 0.00041436013237026325   Iteration 87 of 100, tot loss = 5.284979685969736, l1: 0.00011155713553140315, l2: 0.0004169408326644729   Iteration 88 of 100, tot loss = 5.2925224006175995, l1: 0.00011167785822346394, l2: 0.0004175743814068317   Iteration 89 of 100, tot loss = 5.290931104274279, l1: 0.00011200748804696375, l2: 0.0004170856219807933   Iteration 90 of 100, tot loss = 5.261382325490316, l1: 0.00011167998280951805, l2: 0.000414458249320483   Iteration 91 of 100, tot loss = 5.252891498607593, l1: 0.00011133497241394502, l2: 0.00041395417705004277   Iteration 92 of 100, tot loss = 5.25199438178021, l1: 0.00011137302961920479, l2: 0.0004138264080013513   Iteration 93 of 100, tot loss = 5.307675382142426, l1: 0.00011208283209388384, l2: 0.0004186847050654231   Iteration 94 of 100, tot loss = 5.2877183112692325, l1: 0.00011179774001573876, l2: 0.00041697408999019165   Iteration 95 of 100, tot loss = 5.292572382876748, l1: 0.00011184239452919236, l2: 0.0004174148427984236   Iteration 96 of 100, tot loss = 5.255908489227295, l1: 0.000111274654538344, l2: 0.0004143161935038127   Iteration 97 of 100, tot loss = 5.257252737418892, l1: 0.00011136270239655187, l2: 0.0004143625703045004   Iteration 98 of 100, tot loss = 5.241735249149556, l1: 0.00011109439898917822, l2: 0.00041307912508681494   Iteration 99 of 100, tot loss = 5.23780366627857, l1: 0.00011106005147648645, l2: 0.0004127203140553614   Iteration 100 of 100, tot loss = 5.251728301048279, l1: 0.0001115442832451663, l2: 0.0004136285456479527
   End of epoch 1102; saving model... 

Epoch 1103 of 2000
   Iteration 1 of 100, tot loss = 3.9130845069885254, l1: 0.00010589521116344258, l2: 0.00028541323263198137   Iteration 2 of 100, tot loss = 3.888159155845642, l1: 9.213363955495879e-05, l2: 0.0002966822648886591   Iteration 3 of 100, tot loss = 4.262147665023804, l1: 0.00010020612050235893, l2: 0.00032600865233689547   Iteration 4 of 100, tot loss = 4.2736212611198425, l1: 9.978459820558783e-05, l2: 0.00032757753069745377   Iteration 5 of 100, tot loss = 4.141342449188232, l1: 9.596402815077454e-05, l2: 0.00031817021663300694   Iteration 6 of 100, tot loss = 4.508299271265666, l1: 0.00010190865092833216, l2: 0.00034892127829759073   Iteration 7 of 100, tot loss = 4.439755031040737, l1: 0.000103237790296719, l2: 0.00034073771426587233   Iteration 8 of 100, tot loss = 4.569925487041473, l1: 0.00010512846529309172, l2: 0.0003518640842230525   Iteration 9 of 100, tot loss = 4.738568835788303, l1: 0.00010636660833066951, l2: 0.00036749028044545814   Iteration 10 of 100, tot loss = 4.599722671508789, l1: 0.00010465508457855322, l2: 0.00035531718749552964   Iteration 11 of 100, tot loss = 4.884000994942405, l1: 0.00010956165748542513, l2: 0.0003788384439593012   Iteration 12 of 100, tot loss = 4.647391617298126, l1: 0.00010552534028344478, l2: 0.00035921382247276296   Iteration 13 of 100, tot loss = 4.546436254794781, l1: 0.00010306694793353717, l2: 0.0003515766774608682   Iteration 14 of 100, tot loss = 4.6449265990938455, l1: 0.00010512182312335685, l2: 0.0003593708398901591   Iteration 15 of 100, tot loss = 4.7270930449167885, l1: 0.00010680539271561429, l2: 0.00036590391246136276   Iteration 16 of 100, tot loss = 4.750841483473778, l1: 0.00010793367027872591, l2: 0.0003671504791782354   Iteration 17 of 100, tot loss = 4.731462745105519, l1: 0.00010689592411782702, l2: 0.0003662503517903935   Iteration 18 of 100, tot loss = 4.671751141548157, l1: 0.00010575405354352875, l2: 0.0003614210622294599   Iteration 19 of 100, tot loss = 4.739659271742168, l1: 0.00010614895329222476, l2: 0.0003678169780055453   Iteration 20 of 100, tot loss = 4.8086108565330505, l1: 0.000106976211827714, l2: 0.0003738848776265513   Iteration 21 of 100, tot loss = 4.707606860569546, l1: 0.00010464185574424587, l2: 0.0003661188328849329   Iteration 22 of 100, tot loss = 4.772582184184682, l1: 0.000106578473853667, l2: 0.0003706797479852949   Iteration 23 of 100, tot loss = 4.753604888916016, l1: 0.0001061334778964722, l2: 0.00036922701363674486   Iteration 24 of 100, tot loss = 4.671923408905665, l1: 0.00010434268127331355, l2: 0.00036284966275464586   Iteration 25 of 100, tot loss = 4.678206377029419, l1: 0.0001049914295435883, l2: 0.0003628292115172371   Iteration 26 of 100, tot loss = 4.737763414016137, l1: 0.00010541552574767802, l2: 0.0003683608189861004   Iteration 27 of 100, tot loss = 4.78389209288138, l1: 0.00010557488874635762, l2: 0.0003728143244559221   Iteration 28 of 100, tot loss = 4.7157294154167175, l1: 0.00010456018130103725, l2: 0.00036701276414013203   Iteration 29 of 100, tot loss = 4.670525435743661, l1: 0.00010381144539710006, l2: 0.00036324110185197584   Iteration 30 of 100, tot loss = 4.6647590637207035, l1: 0.00010300705495562094, l2: 0.0003634688546299003   Iteration 31 of 100, tot loss = 4.682304582288189, l1: 0.0001033292151518136, l2: 0.00036490124588336553   Iteration 32 of 100, tot loss = 4.653984881937504, l1: 0.00010265916375828965, l2: 0.00036273932664698805   Iteration 33 of 100, tot loss = 4.629823576320302, l1: 0.00010200523507647273, l2: 0.00036097712467325795   Iteration 34 of 100, tot loss = 4.691469381837284, l1: 0.00010305722496602411, l2: 0.00036608971619417016   Iteration 35 of 100, tot loss = 4.674099602018084, l1: 0.00010340054328220764, l2: 0.000364009419406232   Iteration 36 of 100, tot loss = 4.721747113598718, l1: 0.00010454431483392707, l2: 0.0003676303996245325   Iteration 37 of 100, tot loss = 4.65912102364205, l1: 0.00010318482767419286, l2: 0.00036272727742766   Iteration 38 of 100, tot loss = 4.775756911227577, l1: 0.00010468383985234571, l2: 0.0003728918527485803   Iteration 39 of 100, tot loss = 4.724769439452734, l1: 0.00010403673998856296, l2: 0.0003684402058402506   Iteration 40 of 100, tot loss = 4.746292299032211, l1: 0.00010501373035367578, l2: 0.00036961550213163716   Iteration 41 of 100, tot loss = 4.78306110312299, l1: 0.00010557137053493956, l2: 0.0003727347413986558   Iteration 42 of 100, tot loss = 4.786152482032776, l1: 0.00010538279509221735, l2: 0.0003732324541834671   Iteration 43 of 100, tot loss = 4.769395146259042, l1: 0.00010502485197638486, l2: 0.0003719146628755816   Iteration 44 of 100, tot loss = 4.784490439024839, l1: 0.00010509169723346449, l2: 0.0003733573466888629   Iteration 45 of 100, tot loss = 4.825074677997165, l1: 0.00010614213517530717, l2: 0.0003763653331487957   Iteration 46 of 100, tot loss = 4.809475136839825, l1: 0.00010549218052841277, l2: 0.00037545533395250857   Iteration 47 of 100, tot loss = 4.760967655384794, l1: 0.00010478922489168559, l2: 0.0003713075414626919   Iteration 48 of 100, tot loss = 4.828405722975731, l1: 0.00010610538508141569, l2: 0.0003767351887897045   Iteration 49 of 100, tot loss = 4.806959930731326, l1: 0.00010594271722710596, l2: 0.00037475327761577707   Iteration 50 of 100, tot loss = 4.805064582824707, l1: 0.00010589830679236912, l2: 0.0003746081527788192   Iteration 51 of 100, tot loss = 4.803814270917107, l1: 0.00010602612532880706, l2: 0.0003743553026259749   Iteration 52 of 100, tot loss = 4.774103109653179, l1: 0.00010562589313149847, l2: 0.00037178441869820323   Iteration 53 of 100, tot loss = 4.72010472360647, l1: 0.00010454537976260806, l2: 0.00036746509349026347   Iteration 54 of 100, tot loss = 4.748027883194111, l1: 0.00010462305201346883, l2: 0.00037017973685309427   Iteration 55 of 100, tot loss = 4.71785314950076, l1: 0.00010419170536227863, l2: 0.00036759360995016654   Iteration 56 of 100, tot loss = 4.736902784023966, l1: 0.00010491336589828799, l2: 0.0003687769123124391   Iteration 57 of 100, tot loss = 4.744217431336119, l1: 0.00010519151398687412, l2: 0.0003692302288663198   Iteration 58 of 100, tot loss = 4.72542281602991, l1: 0.00010503474713287091, l2: 0.00036750753397113997   Iteration 59 of 100, tot loss = 4.75453096123065, l1: 0.0001056104823473399, l2: 0.0003698426128613715   Iteration 60 of 100, tot loss = 4.7643022040526075, l1: 0.00010553632382652723, l2: 0.0003708938958880026   Iteration 61 of 100, tot loss = 4.781409120950543, l1: 0.00010559381225306662, l2: 0.0003725470997465652   Iteration 62 of 100, tot loss = 4.778234572179856, l1: 0.0001054007955263519, l2: 0.00037242266215241305   Iteration 63 of 100, tot loss = 4.7710469923322165, l1: 0.00010519598193544273, l2: 0.0003719087175023372   Iteration 64 of 100, tot loss = 4.759374221786857, l1: 0.00010510211279779469, l2: 0.00037083530946802057   Iteration 65 of 100, tot loss = 4.761573444879972, l1: 0.0001055411178430614, l2: 0.0003706162269316757   Iteration 66 of 100, tot loss = 4.745028909408685, l1: 0.00010482179112097418, l2: 0.00036968110031342474   Iteration 67 of 100, tot loss = 4.747538917100251, l1: 0.00010518550459267711, l2: 0.0003695683868744397   Iteration 68 of 100, tot loss = 4.731597832020591, l1: 0.00010522725161161575, l2: 0.00036793253114892114   Iteration 69 of 100, tot loss = 4.697327746861223, l1: 0.00010466949353703176, l2: 0.0003650632804593719   Iteration 70 of 100, tot loss = 4.699270413603101, l1: 0.00010489379928912967, l2: 0.00036503324164576564   Iteration 71 of 100, tot loss = 4.708753701666711, l1: 0.00010484251526551663, l2: 0.00036603285464569663   Iteration 72 of 100, tot loss = 4.697756090097958, l1: 0.00010501334468750024, l2: 0.00036476226382041606   Iteration 73 of 100, tot loss = 4.709109285106398, l1: 0.00010513313025338193, l2: 0.000365777797351132   Iteration 74 of 100, tot loss = 4.711539299101443, l1: 0.00010521695042592221, l2: 0.0003659369786210182   Iteration 75 of 100, tot loss = 4.701946708361308, l1: 0.00010505026119062678, l2: 0.00036514440881243597   Iteration 76 of 100, tot loss = 4.700056739543614, l1: 0.0001050885764829888, l2: 0.0003649170966558526   Iteration 77 of 100, tot loss = 4.661983154036782, l1: 0.00010425724437257711, l2: 0.0003619410703109892   Iteration 78 of 100, tot loss = 4.69135378415768, l1: 0.0001046519639016762, l2: 0.000364483413599933   Iteration 79 of 100, tot loss = 4.731026442745064, l1: 0.00010525973749868689, l2: 0.0003678429060574338   Iteration 80 of 100, tot loss = 4.7318925485014915, l1: 0.00010552899702815921, l2: 0.00036766025732504204   Iteration 81 of 100, tot loss = 4.736154160381835, l1: 0.00010589536225184637, l2: 0.0003677200530310748   Iteration 82 of 100, tot loss = 4.7249065884729715, l1: 0.00010561671279436092, l2: 0.00036687394547298916   Iteration 83 of 100, tot loss = 4.696997790451509, l1: 0.00010521500055812837, l2: 0.0003644847779195329   Iteration 84 of 100, tot loss = 4.71611615447771, l1: 0.00010564329169851372, l2: 0.00036596832294543715   Iteration 85 of 100, tot loss = 4.728699116145863, l1: 0.00010589172923252643, l2: 0.00036697818175204756   Iteration 86 of 100, tot loss = 4.776510952517044, l1: 0.00010664213779057528, l2: 0.00037100895657880926   Iteration 87 of 100, tot loss = 4.78559042256454, l1: 0.00010675494467114345, l2: 0.0003718040964162063   Iteration 88 of 100, tot loss = 4.796767070889473, l1: 0.00010694272375324296, l2: 0.0003727339821621585   Iteration 89 of 100, tot loss = 4.816699396358447, l1: 0.00010752712588644785, l2: 0.00037414281222880347   Iteration 90 of 100, tot loss = 4.8064688563346865, l1: 0.00010747526814359137, l2: 0.00037317161606754073   Iteration 91 of 100, tot loss = 4.801363933217394, l1: 0.0001073856762889261, l2: 0.00037275071580162536   Iteration 92 of 100, tot loss = 4.799322458712951, l1: 0.00010723787683831608, l2: 0.0003726943679161512   Iteration 93 of 100, tot loss = 4.798947420171512, l1: 0.00010690764329704126, l2: 0.00037298709769377746   Iteration 94 of 100, tot loss = 4.780647433818655, l1: 0.00010661198811771527, l2: 0.0003714527540959101   Iteration 95 of 100, tot loss = 4.77048758331098, l1: 0.00010644205858146674, l2: 0.00037060669856145976   Iteration 96 of 100, tot loss = 4.770572238912185, l1: 0.0001063285019426985, l2: 0.00037072872055432526   Iteration 97 of 100, tot loss = 4.77049875628088, l1: 0.00010621529749112165, l2: 0.0003708345769333279   Iteration 98 of 100, tot loss = 4.7752344158231, l1: 0.00010644394421192095, l2: 0.0003710794963455778   Iteration 99 of 100, tot loss = 4.7639878916017935, l1: 0.00010614017104892287, l2: 0.00037025861710197097   Iteration 100 of 100, tot loss = 4.7625005853176114, l1: 0.0001060914559639059, l2: 0.000370158601726871
   End of epoch 1103; saving model... 

Epoch 1104 of 2000
   Iteration 1 of 100, tot loss = 4.609863758087158, l1: 0.00010879742330871522, l2: 0.0003521889739204198   Iteration 2 of 100, tot loss = 5.865830898284912, l1: 0.0001349751400994137, l2: 0.00045160796435084194   Iteration 3 of 100, tot loss = 6.081230004628499, l1: 0.00014293841377366334, l2: 0.0004651845956686884   Iteration 4 of 100, tot loss = 5.545028567314148, l1: 0.00013287894034874626, l2: 0.0004216239249217324   Iteration 5 of 100, tot loss = 6.2813136100769045, l1: 0.00013836380385328083, l2: 0.0004897675651591271   Iteration 6 of 100, tot loss = 5.910379966100057, l1: 0.00013301985260720053, l2: 0.0004580181509178753   Iteration 7 of 100, tot loss = 5.797198363712856, l1: 0.00013122869235563224, l2: 0.00044849115407227406   Iteration 8 of 100, tot loss = 5.726029217243195, l1: 0.00013137725818523904, l2: 0.0004412256785144564   Iteration 9 of 100, tot loss = 5.698796272277832, l1: 0.0001265867783028322, l2: 0.0004432928657883571   Iteration 10 of 100, tot loss = 5.378722214698792, l1: 0.0001214066011016257, l2: 0.00041646563477115706   Iteration 11 of 100, tot loss = 5.208650220524181, l1: 0.00011809509504184297, l2: 0.0004027699403443628   Iteration 12 of 100, tot loss = 5.038547654946645, l1: 0.00011442124135404204, l2: 0.0003894335350196343   Iteration 13 of 100, tot loss = 4.877055241511418, l1: 0.00011124635224964899, l2: 0.00037645918200723827   Iteration 14 of 100, tot loss = 4.758022563798087, l1: 0.00010950256026782361, l2: 0.00036629970626173805   Iteration 15 of 100, tot loss = 4.7051397323608395, l1: 0.00011021578829968348, l2: 0.0003602981955433885   Iteration 16 of 100, tot loss = 4.627090662717819, l1: 0.00010734056968431105, l2: 0.00035536850737116765   Iteration 17 of 100, tot loss = 4.696571406196146, l1: 0.00010831767191548886, l2: 0.00036133947672651093   Iteration 18 of 100, tot loss = 4.634621077113682, l1: 0.00010652815277959841, l2: 0.00035693396219155856   Iteration 19 of 100, tot loss = 4.581393731267829, l1: 0.0001048846254554136, l2: 0.00035325475465996484   Iteration 20 of 100, tot loss = 4.577307426929474, l1: 0.00010392373114882502, l2: 0.0003538070188369602   Iteration 21 of 100, tot loss = 4.575547547567458, l1: 0.0001040233019877979, l2: 0.00035353146016686444   Iteration 22 of 100, tot loss = 4.603800697760149, l1: 0.00010464681756936692, l2: 0.000355733259559863   Iteration 23 of 100, tot loss = 4.6814745094465176, l1: 0.00010670265929429266, l2: 0.0003614447980552264   Iteration 24 of 100, tot loss = 4.6589299738407135, l1: 0.00010647023918863852, l2: 0.00035942276372225024   Iteration 25 of 100, tot loss = 4.732004957199097, l1: 0.00010684705834137276, l2: 0.00036635344265960155   Iteration 26 of 100, tot loss = 4.8307170409422655, l1: 0.00010824657986701753, l2: 0.00037482512841681734   Iteration 27 of 100, tot loss = 4.880429329695525, l1: 0.00010927439062571566, l2: 0.0003787685450839086   Iteration 28 of 100, tot loss = 4.90502826656614, l1: 0.00011042067775893624, l2: 0.00038008215259261694   Iteration 29 of 100, tot loss = 4.952556355246182, l1: 0.00011117255053250119, l2: 0.00038408308896107664   Iteration 30 of 100, tot loss = 4.945331152280172, l1: 0.00011062149011801618, l2: 0.00038391162815969436   Iteration 31 of 100, tot loss = 4.945187791701286, l1: 0.00011011762563289414, l2: 0.0003844011563717598   Iteration 32 of 100, tot loss = 4.900021620094776, l1: 0.00010924492448793899, l2: 0.0003807572402365622   Iteration 33 of 100, tot loss = 4.849992188540372, l1: 0.00010842536787756465, l2: 0.0003765738537450406   Iteration 34 of 100, tot loss = 4.833555964862599, l1: 0.00010884491862003309, l2: 0.00037451068141415496   Iteration 35 of 100, tot loss = 4.884978267124721, l1: 0.00010962301416189543, l2: 0.0003788748149028314   Iteration 36 of 100, tot loss = 4.866546458668179, l1: 0.00010990260529474149, l2: 0.0003767520433434078   Iteration 37 of 100, tot loss = 4.881186072890823, l1: 0.00010990381564919812, l2: 0.00037821479428030953   Iteration 38 of 100, tot loss = 4.829041631598222, l1: 0.00010877932570445792, l2: 0.0003741248395894409   Iteration 39 of 100, tot loss = 4.821263875716772, l1: 0.00010847795802431229, l2: 0.00037364843107449513   Iteration 40 of 100, tot loss = 4.797281098365784, l1: 0.00010812881100719096, l2: 0.00037159930070629343   Iteration 41 of 100, tot loss = 4.846622467041016, l1: 0.00010859633092456156, l2: 0.00037606591756296595   Iteration 42 of 100, tot loss = 4.7861833572387695, l1: 0.00010753428484479497, l2: 0.0003710840530748967   Iteration 43 of 100, tot loss = 4.825530096542003, l1: 0.00010808276453835153, l2: 0.00037447024653882306   Iteration 44 of 100, tot loss = 4.790751858191057, l1: 0.0001067823748764402, l2: 0.0003722928125749935   Iteration 45 of 100, tot loss = 4.821142111884223, l1: 0.0001070961151627772, l2: 0.0003750180980811516   Iteration 46 of 100, tot loss = 4.775334321934244, l1: 0.00010593157244527347, l2: 0.0003716018618433736   Iteration 47 of 100, tot loss = 4.824942593878888, l1: 0.0001072688187668199, l2: 0.00037522544307842294   Iteration 48 of 100, tot loss = 4.793850893775622, l1: 0.0001066150477223952, l2: 0.0003727700444263367   Iteration 49 of 100, tot loss = 4.786961200285931, l1: 0.00010645519722281594, l2: 0.0003722409247919651   Iteration 50 of 100, tot loss = 4.76684618473053, l1: 0.00010658280982170254, l2: 0.0003701018102583475   Iteration 51 of 100, tot loss = 4.750652331931918, l1: 0.00010629724876672102, l2: 0.0003687679854920134   Iteration 52 of 100, tot loss = 4.739426154356736, l1: 0.00010602744371751144, l2: 0.0003679151724589666   Iteration 53 of 100, tot loss = 4.729155135604571, l1: 0.00010597208151835421, l2: 0.0003669434328638193   Iteration 54 of 100, tot loss = 4.742653643643415, l1: 0.00010609982187074988, l2: 0.0003681655434775166   Iteration 55 of 100, tot loss = 4.815335230393843, l1: 0.00010721078803974458, l2: 0.00037432273622305893   Iteration 56 of 100, tot loss = 4.759598674518721, l1: 0.00010600500633829921, l2: 0.00036995486230547873   Iteration 57 of 100, tot loss = 4.738246304947033, l1: 0.0001054867157348015, l2: 0.00036833791614698016   Iteration 58 of 100, tot loss = 4.724155958356528, l1: 0.0001052849508188705, l2: 0.00036713064639178777   Iteration 59 of 100, tot loss = 4.692533709235111, l1: 0.0001047746404996576, l2: 0.0003644787315259514   Iteration 60 of 100, tot loss = 4.726993932326635, l1: 0.00010540376333665336, l2: 0.00036729563022769676   Iteration 61 of 100, tot loss = 4.688438726253197, l1: 0.0001046575986246815, l2: 0.00036418627418928826   Iteration 62 of 100, tot loss = 4.719207607930707, l1: 0.00010525560901442077, l2: 0.00036666515206302245   Iteration 63 of 100, tot loss = 4.706815952346439, l1: 0.00010496659206904633, l2: 0.00036571500314739604   Iteration 64 of 100, tot loss = 4.7329423781484365, l1: 0.00010535039695014348, l2: 0.0003679438409562863   Iteration 65 of 100, tot loss = 4.709634221517122, l1: 0.00010486872706678696, l2: 0.00036609469501015085   Iteration 66 of 100, tot loss = 4.6787397482178426, l1: 0.00010402003918539832, l2: 0.0003638539353480817   Iteration 67 of 100, tot loss = 4.644786153266679, l1: 0.00010323045776814654, l2: 0.0003612481571087586   Iteration 68 of 100, tot loss = 4.624789632418576, l1: 0.00010278020941126776, l2: 0.00035969875342048267   Iteration 69 of 100, tot loss = 4.634016715961954, l1: 0.00010315779297303854, l2: 0.00036024387868499195   Iteration 70 of 100, tot loss = 4.62957284961428, l1: 0.00010312212143617216, l2: 0.0003598351638564574   Iteration 71 of 100, tot loss = 4.651532389748264, l1: 0.00010318931414127598, l2: 0.000361963925378161   Iteration 72 of 100, tot loss = 4.672037252121502, l1: 0.00010293741540105354, l2: 0.0003642663100941314   Iteration 73 of 100, tot loss = 4.7238593771033095, l1: 0.00010388598691437661, l2: 0.00036849995057596126   Iteration 74 of 100, tot loss = 4.720108066056226, l1: 0.00010359348394755412, l2: 0.00036841732248462533   Iteration 75 of 100, tot loss = 4.73878161907196, l1: 0.0001036921690198748, l2: 0.0003701859929909309   Iteration 76 of 100, tot loss = 4.723194578760548, l1: 0.00010360969879816127, l2: 0.0003687097590667882   Iteration 77 of 100, tot loss = 4.745145393656446, l1: 0.00010371301611378946, l2: 0.0003708015236090385   Iteration 78 of 100, tot loss = 4.755469479621985, l1: 0.00010401374521069766, l2: 0.00037153320348200697   Iteration 79 of 100, tot loss = 4.768946658206891, l1: 0.00010448718769365297, l2: 0.00037240747898769906   Iteration 80 of 100, tot loss = 4.820566521584988, l1: 0.00010526010414650955, l2: 0.00037679654924431815   Iteration 81 of 100, tot loss = 4.85852481553584, l1: 0.0001057274338238108, l2: 0.00038012504911073196   Iteration 82 of 100, tot loss = 4.85602311244825, l1: 0.00010582241800254518, l2: 0.0003797798946346497   Iteration 83 of 100, tot loss = 4.854917174362274, l1: 0.00010564569407528413, l2: 0.0003798460249132645   Iteration 84 of 100, tot loss = 4.850557070402872, l1: 0.00010578680879310316, l2: 0.0003792689001004744   Iteration 85 of 100, tot loss = 4.841130793795866, l1: 0.0001056659254947376, l2: 0.00037844715544077404   Iteration 86 of 100, tot loss = 4.842783859995908, l1: 0.00010568522275418725, l2: 0.0003785931648751504   Iteration 87 of 100, tot loss = 4.843326589156842, l1: 0.00010600829179681056, l2: 0.000378324368556469   Iteration 88 of 100, tot loss = 4.858519249341705, l1: 0.00010623718724838744, l2: 0.0003796147390279326   Iteration 89 of 100, tot loss = 4.862654689992412, l1: 0.00010643412666938142, l2: 0.0003798313437584327   Iteration 90 of 100, tot loss = 4.88092899719874, l1: 0.00010698960925057893, l2: 0.00038110329187475146   Iteration 91 of 100, tot loss = 4.872675076945797, l1: 0.00010694963674433274, l2: 0.0003803178727509106   Iteration 92 of 100, tot loss = 4.870652384084204, l1: 0.00010715739015564475, l2: 0.0003799078502039344   Iteration 93 of 100, tot loss = 4.885101050458928, l1: 0.00010719588133587843, l2: 0.0003813142260481473   Iteration 94 of 100, tot loss = 4.899135958641134, l1: 0.00010736500959691067, l2: 0.0003825485885286942   Iteration 95 of 100, tot loss = 4.88380199357083, l1: 0.0001069721026349151, l2: 0.0003814080987419737   Iteration 96 of 100, tot loss = 4.879320319741964, l1: 0.00010654396771769825, l2: 0.00038138806636804173   Iteration 97 of 100, tot loss = 4.884131005129864, l1: 0.00010675980131767123, l2: 0.00038165330122340204   Iteration 98 of 100, tot loss = 4.8769014964298325, l1: 0.00010675312136745375, l2: 0.00038093703024431457   Iteration 99 of 100, tot loss = 4.862094131383029, l1: 0.00010653223003515964, l2: 0.00037967718519138716   Iteration 100 of 100, tot loss = 4.861962708234787, l1: 0.00010635025926603703, l2: 0.0003798460136749782
   End of epoch 1104; saving model... 

Epoch 1105 of 2000
   Iteration 1 of 100, tot loss = 3.98115611076355, l1: 9.634337766328827e-05, l2: 0.0003017722337972373   Iteration 2 of 100, tot loss = 4.011105418205261, l1: 0.00010279502384946682, l2: 0.00029831552819814533   Iteration 3 of 100, tot loss = 4.870431820551555, l1: 0.00011808799414817865, l2: 0.0003689551861801495   Iteration 4 of 100, tot loss = 5.109030544757843, l1: 0.00011191396515641827, l2: 0.00039898909017210826   Iteration 5 of 100, tot loss = 5.687601900100708, l1: 0.00012019749119644985, l2: 0.00044856268796138464   Iteration 6 of 100, tot loss = 5.436651984850566, l1: 0.00011350024093796189, l2: 0.0004301649460103363   Iteration 7 of 100, tot loss = 5.406300919396537, l1: 0.00011168938674797704, l2: 0.00042894069753986387   Iteration 8 of 100, tot loss = 5.4769167602062225, l1: 0.00011565713884920115, l2: 0.00043203453242313117   Iteration 9 of 100, tot loss = 5.358607583575779, l1: 0.00011443883542799288, l2: 0.00042142191927673086   Iteration 10 of 100, tot loss = 5.110256958007812, l1: 0.00010882248279813212, l2: 0.00040220321097876876   Iteration 11 of 100, tot loss = 5.1989006129178135, l1: 0.00011149271151887, l2: 0.00040839735249226743   Iteration 12 of 100, tot loss = 5.345801472663879, l1: 0.00011334918932940734, l2: 0.00042123095772694796   Iteration 13 of 100, tot loss = 5.463933504544771, l1: 0.0001165906542155426, l2: 0.0004298026995876661   Iteration 14 of 100, tot loss = 5.502113887241909, l1: 0.0001178093035767753, l2: 0.00043240208885565935   Iteration 15 of 100, tot loss = 5.384252675374349, l1: 0.00011387619354839748, l2: 0.0004245490786464264   Iteration 16 of 100, tot loss = 5.281450882554054, l1: 0.0001125234139180975, l2: 0.0004156216782575939   Iteration 17 of 100, tot loss = 5.209837731193094, l1: 0.00011197703818598872, l2: 0.00040900674032266527   Iteration 18 of 100, tot loss = 5.19314346048567, l1: 0.00011173309717883563, l2: 0.000407581251541463   Iteration 19 of 100, tot loss = 5.251749854338796, l1: 0.00011332444716428749, l2: 0.0004118505427246227   Iteration 20 of 100, tot loss = 5.094603896141052, l1: 0.0001108658398152329, l2: 0.0003985945542808622   Iteration 21 of 100, tot loss = 5.028219790685744, l1: 0.00010854186430584551, l2: 0.00039428011985451336   Iteration 22 of 100, tot loss = 4.958964911374179, l1: 0.00010720597228183496, l2: 0.0003886905244805596   Iteration 23 of 100, tot loss = 4.967957952748174, l1: 0.00010652500350261107, l2: 0.00039027079701950044   Iteration 24 of 100, tot loss = 4.988159239292145, l1: 0.00010670609087052678, l2: 0.0003921098383822634   Iteration 25 of 100, tot loss = 4.986010818481446, l1: 0.0001066845137393102, l2: 0.0003919165721163154   Iteration 26 of 100, tot loss = 5.035575774999765, l1: 0.00010789280965512332, l2: 0.0003956647706218064   Iteration 27 of 100, tot loss = 5.046728469707348, l1: 0.00010842682213815688, l2: 0.00039624602693409005   Iteration 28 of 100, tot loss = 4.981791121619088, l1: 0.00010733499896429879, l2: 0.0003908441150477821   Iteration 29 of 100, tot loss = 4.904788888733963, l1: 0.0001051504406295235, l2: 0.00038532844989496313   Iteration 30 of 100, tot loss = 4.9482334613800045, l1: 0.00010579006120678969, l2: 0.00038903328531887383   Iteration 31 of 100, tot loss = 4.911675399349582, l1: 0.00010509854567522604, l2: 0.00038606899456241196   Iteration 32 of 100, tot loss = 4.940321691334248, l1: 0.00010589314297249075, l2: 0.00038813902665424393   Iteration 33 of 100, tot loss = 4.912105292984934, l1: 0.00010512519237994583, l2: 0.0003860853374300694   Iteration 34 of 100, tot loss = 4.938722982126124, l1: 0.0001051938786130289, l2: 0.0003886784200885278   Iteration 35 of 100, tot loss = 4.950319392340524, l1: 0.0001054190899594687, l2: 0.00038961284860436403   Iteration 36 of 100, tot loss = 4.932430379920536, l1: 0.00010455719631055318, l2: 0.00038868584104218625   Iteration 37 of 100, tot loss = 4.910820142642872, l1: 0.00010360145215354105, l2: 0.00038748056111806954   Iteration 38 of 100, tot loss = 4.876065938096297, l1: 0.00010322470342017416, l2: 0.00038438188953717286   Iteration 39 of 100, tot loss = 4.822899317130064, l1: 0.00010229346009556395, l2: 0.00037999647051597445   Iteration 40 of 100, tot loss = 4.854077434539795, l1: 0.0001029924904287327, l2: 0.00038241525253397414   Iteration 41 of 100, tot loss = 4.907817026463951, l1: 0.0001029841624563787, l2: 0.00038779754029791345   Iteration 42 of 100, tot loss = 4.928576628367106, l1: 0.00010389369535481645, l2: 0.0003889639677557473   Iteration 43 of 100, tot loss = 4.91376790335012, l1: 0.00010396976468591885, l2: 0.0003874070264214953   Iteration 44 of 100, tot loss = 4.857700380412015, l1: 0.00010306483421696943, l2: 0.0003827052046703598   Iteration 45 of 100, tot loss = 4.897893238067627, l1: 0.00010404591286917113, l2: 0.0003857434112837331   Iteration 46 of 100, tot loss = 4.84306466061136, l1: 0.00010336201692394832, l2: 0.0003809444492419615   Iteration 47 of 100, tot loss = 4.811438621358668, l1: 0.00010318287736191316, l2: 0.0003779609851722427   Iteration 48 of 100, tot loss = 4.80493347843488, l1: 0.00010358757056868247, l2: 0.00037690577755711274   Iteration 49 of 100, tot loss = 4.7526268131878915, l1: 0.00010233682375971457, l2: 0.0003729258579016682   Iteration 50 of 100, tot loss = 4.765339379310608, l1: 0.00010318038577679545, l2: 0.00037335355271352457   Iteration 51 of 100, tot loss = 4.756318686055202, l1: 0.00010261238697007774, l2: 0.0003730194827464099   Iteration 52 of 100, tot loss = 4.699965701653407, l1: 0.00010165758137121044, l2: 0.0003683389899938797   Iteration 53 of 100, tot loss = 4.693997225671445, l1: 0.0001017492416068771, l2: 0.0003676504821446285   Iteration 54 of 100, tot loss = 4.721441979761477, l1: 0.00010242164151148043, l2: 0.00036972255771548524   Iteration 55 of 100, tot loss = 4.732308539477262, l1: 0.00010263295804510232, l2: 0.0003705978975631297   Iteration 56 of 100, tot loss = 4.766726668391909, l1: 0.0001033327959833384, l2: 0.00037333987191751864   Iteration 57 of 100, tot loss = 4.745623592744794, l1: 0.00010306415175651445, l2: 0.0003714982084235536   Iteration 58 of 100, tot loss = 4.7483977243818085, l1: 0.0001033770097600264, l2: 0.00037146276384913203   Iteration 59 of 100, tot loss = 4.741763757447065, l1: 0.00010303744821319893, l2: 0.00037113892830024334   Iteration 60 of 100, tot loss = 4.733775277932485, l1: 0.00010346649772448775, l2: 0.00036991103067218013   Iteration 61 of 100, tot loss = 4.743654442615196, l1: 0.00010343952012366661, l2: 0.000370925925023182   Iteration 62 of 100, tot loss = 4.767759065474233, l1: 0.0001037198320949345, l2: 0.0003730560758060986   Iteration 63 of 100, tot loss = 4.827273312069121, l1: 0.00010439844089209117, l2: 0.0003783288910718901   Iteration 64 of 100, tot loss = 4.86302400007844, l1: 0.00010498898507194099, l2: 0.0003813134153460851   Iteration 65 of 100, tot loss = 4.8940689783829905, l1: 0.00010539879182873008, l2: 0.00038400810623828035   Iteration 66 of 100, tot loss = 4.891039028312221, l1: 0.00010528981874813326, l2: 0.00038381408464346987   Iteration 67 of 100, tot loss = 4.913143282506003, l1: 0.00010583618924685462, l2: 0.00038547813935803057   Iteration 68 of 100, tot loss = 4.907762215417974, l1: 0.00010535738896150553, l2: 0.0003854188330211293   Iteration 69 of 100, tot loss = 4.933062453200852, l1: 0.0001060654468491903, l2: 0.00038724079849603385   Iteration 70 of 100, tot loss = 4.922656014987401, l1: 0.00010616460796362455, l2: 0.00038610099332540165   Iteration 71 of 100, tot loss = 4.942775736392384, l1: 0.00010641114911290778, l2: 0.00038786642384571086   Iteration 72 of 100, tot loss = 4.930682808160782, l1: 0.00010582067140300448, l2: 0.0003872476085638886   Iteration 73 of 100, tot loss = 4.911811743697075, l1: 0.00010547055886764947, l2: 0.00038571061473977687   Iteration 74 of 100, tot loss = 4.9082857660345125, l1: 0.00010568368472033055, l2: 0.0003851448914124253   Iteration 75 of 100, tot loss = 4.910330073038737, l1: 0.00010574532022777324, l2: 0.00038528768656154476   Iteration 76 of 100, tot loss = 4.910411778249238, l1: 0.00010617816348569345, l2: 0.00038486301414913644   Iteration 77 of 100, tot loss = 4.934391993980904, l1: 0.00010642936718181477, l2: 0.00038700983194368225   Iteration 78 of 100, tot loss = 4.961214065551758, l1: 0.00010697093565772192, l2: 0.00038915047069820453   Iteration 79 of 100, tot loss = 4.960845983481105, l1: 0.00010700389202035774, l2: 0.00038908070573712925   Iteration 80 of 100, tot loss = 4.976808995008469, l1: 0.00010738840082922252, l2: 0.0003902924981957767   Iteration 81 of 100, tot loss = 4.96057175706934, l1: 0.00010722908202162082, l2: 0.00038882809328950115   Iteration 82 of 100, tot loss = 4.916729316478822, l1: 0.00010647428692808295, l2: 0.00038519864436948855   Iteration 83 of 100, tot loss = 4.895652906004205, l1: 0.00010633599415459356, l2: 0.0003832292962883857   Iteration 84 of 100, tot loss = 4.885400598957425, l1: 0.0001061954217220773, l2: 0.00038234463811802143   Iteration 85 of 100, tot loss = 4.895144162458532, l1: 0.00010636428611606414, l2: 0.000383150129859774   Iteration 86 of 100, tot loss = 4.920636373896931, l1: 0.00010667842540241572, l2: 0.00038538521191587154   Iteration 87 of 100, tot loss = 4.944186109235917, l1: 0.0001068656228167599, l2: 0.00038755298796211554   Iteration 88 of 100, tot loss = 4.932847911661321, l1: 0.00010662943867084158, l2: 0.00038665535232046915   Iteration 89 of 100, tot loss = 4.9533120701822, l1: 0.00010718799624075392, l2: 0.0003881432105240339   Iteration 90 of 100, tot loss = 4.951758358213636, l1: 0.00010731882951707424, l2: 0.00038785700631302055   Iteration 91 of 100, tot loss = 4.938683350007612, l1: 0.00010709537073725497, l2: 0.0003867729643428862   Iteration 92 of 100, tot loss = 4.949829549893089, l1: 0.00010748849091096022, l2: 0.0003874944641606619   Iteration 93 of 100, tot loss = 4.971723400136476, l1: 0.00010781672002243188, l2: 0.0003893556201928145   Iteration 94 of 100, tot loss = 4.990334837994677, l1: 0.0001081602716093333, l2: 0.00039087321242055003   Iteration 95 of 100, tot loss = 5.004151472292448, l1: 0.00010842156468578125, l2: 0.00039199358265920494   Iteration 96 of 100, tot loss = 4.9927433257301645, l1: 0.00010845409137042832, l2: 0.000390820241439845   Iteration 97 of 100, tot loss = 4.982974949571275, l1: 0.00010826461524648602, l2: 0.0003900328801390379   Iteration 98 of 100, tot loss = 4.979758089902449, l1: 0.00010820629648485326, l2: 0.00038976951283093884   Iteration 99 of 100, tot loss = 4.989409087884305, l1: 0.0001082462930009404, l2: 0.0003906946164506459   Iteration 100 of 100, tot loss = 4.963976509571076, l1: 0.00010783987254399107, l2: 0.0003885577790788375
   End of epoch 1105; saving model... 

Epoch 1106 of 2000
   Iteration 1 of 100, tot loss = 5.568936824798584, l1: 0.0001426421949872747, l2: 0.00041425152448937297   Iteration 2 of 100, tot loss = 5.516298770904541, l1: 0.00013536628830479458, l2: 0.0004162636323599145   Iteration 3 of 100, tot loss = 5.109039942423503, l1: 0.00012688206091600782, l2: 0.00038402196757184964   Iteration 4 of 100, tot loss = 4.423030436038971, l1: 0.00010616052531986497, l2: 0.0003361425442562904   Iteration 5 of 100, tot loss = 4.212506437301636, l1: 9.874540410237387e-05, l2: 0.00032250525837298485   Iteration 6 of 100, tot loss = 4.378472129503886, l1: 0.00010362770626670681, l2: 0.00033421952442343655   Iteration 7 of 100, tot loss = 4.459803206580026, l1: 0.00010542939396275739, l2: 0.00034055094043391624   Iteration 8 of 100, tot loss = 4.2406086921691895, l1: 9.784638405108126e-05, l2: 0.00032621449645375833   Iteration 9 of 100, tot loss = 4.000059896045261, l1: 9.362524603299487e-05, l2: 0.0003063807525904849   Iteration 10 of 100, tot loss = 4.067585444450378, l1: 9.292237627960276e-05, l2: 0.00031383617751998825   Iteration 11 of 100, tot loss = 4.0762246522036465, l1: 9.41463205725251e-05, l2: 0.00031347615268631756   Iteration 12 of 100, tot loss = 4.2285924553871155, l1: 9.667645129714704e-05, l2: 0.00032618280116973136   Iteration 13 of 100, tot loss = 4.197806505056528, l1: 9.715517612326388e-05, l2: 0.00032262548092358676   Iteration 14 of 100, tot loss = 4.429095268249512, l1: 9.97245133476099e-05, l2: 0.00034318502314688103   Iteration 15 of 100, tot loss = 4.267522795995077, l1: 9.70264860370662e-05, l2: 0.00032972580326410633   Iteration 16 of 100, tot loss = 4.29280911386013, l1: 9.766620655682345e-05, l2: 0.00033161471219500527   Iteration 17 of 100, tot loss = 4.5168757017921, l1: 0.00010035711367142059, l2: 0.000351330463993637   Iteration 18 of 100, tot loss = 4.654711816045973, l1: 0.00010236808217693276, l2: 0.0003631031076009903   Iteration 19 of 100, tot loss = 4.5488878676765845, l1: 0.00010072551288574591, l2: 0.0003541632820423202   Iteration 20 of 100, tot loss = 4.84737058877945, l1: 0.00010514885889278958, l2: 0.0003795882083068136   Iteration 21 of 100, tot loss = 4.802126929873512, l1: 0.00010392354704977369, l2: 0.0003762891539649683   Iteration 22 of 100, tot loss = 4.8778109767220235, l1: 0.00010477091051474086, l2: 0.00038301019561582837   Iteration 23 of 100, tot loss = 4.908380777939506, l1: 0.00010584354854274906, l2: 0.00038499453639024466   Iteration 24 of 100, tot loss = 4.835668444633484, l1: 0.00010405972185859962, l2: 0.0003795071297645336   Iteration 25 of 100, tot loss = 4.760069665908813, l1: 0.00010361522101447917, l2: 0.00037239175289869306   Iteration 26 of 100, tot loss = 4.717219646160419, l1: 0.00010242335548145302, l2: 0.0003692986166033034   Iteration 27 of 100, tot loss = 4.739148917021574, l1: 0.00010309444567408516, l2: 0.00037082045282996087   Iteration 28 of 100, tot loss = 4.760742596217564, l1: 0.00010424473488715844, l2: 0.0003718295307148115   Iteration 29 of 100, tot loss = 4.821760736662766, l1: 0.00010526927771620806, l2: 0.00037690680092682355   Iteration 30 of 100, tot loss = 4.837870820363363, l1: 0.00010621939715444265, l2: 0.000377567689671802   Iteration 31 of 100, tot loss = 4.926971789329283, l1: 0.0001072536630794451, l2: 0.0003854435213255666   Iteration 32 of 100, tot loss = 4.931386932730675, l1: 0.00010733228998560662, l2: 0.00038580640830332413   Iteration 33 of 100, tot loss = 4.959525353980787, l1: 0.00010775301041466071, l2: 0.0003881995296317407   Iteration 34 of 100, tot loss = 4.955430998521693, l1: 0.00010785637909173281, l2: 0.0003876867250416099   Iteration 35 of 100, tot loss = 4.927947153363909, l1: 0.00010770722136450266, l2: 0.0003850874981643366   Iteration 36 of 100, tot loss = 4.970882534980774, l1: 0.00010826442758116173, l2: 0.000388823830386779   Iteration 37 of 100, tot loss = 5.008925592577135, l1: 0.00010895883364404703, l2: 0.00039193372956376424   Iteration 38 of 100, tot loss = 5.0178016361437345, l1: 0.00010896516562895972, l2: 0.00039281500166429107   Iteration 39 of 100, tot loss = 5.107376905588003, l1: 0.00011044813483096779, l2: 0.00040028955882940535   Iteration 40 of 100, tot loss = 5.113955819606781, l1: 0.00010978640648318105, l2: 0.0004016091777884867   Iteration 41 of 100, tot loss = 5.179719203855933, l1: 0.00011063854803867842, l2: 0.00040733337433577127   Iteration 42 of 100, tot loss = 5.180451200121925, l1: 0.00011069602857917613, l2: 0.0004073490923647547   Iteration 43 of 100, tot loss = 5.212063711743022, l1: 0.00011113697490695496, l2: 0.00041006939610206455   Iteration 44 of 100, tot loss = 5.16436784917658, l1: 0.00011059305333302208, l2: 0.00040584373114556496   Iteration 45 of 100, tot loss = 5.136319488949246, l1: 0.00011055719636109037, l2: 0.00040307475169861894   Iteration 46 of 100, tot loss = 5.1284273603688115, l1: 0.00011083594567342358, l2: 0.000402006789796439   Iteration 47 of 100, tot loss = 5.149005950765407, l1: 0.00011119436300198667, l2: 0.00040370623111734763   Iteration 48 of 100, tot loss = 5.186385760704677, l1: 0.00011132771904461454, l2: 0.00040731085664447164   Iteration 49 of 100, tot loss = 5.207201247312585, l1: 0.00011173478345213249, l2: 0.000408985341093693   Iteration 50 of 100, tot loss = 5.190220365524292, l1: 0.00011135634333186317, l2: 0.0004076656929100864   Iteration 51 of 100, tot loss = 5.141139656889672, l1: 0.0001105989036909259, l2: 0.0004035150620848963   Iteration 52 of 100, tot loss = 5.103533116670755, l1: 0.00011045778893066409, l2: 0.00039989552309494826   Iteration 53 of 100, tot loss = 5.073211768888077, l1: 0.0001105169159824793, l2: 0.0003968042608148436   Iteration 54 of 100, tot loss = 5.082668127837004, l1: 0.00011052329682570416, l2: 0.0003977435154848973   Iteration 55 of 100, tot loss = 5.095258894833651, l1: 0.00011079135894595476, l2: 0.0003987345299471847   Iteration 56 of 100, tot loss = 5.104549373899188, l1: 0.00011076361891225263, l2: 0.00039969131830730475   Iteration 57 of 100, tot loss = 5.111772420113547, l1: 0.0001111034089581951, l2: 0.00040007383346132803   Iteration 58 of 100, tot loss = 5.102164153395028, l1: 0.00011015723405229249, l2: 0.0004000591820683973   Iteration 59 of 100, tot loss = 5.076393866943101, l1: 0.00011008292729117892, l2: 0.00039755646040726263   Iteration 60 of 100, tot loss = 5.040670271714529, l1: 0.00010991361414198763, l2: 0.00039415341379935855   Iteration 61 of 100, tot loss = 5.006822328098485, l1: 0.00010895871685638443, l2: 0.0003917235167807762   Iteration 62 of 100, tot loss = 5.017784733926097, l1: 0.00010934448624949812, l2: 0.0003924339874966761   Iteration 63 of 100, tot loss = 4.990025849569411, l1: 0.00010893025526976479, l2: 0.0003900723303169278   Iteration 64 of 100, tot loss = 5.000030416995287, l1: 0.00010913098685705336, l2: 0.00039087205573196115   Iteration 65 of 100, tot loss = 4.992725126559918, l1: 0.00010894498476633229, l2: 0.0003903275290772749   Iteration 66 of 100, tot loss = 4.958346659486944, l1: 0.00010797345103955277, l2: 0.0003878612161233005   Iteration 67 of 100, tot loss = 4.931968276180438, l1: 0.00010787226895431627, l2: 0.00038532455974786697   Iteration 68 of 100, tot loss = 4.913212919936461, l1: 0.00010777405896078562, l2: 0.00038354723416405786   Iteration 69 of 100, tot loss = 4.934082504631816, l1: 0.0001076994002674389, l2: 0.0003857088513145952   Iteration 70 of 100, tot loss = 4.882836721624646, l1: 0.00010662935539065594, l2: 0.0003816543178150563   Iteration 71 of 100, tot loss = 4.897942749547287, l1: 0.00010725131439140171, l2: 0.0003825429620788577   Iteration 72 of 100, tot loss = 4.891880656282107, l1: 0.00010713453179454923, l2: 0.00038205353545587667   Iteration 73 of 100, tot loss = 4.9006242670425, l1: 0.00010695533941767804, l2: 0.0003831070891265719   Iteration 74 of 100, tot loss = 4.893305119630453, l1: 0.00010653797545347589, l2: 0.0003827925383292667   Iteration 75 of 100, tot loss = 4.908981022834777, l1: 0.00010696882954410588, l2: 0.0003839292743941769   Iteration 76 of 100, tot loss = 4.8945960449545005, l1: 0.00010699889428092559, l2: 0.00038246071179807924   Iteration 77 of 100, tot loss = 4.894701395715986, l1: 0.00010689221418541312, l2: 0.00038257792691077263   Iteration 78 of 100, tot loss = 4.932625179107372, l1: 0.000107698581697873, l2: 0.00038556393757552054   Iteration 79 of 100, tot loss = 4.907961131651191, l1: 0.00010731249982844802, l2: 0.0003834836145559373   Iteration 80 of 100, tot loss = 4.882543043792248, l1: 0.0001068584969289077, l2: 0.00038139580847200704   Iteration 81 of 100, tot loss = 4.881987911683542, l1: 0.00010665722337212229, l2: 0.00038154156864936934   Iteration 82 of 100, tot loss = 4.872107021692322, l1: 0.00010643713637652648, l2: 0.0003807735670196097   Iteration 83 of 100, tot loss = 4.867697708577995, l1: 0.00010650138679614111, l2: 0.0003802683854865539   Iteration 84 of 100, tot loss = 4.857457829373224, l1: 0.00010627213322593543, l2: 0.00037947365126456134   Iteration 85 of 100, tot loss = 4.89526251484366, l1: 0.00010654232798568795, l2: 0.00038298392473199566   Iteration 86 of 100, tot loss = 4.878759976043257, l1: 0.00010608591113596894, l2: 0.0003817900878222941   Iteration 87 of 100, tot loss = 4.870363332759375, l1: 0.00010586369865648877, l2: 0.0003811726358320414   Iteration 88 of 100, tot loss = 4.884959398345514, l1: 0.00010597140821615044, l2: 0.00038252453255567656   Iteration 89 of 100, tot loss = 4.877254324012927, l1: 0.00010592158724556499, l2: 0.0003818038462208626   Iteration 90 of 100, tot loss = 4.911628393332164, l1: 0.00010631118453198319, l2: 0.0003848516555283115   Iteration 91 of 100, tot loss = 4.88889201263805, l1: 0.00010598173245383366, l2: 0.0003829074695394221   Iteration 92 of 100, tot loss = 4.894779030395591, l1: 0.00010590091124354133, l2: 0.0003835769922538337   Iteration 93 of 100, tot loss = 4.903747139438506, l1: 0.00010594000015908511, l2: 0.0003844347143400541   Iteration 94 of 100, tot loss = 4.89146434373044, l1: 0.00010567429869316835, l2: 0.00038347213635557987   Iteration 95 of 100, tot loss = 4.886697702658804, l1: 0.00010546265548327938, l2: 0.0003832071152589235   Iteration 96 of 100, tot loss = 4.8891340879102545, l1: 0.00010541427415470632, l2: 0.00038349913544758846   Iteration 97 of 100, tot loss = 4.895611417662237, l1: 0.00010527892255154678, l2: 0.00038428221962453086   Iteration 98 of 100, tot loss = 4.891183832470252, l1: 0.00010511005671198328, l2: 0.00038400832718067174   Iteration 99 of 100, tot loss = 4.9023252484774344, l1: 0.00010514804425195443, l2: 0.0003850844811008434   Iteration 100 of 100, tot loss = 4.882059980630874, l1: 0.00010491150838788599, l2: 0.0003832944901660085
   End of epoch 1106; saving model... 

Epoch 1107 of 2000
   Iteration 1 of 100, tot loss = 5.934139251708984, l1: 0.00015025505854282528, l2: 0.0004431589040905237   Iteration 2 of 100, tot loss = 5.4609434604644775, l1: 0.0001393456623191014, l2: 0.0004067487025167793   Iteration 3 of 100, tot loss = 5.540665944417317, l1: 0.00014092659209078798, l2: 0.00041314002010039985   Iteration 4 of 100, tot loss = 6.470550060272217, l1: 0.0001492682822572533, l2: 0.0004977867429261096   Iteration 5 of 100, tot loss = 6.25720853805542, l1: 0.00014074476348469034, l2: 0.00048497610841877756   Iteration 6 of 100, tot loss = 5.88575545946757, l1: 0.00013347051572054625, l2: 0.0004551050466640542   Iteration 7 of 100, tot loss = 5.744143690381732, l1: 0.0001287325919422853, l2: 0.000445681790422116   Iteration 8 of 100, tot loss = 5.706226289272308, l1: 0.0001289571018787683, l2: 0.0004416655392560642   Iteration 9 of 100, tot loss = 5.513302458657159, l1: 0.0001222818197372059, l2: 0.0004290484357625246   Iteration 10 of 100, tot loss = 5.229048061370849, l1: 0.0001148795177869033, l2: 0.0004080252969288267   Iteration 11 of 100, tot loss = 5.147226637059992, l1: 0.00011544315864078023, l2: 0.0003992795126660812   Iteration 12 of 100, tot loss = 4.942692776521047, l1: 0.00011139000343973748, l2: 0.0003828792808538613   Iteration 13 of 100, tot loss = 5.0749535377209005, l1: 0.00011290661551846334, l2: 0.0003945887460409162   Iteration 14 of 100, tot loss = 4.924013137817383, l1: 0.00011018201569511023, l2: 0.0003822193050707158   Iteration 15 of 100, tot loss = 4.726996342341105, l1: 0.00010624913993524387, l2: 0.0003664505008297662   Iteration 16 of 100, tot loss = 5.018766567111015, l1: 0.00011022545595551492, l2: 0.0003916512068826705   Iteration 17 of 100, tot loss = 5.03650445096633, l1: 0.00011013228504452854, l2: 0.0003935181662467692   Iteration 18 of 100, tot loss = 5.006140642695957, l1: 0.00011001377303424913, l2: 0.0003906002966687083   Iteration 19 of 100, tot loss = 4.996371131194265, l1: 0.00010999989861279334, l2: 0.0003896372187515034   Iteration 20 of 100, tot loss = 4.999375474452973, l1: 0.00011153582236147485, l2: 0.0003884017292875797   Iteration 21 of 100, tot loss = 4.896881682532174, l1: 0.00010850258262334613, l2: 0.000381185590023441   Iteration 22 of 100, tot loss = 4.81474739854986, l1: 0.00010726665294152389, l2: 0.00037420809134015474   Iteration 23 of 100, tot loss = 4.9465629743493125, l1: 0.00010992013124885726, l2: 0.0003847361690334409   Iteration 24 of 100, tot loss = 4.826875815788905, l1: 0.00010776165421096569, l2: 0.0003749259303731378   Iteration 25 of 100, tot loss = 4.902867403030395, l1: 0.00010807707920321263, l2: 0.0003822096635121852   Iteration 26 of 100, tot loss = 4.990189341398386, l1: 0.00010909454165532504, l2: 0.0003899243949070716   Iteration 27 of 100, tot loss = 4.966765536202325, l1: 0.00010818613063074062, l2: 0.0003884904252589439   Iteration 28 of 100, tot loss = 5.017902450902121, l1: 0.00010921115959458152, l2: 0.0003925790879293345   Iteration 29 of 100, tot loss = 4.995456210498152, l1: 0.00010815502361671454, l2: 0.0003913905991417966   Iteration 30 of 100, tot loss = 4.997079300880432, l1: 0.000107987095664915, l2: 0.0003917208358567829   Iteration 31 of 100, tot loss = 5.042240012076594, l1: 0.00010922093485620985, l2: 0.00039500306874904183   Iteration 32 of 100, tot loss = 5.0559694692492485, l1: 0.00010985213850744913, l2: 0.0003957448107030359   Iteration 33 of 100, tot loss = 4.956035599564061, l1: 0.00010760851904651093, l2: 0.00038799504303218174   Iteration 34 of 100, tot loss = 4.898690875838785, l1: 0.00010680344733854701, l2: 0.0003830656423815526   Iteration 35 of 100, tot loss = 4.927854435784476, l1: 0.00010762672900455073, l2: 0.0003851587156532332   Iteration 36 of 100, tot loss = 4.878844837347667, l1: 0.00010679557959115805, l2: 0.0003810889057705127   Iteration 37 of 100, tot loss = 4.854582174404247, l1: 0.00010564788818875379, l2: 0.0003798103311596237   Iteration 38 of 100, tot loss = 4.812965418163099, l1: 0.0001052003968461098, l2: 0.000376096146570298   Iteration 39 of 100, tot loss = 4.786902941190279, l1: 0.00010518208355045854, l2: 0.00037350821217748884   Iteration 40 of 100, tot loss = 4.834909117221832, l1: 0.00010586435892037115, l2: 0.00037762655374535826   Iteration 41 of 100, tot loss = 4.831480979919434, l1: 0.00010630402113481356, l2: 0.00037684407697070573   Iteration 42 of 100, tot loss = 4.835041715985253, l1: 0.00010669466566815529, l2: 0.0003768095072397652   Iteration 43 of 100, tot loss = 4.811161640078523, l1: 0.00010625012746882127, l2: 0.00037486603731278677   Iteration 44 of 100, tot loss = 4.814103950153697, l1: 0.00010581032951119018, l2: 0.0003756000663667113   Iteration 45 of 100, tot loss = 4.79189756181505, l1: 0.000105921501138558, l2: 0.0003732682564683879   Iteration 46 of 100, tot loss = 4.784214351488196, l1: 0.0001062306071854318, l2: 0.0003721908291007149   Iteration 47 of 100, tot loss = 4.779303581156629, l1: 0.00010611687089004455, l2: 0.0003718134889532713   Iteration 48 of 100, tot loss = 4.752258246143659, l1: 0.00010554198494598192, l2: 0.0003696838415028954   Iteration 49 of 100, tot loss = 4.7427665311463025, l1: 0.00010482555600382122, l2: 0.0003694510987213794   Iteration 50 of 100, tot loss = 4.741424174308777, l1: 0.00010485457794857212, l2: 0.0003692878413130529   Iteration 51 of 100, tot loss = 4.7124415799683215, l1: 0.00010425450583330959, l2: 0.00036698965356895223   Iteration 52 of 100, tot loss = 4.684848679946019, l1: 0.00010401289812132466, l2: 0.0003644719713170511   Iteration 53 of 100, tot loss = 4.679600405243208, l1: 0.00010348614214651532, l2: 0.0003644738995408604   Iteration 54 of 100, tot loss = 4.661990779417533, l1: 0.0001030247157258499, l2: 0.0003631743634792252   Iteration 55 of 100, tot loss = 4.647460889816284, l1: 0.00010310363789110191, l2: 0.00036164245261302726   Iteration 56 of 100, tot loss = 4.682972741978509, l1: 0.00010356765038263152, l2: 0.0003647296247696171   Iteration 57 of 100, tot loss = 4.698765591571205, l1: 0.00010348358786391225, l2: 0.00036639297225823003   Iteration 58 of 100, tot loss = 4.649931204730067, l1: 0.00010237503475168366, l2: 0.00036261808669098234   Iteration 59 of 100, tot loss = 4.635391465688156, l1: 0.00010209582806209702, l2: 0.0003614433191645638   Iteration 60 of 100, tot loss = 4.638002049922943, l1: 0.00010206131470719507, l2: 0.00036173889093333856   Iteration 61 of 100, tot loss = 4.601153541783818, l1: 0.00010157555898033144, l2: 0.00035853979590951966   Iteration 62 of 100, tot loss = 4.611724019050598, l1: 0.00010198645360369776, l2: 0.0003591859494918026   Iteration 63 of 100, tot loss = 4.662280154606653, l1: 0.00010307127353920054, l2: 0.00036315674341971143   Iteration 64 of 100, tot loss = 4.668868076056242, l1: 0.00010301497832188033, l2: 0.0003638718310412514   Iteration 65 of 100, tot loss = 4.65496636904203, l1: 0.00010300739399658946, l2: 0.00036248924465885813   Iteration 66 of 100, tot loss = 4.657598076444684, l1: 0.00010282353821711502, l2: 0.0003629362708483258   Iteration 67 of 100, tot loss = 4.620199662535938, l1: 0.00010203933181482673, l2: 0.0003599806357724175   Iteration 68 of 100, tot loss = 4.622478720019846, l1: 0.00010192065196058727, l2: 0.0003603272215353654   Iteration 69 of 100, tot loss = 4.63157079876333, l1: 0.00010212717962280874, l2: 0.00036102990144053877   Iteration 70 of 100, tot loss = 4.621779683658055, l1: 0.00010233916062653796, l2: 0.0003598388089033376   Iteration 71 of 100, tot loss = 4.625564699441615, l1: 0.00010248731140704812, l2: 0.0003600691594380203   Iteration 72 of 100, tot loss = 4.642731630139881, l1: 0.00010268708466456801, l2: 0.0003615860793312701   Iteration 73 of 100, tot loss = 4.673686814634768, l1: 0.0001032966409258908, l2: 0.00036407204118416936   Iteration 74 of 100, tot loss = 4.668139957092904, l1: 0.00010327212525275222, l2: 0.0003635418712243287   Iteration 75 of 100, tot loss = 4.679420226414998, l1: 0.00010339076140856681, l2: 0.0003645512620763232   Iteration 76 of 100, tot loss = 4.64935512919175, l1: 0.00010303433923821246, l2: 0.0003619011745046775   Iteration 77 of 100, tot loss = 4.671174259928914, l1: 0.0001033694144960511, l2: 0.00036374801224061325   Iteration 78 of 100, tot loss = 4.715935645959316, l1: 0.00010421898831480529, l2: 0.00036737457766997605   Iteration 79 of 100, tot loss = 4.718522264987608, l1: 0.0001041567719985784, l2: 0.0003676954561054612   Iteration 80 of 100, tot loss = 4.704868343472481, l1: 0.00010408267694401729, l2: 0.0003664041592855938   Iteration 81 of 100, tot loss = 4.7191119812153, l1: 0.00010449227345726499, l2: 0.00036741892611722886   Iteration 82 of 100, tot loss = 4.730070343831691, l1: 0.00010444757434725477, l2: 0.00036855946151573754   Iteration 83 of 100, tot loss = 4.746303969118968, l1: 0.00010453542212833627, l2: 0.00037009497657025523   Iteration 84 of 100, tot loss = 4.749082443260011, l1: 0.00010469557418262065, l2: 0.0003702126719179519   Iteration 85 of 100, tot loss = 4.7664940693799185, l1: 0.00010495698186466197, l2: 0.00037169242688619037   Iteration 86 of 100, tot loss = 4.7745347383410435, l1: 0.0001051774801199763, l2: 0.0003722759950527012   Iteration 87 of 100, tot loss = 4.7718157850462815, l1: 0.00010485219908510228, l2: 0.00037232938091452607   Iteration 88 of 100, tot loss = 4.784755400635979, l1: 0.00010519877125923152, l2: 0.000373276770419166   Iteration 89 of 100, tot loss = 4.809642451532771, l1: 0.00010571885891448381, l2: 0.0003752453881185083   Iteration 90 of 100, tot loss = 4.795272541046143, l1: 0.00010556461707488375, l2: 0.00037396263893848907   Iteration 91 of 100, tot loss = 4.807638210254711, l1: 0.00010563289022404485, l2: 0.00037513093275765144   Iteration 92 of 100, tot loss = 4.809476131978243, l1: 0.00010557437342314465, l2: 0.0003753732418535156   Iteration 93 of 100, tot loss = 4.799913475590367, l1: 0.00010517469604320634, l2: 0.0003748166538219178   Iteration 94 of 100, tot loss = 4.8328548771269775, l1: 0.00010584163105027363, l2: 0.00037744385902423096   Iteration 95 of 100, tot loss = 4.809950650365729, l1: 0.00010549528367272088, l2: 0.0003754997835800934   Iteration 96 of 100, tot loss = 4.801032366851966, l1: 0.00010535337230521691, l2: 0.00037474986659920734   Iteration 97 of 100, tot loss = 4.787169704732206, l1: 0.00010524913076804615, l2: 0.00037346784185651765   Iteration 98 of 100, tot loss = 4.8256210137386715, l1: 0.0001057222403799141, l2: 0.00037683986293090204   Iteration 99 of 100, tot loss = 4.829169270968197, l1: 0.00010578219727830575, l2: 0.0003771347320354026   Iteration 100 of 100, tot loss = 4.806189820766449, l1: 0.00010538331491261488, l2: 0.00037523566934396515
   End of epoch 1107; saving model... 

Epoch 1108 of 2000
   Iteration 1 of 100, tot loss = 5.392458915710449, l1: 0.00011551543866517022, l2: 0.00042373043834231794   Iteration 2 of 100, tot loss = 4.751258373260498, l1: 9.028942440636456e-05, l2: 0.00038483639946207404   Iteration 3 of 100, tot loss = 5.2939761479695635, l1: 0.00011044475832022727, l2: 0.0004189528602485855   Iteration 4 of 100, tot loss = 5.6159796714782715, l1: 0.00011612053276621737, l2: 0.00044547743164002895   Iteration 5 of 100, tot loss = 5.215524053573608, l1: 0.0001113239413825795, l2: 0.000410228461259976   Iteration 6 of 100, tot loss = 5.3418556451797485, l1: 0.00011563208924296002, l2: 0.00041855346838322777   Iteration 7 of 100, tot loss = 5.038622243063791, l1: 0.00010956385397418802, l2: 0.000394298362412623   Iteration 8 of 100, tot loss = 4.99578857421875, l1: 0.00010696895242290339, l2: 0.00039260990160983056   Iteration 9 of 100, tot loss = 4.9294448958502874, l1: 0.00010612265436470302, l2: 0.000386821830438243   Iteration 10 of 100, tot loss = 4.843248128890991, l1: 0.00010346935305278748, l2: 0.00038085545820649714   Iteration 11 of 100, tot loss = 5.101104909723455, l1: 0.00011005359754728323, l2: 0.0004000568905294958   Iteration 12 of 100, tot loss = 4.96701314051946, l1: 0.00010675808456047282, l2: 0.00038994322676444426   Iteration 13 of 100, tot loss = 4.809149742126465, l1: 0.00010438278919676892, l2: 0.0003765321833690485   Iteration 14 of 100, tot loss = 4.989530324935913, l1: 0.00010818149010966798, l2: 0.00039077154152827073   Iteration 15 of 100, tot loss = 5.0106940269470215, l1: 0.00010837689090597754, l2: 0.0003926925089520713   Iteration 16 of 100, tot loss = 4.920560643076897, l1: 0.00010769013715616893, l2: 0.0003843659233098151   Iteration 17 of 100, tot loss = 4.986013146007762, l1: 0.0001082060334738344, l2: 0.00039039527901503095   Iteration 18 of 100, tot loss = 5.000713229179382, l1: 0.00010812779631426868, l2: 0.0003919435241388985   Iteration 19 of 100, tot loss = 5.166919745896992, l1: 0.00011156439148563597, l2: 0.000405127582397606   Iteration 20 of 100, tot loss = 5.109153950214386, l1: 0.00011034412309527398, l2: 0.0004005712718935683   Iteration 21 of 100, tot loss = 5.066122929255168, l1: 0.00011000610157636748, l2: 0.00039660619083969363   Iteration 22 of 100, tot loss = 5.010223887183449, l1: 0.00010864404049987735, l2: 0.0003923783475660126   Iteration 23 of 100, tot loss = 4.999840570532757, l1: 0.00010678935992156924, l2: 0.0003931946968963451   Iteration 24 of 100, tot loss = 4.8464370767275495, l1: 0.00010375997029162438, l2: 0.0003808837370039934   Iteration 25 of 100, tot loss = 4.965035152435303, l1: 0.00010541758369072341, l2: 0.0003910859316238202   Iteration 26 of 100, tot loss = 5.087077232507559, l1: 0.00010756650856190111, l2: 0.0004011412151717545   Iteration 27 of 100, tot loss = 5.1896538734436035, l1: 0.00010802688498019793, l2: 0.00041093850315384635   Iteration 28 of 100, tot loss = 5.219392214502607, l1: 0.00010721182908517741, l2: 0.000414727394205069   Iteration 29 of 100, tot loss = 5.212242636187323, l1: 0.00010768028369056992, l2: 0.0004135439825216148   Iteration 30 of 100, tot loss = 5.25764323870341, l1: 0.00010858804149999438, l2: 0.00041717628506982387   Iteration 31 of 100, tot loss = 5.186715956657164, l1: 0.00010809487479667539, l2: 0.00041057672393846235   Iteration 32 of 100, tot loss = 5.214747816324234, l1: 0.00010824653838881204, l2: 0.000413228245861319   Iteration 33 of 100, tot loss = 5.134355573943167, l1: 0.00010713498285798723, l2: 0.00040630057694877246   Iteration 34 of 100, tot loss = 5.075035726322847, l1: 0.00010573666291105944, l2: 0.00040176691190401255   Iteration 35 of 100, tot loss = 5.098145621163504, l1: 0.00010667837590777448, l2: 0.0004031361881061457   Iteration 36 of 100, tot loss = 5.01277987824546, l1: 0.00010524474419071339, l2: 0.0003960332455284919   Iteration 37 of 100, tot loss = 4.962394888336594, l1: 0.00010434557515435626, l2: 0.0003918939157203469   Iteration 38 of 100, tot loss = 4.9699990184683545, l1: 0.00010524184625338469, l2: 0.0003917580587241039   Iteration 39 of 100, tot loss = 4.907436578701704, l1: 0.00010429836234274822, l2: 0.0003864452982013926   Iteration 40 of 100, tot loss = 4.93977278470993, l1: 0.00010451306770846714, l2: 0.00038946421282162194   Iteration 41 of 100, tot loss = 4.896210798403112, l1: 0.00010374590943328945, l2: 0.0003858751721364991   Iteration 42 of 100, tot loss = 4.97841493288676, l1: 0.00010545948351223377, l2: 0.00039238201116815964   Iteration 43 of 100, tot loss = 5.0320863280185435, l1: 0.00010641835708452692, l2: 0.0003967902768297706   Iteration 44 of 100, tot loss = 5.091506730426442, l1: 0.00010703873208099553, l2: 0.0004021119412672388   Iteration 45 of 100, tot loss = 5.10087055630154, l1: 0.00010762428024059368, l2: 0.00040246277623939223   Iteration 46 of 100, tot loss = 5.115325088086336, l1: 0.00010743386614510952, l2: 0.00040409864326964294   Iteration 47 of 100, tot loss = 5.101553156020794, l1: 0.00010752368116493714, l2: 0.00040263163580365997   Iteration 48 of 100, tot loss = 5.081606596708298, l1: 0.0001070937726884343, l2: 0.0004010668885712221   Iteration 49 of 100, tot loss = 5.083330047373869, l1: 0.00010693057206202754, l2: 0.00040140243414052933   Iteration 50 of 100, tot loss = 5.0439128494262695, l1: 0.00010626705246977508, l2: 0.0003981242341978941   Iteration 51 of 100, tot loss = 4.99068351352916, l1: 0.00010538862018923585, l2: 0.0003936797328802355   Iteration 52 of 100, tot loss = 4.978778605277721, l1: 0.00010511299506805908, l2: 0.0003927648676136204   Iteration 53 of 100, tot loss = 4.964870241453063, l1: 0.00010508589129697286, l2: 0.00039140113510239763   Iteration 54 of 100, tot loss = 4.988097469011943, l1: 0.0001053953080069429, l2: 0.0003934144407577588   Iteration 55 of 100, tot loss = 5.016669598492709, l1: 0.0001057205707184039, l2: 0.00039594639079454776   Iteration 56 of 100, tot loss = 5.028960121529443, l1: 0.00010618857114812792, l2: 0.0003967074422429765   Iteration 57 of 100, tot loss = 5.042711839341281, l1: 0.00010663783164968583, l2: 0.00039763335358625147   Iteration 58 of 100, tot loss = 5.0230963024599795, l1: 0.00010642684431112339, l2: 0.0003958827872903102   Iteration 59 of 100, tot loss = 5.006860737073219, l1: 0.00010593180331081878, l2: 0.000394754271961204   Iteration 60 of 100, tot loss = 4.982027475039164, l1: 0.00010517994996916968, l2: 0.0003930227989864458   Iteration 61 of 100, tot loss = 5.018774282736857, l1: 0.0001057703516753314, l2: 0.00039610707859001046   Iteration 62 of 100, tot loss = 5.026361242417367, l1: 0.00010604387364676973, l2: 0.0003965922527816973   Iteration 63 of 100, tot loss = 5.00397076682439, l1: 0.00010545052657742793, l2: 0.000394946552183856   Iteration 64 of 100, tot loss = 4.946148749440908, l1: 0.00010433899132067381, l2: 0.0003902758855929278   Iteration 65 of 100, tot loss = 4.962376180061927, l1: 0.00010470514753251337, l2: 0.0003915324724333074   Iteration 66 of 100, tot loss = 4.953142191424514, l1: 0.00010415636086580696, l2: 0.0003911578605766408   Iteration 67 of 100, tot loss = 4.921476282290558, l1: 0.00010370893429733687, l2: 0.00038843869610425476   Iteration 68 of 100, tot loss = 4.897808888379266, l1: 0.00010353976417451016, l2: 0.0003862411268688875   Iteration 69 of 100, tot loss = 4.8950493024743125, l1: 0.00010366150583148988, l2: 0.0003858434267939352   Iteration 70 of 100, tot loss = 4.882774775368826, l1: 0.00010380662866477256, l2: 0.0003844708509859629   Iteration 71 of 100, tot loss = 4.879715805322352, l1: 0.00010378399865273846, l2: 0.0003841875838755276   Iteration 72 of 100, tot loss = 4.866449379258686, l1: 0.00010379605570657684, l2: 0.00038284888412615855   Iteration 73 of 100, tot loss = 4.851273739174621, l1: 0.00010353174665185292, l2: 0.0003815956291350319   Iteration 74 of 100, tot loss = 4.8511360013807145, l1: 0.00010344459573192384, l2: 0.00038166900619241486   Iteration 75 of 100, tot loss = 4.874349352518718, l1: 0.00010363298492544952, l2: 0.0003838019525088991   Iteration 76 of 100, tot loss = 4.861434839273754, l1: 0.00010355726221620755, l2: 0.00038258622398164955   Iteration 77 of 100, tot loss = 4.844669311077563, l1: 0.00010332793983677704, l2: 0.00038113899346797834   Iteration 78 of 100, tot loss = 4.8593158049461165, l1: 0.00010371226133616008, l2: 0.00038221932127504633   Iteration 79 of 100, tot loss = 4.915055293071119, l1: 0.0001047536471686945, l2: 0.00038675188408911085   Iteration 80 of 100, tot loss = 4.901163658499717, l1: 0.00010416720588182215, l2: 0.00038594916204601757   Iteration 81 of 100, tot loss = 4.920557766784857, l1: 0.00010451145175870301, l2: 0.00038754432710077336   Iteration 82 of 100, tot loss = 4.892500909363351, l1: 0.00010435474648214195, l2: 0.00038489534660872843   Iteration 83 of 100, tot loss = 4.90051457106349, l1: 0.00010453939282879548, l2: 0.00038551206645101355   Iteration 84 of 100, tot loss = 4.898031141076769, l1: 0.00010443609348190616, l2: 0.0003853670227510433   Iteration 85 of 100, tot loss = 4.911833872514612, l1: 0.00010496632680502336, l2: 0.0003862170627518721   Iteration 86 of 100, tot loss = 4.8974920954815175, l1: 0.0001047708659783428, l2: 0.0003849783457807525   Iteration 87 of 100, tot loss = 4.889276381196646, l1: 0.00010464634480698826, l2: 0.00038428129558050044   Iteration 88 of 100, tot loss = 4.902144841172478, l1: 0.0001046513958830143, l2: 0.0003855630901935828   Iteration 89 of 100, tot loss = 4.8999760525949885, l1: 0.00010441161013271116, l2: 0.0003855859969017943   Iteration 90 of 100, tot loss = 4.881521150800917, l1: 0.00010377731939823005, l2: 0.0003843747973506753   Iteration 91 of 100, tot loss = 4.850494167306921, l1: 0.0001031023497849314, l2: 0.0003819470687201349   Iteration 92 of 100, tot loss = 4.857905691084654, l1: 0.00010329858694445727, l2: 0.0003824919837390316   Iteration 93 of 100, tot loss = 4.845599979482671, l1: 0.00010299831320549192, l2: 0.0003815616862172441   Iteration 94 of 100, tot loss = 4.886418418681368, l1: 0.00010369016494455826, l2: 0.0003849516788056675   Iteration 95 of 100, tot loss = 4.876063580262033, l1: 0.00010370031454122479, l2: 0.0003839060452791225   Iteration 96 of 100, tot loss = 4.89014479269584, l1: 0.00010404992061315473, l2: 0.00038496456030164455   Iteration 97 of 100, tot loss = 4.872218616229972, l1: 0.00010375849224240567, l2: 0.0003834633710530116   Iteration 98 of 100, tot loss = 4.905922210946375, l1: 0.0001043516027233896, l2: 0.00038624061978355585   Iteration 99 of 100, tot loss = 4.902214881145593, l1: 0.00010440131375093408, l2: 0.0003858201759585151   Iteration 100 of 100, tot loss = 4.908224222660064, l1: 0.00010473306647327263, l2: 0.00038608935719821604
   End of epoch 1108; saving model... 

Epoch 1109 of 2000
   Iteration 1 of 100, tot loss = 2.7820916175842285, l1: 8.586147305322811e-05, l2: 0.00019234768114984035   Iteration 2 of 100, tot loss = 3.153739809989929, l1: 8.824227552395314e-05, l2: 0.00022713170619681478   Iteration 3 of 100, tot loss = 3.974857727686564, l1: 0.00010002200239493202, l2: 0.00029746377064536017   Iteration 4 of 100, tot loss = 4.71859747171402, l1: 0.00011523204011609778, l2: 0.00035662770096678287   Iteration 5 of 100, tot loss = 4.466517400741577, l1: 0.00011057258961955085, l2: 0.00033607914811000227   Iteration 6 of 100, tot loss = 4.526907881100972, l1: 0.00011050276467964674, l2: 0.0003421880246605724   Iteration 7 of 100, tot loss = 4.447806324277606, l1: 0.0001089780593507125, l2: 0.0003358025735776339   Iteration 8 of 100, tot loss = 4.483094364404678, l1: 0.00010964768534904579, l2: 0.00033866174999275245   Iteration 9 of 100, tot loss = 4.384279595481025, l1: 0.00010819711880887755, l2: 0.00033023083880026307   Iteration 10 of 100, tot loss = 4.365487122535706, l1: 0.0001048407233611215, l2: 0.0003317079856060445   Iteration 11 of 100, tot loss = 4.285707733847878, l1: 0.00010349233608311889, l2: 0.00032507843570783734   Iteration 12 of 100, tot loss = 4.6300844351450605, l1: 0.0001078047243936453, l2: 0.00035520371845147264   Iteration 13 of 100, tot loss = 4.674861687880296, l1: 0.00010888061940652103, l2: 0.00035860554691260826   Iteration 14 of 100, tot loss = 4.521386453083584, l1: 0.00010437864953668654, l2: 0.0003477599924995697   Iteration 15 of 100, tot loss = 4.530034828186035, l1: 0.00010493250980895634, l2: 0.00034807097205581764   Iteration 16 of 100, tot loss = 4.609077841043472, l1: 0.00010579908803265425, l2: 0.000355108695657691   Iteration 17 of 100, tot loss = 4.570267452913172, l1: 0.0001060805272358908, l2: 0.00035094621810404696   Iteration 18 of 100, tot loss = 4.4580109516779585, l1: 0.00010346568544466411, l2: 0.00034233540888332453   Iteration 19 of 100, tot loss = 4.474086171702335, l1: 0.00010291181675519941, l2: 0.0003444968001292038   Iteration 20 of 100, tot loss = 4.427707707881927, l1: 0.00010230227326246677, l2: 0.00034046849687001667   Iteration 21 of 100, tot loss = 4.56777074223473, l1: 0.00010492452513842312, l2: 0.0003518525481922552   Iteration 22 of 100, tot loss = 4.671326691454107, l1: 0.0001057830329293872, l2: 0.00036134963408006695   Iteration 23 of 100, tot loss = 4.6762197224990185, l1: 0.0001050733403446983, l2: 0.00036254862852333844   Iteration 24 of 100, tot loss = 4.729290177424748, l1: 0.0001063979787735055, l2: 0.0003665310365856082   Iteration 25 of 100, tot loss = 4.726970777511597, l1: 0.0001057129977561999, l2: 0.0003669840778457001   Iteration 26 of 100, tot loss = 4.7524095590297994, l1: 0.00010672230186733381, l2: 0.0003685186524168015   Iteration 27 of 100, tot loss = 4.758547456176193, l1: 0.00010634270155504864, l2: 0.00036951204216435416   Iteration 28 of 100, tot loss = 4.669660559722355, l1: 0.00010410490033661648, l2: 0.0003628611541768935   Iteration 29 of 100, tot loss = 4.770289462188194, l1: 0.000105876376594473, l2: 0.0003711525681990616   Iteration 30 of 100, tot loss = 4.8169824520746864, l1: 0.0001070504938979866, l2: 0.00037464774892820665   Iteration 31 of 100, tot loss = 4.804887271696521, l1: 0.00010711868585091114, l2: 0.0003733700394795667   Iteration 32 of 100, tot loss = 4.793885312974453, l1: 0.00010656723509328003, l2: 0.00037282129414961673   Iteration 33 of 100, tot loss = 4.711536234075373, l1: 0.0001050975789081404, l2: 0.00036605604250463796   Iteration 34 of 100, tot loss = 4.734727985718671, l1: 0.00010455197340942344, l2: 0.0003689208234016619   Iteration 35 of 100, tot loss = 4.697125625610352, l1: 0.00010414087934935066, l2: 0.000365571681010936   Iteration 36 of 100, tot loss = 4.652385950088501, l1: 0.00010295445534009357, l2: 0.00036228413713009405   Iteration 37 of 100, tot loss = 4.680464821892816, l1: 0.00010315444592993731, l2: 0.0003648920337090621   Iteration 38 of 100, tot loss = 4.6391988302532, l1: 0.00010259547774407302, l2: 0.000361324403217479   Iteration 39 of 100, tot loss = 4.643362852243277, l1: 0.00010302334283971085, l2: 0.0003613129400689967   Iteration 40 of 100, tot loss = 4.672265148162841, l1: 0.00010312490094293026, l2: 0.00036410161083040294   Iteration 41 of 100, tot loss = 4.695559210893585, l1: 0.00010312799580477564, l2: 0.0003664279225276692   Iteration 42 of 100, tot loss = 4.663416839781261, l1: 0.00010285392194678675, l2: 0.00036348775964662697   Iteration 43 of 100, tot loss = 4.621254377586897, l1: 0.00010163341603190422, l2: 0.00036049201890967007   Iteration 44 of 100, tot loss = 4.612771164287221, l1: 0.00010158808510657929, l2: 0.000359689028052592   Iteration 45 of 100, tot loss = 4.59081916809082, l1: 0.00010137250283150933, l2: 0.00035770941079438977   Iteration 46 of 100, tot loss = 4.579890645068625, l1: 0.00010112915072607828, l2: 0.0003568599106232717   Iteration 47 of 100, tot loss = 4.582508685741018, l1: 0.00010104164485188022, l2: 0.00035720922088775626   Iteration 48 of 100, tot loss = 4.570406814416249, l1: 0.00010053203103173776, l2: 0.0003565086472008261   Iteration 49 of 100, tot loss = 4.572370675145363, l1: 9.982669819562164e-05, l2: 0.00035741036592707115   Iteration 50 of 100, tot loss = 4.550186576843262, l1: 9.974417589546647e-05, l2: 0.00035527447849744933   Iteration 51 of 100, tot loss = 4.556949503281537, l1: 0.00010003404360545305, l2: 0.0003556609042468207   Iteration 52 of 100, tot loss = 4.554641072566692, l1: 0.00010045125565319232, l2: 0.00035501284946803935   Iteration 53 of 100, tot loss = 4.515368798993668, l1: 0.00010004586572835731, l2: 0.0003514910121733885   Iteration 54 of 100, tot loss = 4.486406926755552, l1: 9.948466814185613e-05, l2: 0.00034915602251809713   Iteration 55 of 100, tot loss = 4.526565317674117, l1: 9.984252068203535e-05, l2: 0.00035281400910621   Iteration 56 of 100, tot loss = 4.6014238851411005, l1: 0.0001011447131661823, l2: 0.0003589976734344548   Iteration 57 of 100, tot loss = 4.612244831888299, l1: 0.00010141187375299061, l2: 0.0003598126071697232   Iteration 58 of 100, tot loss = 4.587754907279179, l1: 0.00010112310080550178, l2: 0.0003576523876259232   Iteration 59 of 100, tot loss = 4.5998661962606135, l1: 0.00010155151496785378, l2: 0.000358435102359597   Iteration 60 of 100, tot loss = 4.628857851028442, l1: 0.00010231230923333594, l2: 0.0003605734736386997   Iteration 61 of 100, tot loss = 4.667374227867752, l1: 0.00010276259693410629, l2: 0.0003639748238874447   Iteration 62 of 100, tot loss = 4.635956145101978, l1: 0.00010216169744859555, l2: 0.0003614339155084904   Iteration 63 of 100, tot loss = 4.613911613585457, l1: 0.00010192878829594332, l2: 0.00035946237155243694   Iteration 64 of 100, tot loss = 4.609972856938839, l1: 0.00010184245951450066, l2: 0.0003591548249914922   Iteration 65 of 100, tot loss = 4.639603321368877, l1: 0.00010227276154462463, l2: 0.00036168756935064895   Iteration 66 of 100, tot loss = 4.629335006078084, l1: 0.00010208321212055374, l2: 0.0003608502873138412   Iteration 67 of 100, tot loss = 4.643591012527693, l1: 0.00010240100272679215, l2: 0.00036195809705314965   Iteration 68 of 100, tot loss = 4.634411776767058, l1: 0.00010271835018262637, l2: 0.0003607228258367874   Iteration 69 of 100, tot loss = 4.612049462138742, l1: 0.00010211138789494778, l2: 0.00035909355674580115   Iteration 70 of 100, tot loss = 4.619634798594883, l1: 0.00010223521530861035, l2: 0.00035972826236892224   Iteration 71 of 100, tot loss = 4.618367490634112, l1: 0.00010249034184339443, l2: 0.000359346405365242   Iteration 72 of 100, tot loss = 4.60812763704194, l1: 0.00010222469518339494, l2: 0.00035858806647108093   Iteration 73 of 100, tot loss = 4.586749540616388, l1: 0.00010194609078023685, l2: 0.00035672886149594175   Iteration 74 of 100, tot loss = 4.576742420325408, l1: 0.00010180735045798576, l2: 0.00035586688967226575   Iteration 75 of 100, tot loss = 4.574597069422404, l1: 0.00010167559104350706, l2: 0.00035578411363530906   Iteration 76 of 100, tot loss = 4.568541875011043, l1: 0.00010175586871092061, l2: 0.0003550983166956269   Iteration 77 of 100, tot loss = 4.542851398517559, l1: 0.00010121376232876043, l2: 0.00035307137522386846   Iteration 78 of 100, tot loss = 4.548518272546621, l1: 0.00010120233892872094, l2: 0.0003536494858473396   Iteration 79 of 100, tot loss = 4.517556893674633, l1: 0.00010046511914292233, l2: 0.0003512905677780509   Iteration 80 of 100, tot loss = 4.517540177702903, l1: 0.0001002922216684965, l2: 0.0003514617939799791   Iteration 81 of 100, tot loss = 4.540962281050505, l1: 0.00010053841348659868, l2: 0.0003535578125817586   Iteration 82 of 100, tot loss = 4.5555164668618175, l1: 0.00010051235355005214, l2: 0.0003550392916289771   Iteration 83 of 100, tot loss = 4.589158055293991, l1: 0.00010123216853538775, l2: 0.0003576836357992816   Iteration 84 of 100, tot loss = 4.621398468812306, l1: 0.00010172734064759598, l2: 0.00036041250513378707   Iteration 85 of 100, tot loss = 4.606911552653593, l1: 0.00010182958007242311, l2: 0.0003588615740334396   Iteration 86 of 100, tot loss = 4.6079913959946746, l1: 0.0001017974684865019, l2: 0.0003590016697904333   Iteration 87 of 100, tot loss = 4.656482751342072, l1: 0.00010262973872200725, l2: 0.0003630185351591964   Iteration 88 of 100, tot loss = 4.6388735662807115, l1: 0.0001021827749189635, l2: 0.00036170458026225043   Iteration 89 of 100, tot loss = 4.619468370180451, l1: 0.00010163161702999542, l2: 0.00036031521874443326   Iteration 90 of 100, tot loss = 4.633017585012648, l1: 0.00010187408952333499, l2: 0.0003614276681522218   Iteration 91 of 100, tot loss = 4.634101251979451, l1: 0.00010174003222306096, l2: 0.00036167009214865516   Iteration 92 of 100, tot loss = 4.653802729171256, l1: 0.00010187798140933687, l2: 0.00036350229034407835   Iteration 93 of 100, tot loss = 4.672629753748576, l1: 0.00010226973671539967, l2: 0.00036499323714549544   Iteration 94 of 100, tot loss = 4.683367675923287, l1: 0.00010236055285086549, l2: 0.00036597621309486755   Iteration 95 of 100, tot loss = 4.6787269366414925, l1: 0.0001019214422022311, l2: 0.0003659512499947787   Iteration 96 of 100, tot loss = 4.692033626139164, l1: 0.00010220331140923615, l2: 0.0003670000501188042   Iteration 97 of 100, tot loss = 4.668288540594356, l1: 0.00010157450228736295, l2: 0.00036525435070027164   Iteration 98 of 100, tot loss = 4.676867621285575, l1: 0.00010165943459302367, l2: 0.0003660273264822721   Iteration 99 of 100, tot loss = 4.685856274884157, l1: 0.00010207313655451118, l2: 0.00036651248993079243   Iteration 100 of 100, tot loss = 4.716214122772217, l1: 0.00010274741067405558, l2: 0.0003688740005600266
   End of epoch 1109; saving model... 

Epoch 1110 of 2000
   Iteration 1 of 100, tot loss = 5.503759384155273, l1: 0.00011456404899945483, l2: 0.0004358119040261954   Iteration 2 of 100, tot loss = 6.15529727935791, l1: 0.00011731002814485691, l2: 0.0004982196987839416   Iteration 3 of 100, tot loss = 5.164522012074788, l1: 0.00010565562600580354, l2: 0.00041079657967202365   Iteration 4 of 100, tot loss = 5.2849860191345215, l1: 0.00011526548405527137, l2: 0.00041323312325403094   Iteration 5 of 100, tot loss = 5.057764911651612, l1: 0.00011263751948717982, l2: 0.00039313897141255436   Iteration 6 of 100, tot loss = 4.941981395085652, l1: 0.0001136388042747664, l2: 0.0003805593344926213   Iteration 7 of 100, tot loss = 4.6295398984636575, l1: 0.00010569867299636826, l2: 0.0003572553146763572   Iteration 8 of 100, tot loss = 4.67193603515625, l1: 0.0001071183414751431, l2: 0.000360075257049175   Iteration 9 of 100, tot loss = 4.594712654749553, l1: 0.00010419592662300501, l2: 0.0003552753348938293   Iteration 10 of 100, tot loss = 4.903416800498962, l1: 0.00010795850248541683, l2: 0.000382383173564449   Iteration 11 of 100, tot loss = 4.94383489001881, l1: 0.00010795126581797376, l2: 0.00038643221804787487   Iteration 12 of 100, tot loss = 4.881247977415721, l1: 0.00010755096324525464, l2: 0.000380573830625508   Iteration 13 of 100, tot loss = 4.915314839436458, l1: 0.00010799439881408874, l2: 0.0003835370813388951   Iteration 14 of 100, tot loss = 4.713765297617231, l1: 0.00010393438554144398, l2: 0.00036744214074652907   Iteration 15 of 100, tot loss = 4.6772533257802325, l1: 0.00010386682139748396, l2: 0.00036385850883865106   Iteration 16 of 100, tot loss = 4.597513809800148, l1: 0.00010115492386830738, l2: 0.00035859645595337497   Iteration 17 of 100, tot loss = 4.684212670606725, l1: 0.00010277661974267925, l2: 0.00036564464629490804   Iteration 18 of 100, tot loss = 4.705504986974928, l1: 0.00010285382045872716, l2: 0.00036769667915197916   Iteration 19 of 100, tot loss = 4.711446724439922, l1: 0.0001044249672160827, l2: 0.0003667197053896655   Iteration 20 of 100, tot loss = 4.676976191997528, l1: 0.00010436637239763513, l2: 0.0003633312466263305   Iteration 21 of 100, tot loss = 4.640782889865694, l1: 0.00010336738237778523, l2: 0.0003607109074004083   Iteration 22 of 100, tot loss = 4.611373337832364, l1: 0.00010184821506052025, l2: 0.00035928911992496893   Iteration 23 of 100, tot loss = 4.549795834914498, l1: 0.00010130411196176125, l2: 0.0003536754723071404   Iteration 24 of 100, tot loss = 4.544979929924011, l1: 0.00010243698246389006, l2: 0.00035206101165385917   Iteration 25 of 100, tot loss = 4.54325387954712, l1: 0.00010105284076416865, l2: 0.0003532725491095334   Iteration 26 of 100, tot loss = 4.549220451941857, l1: 0.00010140165138112094, l2: 0.0003535203958073488   Iteration 27 of 100, tot loss = 4.5641350746154785, l1: 0.00010265111362699557, l2: 0.00035376239598176824   Iteration 28 of 100, tot loss = 4.658913646425519, l1: 0.000104046991119893, l2: 0.00036184437651952194   Iteration 29 of 100, tot loss = 4.7727137434071505, l1: 0.00010565787971276662, l2: 0.0003716134966013889   Iteration 30 of 100, tot loss = 4.824678564071656, l1: 0.00010658317963437487, l2: 0.00037588467724466074   Iteration 31 of 100, tot loss = 4.768691578219014, l1: 0.00010602546559136001, l2: 0.0003708436927427688   Iteration 32 of 100, tot loss = 4.72623185813427, l1: 0.00010538988453845377, l2: 0.00036723330231325235   Iteration 33 of 100, tot loss = 4.686663223035408, l1: 0.00010372093818273932, l2: 0.0003649453856544851   Iteration 34 of 100, tot loss = 4.686279030407176, l1: 0.00010312791463922408, l2: 0.0003654999891296029   Iteration 35 of 100, tot loss = 4.613753604888916, l1: 0.00010181864916895783, l2: 0.00035955671171125555   Iteration 36 of 100, tot loss = 4.676466147104899, l1: 0.00010258868708964049, l2: 0.00036505792862347636   Iteration 37 of 100, tot loss = 4.69802426003121, l1: 0.00010314029556891025, l2: 0.00036666213103721067   Iteration 38 of 100, tot loss = 4.71769296495538, l1: 0.00010392361592063313, l2: 0.00036784568050949787   Iteration 39 of 100, tot loss = 4.828764145190899, l1: 0.00010540938157459888, l2: 0.0003774670327309137   Iteration 40 of 100, tot loss = 4.868204164505005, l1: 0.0001060825929926068, l2: 0.0003807378227065783   Iteration 41 of 100, tot loss = 4.830850072023345, l1: 0.00010471728109689315, l2: 0.00037836772531679854   Iteration 42 of 100, tot loss = 4.789052747544789, l1: 0.00010369831259248756, l2: 0.0003752069612909552   Iteration 43 of 100, tot loss = 4.776430529217388, l1: 0.00010343770850869437, l2: 0.00037420534347400587   Iteration 44 of 100, tot loss = 4.780788389119235, l1: 0.00010379960564354074, l2: 0.00037427923234645277   Iteration 45 of 100, tot loss = 4.783440261416965, l1: 0.0001038655123718652, l2: 0.0003744785130644838   Iteration 46 of 100, tot loss = 4.776298533315244, l1: 0.00010381437799799681, l2: 0.0003738154748292721   Iteration 47 of 100, tot loss = 4.78611288679407, l1: 0.0001035727758828719, l2: 0.00037503851258116676   Iteration 48 of 100, tot loss = 4.788102507591248, l1: 0.00010403608613766362, l2: 0.00037477416420491255   Iteration 49 of 100, tot loss = 4.77767647529135, l1: 0.00010418162120765132, l2: 0.0003735860263244534   Iteration 50 of 100, tot loss = 4.8331556224823, l1: 0.00010523801938688848, l2: 0.0003780775424093008   Iteration 51 of 100, tot loss = 4.791930063098085, l1: 0.00010464131321664444, l2: 0.0003745516927059119   Iteration 52 of 100, tot loss = 4.773624640244704, l1: 0.00010405827540055795, l2: 0.00037330418826268916   Iteration 53 of 100, tot loss = 4.762067362947284, l1: 0.00010382174260261961, l2: 0.0003723849936164388   Iteration 54 of 100, tot loss = 4.738495720757379, l1: 0.0001030633721475843, l2: 0.0003707861995078727   Iteration 55 of 100, tot loss = 4.794892692565918, l1: 0.00010414892147475092, l2: 0.0003753403474216942   Iteration 56 of 100, tot loss = 4.816982669489724, l1: 0.00010451425656096294, l2: 0.0003771840100463513   Iteration 57 of 100, tot loss = 4.864130455150939, l1: 0.00010555137487244792, l2: 0.00038086167018934106   Iteration 58 of 100, tot loss = 4.888137899596115, l1: 0.00010565883556033629, l2: 0.0003831549540085814   Iteration 59 of 100, tot loss = 4.897568686533782, l1: 0.00010603629031674384, l2: 0.00038372057808713073   Iteration 60 of 100, tot loss = 4.883210245768229, l1: 0.00010582158447505207, l2: 0.0003824994397291448   Iteration 61 of 100, tot loss = 4.880643461571365, l1: 0.00010600355035123477, l2: 0.00038206079553957783   Iteration 62 of 100, tot loss = 4.838120564337699, l1: 0.0001051334572454629, l2: 0.0003786785990439145   Iteration 63 of 100, tot loss = 4.792972468194508, l1: 0.00010408985286226703, l2: 0.00037520739369072196   Iteration 64 of 100, tot loss = 4.776039959862828, l1: 0.00010397837297659862, l2: 0.0003736256228421553   Iteration 65 of 100, tot loss = 4.793325015214774, l1: 0.00010404487324054711, l2: 0.000375287628132635   Iteration 66 of 100, tot loss = 4.77719055703192, l1: 0.00010389777856868353, l2: 0.0003738212767392256   Iteration 67 of 100, tot loss = 4.7714159613225, l1: 0.0001041506254852666, l2: 0.00037299097040424873   Iteration 68 of 100, tot loss = 4.747754224959542, l1: 0.0001038172999602359, l2: 0.00037095812234945376   Iteration 69 of 100, tot loss = 4.759363238362298, l1: 0.000103915698528526, l2: 0.0003720206251550142   Iteration 70 of 100, tot loss = 4.730069615159716, l1: 0.00010343003897495302, l2: 0.00036957692250975276   Iteration 71 of 100, tot loss = 4.709238732364816, l1: 0.00010330268385997174, l2: 0.0003676211892243501   Iteration 72 of 100, tot loss = 4.724724916948213, l1: 0.00010344842454893903, l2: 0.0003690240670645532   Iteration 73 of 100, tot loss = 4.722105217306582, l1: 0.00010343458605547474, l2: 0.000368775936020602   Iteration 74 of 100, tot loss = 4.744873976385271, l1: 0.0001039730549736477, l2: 0.0003705143429436469   Iteration 75 of 100, tot loss = 4.725451324780782, l1: 0.00010374426589502643, l2: 0.00036880086719368895   Iteration 76 of 100, tot loss = 4.719392382784894, l1: 0.00010380143670006778, l2: 0.00036813780216585944   Iteration 77 of 100, tot loss = 4.745973254179026, l1: 0.00010426342043218311, l2: 0.00037033390593956916   Iteration 78 of 100, tot loss = 4.763972124992272, l1: 0.0001044572551715343, l2: 0.00037193995824334427   Iteration 79 of 100, tot loss = 4.787026787105995, l1: 0.00010478789367362315, l2: 0.00037391478589240794   Iteration 80 of 100, tot loss = 4.766042290627956, l1: 0.00010471337182025309, l2: 0.00037189085815043655   Iteration 81 of 100, tot loss = 4.751190631477921, l1: 0.00010446593345769532, l2: 0.00037065313059698654   Iteration 82 of 100, tot loss = 4.725855314150089, l1: 0.0001039325716857736, l2: 0.0003686529607654027   Iteration 83 of 100, tot loss = 4.7270233070994, l1: 0.00010408282679532296, l2: 0.00036861950528796716   Iteration 84 of 100, tot loss = 4.727324438946588, l1: 0.00010438293139443322, l2: 0.000368349513640472   Iteration 85 of 100, tot loss = 4.7454895342097565, l1: 0.00010454346505438855, l2: 0.00037000548909418287   Iteration 86 of 100, tot loss = 4.71421251879182, l1: 0.00010381421021139791, l2: 0.00036760704232471843   Iteration 87 of 100, tot loss = 4.708480052564336, l1: 0.00010368584154438677, l2: 0.0003671621642511702   Iteration 88 of 100, tot loss = 4.6995227648453275, l1: 0.00010366429846313126, l2: 0.00036628797848110975   Iteration 89 of 100, tot loss = 4.677594721987006, l1: 0.00010307469792576412, l2: 0.0003646847748150526   Iteration 90 of 100, tot loss = 4.689786366621653, l1: 0.0001031550928423207, l2: 0.00036582354385043597   Iteration 91 of 100, tot loss = 4.670532254072336, l1: 0.00010292035173963766, l2: 0.00036413287379328613   Iteration 92 of 100, tot loss = 4.678793125826379, l1: 0.00010319791738805614, l2: 0.00036468139518072587   Iteration 93 of 100, tot loss = 4.673654270428483, l1: 0.00010323196638441864, l2: 0.00036413346039211396   Iteration 94 of 100, tot loss = 4.69644651641237, l1: 0.00010372282652477883, l2: 0.0003659218250020863   Iteration 95 of 100, tot loss = 4.697879526489659, l1: 0.00010386905368680968, l2: 0.00036591889894273326   Iteration 96 of 100, tot loss = 4.730850876619418, l1: 0.00010438887629031039, l2: 0.0003686962115049634   Iteration 97 of 100, tot loss = 4.741411996870926, l1: 0.00010455490059798977, l2: 0.0003695862992431431   Iteration 98 of 100, tot loss = 4.7304617421967645, l1: 0.00010419865522672641, l2: 0.00036884751913318295   Iteration 99 of 100, tot loss = 4.724584019545353, l1: 0.00010422877466859946, l2: 0.0003682296276868631   Iteration 100 of 100, tot loss = 4.7082088339328765, l1: 0.0001037534244460403, l2: 0.0003670674591558054
   End of epoch 1110; saving model... 

Epoch 1111 of 2000
   Iteration 1 of 100, tot loss = 7.1956048011779785, l1: 0.00014283572090789676, l2: 0.0005767247639596462   Iteration 2 of 100, tot loss = 5.760761022567749, l1: 0.00012881819930044003, l2: 0.00044725791667588055   Iteration 3 of 100, tot loss = 4.5780307451883955, l1: 0.00010566986748017371, l2: 0.0003521332109812647   Iteration 4 of 100, tot loss = 4.290156900882721, l1: 9.38582670642063e-05, l2: 0.0003351574268890545   Iteration 5 of 100, tot loss = 4.310137319564819, l1: 9.229650313500315e-05, l2: 0.00033871723571792244   Iteration 6 of 100, tot loss = 4.648265480995178, l1: 9.882340236799791e-05, l2: 0.00036600314585181576   Iteration 7 of 100, tot loss = 4.741047280175345, l1: 0.00010004476644098759, l2: 0.0003740599620089467   Iteration 8 of 100, tot loss = 4.65959432721138, l1: 9.967917594622122e-05, l2: 0.0003662802555481903   Iteration 9 of 100, tot loss = 4.737770954767863, l1: 0.00010130332035866256, l2: 0.0003724737746071898   Iteration 10 of 100, tot loss = 4.79575879573822, l1: 0.00010057080144179053, l2: 0.0003790050803218037   Iteration 11 of 100, tot loss = 4.821628028696233, l1: 0.00010083865609803152, l2: 0.0003813241518483582   Iteration 12 of 100, tot loss = 4.939485649267833, l1: 0.00010364899632501572, l2: 0.0003902995740645565   Iteration 13 of 100, tot loss = 5.012775182723999, l1: 0.00010546308756322385, l2: 0.0003958144381893082   Iteration 14 of 100, tot loss = 4.8922344616481235, l1: 0.00010493729028634593, l2: 0.00038428616452230405   Iteration 15 of 100, tot loss = 5.031352519989014, l1: 0.00010723110317485407, l2: 0.0003959041573883345   Iteration 16 of 100, tot loss = 4.926560968160629, l1: 0.00010630479800965986, l2: 0.00038635130658803973   Iteration 17 of 100, tot loss = 4.963590032914105, l1: 0.00010699151522096465, l2: 0.00038936749607434165   Iteration 18 of 100, tot loss = 4.900855210092333, l1: 0.00010737145930761471, l2: 0.000382714068918075   Iteration 19 of 100, tot loss = 4.839801675394962, l1: 0.0001052342034343287, l2: 0.00037874597106374015   Iteration 20 of 100, tot loss = 4.872476255893707, l1: 0.00010629376156430226, l2: 0.00038095387135399504   Iteration 21 of 100, tot loss = 4.770117123921712, l1: 0.00010465890989594517, l2: 0.0003723528096175176   Iteration 22 of 100, tot loss = 4.79157567024231, l1: 0.00010532695821232416, l2: 0.0003738306173049337   Iteration 23 of 100, tot loss = 4.784846761952275, l1: 0.00010629862916412885, l2: 0.00037218605449079007   Iteration 24 of 100, tot loss = 4.702286074558894, l1: 0.00010512712530423111, l2: 0.0003651014897817125   Iteration 25 of 100, tot loss = 4.732996339797974, l1: 0.0001051997861941345, l2: 0.00036809985642321407   Iteration 26 of 100, tot loss = 4.757828263136057, l1: 0.00010623226290380654, l2: 0.00036955057294107974   Iteration 27 of 100, tot loss = 4.725371245984678, l1: 0.00010553263537413268, l2: 0.000367004498710028   Iteration 28 of 100, tot loss = 4.717199470315661, l1: 0.00010588823767778064, l2: 0.0003658317177074163   Iteration 29 of 100, tot loss = 4.68284167914555, l1: 0.00010523891251068562, l2: 0.00036304526368220307   Iteration 30 of 100, tot loss = 4.684753783543905, l1: 0.0001053641196146297, l2: 0.00036311126605141907   Iteration 31 of 100, tot loss = 4.725582738076487, l1: 0.00010633398027669998, l2: 0.0003662243017911791   Iteration 32 of 100, tot loss = 4.690548539161682, l1: 0.00010589897328827647, l2: 0.0003631558893175679   Iteration 33 of 100, tot loss = 4.65381974884958, l1: 0.00010480342439912033, l2: 0.0003605785596212654   Iteration 34 of 100, tot loss = 4.698655731537762, l1: 0.00010516902929178767, l2: 0.0003646965522099944   Iteration 35 of 100, tot loss = 4.767288671221052, l1: 0.00010562441498872691, l2: 0.00037110446054222326   Iteration 36 of 100, tot loss = 4.772697356012133, l1: 0.00010614196536254085, l2: 0.0003711277782309076   Iteration 37 of 100, tot loss = 4.7623540001946525, l1: 0.00010567474355166023, l2: 0.0003705606646395314   Iteration 38 of 100, tot loss = 4.772940723519576, l1: 0.00010529276704274755, l2: 0.00037200131229933744   Iteration 39 of 100, tot loss = 4.729856393276116, l1: 0.00010470022075773719, l2: 0.00036828542532673915   Iteration 40 of 100, tot loss = 4.677278161048889, l1: 0.0001033387016832421, l2: 0.0003643891210231232   Iteration 41 of 100, tot loss = 4.668448413290629, l1: 0.00010324982532637944, l2: 0.00036359502310671547   Iteration 42 of 100, tot loss = 4.669836816333589, l1: 0.00010304632910230707, l2: 0.0003639373590842643   Iteration 43 of 100, tot loss = 4.73780113042787, l1: 0.00010423277151773597, l2: 0.0003695473481689818   Iteration 44 of 100, tot loss = 4.756601507013494, l1: 0.00010497824744313469, l2: 0.0003706819099368824   Iteration 45 of 100, tot loss = 4.7484710905287, l1: 0.0001049713557424386, l2: 0.00036987576006342553   Iteration 46 of 100, tot loss = 4.775922609412151, l1: 0.00010595174970386206, l2: 0.00037164051742444786   Iteration 47 of 100, tot loss = 4.7597912828973, l1: 0.00010622476908425364, l2: 0.0003697543650830878   Iteration 48 of 100, tot loss = 4.738714873790741, l1: 0.00010600342958847857, l2: 0.00036786806382830645   Iteration 49 of 100, tot loss = 4.731550732437445, l1: 0.00010587770958984632, l2: 0.00036727736985586506   Iteration 50 of 100, tot loss = 4.727897577285766, l1: 0.00010591175865556579, l2: 0.00036687800515210255   Iteration 51 of 100, tot loss = 4.6893403857362035, l1: 0.00010499962745063171, l2: 0.000363934417049784   Iteration 52 of 100, tot loss = 4.778117528328528, l1: 0.00010630259769711232, l2: 0.00037150916054647847   Iteration 53 of 100, tot loss = 4.790881048958257, l1: 0.00010646882123491762, l2: 0.0003726192890613709   Iteration 54 of 100, tot loss = 4.769756078720093, l1: 0.00010651974049256683, l2: 0.00037045587237958833   Iteration 55 of 100, tot loss = 4.777133118022572, l1: 0.00010638448934928005, l2: 0.00037132882809435777   Iteration 56 of 100, tot loss = 4.7616963641984125, l1: 0.00010613138504465627, l2: 0.0003700382569929518   Iteration 57 of 100, tot loss = 4.773480273129647, l1: 0.00010688313488029515, l2: 0.0003704648973469279   Iteration 58 of 100, tot loss = 4.754320152874651, l1: 0.00010636884558309624, l2: 0.0003690631741031619   Iteration 59 of 100, tot loss = 4.758600323887195, l1: 0.00010634227750785412, l2: 0.00036951775911216764   Iteration 60 of 100, tot loss = 4.787858478228251, l1: 0.00010694421707739821, l2: 0.00037184163520578297   Iteration 61 of 100, tot loss = 4.752525270962324, l1: 0.00010610413802005198, l2: 0.0003691483933104538   Iteration 62 of 100, tot loss = 4.722203931500835, l1: 0.00010575116151988866, l2: 0.0003664692356382617   Iteration 63 of 100, tot loss = 4.718479451679048, l1: 0.00010568233558423965, l2: 0.0003661656138276504   Iteration 64 of 100, tot loss = 4.764835894107819, l1: 0.00010614244354201219, l2: 0.0003703411500737275   Iteration 65 of 100, tot loss = 4.774444514054519, l1: 0.00010643331054150342, l2: 0.0003710111453144166   Iteration 66 of 100, tot loss = 4.756718346566865, l1: 0.00010601357754432794, l2: 0.00036965826174392447   Iteration 67 of 100, tot loss = 4.730841526344641, l1: 0.0001055327227379112, l2: 0.00036755143473647645   Iteration 68 of 100, tot loss = 4.7136880299624275, l1: 0.00010516243117522476, l2: 0.0003662063767585684   Iteration 69 of 100, tot loss = 4.709078512329986, l1: 0.00010525237514124508, l2: 0.0003656554811944564   Iteration 70 of 100, tot loss = 4.705112191608974, l1: 0.00010542434954134349, l2: 0.0003650868748081848   Iteration 71 of 100, tot loss = 4.706054714364066, l1: 0.00010534960442078366, l2: 0.0003652558723484284   Iteration 72 of 100, tot loss = 4.732411576641931, l1: 0.00010590958239239019, l2: 0.0003673315808959564   Iteration 73 of 100, tot loss = 4.733853810454068, l1: 0.0001061396304141628, l2: 0.00036724575608649787   Iteration 74 of 100, tot loss = 4.749772464906847, l1: 0.00010646382823422853, l2: 0.0003685134237260885   Iteration 75 of 100, tot loss = 4.748745231628418, l1: 0.0001067369378870353, l2: 0.0003681375905095289   Iteration 76 of 100, tot loss = 4.808304661198666, l1: 0.00010777774380250393, l2: 0.00037305272685204586   Iteration 77 of 100, tot loss = 4.8020873007836276, l1: 0.00010767776257125661, l2: 0.0003725309718654237   Iteration 78 of 100, tot loss = 4.821373866154597, l1: 0.00010789145903217081, l2: 0.00037424593122126773   Iteration 79 of 100, tot loss = 4.801493943492068, l1: 0.00010756245420809506, l2: 0.0003725869438899799   Iteration 80 of 100, tot loss = 4.800756493210793, l1: 0.00010766036402856116, l2: 0.00037241528916638346   Iteration 81 of 100, tot loss = 4.8091553376044756, l1: 0.00010756446737767324, l2: 0.0003733510700525877   Iteration 82 of 100, tot loss = 4.790032020429286, l1: 0.00010750562175483693, l2: 0.0003714975840610838   Iteration 83 of 100, tot loss = 4.771179681800934, l1: 0.00010685891771046669, l2: 0.00037025905414821066   Iteration 84 of 100, tot loss = 4.781556112425668, l1: 0.00010726275346920981, l2: 0.0003708928610042979   Iteration 85 of 100, tot loss = 4.787009211147533, l1: 0.0001073882301331169, l2: 0.00037131269414470916   Iteration 86 of 100, tot loss = 4.779698238816372, l1: 0.00010711764336393085, l2: 0.00037085218388901287   Iteration 87 of 100, tot loss = 4.791042980106398, l1: 0.00010737709374228578, l2: 0.00037172720760867084   Iteration 88 of 100, tot loss = 4.792239032008431, l1: 0.00010724842063196279, l2: 0.0003719754862355661   Iteration 89 of 100, tot loss = 4.765492616074808, l1: 0.00010680304334977684, l2: 0.0003697462218062785   Iteration 90 of 100, tot loss = 4.751311816109551, l1: 0.00010672180109799633, l2: 0.0003684093841002323   Iteration 91 of 100, tot loss = 4.744748539977021, l1: 0.00010669574432220863, l2: 0.0003677791131309817   Iteration 92 of 100, tot loss = 4.736139999783558, l1: 0.00010670100765191424, l2: 0.0003669129957574035   Iteration 93 of 100, tot loss = 4.733473846989293, l1: 0.00010694101795814531, l2: 0.0003664063701909336   Iteration 94 of 100, tot loss = 4.737838422998469, l1: 0.00010716843098634854, l2: 0.00036661541464049966   Iteration 95 of 100, tot loss = 4.732066523401361, l1: 0.00010693378916537193, l2: 0.0003662728666217606   Iteration 96 of 100, tot loss = 4.724164431293805, l1: 0.00010689929710376116, l2: 0.0003655171493240535   Iteration 97 of 100, tot loss = 4.694591281340294, l1: 0.00010622117068911375, l2: 0.00036323796066613966   Iteration 98 of 100, tot loss = 4.6780450854982645, l1: 0.00010605352753725815, l2: 0.00036175098440643135   Iteration 99 of 100, tot loss = 4.697313653098212, l1: 0.00010644259431629883, l2: 0.0003632887746054547   Iteration 100 of 100, tot loss = 4.701192166805267, l1: 0.00010614119448291603, l2: 0.0003639780259982217
   End of epoch 1111; saving model... 

Epoch 1112 of 2000
   Iteration 1 of 100, tot loss = 7.273505210876465, l1: 0.00011513881327118725, l2: 0.0006122117047198117   Iteration 2 of 100, tot loss = 5.897584915161133, l1: 0.00011098333925474435, l2: 0.00047877515316940844   Iteration 3 of 100, tot loss = 5.964728355407715, l1: 0.00012118380982428789, l2: 0.0004752890090458095   Iteration 4 of 100, tot loss = 5.49380624294281, l1: 0.00011905444989679381, l2: 0.00043032615940319374   Iteration 5 of 100, tot loss = 5.415567207336426, l1: 0.0001132850768044591, l2: 0.00042827163124457   Iteration 6 of 100, tot loss = 5.133743365605672, l1: 0.00010706149744995248, l2: 0.00040631282899994403   Iteration 7 of 100, tot loss = 4.804738828114101, l1: 9.915943831271891e-05, l2: 0.000381314436838563   Iteration 8 of 100, tot loss = 4.96500638127327, l1: 0.00010537948219280224, l2: 0.00039112114973249845   Iteration 9 of 100, tot loss = 4.899942848417494, l1: 0.00010495418326980952, l2: 0.0003850400971714407   Iteration 10 of 100, tot loss = 4.969472861289978, l1: 0.00010563926916802302, l2: 0.00039130800869315865   Iteration 11 of 100, tot loss = 5.206146652048284, l1: 0.0001088774191554297, l2: 0.0004117372392846102   Iteration 12 of 100, tot loss = 5.181349694728851, l1: 0.00011056501170969568, l2: 0.0004075699519792882   Iteration 13 of 100, tot loss = 5.22678725536053, l1: 0.00011345292351996669, l2: 0.00040922579892839375   Iteration 14 of 100, tot loss = 5.115547299385071, l1: 0.00011254125586544563, l2: 0.0003990134719060734   Iteration 15 of 100, tot loss = 5.035364929835001, l1: 0.00010915286014399802, l2: 0.00039438363164663316   Iteration 16 of 100, tot loss = 5.089127376675606, l1: 0.00010945063877443317, l2: 0.0003994620983576169   Iteration 17 of 100, tot loss = 5.036761466194601, l1: 0.00011004661706358413, l2: 0.00039362952948602685   Iteration 18 of 100, tot loss = 5.051385733816359, l1: 0.0001097870537907713, l2: 0.0003953515196270827   Iteration 19 of 100, tot loss = 5.090092947608547, l1: 0.0001099313414765914, l2: 0.0003990779545060114   Iteration 20 of 100, tot loss = 5.08217111825943, l1: 0.00010802662873174996, l2: 0.00040019048465183004   Iteration 21 of 100, tot loss = 5.0795760949452715, l1: 0.00010877480339591525, l2: 0.0003991828083858958   Iteration 22 of 100, tot loss = 5.139204621315002, l1: 0.00011029025666754354, l2: 0.0004036302085627209   Iteration 23 of 100, tot loss = 5.179732664771702, l1: 0.00011137701917702898, l2: 0.00040659625032592726   Iteration 24 of 100, tot loss = 5.173420240481694, l1: 0.00011094266786434066, l2: 0.0004063993595385303   Iteration 25 of 100, tot loss = 5.149463529586792, l1: 0.00011130150320241227, l2: 0.00040364485350437464   Iteration 26 of 100, tot loss = 5.1980826029410725, l1: 0.00011209655601235751, l2: 0.0004077117068496031   Iteration 27 of 100, tot loss = 5.163060232445046, l1: 0.0001107533020398545, l2: 0.00040555272479886534   Iteration 28 of 100, tot loss = 5.1627441218921115, l1: 0.00011119271636874015, l2: 0.0004050816995524136   Iteration 29 of 100, tot loss = 5.1396941563178755, l1: 0.00011136296761206126, l2: 0.0004026064516751674   Iteration 30 of 100, tot loss = 5.069769326845805, l1: 0.00010959398156652848, l2: 0.00039738295502805463   Iteration 31 of 100, tot loss = 5.065501866802093, l1: 0.00010991306325811292, l2: 0.0003966371277196994   Iteration 32 of 100, tot loss = 5.0210216492414474, l1: 0.00010917481176875299, l2: 0.0003929273570975056   Iteration 33 of 100, tot loss = 5.031682939240427, l1: 0.00010933858103464556, l2: 0.0003938297159038484   Iteration 34 of 100, tot loss = 5.042443906559663, l1: 0.00010955428939162041, l2: 0.0003946901030276957   Iteration 35 of 100, tot loss = 4.9799788747515, l1: 0.0001085032509373767, l2: 0.00038949463840773596   Iteration 36 of 100, tot loss = 5.0376060009002686, l1: 0.00010970066917555717, l2: 0.0003940599332660592   Iteration 37 of 100, tot loss = 5.010679219220136, l1: 0.0001090008915304458, l2: 0.0003920670325798612   Iteration 38 of 100, tot loss = 4.986023488797639, l1: 0.00010876810617781685, l2: 0.0003898342451736282   Iteration 39 of 100, tot loss = 4.951415000817715, l1: 0.00010885331087196484, l2: 0.00038628819143363775   Iteration 40 of 100, tot loss = 4.8783970952033995, l1: 0.00010765443166746991, l2: 0.0003801852803007932   Iteration 41 of 100, tot loss = 4.874865183016149, l1: 0.00010742357699200511, l2: 0.0003800629435828319   Iteration 42 of 100, tot loss = 4.841287226904006, l1: 0.00010671463037613735, l2: 0.00037741409427586144   Iteration 43 of 100, tot loss = 4.797074833581614, l1: 0.00010590282299956523, l2: 0.0003738046622412756   Iteration 44 of 100, tot loss = 4.80618017911911, l1: 0.00010652168467376296, l2: 0.00037409633461555296   Iteration 45 of 100, tot loss = 4.747436253229777, l1: 0.00010530229919822887, l2: 0.00036944132783294967   Iteration 46 of 100, tot loss = 4.718245449273483, l1: 0.00010517836056786346, l2: 0.00036664618591717243   Iteration 47 of 100, tot loss = 4.722332807297402, l1: 0.00010478746968301687, l2: 0.0003674458124070488   Iteration 48 of 100, tot loss = 4.73123125731945, l1: 0.00010498996304401469, l2: 0.0003681331642534739   Iteration 49 of 100, tot loss = 4.761244846850025, l1: 0.00010589261023132891, l2: 0.00037023187579340015   Iteration 50 of 100, tot loss = 4.76690776348114, l1: 0.00010622583926306106, l2: 0.0003704649378778413   Iteration 51 of 100, tot loss = 4.775654862908756, l1: 0.00010681396858725587, l2: 0.0003707515191076798   Iteration 52 of 100, tot loss = 4.774792235631209, l1: 0.00010727572323678312, l2: 0.00037020350170608325   Iteration 53 of 100, tot loss = 4.756582489553487, l1: 0.00010690019210088858, l2: 0.00036875805806561884   Iteration 54 of 100, tot loss = 4.735379249961288, l1: 0.00010701295509028973, l2: 0.0003665249710643871   Iteration 55 of 100, tot loss = 4.71374021876942, l1: 0.0001062813971657306, l2: 0.00036509262598966334   Iteration 56 of 100, tot loss = 4.719190333570753, l1: 0.00010604265285759798, l2: 0.0003658763822126535   Iteration 57 of 100, tot loss = 4.722703916984692, l1: 0.00010605082487720146, l2: 0.00036621956908340126   Iteration 58 of 100, tot loss = 4.717595174394805, l1: 0.00010643163891629605, l2: 0.00036532788061745594   Iteration 59 of 100, tot loss = 4.676600702738358, l1: 0.00010581735915689871, l2: 0.00036184271349634786   Iteration 60 of 100, tot loss = 4.691731870174408, l1: 0.0001062040742302391, l2: 0.00036296911566751077   Iteration 61 of 100, tot loss = 4.752943871451206, l1: 0.00010632177050030775, l2: 0.00036897261857940646   Iteration 62 of 100, tot loss = 4.728450294463865, l1: 0.00010554945437170025, l2: 0.00036729557717579507   Iteration 63 of 100, tot loss = 4.751252019216144, l1: 0.00010586375719867647, l2: 0.00036926144671197683   Iteration 64 of 100, tot loss = 4.778653051704168, l1: 0.0001059080399272716, l2: 0.00037195726690697484   Iteration 65 of 100, tot loss = 4.770991226343008, l1: 0.00010573626439928866, l2: 0.00037136285995634704   Iteration 66 of 100, tot loss = 4.723481543136366, l1: 0.00010485374806387082, l2: 0.00036749440782428297   Iteration 67 of 100, tot loss = 4.739315406600041, l1: 0.00010456962600209639, l2: 0.000369361915613469   Iteration 68 of 100, tot loss = 4.751225531101227, l1: 0.00010494764217796996, l2: 0.0003701749123780139   Iteration 69 of 100, tot loss = 4.732000917628191, l1: 0.00010454342813371086, l2: 0.00036865666502769494   Iteration 70 of 100, tot loss = 4.822075775691441, l1: 0.00010580468011279923, l2: 0.0003764028995647095   Iteration 71 of 100, tot loss = 4.831247611784599, l1: 0.00010614511719863848, l2: 0.0003769796464221657   Iteration 72 of 100, tot loss = 4.833588474326664, l1: 0.00010594499270963651, l2: 0.00037741385656570655   Iteration 73 of 100, tot loss = 4.8181383021890305, l1: 0.00010570712820730452, l2: 0.00037610670372250504   Iteration 74 of 100, tot loss = 4.811456509538599, l1: 0.00010562476770271157, l2: 0.00037552088493563086   Iteration 75 of 100, tot loss = 4.796745862960815, l1: 0.00010548260261809143, l2: 0.00037419198527156064   Iteration 76 of 100, tot loss = 4.803753937545576, l1: 0.00010516562298634151, l2: 0.00037520977198973445   Iteration 77 of 100, tot loss = 4.818764819727315, l1: 0.00010538157915685919, l2: 0.0003764949039353493   Iteration 78 of 100, tot loss = 4.8136113576399975, l1: 0.00010533242894803736, l2: 0.0003760287078106418   Iteration 79 of 100, tot loss = 4.8006818234166015, l1: 0.00010513556955192132, l2: 0.00037493261370860934   Iteration 80 of 100, tot loss = 4.77617255449295, l1: 0.00010488107877790754, l2: 0.00037273617781465875   Iteration 81 of 100, tot loss = 4.795855898915986, l1: 0.00010516602704251173, l2: 0.000374419564369744   Iteration 82 of 100, tot loss = 4.835651258142983, l1: 0.00010565348845663788, l2: 0.00037791163918971107   Iteration 83 of 100, tot loss = 4.81032173317599, l1: 0.00010522902067098487, l2: 0.0003758031544934801   Iteration 84 of 100, tot loss = 4.829027868452526, l1: 0.00010559400068034717, l2: 0.00037730878778217757   Iteration 85 of 100, tot loss = 4.812075623343973, l1: 0.00010555399806916659, l2: 0.00037565356572432556   Iteration 86 of 100, tot loss = 4.838648721229198, l1: 0.00010570446994638904, l2: 0.00037816040377609085   Iteration 87 of 100, tot loss = 4.8535935385473845, l1: 0.00010601548470814991, l2: 0.00037934387038493977   Iteration 88 of 100, tot loss = 4.833336125720631, l1: 0.0001059807641468069, l2: 0.00037735284968221094   Iteration 89 of 100, tot loss = 4.8256694654400425, l1: 0.00010613950417904193, l2: 0.00037642744362610653   Iteration 90 of 100, tot loss = 4.809962492518955, l1: 0.00010589149529146703, l2: 0.0003751047552214004   Iteration 91 of 100, tot loss = 4.840777310696277, l1: 0.00010655633037798567, l2: 0.0003775214019377337   Iteration 92 of 100, tot loss = 4.826961885327878, l1: 0.00010619998667370173, l2: 0.0003764962031867902   Iteration 93 of 100, tot loss = 4.8220902104531564, l1: 0.00010639031969858284, l2: 0.0003758187023034039   Iteration 94 of 100, tot loss = 4.814033559028139, l1: 0.00010613281626726174, l2: 0.00037527054054736894   Iteration 95 of 100, tot loss = 4.81530351638794, l1: 0.00010609763439309685, l2: 0.00037543271824496945   Iteration 96 of 100, tot loss = 4.819985677798589, l1: 0.00010628511385372501, l2: 0.00037571345501419273   Iteration 97 of 100, tot loss = 4.803322393869617, l1: 0.00010593976473979431, l2: 0.0003743924759508389   Iteration 98 of 100, tot loss = 4.785301417720561, l1: 0.0001055886159334403, l2: 0.0003729415271961016   Iteration 99 of 100, tot loss = 4.784577263726129, l1: 0.00010528058937963597, l2: 0.000373177138195994   Iteration 100 of 100, tot loss = 4.800350589752197, l1: 0.00010552318144618766, l2: 0.00037451187861734067
   End of epoch 1112; saving model... 

Epoch 1113 of 2000
   Iteration 1 of 100, tot loss = 2.5414483547210693, l1: 5.5988941312534735e-05, l2: 0.00019815590349026024   Iteration 2 of 100, tot loss = 3.88770854473114, l1: 9.397593930771109e-05, l2: 0.0002947949105873704   Iteration 3 of 100, tot loss = 4.806512435277303, l1: 0.00011337154016170341, l2: 0.0003672797077645858   Iteration 4 of 100, tot loss = 4.389813482761383, l1: 9.884153405437246e-05, l2: 0.0003401398134883493   Iteration 5 of 100, tot loss = 4.923006010055542, l1: 0.00010351948731113225, l2: 0.00038878110935911534   Iteration 6 of 100, tot loss = 4.796960790952046, l1: 9.809842716398028e-05, l2: 0.0003815976524492726   Iteration 7 of 100, tot loss = 4.735337836401803, l1: 0.0001018768536076615, l2: 0.0003716569294088653   Iteration 8 of 100, tot loss = 4.660867661237717, l1: 0.00010376124737376813, l2: 0.00036232551792636514   Iteration 9 of 100, tot loss = 4.77049199740092, l1: 0.00010228894502183216, l2: 0.0003747602523718443   Iteration 10 of 100, tot loss = 4.965520215034485, l1: 0.00010687142930692062, l2: 0.00038968059234321115   Iteration 11 of 100, tot loss = 5.1617398045279765, l1: 0.00010989343784537844, l2: 0.0004062805409458551   Iteration 12 of 100, tot loss = 5.0705856283505755, l1: 0.00010870230653381441, l2: 0.00039835625648265705   Iteration 13 of 100, tot loss = 5.097610015135545, l1: 0.00010842811305057973, l2: 0.0004013328893611637   Iteration 14 of 100, tot loss = 5.389070766312735, l1: 0.00011341512176191568, l2: 0.00042549195599609187   Iteration 15 of 100, tot loss = 5.409273672103882, l1: 0.00011391041577250387, l2: 0.00042701695153179266   Iteration 16 of 100, tot loss = 5.351548507809639, l1: 0.00011282124751232914, l2: 0.0004223336036375258   Iteration 17 of 100, tot loss = 5.288797392564661, l1: 0.00011180750409360317, l2: 0.0004170722352094291   Iteration 18 of 100, tot loss = 5.235150270991856, l1: 0.00011190439019830794, l2: 0.0004116106364462111   Iteration 19 of 100, tot loss = 5.218924710625096, l1: 0.00011092110033036749, l2: 0.00041097136961884404   Iteration 20 of 100, tot loss = 5.16854897737503, l1: 0.00011004823936673347, l2: 0.0004068066569743678   Iteration 21 of 100, tot loss = 5.129446654092698, l1: 0.00010837391406918566, l2: 0.0004045707506260702   Iteration 22 of 100, tot loss = 5.200084133581682, l1: 0.00010974237573629415, l2: 0.0004102660353634168   Iteration 23 of 100, tot loss = 5.1947857504305635, l1: 0.00010968605815635428, l2: 0.0004097925165020253   Iteration 24 of 100, tot loss = 5.175971815983455, l1: 0.0001101310311544997, l2: 0.00040746614952998544   Iteration 25 of 100, tot loss = 5.246098775863647, l1: 0.00011076154129114002, l2: 0.0004138483351562172   Iteration 26 of 100, tot loss = 5.165931857549227, l1: 0.00010902514595027942, l2: 0.00040756803914868773   Iteration 27 of 100, tot loss = 5.105070493839405, l1: 0.00010795570226575903, l2: 0.00040255134593246986   Iteration 28 of 100, tot loss = 5.138425239494869, l1: 0.0001089903079056447, l2: 0.00040485221584926227   Iteration 29 of 100, tot loss = 5.085892258019283, l1: 0.0001078247165925608, l2: 0.00040076450938519474   Iteration 30 of 100, tot loss = 5.048722982406616, l1: 0.00010740788105370788, l2: 0.0003974644176196307   Iteration 31 of 100, tot loss = 5.072973589743337, l1: 0.00010754812847740288, l2: 0.0003997492303531016   Iteration 32 of 100, tot loss = 5.041636735200882, l1: 0.00010737740126387507, l2: 0.0003967862721765414   Iteration 33 of 100, tot loss = 5.086670991146203, l1: 0.00010843242224919695, l2: 0.00040023467701041335   Iteration 34 of 100, tot loss = 5.059712970958037, l1: 0.00010801663580316338, l2: 0.0003979546617141322   Iteration 35 of 100, tot loss = 5.071574006761823, l1: 0.00010839097792216177, l2: 0.0003987664232097034   Iteration 36 of 100, tot loss = 5.085135830773248, l1: 0.00010859869528050897, l2: 0.0003999148886780151   Iteration 37 of 100, tot loss = 5.073207429937415, l1: 0.00010834272492305704, l2: 0.000398978018327742   Iteration 38 of 100, tot loss = 5.069052219390869, l1: 0.00010850550652800226, l2: 0.0003983997150115963   Iteration 39 of 100, tot loss = 5.0846255253522825, l1: 0.00010872196976262599, l2: 0.00039974058148427267   Iteration 40 of 100, tot loss = 5.027705270051956, l1: 0.00010751145910035121, l2: 0.0003952590668632183   Iteration 41 of 100, tot loss = 5.064932282378034, l1: 0.00010814781072861856, l2: 0.00039834541583819903   Iteration 42 of 100, tot loss = 5.107096779914129, l1: 0.00010920051395909727, l2: 0.0004015091627869489   Iteration 43 of 100, tot loss = 5.119759365569713, l1: 0.0001099808031603759, l2: 0.0004019951326014517   Iteration 44 of 100, tot loss = 5.182176356965845, l1: 0.00011132070639260664, l2: 0.00040689692832529545   Iteration 45 of 100, tot loss = 5.214924414952596, l1: 0.00011176672948446746, l2: 0.000409725710697886   Iteration 46 of 100, tot loss = 5.155586512192436, l1: 0.00011073923135130747, l2: 0.00040481941845348996   Iteration 47 of 100, tot loss = 5.115162824062591, l1: 0.0001097579246406701, l2: 0.00040175835663908496   Iteration 48 of 100, tot loss = 5.112781797846158, l1: 0.00010973452496424822, l2: 0.0004015436528182666   Iteration 49 of 100, tot loss = 5.138556884259594, l1: 0.00011046688777408848, l2: 0.00040338879833663147   Iteration 50 of 100, tot loss = 5.178698134422302, l1: 0.00011127248420962132, l2: 0.0004065973270917311   Iteration 51 of 100, tot loss = 5.194735513013952, l1: 0.00011195499959570702, l2: 0.0004075185496213974   Iteration 52 of 100, tot loss = 5.167515873908997, l1: 0.00011144825111841783, l2: 0.00040530333409641083   Iteration 53 of 100, tot loss = 5.223401114625751, l1: 0.0001117018768607797, l2: 0.00041063823286918396   Iteration 54 of 100, tot loss = 5.209141987341422, l1: 0.00011129346064451427, l2: 0.00040962073641518754   Iteration 55 of 100, tot loss = 5.1471108783375135, l1: 0.00011028560673532246, l2: 0.0004044254794611003   Iteration 56 of 100, tot loss = 5.122485348156521, l1: 0.00010999776720512142, l2: 0.0004022507660660527   Iteration 57 of 100, tot loss = 5.114769826855576, l1: 0.00011015078812386236, l2: 0.00040132619298610575   Iteration 58 of 100, tot loss = 5.112078140521872, l1: 0.00011029489372770607, l2: 0.0004009129180962316   Iteration 59 of 100, tot loss = 5.131646358360679, l1: 0.00011026462774587621, l2: 0.0004029000058111151   Iteration 60 of 100, tot loss = 5.076943337917328, l1: 0.00010915551168485156, l2: 0.00039853881996047374   Iteration 61 of 100, tot loss = 5.078268273932035, l1: 0.00010940855072923844, l2: 0.00039841827495619046   Iteration 62 of 100, tot loss = 5.061879538720654, l1: 0.00010921220599547885, l2: 0.00039697574609289725   Iteration 63 of 100, tot loss = 5.060416679533701, l1: 0.00010935506719443578, l2: 0.00039668659917965884   Iteration 64 of 100, tot loss = 5.042843211442232, l1: 0.00010953515180744944, l2: 0.00039474916775361635   Iteration 65 of 100, tot loss = 5.0338220339555, l1: 0.00010884678737896207, l2: 0.00039453541439098236   Iteration 66 of 100, tot loss = 5.0550091808492486, l1: 0.00010897317632484236, l2: 0.00039652774020479143   Iteration 67 of 100, tot loss = 5.073289760902746, l1: 0.00010955266363670312, l2: 0.0003977763108603323   Iteration 68 of 100, tot loss = 5.061016703353209, l1: 0.00010937842569760773, l2: 0.00039672324355171225   Iteration 69 of 100, tot loss = 5.047934736030689, l1: 0.00010881888132037905, l2: 0.0003959745914418844   Iteration 70 of 100, tot loss = 5.05887702873775, l1: 0.0001092943916384164, l2: 0.00039659331045446117   Iteration 71 of 100, tot loss = 5.078905135812894, l1: 0.00010990969469634698, l2: 0.00039798081778711313   Iteration 72 of 100, tot loss = 5.05947431590822, l1: 0.00010965795036099735, l2: 0.0003962894798961416   Iteration 73 of 100, tot loss = 5.054405604323295, l1: 0.00010992320307669204, l2: 0.0003955173557337169   Iteration 74 of 100, tot loss = 5.019764594129614, l1: 0.00010932838754465715, l2: 0.00039264807030023704   Iteration 75 of 100, tot loss = 4.979183910687764, l1: 0.00010857613728148863, l2: 0.00038934225235910463   Iteration 76 of 100, tot loss = 4.984000640480142, l1: 0.00010887802014430666, l2: 0.00038952204283161434   Iteration 77 of 100, tot loss = 4.965893598345967, l1: 0.00010881515793568854, l2: 0.0003877742007728545   Iteration 78 of 100, tot loss = 4.975811819235484, l1: 0.00010911333009015578, l2: 0.00038846785061580775   Iteration 79 of 100, tot loss = 4.969228830518602, l1: 0.00010875126021277584, l2: 0.00038817162162040747   Iteration 80 of 100, tot loss = 4.976802505552769, l1: 0.00010902476551564178, l2: 0.00038865548394824146   Iteration 81 of 100, tot loss = 4.969087307835802, l1: 0.00010925578654105602, l2: 0.00038765294295534444   Iteration 82 of 100, tot loss = 4.953718541598901, l1: 0.00010902248562537929, l2: 0.0003863493672347764   Iteration 83 of 100, tot loss = 4.965380836682147, l1: 0.00010934257758879496, l2: 0.0003871955045888829   Iteration 84 of 100, tot loss = 4.954622145209994, l1: 0.00010924048451886934, l2: 0.00038622172879903313   Iteration 85 of 100, tot loss = 4.959652930147508, l1: 0.00010895315816228771, l2: 0.0003870121335186174   Iteration 86 of 100, tot loss = 4.962046707785407, l1: 0.0001090651135729236, l2: 0.00038713955562047356   Iteration 87 of 100, tot loss = 4.94798211256663, l1: 0.00010910712151745355, l2: 0.0003856910881385657   Iteration 88 of 100, tot loss = 4.952770768241449, l1: 0.00010931780723860192, l2: 0.0003859592677393018   Iteration 89 of 100, tot loss = 4.938122841749299, l1: 0.00010918194229918627, l2: 0.0003846303399278144   Iteration 90 of 100, tot loss = 4.928081476688385, l1: 0.0001090429677181722, l2: 0.000383765177846524   Iteration 91 of 100, tot loss = 4.926523707725189, l1: 0.00010918939569619597, l2: 0.0003834629729121494   Iteration 92 of 100, tot loss = 4.928590070942174, l1: 0.00010919547434162308, l2: 0.00038366353075379385   Iteration 93 of 100, tot loss = 4.92824121316274, l1: 0.00010921563666817101, l2: 0.0003836084827242459   Iteration 94 of 100, tot loss = 4.920320887514886, l1: 0.00010901342922923195, l2: 0.0003830186576530476   Iteration 95 of 100, tot loss = 4.896079521430166, l1: 0.00010845212914558176, l2: 0.0003811558209188086   Iteration 96 of 100, tot loss = 4.89485186462601, l1: 0.00010867569331670286, l2: 0.00038080949101034395   Iteration 97 of 100, tot loss = 4.89613844807615, l1: 0.00010874851368404877, l2: 0.00038086532881560244   Iteration 98 of 100, tot loss = 4.878967802135312, l1: 0.00010828903028521716, l2: 0.00037960774753461305   Iteration 99 of 100, tot loss = 4.874026896977665, l1: 0.0001085019711804261, l2: 0.0003789007161612237   Iteration 100 of 100, tot loss = 4.9258435642719265, l1: 0.0001092511856040801, l2: 0.00038333316799253224
   End of epoch 1113; saving model... 

Epoch 1114 of 2000
   Iteration 1 of 100, tot loss = 4.869146823883057, l1: 9.784742724150419e-05, l2: 0.0003890672523993999   Iteration 2 of 100, tot loss = 5.528902053833008, l1: 0.00011709759564837441, l2: 0.0004357926081866026   Iteration 3 of 100, tot loss = 5.69854211807251, l1: 0.00012320398915714273, l2: 0.0004466502189946671   Iteration 4 of 100, tot loss = 5.871106266975403, l1: 0.00012168157445557881, l2: 0.00046542905329260975   Iteration 5 of 100, tot loss = 6.132956027984619, l1: 0.00012322922557359562, l2: 0.0004900663741864264   Iteration 6 of 100, tot loss = 5.906446218490601, l1: 0.00011953327217876601, l2: 0.00047111134335864335   Iteration 7 of 100, tot loss = 5.989516939435687, l1: 0.0001231893412685687, l2: 0.0004757623412712876   Iteration 8 of 100, tot loss = 5.722829967737198, l1: 0.00012134522603446385, l2: 0.0004509377613430843   Iteration 9 of 100, tot loss = 5.883344517813788, l1: 0.00012224714333165644, l2: 0.0004660872962429292   Iteration 10 of 100, tot loss = 5.684383583068848, l1: 0.00012020927970297634, l2: 0.00044822906784247607   Iteration 11 of 100, tot loss = 5.566928169944069, l1: 0.00011804666891376573, l2: 0.0004386461382223801   Iteration 12 of 100, tot loss = 5.3676130175590515, l1: 0.00011492894251811474, l2: 0.00042183234957822907   Iteration 13 of 100, tot loss = 5.527972203034621, l1: 0.00011732428538380191, l2: 0.00043547292723535345   Iteration 14 of 100, tot loss = 5.506483980587551, l1: 0.00011675395632794659, l2: 0.00043389443349691907   Iteration 15 of 100, tot loss = 5.312410068511963, l1: 0.00011226677670492791, l2: 0.00041897422343026844   Iteration 16 of 100, tot loss = 5.424811363220215, l1: 0.00011424279932725767, l2: 0.0004282383279132773   Iteration 17 of 100, tot loss = 5.396201133728027, l1: 0.00011365506168872612, l2: 0.00042596504328471115   Iteration 18 of 100, tot loss = 5.522651937272814, l1: 0.00011637791390563102, l2: 0.00043588727405424125   Iteration 19 of 100, tot loss = 5.52413817455894, l1: 0.00011761786701850054, l2: 0.0004347959452149409   Iteration 20 of 100, tot loss = 5.35996288061142, l1: 0.0001153135852291598, l2: 0.00042068269758601675   Iteration 21 of 100, tot loss = 5.346037217548916, l1: 0.00011506195230703313, l2: 0.0004195417658636524   Iteration 22 of 100, tot loss = 5.41754324869676, l1: 0.00011627684861278712, l2: 0.00042547747183231297   Iteration 23 of 100, tot loss = 5.3048606126204785, l1: 0.00011324202363460284, l2: 0.0004172440334830595   Iteration 24 of 100, tot loss = 5.294984181722005, l1: 0.00011292676542022188, l2: 0.0004165716491115745   Iteration 25 of 100, tot loss = 5.304636993408203, l1: 0.00011390472092898563, l2: 0.0004165589751210064   Iteration 26 of 100, tot loss = 5.316241392722497, l1: 0.00011450917400697318, l2: 0.00041711496081776346   Iteration 27 of 100, tot loss = 5.271959622701009, l1: 0.00011299880268914556, l2: 0.0004141971553003208   Iteration 28 of 100, tot loss = 5.378871236528669, l1: 0.00011508974213419216, l2: 0.00042279737473498765   Iteration 29 of 100, tot loss = 5.419952507676749, l1: 0.0001157210303815724, l2: 0.00042627421348227253   Iteration 30 of 100, tot loss = 5.437284183502197, l1: 0.00011486896813342659, l2: 0.0004288594422784323   Iteration 31 of 100, tot loss = 5.406585016558247, l1: 0.00011489696583838292, l2: 0.0004257615270739001   Iteration 32 of 100, tot loss = 5.381338402628899, l1: 0.00011416134566388791, l2: 0.0004239724858052796   Iteration 33 of 100, tot loss = 5.342103784734553, l1: 0.00011286125443417184, l2: 0.00042134911530989814   Iteration 34 of 100, tot loss = 5.431633893181296, l1: 0.00011432279943084508, l2: 0.0004288405804853777   Iteration 35 of 100, tot loss = 5.446655423300607, l1: 0.00011486446746857836, l2: 0.0004298010658073638   Iteration 36 of 100, tot loss = 5.499537388483684, l1: 0.00011558004047805702, l2: 0.00043437368973779183   Iteration 37 of 100, tot loss = 5.483427060616983, l1: 0.0001151617815029002, l2: 0.00043318091539313664   Iteration 38 of 100, tot loss = 5.495430381674516, l1: 0.00011505493488297553, l2: 0.00043448809485294317   Iteration 39 of 100, tot loss = 5.570662021636963, l1: 0.00011655501088497635, l2: 0.0004405111823684703   Iteration 40 of 100, tot loss = 5.502218478918076, l1: 0.00011580423306440934, l2: 0.00043441760608402545   Iteration 41 of 100, tot loss = 5.4766390207337174, l1: 0.00011556679716549541, l2: 0.0004320970957207198   Iteration 42 of 100, tot loss = 5.433640769549778, l1: 0.00011506568314091834, l2: 0.000428298384663538   Iteration 43 of 100, tot loss = 5.421493302944095, l1: 0.00011440651233410896, l2: 0.0004277428091628217   Iteration 44 of 100, tot loss = 5.445348430763591, l1: 0.00011514056198145475, l2: 0.00042939427228041245   Iteration 45 of 100, tot loss = 5.473416948318482, l1: 0.00011554804303967912, l2: 0.00043179364325219975   Iteration 46 of 100, tot loss = 5.428444302600363, l1: 0.00011420232603204963, l2: 0.0004286420959903373   Iteration 47 of 100, tot loss = 5.389316776965527, l1: 0.00011349821053296724, l2: 0.00042543345857956545   Iteration 48 of 100, tot loss = 5.40122106174628, l1: 0.00011411249882560999, l2: 0.00042600959901998675   Iteration 49 of 100, tot loss = 5.400249398484522, l1: 0.00011415617826765365, l2: 0.00042586875289479006   Iteration 50 of 100, tot loss = 5.368164525032044, l1: 0.0001136098225833848, l2: 0.0004232066214899532   Iteration 51 of 100, tot loss = 5.371278627246034, l1: 0.00011343706037401788, l2: 0.0004236907940745975   Iteration 52 of 100, tot loss = 5.327125017459576, l1: 0.00011248362674310696, l2: 0.0004202288666699762   Iteration 53 of 100, tot loss = 5.313606235216248, l1: 0.00011267000081716105, l2: 0.0004186906139607945   Iteration 54 of 100, tot loss = 5.300671639265837, l1: 0.00011222214878351358, l2: 0.0004178450070429352   Iteration 55 of 100, tot loss = 5.301551584763961, l1: 0.00011265852332475002, l2: 0.0004174966277787462   Iteration 56 of 100, tot loss = 5.253325130258288, l1: 0.0001116672712149531, l2: 0.00041366523484092407   Iteration 57 of 100, tot loss = 5.243907393070689, l1: 0.00011184150909621007, l2: 0.00041254922331988874   Iteration 58 of 100, tot loss = 5.236881387644801, l1: 0.00011194301711431511, l2: 0.0004117451144025083   Iteration 59 of 100, tot loss = 5.2029327093544655, l1: 0.00011147329743095216, l2: 0.00040881996663579305   Iteration 60 of 100, tot loss = 5.176084323724111, l1: 0.00011104006443929393, l2: 0.0004065683613589499   Iteration 61 of 100, tot loss = 5.147130977912028, l1: 0.00011066787591830017, l2: 0.00040404521550059503   Iteration 62 of 100, tot loss = 5.141113938823823, l1: 0.00011071478682776703, l2: 0.0004033966010132234   Iteration 63 of 100, tot loss = 5.171264614377703, l1: 0.00011156894031464906, l2: 0.0004055575153047574   Iteration 64 of 100, tot loss = 5.122047688812017, l1: 0.00011067779394124955, l2: 0.00040152696919903974   Iteration 65 of 100, tot loss = 5.088746701754056, l1: 0.00011054591103367364, l2: 0.00039832875347481323   Iteration 66 of 100, tot loss = 5.0605652404553965, l1: 0.00011045006172163553, l2: 0.000395606456687815   Iteration 67 of 100, tot loss = 5.055635110655827, l1: 0.00011015873076133569, l2: 0.00039540477524469816   Iteration 68 of 100, tot loss = 5.101931487812715, l1: 0.00011056233426429303, l2: 0.00039963080951779643   Iteration 69 of 100, tot loss = 5.145995734394461, l1: 0.00011134808480257061, l2: 0.0004032514837529996   Iteration 70 of 100, tot loss = 5.129735864911761, l1: 0.000111081687022566, l2: 0.0004018918947882152   Iteration 71 of 100, tot loss = 5.119004840582189, l1: 0.00011093502177227236, l2: 0.00040096545741158786   Iteration 72 of 100, tot loss = 5.112841407457988, l1: 0.00011065310950976305, l2: 0.0004006310268272904   Iteration 73 of 100, tot loss = 5.1154390295890915, l1: 0.00011061697694782346, l2: 0.00040092692137951003   Iteration 74 of 100, tot loss = 5.136408006822741, l1: 0.00011077434769627084, l2: 0.0004028664485746491   Iteration 75 of 100, tot loss = 5.118910624186198, l1: 0.00011079290299676359, l2: 0.00040109815502849716   Iteration 76 of 100, tot loss = 5.170865535736084, l1: 0.00011166397368804992, l2: 0.00040542257537660927   Iteration 77 of 100, tot loss = 5.1318005400818665, l1: 0.00011099960451625143, l2: 0.0004021804450932503   Iteration 78 of 100, tot loss = 5.102816306627714, l1: 0.00011052024600734838, l2: 0.0003997613802755204   Iteration 79 of 100, tot loss = 5.082095870488806, l1: 0.00011008983213436801, l2: 0.0003981197505594007   Iteration 80 of 100, tot loss = 5.111870819330216, l1: 0.00011053561884182273, l2: 0.00040065145840344484   Iteration 81 of 100, tot loss = 5.121000266369478, l1: 0.00011072335671346037, l2: 0.0004013766653916258   Iteration 82 of 100, tot loss = 5.130436344844539, l1: 0.00011086672659474993, l2: 0.0004021769035295243   Iteration 83 of 100, tot loss = 5.11056948857135, l1: 0.00011056694715825212, l2: 0.0004004899974132563   Iteration 84 of 100, tot loss = 5.148438391231355, l1: 0.00011131994714170495, l2: 0.0004035238875769123   Iteration 85 of 100, tot loss = 5.164102004556095, l1: 0.0001113899631651721, l2: 0.000405020232873914   Iteration 86 of 100, tot loss = 5.164157113363577, l1: 0.0001114442658395409, l2: 0.00040497144089935913   Iteration 87 of 100, tot loss = 5.158809513881288, l1: 0.0001112053234527562, l2: 0.0004046756232922747   Iteration 88 of 100, tot loss = 5.19212191213261, l1: 0.00011192224445205119, l2: 0.000407289941490903   Iteration 89 of 100, tot loss = 5.1606752363483555, l1: 0.00011134917849120213, l2: 0.0004047183400043548   Iteration 90 of 100, tot loss = 5.150751066207886, l1: 0.00011102110972084726, l2: 0.000404053991951514   Iteration 91 of 100, tot loss = 5.129138453976139, l1: 0.00011058081896747923, l2: 0.0004023330215464479   Iteration 92 of 100, tot loss = 5.102746292300846, l1: 0.00011026518064761109, l2: 0.00040000944389485875   Iteration 93 of 100, tot loss = 5.13519529373415, l1: 0.00011090973496339433, l2: 0.00040260978983164394   Iteration 94 of 100, tot loss = 5.126521924708752, l1: 0.000110991962115959, l2: 0.0004016602259535501   Iteration 95 of 100, tot loss = 5.123660878131264, l1: 0.00011110877438975303, l2: 0.00040125730907005305   Iteration 96 of 100, tot loss = 5.126676551997662, l1: 0.00011109740345697598, l2: 0.0004015702476560061   Iteration 97 of 100, tot loss = 5.119343602780214, l1: 0.00011110204537807167, l2: 0.0004008323105397759   Iteration 98 of 100, tot loss = 5.109925092483054, l1: 0.00011101721893468567, l2: 0.0003999752860886938   Iteration 99 of 100, tot loss = 5.109871124980425, l1: 0.00011112484310734829, l2: 0.00039986226527782327   Iteration 100 of 100, tot loss = 5.10953816652298, l1: 0.00011101547694124748, l2: 0.0003999383354675956
   End of epoch 1114; saving model... 

Epoch 1115 of 2000
   Iteration 1 of 100, tot loss = 6.284957408905029, l1: 0.00011138245463371277, l2: 0.0005171133088879287   Iteration 2 of 100, tot loss = 5.26116681098938, l1: 0.00010780432057799771, l2: 0.000418312381953001   Iteration 3 of 100, tot loss = 5.416245301564534, l1: 0.00011767354347587873, l2: 0.00042395098716951907   Iteration 4 of 100, tot loss = 5.525852918624878, l1: 0.00011612299931584857, l2: 0.00043646229460136965   Iteration 5 of 100, tot loss = 5.475756740570068, l1: 0.00011298825265839695, l2: 0.00043458741856738925   Iteration 6 of 100, tot loss = 5.441932439804077, l1: 0.00011511953683414806, l2: 0.00042907370273799944   Iteration 7 of 100, tot loss = 5.472880908421108, l1: 0.00011648407338985376, l2: 0.00043080401623488536   Iteration 8 of 100, tot loss = 5.575586676597595, l1: 0.0001156210510089295, l2: 0.000441937616415089   Iteration 9 of 100, tot loss = 5.593423366546631, l1: 0.00011877058811175327, l2: 0.00044057174818590283   Iteration 10 of 100, tot loss = 5.450498151779175, l1: 0.00011613524693530053, l2: 0.0004289145668735728   Iteration 11 of 100, tot loss = 5.68711553920399, l1: 0.00012106004710817201, l2: 0.00044765150629576635   Iteration 12 of 100, tot loss = 5.667374014854431, l1: 0.0001216580737188148, l2: 0.0004450793276191689   Iteration 13 of 100, tot loss = 5.5793071159949665, l1: 0.0001198383363841388, l2: 0.000438092372720488   Iteration 14 of 100, tot loss = 5.455980164664132, l1: 0.00011713796889775299, l2: 0.00042846004570102584   Iteration 15 of 100, tot loss = 5.422094662984212, l1: 0.00011675768037093804, l2: 0.0004254517863349368   Iteration 16 of 100, tot loss = 5.453754633665085, l1: 0.00011726031834768946, l2: 0.0004281151432223851   Iteration 17 of 100, tot loss = 5.373455243952134, l1: 0.0001161887651987319, l2: 0.0004211567575111985   Iteration 18 of 100, tot loss = 5.2674343718422785, l1: 0.00011568555621326798, l2: 0.00041105787952094234   Iteration 19 of 100, tot loss = 5.303253236569856, l1: 0.00011496411331647418, l2: 0.00041536120988894254   Iteration 20 of 100, tot loss = 5.2971380114555355, l1: 0.0001146951544797048, l2: 0.00041501864689053036   Iteration 21 of 100, tot loss = 5.323066586539859, l1: 0.00011515985326176243, l2: 0.000417146806111781   Iteration 22 of 100, tot loss = 5.356558853929693, l1: 0.00011628024102802473, l2: 0.0004193756460815414   Iteration 23 of 100, tot loss = 5.394977807998657, l1: 0.00011729760950876643, l2: 0.00042220017222343415   Iteration 24 of 100, tot loss = 5.365441630283992, l1: 0.0001164895478117008, l2: 0.00042005461667334504   Iteration 25 of 100, tot loss = 5.338321695327759, l1: 0.00011613055306952446, l2: 0.00041770161769818517   Iteration 26 of 100, tot loss = 5.35694629412431, l1: 0.00011653289122757717, l2: 0.00041916173862633656   Iteration 27 of 100, tot loss = 5.277303951757926, l1: 0.00011551804938655416, l2: 0.00041221234628378796   Iteration 28 of 100, tot loss = 5.3138220225061685, l1: 0.000115758068204741, l2: 0.00041562413389328867   Iteration 29 of 100, tot loss = 5.246589800407147, l1: 0.00011446880746697043, l2: 0.00041019017226062715   Iteration 30 of 100, tot loss = 5.192018206914266, l1: 0.00011344929953338578, l2: 0.0004057525210858633   Iteration 31 of 100, tot loss = 5.187831755607359, l1: 0.00011302545998102775, l2: 0.00040575771615089425   Iteration 32 of 100, tot loss = 5.240461990237236, l1: 0.00011441611081863812, l2: 0.00040963008905237075   Iteration 33 of 100, tot loss = 5.230211171236905, l1: 0.00011405382776336575, l2: 0.00040896729104023314   Iteration 34 of 100, tot loss = 5.225709059659173, l1: 0.00011447107381707825, l2: 0.00040809983329381794   Iteration 35 of 100, tot loss = 5.1977200644356865, l1: 0.00011418562187048208, l2: 0.0004055863853344428   Iteration 36 of 100, tot loss = 5.147606352965037, l1: 0.00011357077831538238, l2: 0.0004011898579645074   Iteration 37 of 100, tot loss = 5.231658220291138, l1: 0.00011443923958231116, l2: 0.00040872658231622865   Iteration 38 of 100, tot loss = 5.177544104425531, l1: 0.00011331792714037492, l2: 0.0004044364836054707   Iteration 39 of 100, tot loss = 5.246271512447259, l1: 0.00011408842804555137, l2: 0.0004105387242116894   Iteration 40 of 100, tot loss = 5.238051724433899, l1: 0.00011373813667887589, l2: 0.00041006703613675197   Iteration 41 of 100, tot loss = 5.210772118917325, l1: 0.00011329461888569157, l2: 0.00040778259337857   Iteration 42 of 100, tot loss = 5.13985017935435, l1: 0.00011212944411367754, l2: 0.00040185557424722774   Iteration 43 of 100, tot loss = 5.080575704574585, l1: 0.00011120903912151969, l2: 0.0003968485313613877   Iteration 44 of 100, tot loss = 5.0755714123899285, l1: 0.00011137400096314113, l2: 0.000396183140599698   Iteration 45 of 100, tot loss = 5.072519848081801, l1: 0.00011146564146555547, l2: 0.0003957863441125179   Iteration 46 of 100, tot loss = 5.061579616173454, l1: 0.00011067482236008483, l2: 0.00039548314025919154   Iteration 47 of 100, tot loss = 5.1121041673295045, l1: 0.0001114901126439168, l2: 0.0003997203054902282   Iteration 48 of 100, tot loss = 5.0697954793771105, l1: 0.00011043131735277711, l2: 0.00039654823225040065   Iteration 49 of 100, tot loss = 5.034394609684846, l1: 0.00010975935568852464, l2: 0.00039368010706942986   Iteration 50 of 100, tot loss = 5.0316060304641725, l1: 0.00010918087675236165, l2: 0.00039397972781443966   Iteration 51 of 100, tot loss = 5.001518726348877, l1: 0.00010864660497663506, l2: 0.0003915052691488253   Iteration 52 of 100, tot loss = 4.951165712796724, l1: 0.00010798225020362924, l2: 0.000387134322618994   Iteration 53 of 100, tot loss = 4.914516763867073, l1: 0.00010747307903827432, l2: 0.00038397859892904545   Iteration 54 of 100, tot loss = 4.918754083138925, l1: 0.00010756849951576442, l2: 0.00038430691051029564   Iteration 55 of 100, tot loss = 4.889153268120506, l1: 0.00010730085182744501, l2: 0.00038161447657991877   Iteration 56 of 100, tot loss = 4.8765464297362735, l1: 0.0001074300603899506, l2: 0.00038022458440017156   Iteration 57 of 100, tot loss = 4.877479331535206, l1: 0.00010710508190401781, l2: 0.0003806428538059424   Iteration 58 of 100, tot loss = 4.885325740123617, l1: 0.00010688761878931285, l2: 0.0003816449574640438   Iteration 59 of 100, tot loss = 4.878850059994196, l1: 0.00010683762451724576, l2: 0.00038104738336711553   Iteration 60 of 100, tot loss = 4.915963129202525, l1: 0.00010767831251238628, l2: 0.00038391800262616016   Iteration 61 of 100, tot loss = 4.878641452945646, l1: 0.00010731074012926643, l2: 0.00038055340701248497   Iteration 62 of 100, tot loss = 4.8727920247662455, l1: 0.00010710995282410764, l2: 0.0003801692517064962   Iteration 63 of 100, tot loss = 4.872152899938916, l1: 0.00010721718810225231, l2: 0.00037999810407950824   Iteration 64 of 100, tot loss = 4.906246427446604, l1: 0.0001076366911547666, l2: 0.00038298795357150084   Iteration 65 of 100, tot loss = 4.873371285658616, l1: 0.00010683203704852181, l2: 0.0003805050933106731   Iteration 66 of 100, tot loss = 4.887011022278757, l1: 0.00010647769136774127, l2: 0.000382223412805505   Iteration 67 of 100, tot loss = 4.902436042899516, l1: 0.0001069817826918104, l2: 0.00038326182337512554   Iteration 68 of 100, tot loss = 4.8750797404962425, l1: 0.00010650973961353028, l2: 0.0003809982359491508   Iteration 69 of 100, tot loss = 4.856227988782137, l1: 0.00010604028827493903, l2: 0.0003795825122970332   Iteration 70 of 100, tot loss = 4.9051253625324795, l1: 0.00010698926645480761, l2: 0.0003835232708037698   Iteration 71 of 100, tot loss = 4.876327047885304, l1: 0.00010648559507156367, l2: 0.00038114711039693056   Iteration 72 of 100, tot loss = 4.86355812019772, l1: 0.00010633361630930772, l2: 0.00038002219642092113   Iteration 73 of 100, tot loss = 4.871593129144956, l1: 0.00010640818232188818, l2: 0.00038075113086163845   Iteration 74 of 100, tot loss = 4.845100589700647, l1: 0.00010600022152003303, l2: 0.0003785098379012197   Iteration 75 of 100, tot loss = 4.857586765289307, l1: 0.00010621437366353347, l2: 0.0003795443036748717   Iteration 76 of 100, tot loss = 4.909536932644091, l1: 0.00010730024749998273, l2: 0.00038365344631816506   Iteration 77 of 100, tot loss = 4.874631160265439, l1: 0.00010684653940873656, l2: 0.00038061657725446316   Iteration 78 of 100, tot loss = 4.838158717522254, l1: 0.00010605134724382646, l2: 0.00037776452518301085   Iteration 79 of 100, tot loss = 4.884238267246681, l1: 0.00010674049996010535, l2: 0.0003816833278632876   Iteration 80 of 100, tot loss = 4.913793241977691, l1: 0.00010713137744460255, l2: 0.00038424794747697887   Iteration 81 of 100, tot loss = 4.918960035583119, l1: 0.00010725938342080486, l2: 0.0003846366208888889   Iteration 82 of 100, tot loss = 4.945608104147563, l1: 0.00010772698566675322, l2: 0.00038683382604731155   Iteration 83 of 100, tot loss = 4.9401672030069745, l1: 0.00010792041128692049, l2: 0.0003860963104188958   Iteration 84 of 100, tot loss = 4.925505555811382, l1: 0.00010785010116316178, l2: 0.0003847004556432477   Iteration 85 of 100, tot loss = 4.9130831157459935, l1: 0.00010781849842340521, l2: 0.00038348981436095474   Iteration 86 of 100, tot loss = 4.898661857427553, l1: 0.0001075036499207991, l2: 0.00038236253681663034   Iteration 87 of 100, tot loss = 4.897628608791307, l1: 0.00010771690529969188, l2: 0.0003820459568142441   Iteration 88 of 100, tot loss = 4.917211510918357, l1: 0.00010813802030249711, l2: 0.0003835831321339356   Iteration 89 of 100, tot loss = 4.953056581904379, l1: 0.00010877284446987584, l2: 0.0003865328151405674   Iteration 90 of 100, tot loss = 4.974535650677151, l1: 0.00010897878512170994, l2: 0.0003884747816805935   Iteration 91 of 100, tot loss = 4.972809744405223, l1: 0.00010894413004120157, l2: 0.0003883368461994086   Iteration 92 of 100, tot loss = 4.937765515368918, l1: 0.00010820625155484659, l2: 0.00038557030173884095   Iteration 93 of 100, tot loss = 4.9840243554884385, l1: 0.00010891411836372978, l2: 0.0003894883192642542   Iteration 94 of 100, tot loss = 4.982950804081369, l1: 0.00010906352083034011, l2: 0.00038923156132520314   Iteration 95 of 100, tot loss = 5.004888263501619, l1: 0.00010923027703507901, l2: 0.00039125855102228294   Iteration 96 of 100, tot loss = 5.0001592834790545, l1: 0.00010912326429964499, l2: 0.0003908926654124419   Iteration 97 of 100, tot loss = 5.00587692457376, l1: 0.00010942472258226265, l2: 0.00039116297137429076   Iteration 98 of 100, tot loss = 4.998298187645114, l1: 0.00010934095653468904, l2: 0.0003904888638929578   Iteration 99 of 100, tot loss = 4.980161308038114, l1: 0.00010892235704978711, l2: 0.00038909377524925566   Iteration 100 of 100, tot loss = 4.97537903547287, l1: 0.00010891646303207381, l2: 0.0003886214418162126
   End of epoch 1115; saving model... 

Epoch 1116 of 2000
   Iteration 1 of 100, tot loss = 3.2578775882720947, l1: 8.23416339699179e-05, l2: 0.0002434461348457262   Iteration 2 of 100, tot loss = 4.552472949028015, l1: 0.00010902403300860897, l2: 0.00034622327802935615   Iteration 3 of 100, tot loss = 4.938502073287964, l1: 0.00011647265152229617, l2: 0.00037737757763049257   Iteration 4 of 100, tot loss = 4.437455773353577, l1: 0.00010665528498066124, l2: 0.00033709030685713515   Iteration 5 of 100, tot loss = 4.645972728729248, l1: 0.00010447207605466247, l2: 0.0003601252043154091   Iteration 6 of 100, tot loss = 5.022462765375773, l1: 0.0001061354511572669, l2: 0.0003961108304793015   Iteration 7 of 100, tot loss = 5.128175939832415, l1: 0.00010655144745084857, l2: 0.00040626615269242654   Iteration 8 of 100, tot loss = 5.127901017665863, l1: 0.00010785496397147654, l2: 0.00040493514825357124   Iteration 9 of 100, tot loss = 5.150443924797906, l1: 0.00010864933493495401, l2: 0.0004063950683404174   Iteration 10 of 100, tot loss = 5.010500097274781, l1: 0.00010581576425465755, l2: 0.0003952342551201582   Iteration 11 of 100, tot loss = 4.8594654906879775, l1: 0.00010265364438634028, l2: 0.00038329291485503995   Iteration 12 of 100, tot loss = 4.7585022648175554, l1: 9.934445552062243e-05, l2: 0.0003765057796651187   Iteration 13 of 100, tot loss = 4.802473526734572, l1: 9.94908998389012e-05, l2: 0.0003807564610794473   Iteration 14 of 100, tot loss = 4.728642429624285, l1: 9.80712081530198e-05, l2: 0.0003747930417635611   Iteration 15 of 100, tot loss = 4.7552414258321125, l1: 0.00010042152571259067, l2: 0.00037510262336581945   Iteration 16 of 100, tot loss = 4.790644139051437, l1: 0.00010266325944030541, l2: 0.00037640116170223337   Iteration 17 of 100, tot loss = 4.7747672024895165, l1: 0.00010366984711521688, l2: 0.00037380687974612503   Iteration 18 of 100, tot loss = 4.847834295696682, l1: 0.00010426067213605468, l2: 0.0003805227622958935   Iteration 19 of 100, tot loss = 4.727471790815654, l1: 0.00010213394431813963, l2: 0.00037061324033665617   Iteration 20 of 100, tot loss = 4.7002281785011295, l1: 0.00010183171434618998, l2: 0.0003681911090097856   Iteration 21 of 100, tot loss = 4.77620412054516, l1: 0.00010378329517109142, l2: 0.0003738371216015713   Iteration 22 of 100, tot loss = 4.8204273852435024, l1: 0.0001040079087860332, l2: 0.0003780348349457861   Iteration 23 of 100, tot loss = 4.806781841361004, l1: 0.00010430108807707691, l2: 0.00037637709971497077   Iteration 24 of 100, tot loss = 4.7424420813719435, l1: 0.00010260331570558871, l2: 0.0003716408955369843   Iteration 25 of 100, tot loss = 4.7249265384674075, l1: 0.00010302767565008253, l2: 0.0003694649814860895   Iteration 26 of 100, tot loss = 4.630382281083327, l1: 0.00010063495532309529, l2: 0.0003624032760853879   Iteration 27 of 100, tot loss = 4.655650015230532, l1: 0.00010177875446970575, l2: 0.00036378625049514487   Iteration 28 of 100, tot loss = 4.555561695780073, l1: 9.952731995456685e-05, l2: 0.0003560288527556362   Iteration 29 of 100, tot loss = 4.6321033444897886, l1: 9.978817289610843e-05, l2: 0.0003634221648098901   Iteration 30 of 100, tot loss = 4.603126970926921, l1: 9.973272135539446e-05, l2: 0.00036057997931493446   Iteration 31 of 100, tot loss = 4.559537018499067, l1: 9.892926719922182e-05, l2: 0.0003570244387505696   Iteration 32 of 100, tot loss = 4.559502772986889, l1: 9.832119519614935e-05, l2: 0.0003576290860110021   Iteration 33 of 100, tot loss = 4.515090935158007, l1: 9.683701456665541e-05, l2: 0.0003546720823082565   Iteration 34 of 100, tot loss = 4.459043047007392, l1: 9.631395112271147e-05, l2: 0.00034959035667056655   Iteration 35 of 100, tot loss = 4.445498105457851, l1: 9.640033178064706e-05, l2: 0.00034814948157873004   Iteration 36 of 100, tot loss = 4.486154615879059, l1: 9.78612129579738e-05, l2: 0.00035075425007057167   Iteration 37 of 100, tot loss = 4.52101477416786, l1: 9.884074353141005e-05, l2: 0.00035326073545572425   Iteration 38 of 100, tot loss = 4.498941515621386, l1: 9.894143142063465e-05, l2: 0.00035095272170637096   Iteration 39 of 100, tot loss = 4.511485228171716, l1: 9.984235340454735e-05, l2: 0.00035130617078931   Iteration 40 of 100, tot loss = 4.511621123552322, l1: 9.981674356822623e-05, l2: 0.00035134536992700307   Iteration 41 of 100, tot loss = 4.515269017801052, l1: 0.00010041914871047682, l2: 0.00035110775419658547   Iteration 42 of 100, tot loss = 4.485255054065159, l1: 0.00010031722628073545, l2: 0.00034820828018599146   Iteration 43 of 100, tot loss = 4.473407451496568, l1: 0.00010066339915325908, l2: 0.00034667734736251797   Iteration 44 of 100, tot loss = 4.457975035363978, l1: 0.00010048503810883796, l2: 0.00034531246637925506   Iteration 45 of 100, tot loss = 4.463686005274455, l1: 0.00010055086620721138, l2: 0.00034581773539280725   Iteration 46 of 100, tot loss = 4.470943870751754, l1: 0.00010034441394331542, l2: 0.00034674997352893746   Iteration 47 of 100, tot loss = 4.492548288183009, l1: 0.00010098916236335966, l2: 0.00034826566735183466   Iteration 48 of 100, tot loss = 4.5195616235335665, l1: 0.00010161329873881186, l2: 0.0003503428639911969   Iteration 49 of 100, tot loss = 4.530895607812064, l1: 0.0001010230811590766, l2: 0.0003520664800557175   Iteration 50 of 100, tot loss = 4.541669869422913, l1: 0.0001014288462465629, l2: 0.00035273814050015064   Iteration 51 of 100, tot loss = 4.601766076742434, l1: 0.0001025599254610236, l2: 0.00035761668193884486   Iteration 52 of 100, tot loss = 4.599627334337968, l1: 0.00010196736660033751, l2: 0.00035799536663734424   Iteration 53 of 100, tot loss = 4.585250193218015, l1: 0.00010199207737866157, l2: 0.00035653294191502455   Iteration 54 of 100, tot loss = 4.631077373469317, l1: 0.0001029981067404151, l2: 0.0003601096309635236   Iteration 55 of 100, tot loss = 4.58016301501881, l1: 0.00010185076098423451, l2: 0.0003561655408702791   Iteration 56 of 100, tot loss = 4.549877737249647, l1: 0.00010108979371839919, l2: 0.00035389798046838095   Iteration 57 of 100, tot loss = 4.533752353567826, l1: 0.0001009010869765597, l2: 0.0003524741485690404   Iteration 58 of 100, tot loss = 4.497527961073251, l1: 0.00010004639719634976, l2: 0.000349706399368909   Iteration 59 of 100, tot loss = 4.555589983018778, l1: 0.00010086250854251607, l2: 0.0003546964906459927   Iteration 60 of 100, tot loss = 4.516531538963318, l1: 9.98443581314253e-05, l2: 0.00035180879664646153   Iteration 61 of 100, tot loss = 4.503938096468566, l1: 9.947092925746101e-05, l2: 0.00035092288119619196   Iteration 62 of 100, tot loss = 4.508501552766369, l1: 9.91257263282727e-05, l2: 0.00035172442960996   Iteration 63 of 100, tot loss = 4.47187766574678, l1: 9.836216711500172e-05, l2: 0.0003488256001479125   Iteration 64 of 100, tot loss = 4.4769861325621605, l1: 9.833853079044275e-05, l2: 0.0003493600827368937   Iteration 65 of 100, tot loss = 4.459158354539138, l1: 9.806652272415634e-05, l2: 0.00034784931317867284   Iteration 66 of 100, tot loss = 4.442132545239998, l1: 9.76191617651652e-05, l2: 0.0003465940930199753   Iteration 67 of 100, tot loss = 4.496264785083373, l1: 9.836556669889338e-05, l2: 0.00035126091119200823   Iteration 68 of 100, tot loss = 4.483582479112289, l1: 9.822760196531068e-05, l2: 0.0003501306451011367   Iteration 69 of 100, tot loss = 4.491402283958767, l1: 9.860218905605902e-05, l2: 0.0003505380382589267   Iteration 70 of 100, tot loss = 4.470154922349113, l1: 9.841899087145325e-05, l2: 0.0003485964999916697   Iteration 71 of 100, tot loss = 4.477102699414106, l1: 9.853604800374457e-05, l2: 0.0003491742206445541   Iteration 72 of 100, tot loss = 4.456279625495275, l1: 9.841470086434533e-05, l2: 0.00034721326038480684   Iteration 73 of 100, tot loss = 4.440853572871587, l1: 9.788331834436713e-05, l2: 0.0003462020379143499   Iteration 74 of 100, tot loss = 4.408953325168507, l1: 9.738787412061356e-05, l2: 0.0003435074572366461   Iteration 75 of 100, tot loss = 4.427944653828939, l1: 9.803933625031884e-05, l2: 0.0003447551282200341   Iteration 76 of 100, tot loss = 4.431379299414785, l1: 9.793739139384247e-05, l2: 0.0003452005378341327   Iteration 77 of 100, tot loss = 4.468564200710941, l1: 9.869435702317528e-05, l2: 0.00034816206191732945   Iteration 78 of 100, tot loss = 4.512285049145039, l1: 9.943825586024337e-05, l2: 0.00035179024789846526   Iteration 79 of 100, tot loss = 4.508960102177873, l1: 9.933880753549923e-05, l2: 0.0003515572017839744   Iteration 80 of 100, tot loss = 4.517853319644928, l1: 9.941218859239598e-05, l2: 0.0003523731424138532   Iteration 81 of 100, tot loss = 4.539578231764429, l1: 9.981419143402075e-05, l2: 0.00035414363082963973   Iteration 82 of 100, tot loss = 4.544477864009578, l1: 9.980942781023658e-05, l2: 0.0003546383574380676   Iteration 83 of 100, tot loss = 4.546388700783971, l1: 9.994129452178345e-05, l2: 0.0003546975745541795   Iteration 84 of 100, tot loss = 4.54874126684098, l1: 0.00010024833829361124, l2: 0.0003546257872325701   Iteration 85 of 100, tot loss = 4.576123607859892, l1: 0.00010073920463636407, l2: 0.00035687315514009886   Iteration 86 of 100, tot loss = 4.586302352506061, l1: 0.00010079115777761068, l2: 0.0003578390763344265   Iteration 87 of 100, tot loss = 4.5929218599166, l1: 0.0001010039632928547, l2: 0.00035828822135159895   Iteration 88 of 100, tot loss = 4.606736486608332, l1: 0.00010142249481181021, l2: 0.0003592511521591487   Iteration 89 of 100, tot loss = 4.602950267577439, l1: 0.00010151021330345297, l2: 0.0003587848118053791   Iteration 90 of 100, tot loss = 4.603892130321927, l1: 0.00010173948329692293, l2: 0.00035864972790780787   Iteration 91 of 100, tot loss = 4.598361638876108, l1: 0.00010184101895672259, l2: 0.00035799514316150327   Iteration 92 of 100, tot loss = 4.596676074940225, l1: 0.00010183373809053117, l2: 0.0003578338676205654   Iteration 93 of 100, tot loss = 4.605201218717841, l1: 0.00010215746215669818, l2: 0.0003583626575975789   Iteration 94 of 100, tot loss = 4.614722520747083, l1: 0.00010223885266818701, l2: 0.0003592333977142884   Iteration 95 of 100, tot loss = 4.629211099524247, l1: 0.000102585922056613, l2: 0.0003603351861544836   Iteration 96 of 100, tot loss = 4.660297056039174, l1: 0.00010287557673412569, l2: 0.00036315412717158324   Iteration 97 of 100, tot loss = 4.682997639646235, l1: 0.00010290802775980598, l2: 0.0003653917344318242   Iteration 98 of 100, tot loss = 4.67391536673721, l1: 0.00010247536200188975, l2: 0.00036491617297264273   Iteration 99 of 100, tot loss = 4.665787528259585, l1: 0.00010227832822376072, l2: 0.00036430042287019656   Iteration 100 of 100, tot loss = 4.668750524520874, l1: 0.00010252785716147628, l2: 0.0003643471935356501
   End of epoch 1116; saving model... 

Epoch 1117 of 2000
   Iteration 1 of 100, tot loss = 5.571417331695557, l1: 0.00013950405991636217, l2: 0.00041763766785152256   Iteration 2 of 100, tot loss = 6.024580240249634, l1: 0.00014158177509671077, l2: 0.00046087625378277153   Iteration 3 of 100, tot loss = 4.987173398335774, l1: 0.00012069311439214896, l2: 0.0003780242307887723   Iteration 4 of 100, tot loss = 4.848487496376038, l1: 0.00011587803237489425, l2: 0.0003689707154990174   Iteration 5 of 100, tot loss = 4.960860538482666, l1: 0.00011315228330204264, l2: 0.0003829337714705616   Iteration 6 of 100, tot loss = 5.107412735621135, l1: 0.00011444202027632855, l2: 0.0003962992486776784   Iteration 7 of 100, tot loss = 5.274424689156668, l1: 0.00011416360627793307, l2: 0.0004132788620024387   Iteration 8 of 100, tot loss = 5.188662111759186, l1: 0.00011072166944359196, l2: 0.0004081445404153783   Iteration 9 of 100, tot loss = 5.022409068213569, l1: 0.00010662180688490884, l2: 0.00039561909701054293   Iteration 10 of 100, tot loss = 5.099505424499512, l1: 0.00010709102716646157, l2: 0.00040285951108671726   Iteration 11 of 100, tot loss = 4.892096519470215, l1: 0.00010456261026609519, l2: 0.0003846470380350101   Iteration 12 of 100, tot loss = 4.910077929496765, l1: 0.00010428727728140075, l2: 0.00038672051232424565   Iteration 13 of 100, tot loss = 4.7526436769045315, l1: 0.00010174582600414466, l2: 0.0003735185388135366   Iteration 14 of 100, tot loss = 4.714254975318909, l1: 0.00010093633682117797, l2: 0.0003704891579608167   Iteration 15 of 100, tot loss = 4.829790576299032, l1: 0.0001034036807444257, l2: 0.00037957537278998643   Iteration 16 of 100, tot loss = 4.76838681101799, l1: 0.00010227810253127245, l2: 0.0003745605745280045   Iteration 17 of 100, tot loss = 4.59504368725945, l1: 9.925955900518388e-05, l2: 0.000360244806335472   Iteration 18 of 100, tot loss = 4.586655722724067, l1: 9.710829504506869e-05, l2: 0.0003615572738150756   Iteration 19 of 100, tot loss = 4.57670417584871, l1: 9.564098467021005e-05, l2: 0.00036202943087310384   Iteration 20 of 100, tot loss = 4.701344203948975, l1: 9.74100454186555e-05, l2: 0.000372724374756217   Iteration 21 of 100, tot loss = 4.662591922850836, l1: 9.73330173846556e-05, l2: 0.0003689261738188742   Iteration 22 of 100, tot loss = 4.749311132864519, l1: 9.830210050991313e-05, l2: 0.000376629012118263   Iteration 23 of 100, tot loss = 4.723182771516883, l1: 9.867462864823882e-05, l2: 0.0003736436483450234   Iteration 24 of 100, tot loss = 4.741040974855423, l1: 9.830484062452645e-05, l2: 0.00037579925628961064   Iteration 25 of 100, tot loss = 4.737132921218872, l1: 9.842162631684914e-05, l2: 0.00037529166438616813   Iteration 26 of 100, tot loss = 4.740418443313012, l1: 9.95084967423911e-05, l2: 0.0003745333457258172   Iteration 27 of 100, tot loss = 4.725496618836014, l1: 9.932483172397715e-05, l2: 0.00037322482697803663   Iteration 28 of 100, tot loss = 4.633626878261566, l1: 9.751219580981083e-05, l2: 0.00036585048863863837   Iteration 29 of 100, tot loss = 4.639985059869701, l1: 9.855455297430784e-05, l2: 0.00036544395020584865   Iteration 30 of 100, tot loss = 4.664560532569885, l1: 9.926450584316627e-05, l2: 0.0003671915454712386   Iteration 31 of 100, tot loss = 4.628379621813374, l1: 9.824264082952493e-05, l2: 0.0003645953190918531   Iteration 32 of 100, tot loss = 4.687159985303879, l1: 9.915580403685453e-05, l2: 0.000369560190847551   Iteration 33 of 100, tot loss = 4.705044153964881, l1: 0.00010034885428577775, l2: 0.0003701555577925209   Iteration 34 of 100, tot loss = 4.708986969555125, l1: 0.00010067628874607822, l2: 0.00037022240490734796   Iteration 35 of 100, tot loss = 4.759270204816546, l1: 0.00010144360962190798, l2: 0.00037448340777440793   Iteration 36 of 100, tot loss = 4.771788663334316, l1: 0.00010255212368469479, l2: 0.00037462674016650353   Iteration 37 of 100, tot loss = 4.7026689954706145, l1: 0.00010153235485336768, l2: 0.00036873454204445854   Iteration 38 of 100, tot loss = 4.695760532429344, l1: 0.00010202126251993162, l2: 0.0003675547888729182   Iteration 39 of 100, tot loss = 4.690299541522295, l1: 0.00010278103078375212, l2: 0.00036624892135688034   Iteration 40 of 100, tot loss = 4.748696058988571, l1: 0.00010368177918280707, l2: 0.00037118782456673215   Iteration 41 of 100, tot loss = 4.720554392512252, l1: 0.00010282793877328315, l2: 0.00036922749806275   Iteration 42 of 100, tot loss = 4.695857093447731, l1: 0.00010243688822291526, l2: 0.00036714881848421923   Iteration 43 of 100, tot loss = 4.672613997792089, l1: 0.00010262056559102263, l2: 0.0003646408315331047   Iteration 44 of 100, tot loss = 4.710960702462629, l1: 0.00010331227448742456, l2: 0.00036778379415574653   Iteration 45 of 100, tot loss = 4.680476421780057, l1: 0.00010257649248362416, l2: 0.00036547114820374796   Iteration 46 of 100, tot loss = 4.650258074636045, l1: 0.00010181210192335446, l2: 0.00036321370451174596   Iteration 47 of 100, tot loss = 4.667668352735803, l1: 0.00010255879061197505, l2: 0.0003642080431591046   Iteration 48 of 100, tot loss = 4.633293017745018, l1: 0.00010197693942852008, l2: 0.0003613523610207873   Iteration 49 of 100, tot loss = 4.633469985455883, l1: 0.00010172957738585846, l2: 0.0003616174199494856   Iteration 50 of 100, tot loss = 4.66222050189972, l1: 0.00010193064357736148, l2: 0.0003642914054216817   Iteration 51 of 100, tot loss = 4.70379319377974, l1: 0.00010251965874226252, l2: 0.00036785965979409713   Iteration 52 of 100, tot loss = 4.660715584571545, l1: 0.00010194582710727655, l2: 0.00036412573004222044   Iteration 53 of 100, tot loss = 4.655864828037766, l1: 0.00010225563246936906, l2: 0.00036333084907075216   Iteration 54 of 100, tot loss = 4.653312546235544, l1: 0.00010262380197269117, l2: 0.0003627074515356475   Iteration 55 of 100, tot loss = 4.648528302799572, l1: 0.00010245394197144462, l2: 0.0003623988868308846   Iteration 56 of 100, tot loss = 4.67026178751673, l1: 0.0001032492038055872, l2: 0.0003637769729851113   Iteration 57 of 100, tot loss = 4.672232841190539, l1: 0.00010356640102639958, l2: 0.00036365688107540146   Iteration 58 of 100, tot loss = 4.657662868499756, l1: 0.00010343322732108485, l2: 0.0003623330575747191   Iteration 59 of 100, tot loss = 4.630070589356503, l1: 0.00010315626719078618, l2: 0.00035985078962551335   Iteration 60 of 100, tot loss = 4.663767790794372, l1: 0.00010328847844599901, l2: 0.000363088299248678   Iteration 61 of 100, tot loss = 4.712129084790339, l1: 0.00010383363894747997, l2: 0.00036737926735649586   Iteration 62 of 100, tot loss = 4.689682045290547, l1: 0.00010335701881558634, l2: 0.0003656111835997792   Iteration 63 of 100, tot loss = 4.680609937698122, l1: 0.00010316871339455247, l2: 0.0003648922785133537   Iteration 64 of 100, tot loss = 4.663107872009277, l1: 0.00010276842556322663, l2: 0.00036354235953695024   Iteration 65 of 100, tot loss = 4.663163031064547, l1: 0.00010253412384862224, l2: 0.00036378217673000806   Iteration 66 of 100, tot loss = 4.646573846990412, l1: 0.00010234967086227809, l2: 0.000362307711232764   Iteration 67 of 100, tot loss = 4.634311619089611, l1: 0.00010229475392678531, l2: 0.0003611364053499732   Iteration 68 of 100, tot loss = 4.638124094289892, l1: 0.00010264479640361114, l2: 0.0003611676100477138   Iteration 69 of 100, tot loss = 4.619665529416955, l1: 0.00010237816478247228, l2: 0.00035958838534365964   Iteration 70 of 100, tot loss = 4.600582429340908, l1: 0.00010174707624953174, l2: 0.00035831116422611686   Iteration 71 of 100, tot loss = 4.57618699275272, l1: 0.0001013695091131294, l2: 0.000356249187820473   Iteration 72 of 100, tot loss = 4.565792169835833, l1: 0.00010120106450510018, l2: 0.00035537814983399585   Iteration 73 of 100, tot loss = 4.600545830922584, l1: 0.0001015378523969621, l2: 0.00035851672833964025   Iteration 74 of 100, tot loss = 4.573902877601418, l1: 0.00010077446505248018, l2: 0.0003566158205078839   Iteration 75 of 100, tot loss = 4.567363777160645, l1: 0.00010046936290261026, l2: 0.00035626701292737077   Iteration 76 of 100, tot loss = 4.600883076065465, l1: 0.00010095714117740404, l2: 0.00035913116478600147   Iteration 77 of 100, tot loss = 4.5934605660376615, l1: 0.0001009257078282656, l2: 0.0003584203472460753   Iteration 78 of 100, tot loss = 4.563726831705142, l1: 0.00010056983629534117, l2: 0.00035580284524117957   Iteration 79 of 100, tot loss = 4.535578477231762, l1: 9.985118074699222e-05, l2: 0.000353706665366558   Iteration 80 of 100, tot loss = 4.576755157113075, l1: 0.00010042053299912368, l2: 0.0003572549809177872   Iteration 81 of 100, tot loss = 4.5614784911826805, l1: 0.00010025893398388116, l2: 0.0003558889133453277   Iteration 82 of 100, tot loss = 4.601184199496013, l1: 0.00010075532084489923, l2: 0.0003593630972318351   Iteration 83 of 100, tot loss = 4.60928969210889, l1: 0.00010094943052053968, l2: 0.0003599795366985252   Iteration 84 of 100, tot loss = 4.579018022332873, l1: 0.00010043606748478764, l2: 0.000357465732598766   Iteration 85 of 100, tot loss = 4.63023803654839, l1: 0.00010113105462925673, l2: 0.0003618927460218615   Iteration 86 of 100, tot loss = 4.633898260981538, l1: 0.00010083928760269733, l2: 0.00036255053520679127   Iteration 87 of 100, tot loss = 4.627643401595368, l1: 0.00010051427123199709, l2: 0.0003622500654095891   Iteration 88 of 100, tot loss = 4.614668369293213, l1: 0.0001001769354099286, l2: 0.00036128989813732915   Iteration 89 of 100, tot loss = 4.622834355643626, l1: 0.000100148431709876, l2: 0.0003621350006550927   Iteration 90 of 100, tot loss = 4.658954879972669, l1: 0.00010047610837015479, l2: 0.00036541937636987615   Iteration 91 of 100, tot loss = 4.638734560746413, l1: 0.00010009860960673285, l2: 0.0003637748434282532   Iteration 92 of 100, tot loss = 4.6295888890390815, l1: 0.00010019133930219813, l2: 0.00036276754642882304   Iteration 93 of 100, tot loss = 4.632759786421253, l1: 0.00010038670112525353, l2: 0.00036288927398717174   Iteration 94 of 100, tot loss = 4.618816908369673, l1: 0.00010029517521770879, l2: 0.0003615865120701393   Iteration 95 of 100, tot loss = 4.60183421687076, l1: 9.989587465258267e-05, l2: 0.00036028754364356006   Iteration 96 of 100, tot loss = 4.607240175207456, l1: 0.0001000597045504037, l2: 0.00036066430948267225   Iteration 97 of 100, tot loss = 4.608473168205969, l1: 9.997951936609227e-05, l2: 0.0003608677940702392   Iteration 98 of 100, tot loss = 4.61710033124807, l1: 0.00010026005486220036, l2: 0.000361449974921665   Iteration 99 of 100, tot loss = 4.601619956469295, l1: 0.00010004332003206005, l2: 0.0003601186722105444   Iteration 100 of 100, tot loss = 4.594696071147919, l1: 0.00010015660507633583, l2: 0.00035931299862568267
   End of epoch 1117; saving model... 

Epoch 1118 of 2000
   Iteration 1 of 100, tot loss = 4.436100006103516, l1: 0.00011570670176297426, l2: 0.0003279032825957984   Iteration 2 of 100, tot loss = 4.532426357269287, l1: 9.469663200434297e-05, l2: 0.0003585459926398471   Iteration 3 of 100, tot loss = 4.355534394582112, l1: 0.00010003618808696046, l2: 0.00033551724239562947   Iteration 4 of 100, tot loss = 4.953211426734924, l1: 0.00010767200183181558, l2: 0.00038764912460464984   Iteration 5 of 100, tot loss = 5.096461486816406, l1: 0.00010904493829002603, l2: 0.0004006011935416609   Iteration 6 of 100, tot loss = 4.72173269589742, l1: 9.9590675138946e-05, l2: 0.0003725825784689126   Iteration 7 of 100, tot loss = 4.291129759379795, l1: 9.229227050257447e-05, l2: 0.0003368206920900515   Iteration 8 of 100, tot loss = 4.2743410766124725, l1: 9.550856384521467e-05, l2: 0.00033192553019034676   Iteration 9 of 100, tot loss = 4.456205659442478, l1: 0.00010079807584083432, l2: 0.00034482247752344445   Iteration 10 of 100, tot loss = 4.365213465690613, l1: 9.677945199655369e-05, l2: 0.00033974188263528047   Iteration 11 of 100, tot loss = 4.353539445183494, l1: 9.870070128024302e-05, l2: 0.00033665323545309633   Iteration 12 of 100, tot loss = 4.467535038789113, l1: 9.781445199526691e-05, l2: 0.00034893904618608457   Iteration 13 of 100, tot loss = 4.48716924740718, l1: 9.912466003487889e-05, l2: 0.0003495922607656282   Iteration 14 of 100, tot loss = 4.468456489699228, l1: 9.822036320526552e-05, l2: 0.0003486252826405689   Iteration 15 of 100, tot loss = 4.471350113550822, l1: 9.753980654447029e-05, l2: 0.0003495952017450084   Iteration 16 of 100, tot loss = 4.469234272837639, l1: 9.920114553096937e-05, l2: 0.00034772227809298784   Iteration 17 of 100, tot loss = 4.461744154200835, l1: 0.00010009628878084614, l2: 0.0003460781213909607   Iteration 18 of 100, tot loss = 4.520172900623745, l1: 0.00010177233343711123, l2: 0.00035024495122747286   Iteration 19 of 100, tot loss = 4.595870683067723, l1: 0.00010211771089081212, l2: 0.0003574693508103098   Iteration 20 of 100, tot loss = 4.69965945482254, l1: 0.0001034204618918011, l2: 0.000366545477299951   Iteration 21 of 100, tot loss = 4.921455667132423, l1: 0.00010598940432481911, l2: 0.00038615615561693196   Iteration 22 of 100, tot loss = 4.857392701235685, l1: 0.0001049859879871788, l2: 0.0003807532749223438   Iteration 23 of 100, tot loss = 4.900828216386878, l1: 0.0001053832700405189, l2: 0.0003846995456589629   Iteration 24 of 100, tot loss = 4.9114306171735125, l1: 0.00010531989755691029, l2: 0.0003858231599830712   Iteration 25 of 100, tot loss = 4.772390236854553, l1: 0.00010282924224156886, l2: 0.00037440977757796645   Iteration 26 of 100, tot loss = 4.812666310713841, l1: 0.00010420363553119107, l2: 0.0003770629924614556   Iteration 27 of 100, tot loss = 5.0000499133710505, l1: 0.00010553745313599292, l2: 0.00039446753604958457   Iteration 28 of 100, tot loss = 5.07822260260582, l1: 0.00010623730953998997, l2: 0.0004015849491614582   Iteration 29 of 100, tot loss = 5.08191533335324, l1: 0.00010645298883399188, l2: 0.000401738543336376   Iteration 30 of 100, tot loss = 4.98167579571406, l1: 0.00010466589325612101, l2: 0.000393501685175579   Iteration 31 of 100, tot loss = 4.912432189910643, l1: 0.00010317402038415293, l2: 0.0003880691970886302   Iteration 32 of 100, tot loss = 4.972681920975447, l1: 0.0001037363258546975, l2: 0.00039353186411972274   Iteration 33 of 100, tot loss = 5.00188993323933, l1: 0.00010385969014323288, l2: 0.00039632930146939486   Iteration 34 of 100, tot loss = 4.992753712569966, l1: 0.00010360237266628675, l2: 0.000395672998017878   Iteration 35 of 100, tot loss = 4.935526319912502, l1: 0.00010288419393223843, l2: 0.0003906684374669567   Iteration 36 of 100, tot loss = 4.916053271955914, l1: 0.00010277054530484343, l2: 0.0003888347819156479   Iteration 37 of 100, tot loss = 4.869481418583844, l1: 0.00010143921994116485, l2: 0.0003855089225170731   Iteration 38 of 100, tot loss = 4.870813153292003, l1: 0.00010163364899390696, l2: 0.0003854476668302117   Iteration 39 of 100, tot loss = 4.814658822157444, l1: 0.00010101574606289013, l2: 0.00038045013692074764   Iteration 40 of 100, tot loss = 4.782048389315605, l1: 0.00010026917307186523, l2: 0.0003779356669838307   Iteration 41 of 100, tot loss = 4.762678032968102, l1: 9.978276107065584e-05, l2: 0.00037648504359999717   Iteration 42 of 100, tot loss = 4.715744265488216, l1: 9.952534821336268e-05, l2: 0.0003720490796175519   Iteration 43 of 100, tot loss = 4.736306420592374, l1: 9.966348468794401e-05, l2: 0.0003739671580639646   Iteration 44 of 100, tot loss = 4.720659773458134, l1: 9.962899069002809e-05, l2: 0.00037243698732874526   Iteration 45 of 100, tot loss = 4.692846851878696, l1: 9.913745703266209e-05, l2: 0.000370147228305642   Iteration 46 of 100, tot loss = 4.673302523467852, l1: 9.913688597368056e-05, l2: 0.0003681933661067413   Iteration 47 of 100, tot loss = 4.680393236748715, l1: 9.893709268889073e-05, l2: 0.0003691022306002003   Iteration 48 of 100, tot loss = 4.665610618889332, l1: 9.900944792207156e-05, l2: 0.00036755161363544175   Iteration 49 of 100, tot loss = 4.6345179299918975, l1: 9.861770399302073e-05, l2: 0.0003648340884401292   Iteration 50 of 100, tot loss = 4.617036035060883, l1: 9.880501173029188e-05, l2: 0.00036289859126554803   Iteration 51 of 100, tot loss = 4.612610066638274, l1: 9.856686800976247e-05, l2: 0.0003626941381976483   Iteration 52 of 100, tot loss = 4.563909106529676, l1: 9.795849592382392e-05, l2: 0.0003584324141019561   Iteration 53 of 100, tot loss = 4.584573221656512, l1: 9.748289853890146e-05, l2: 0.00036097442284270347   Iteration 54 of 100, tot loss = 4.573336802147053, l1: 9.716406303697213e-05, l2: 0.00036016961638341417   Iteration 55 of 100, tot loss = 4.5811257600784305, l1: 9.77442861106035e-05, l2: 0.00036036828929685396   Iteration 56 of 100, tot loss = 4.617127808076995, l1: 9.883739656808237e-05, l2: 0.0003628753836112862   Iteration 57 of 100, tot loss = 4.60292809260519, l1: 9.805368915151216e-05, l2: 0.0003622391196314997   Iteration 58 of 100, tot loss = 4.579530894756317, l1: 9.755413037933537e-05, l2: 0.00036039895850739536   Iteration 59 of 100, tot loss = 4.53867820763992, l1: 9.70364011571569e-05, l2: 0.00035683141890084527   Iteration 60 of 100, tot loss = 4.573295289278031, l1: 9.737866845777413e-05, l2: 0.0003599508595167814   Iteration 61 of 100, tot loss = 4.568653933337478, l1: 9.770961332833394e-05, l2: 0.00035915577895824844   Iteration 62 of 100, tot loss = 4.525637009451466, l1: 9.71167549235566e-05, l2: 0.0003554469451867795   Iteration 63 of 100, tot loss = 4.56298976285117, l1: 9.776693944346219e-05, l2: 0.0003585320361982292   Iteration 64 of 100, tot loss = 4.615848718211055, l1: 9.868562779047352e-05, l2: 0.0003628992428730271   Iteration 65 of 100, tot loss = 4.631134163416349, l1: 9.900642348489222e-05, l2: 0.0003641069909253229   Iteration 66 of 100, tot loss = 4.643524637728026, l1: 9.925281770075813e-05, l2: 0.00036509964367283055   Iteration 67 of 100, tot loss = 4.630587394557782, l1: 9.889713594509261e-05, l2: 0.0003641616010520635   Iteration 68 of 100, tot loss = 4.648196860271342, l1: 9.932032544919126e-05, l2: 0.00036549935804727925   Iteration 69 of 100, tot loss = 4.673681281614995, l1: 9.947512191264768e-05, l2: 0.0003678930037152157   Iteration 70 of 100, tot loss = 4.666529902390072, l1: 9.97848115470593e-05, l2: 0.00036686817620648073   Iteration 71 of 100, tot loss = 4.678570628166199, l1: 0.00010039039707536035, l2: 0.0003674666630514097   Iteration 72 of 100, tot loss = 4.704846403665012, l1: 0.000100845014357118, l2: 0.00036963962358842965   Iteration 73 of 100, tot loss = 4.698943335716039, l1: 0.00010065515898407297, l2: 0.0003692391725202818   Iteration 74 of 100, tot loss = 4.704838677032574, l1: 0.0001008871456449285, l2: 0.0003695967200139814   Iteration 75 of 100, tot loss = 4.676079265276591, l1: 0.00010056447422054286, l2: 0.00036704345043593394   Iteration 76 of 100, tot loss = 4.683616486034896, l1: 0.00010092355727178266, l2: 0.0003674380892143266   Iteration 77 of 100, tot loss = 4.67539411241358, l1: 0.0001007836803796931, l2: 0.0003667557285115873   Iteration 78 of 100, tot loss = 4.6606629093488054, l1: 0.00010087123537698809, l2: 0.0003651950531863034   Iteration 79 of 100, tot loss = 4.648358802252178, l1: 0.00010046780710270676, l2: 0.0003643680706139819   Iteration 80 of 100, tot loss = 4.6579918071627615, l1: 0.00010036740904979524, l2: 0.0003654317692053155   Iteration 81 of 100, tot loss = 4.633267218683973, l1: 9.96751654900346e-05, l2: 0.0003636515538357658   Iteration 82 of 100, tot loss = 4.671148804629722, l1: 0.00010010767800849862, l2: 0.0003670071998452086   Iteration 83 of 100, tot loss = 4.65177111309695, l1: 9.984495470662187e-05, l2: 0.00036533215417700586   Iteration 84 of 100, tot loss = 4.65314631093116, l1: 0.00010009346511824766, l2: 0.000365221163519891   Iteration 85 of 100, tot loss = 4.6326262011247525, l1: 9.971343610873994e-05, l2: 0.0003635491814587594   Iteration 86 of 100, tot loss = 4.631657743176748, l1: 9.986551146518418e-05, l2: 0.00036330026020106403   Iteration 87 of 100, tot loss = 4.657421160018307, l1: 0.00010056665055815066, l2: 0.00036517546285377367   Iteration 88 of 100, tot loss = 4.629770583727143, l1: 9.994066017911378e-05, l2: 0.00036303639568690085   Iteration 89 of 100, tot loss = 4.620664110344447, l1: 9.99568014623456e-05, l2: 0.0003621096072360099   Iteration 90 of 100, tot loss = 4.6139195349481374, l1: 9.948430565095299e-05, l2: 0.0003619076456137312   Iteration 91 of 100, tot loss = 4.6008685905854785, l1: 9.961807590226162e-05, l2: 0.00036046878101462804   Iteration 92 of 100, tot loss = 4.587223585533059, l1: 9.920551952824968e-05, l2: 0.0003595168367926659   Iteration 93 of 100, tot loss = 4.586822354665366, l1: 9.891905633154355e-05, l2: 0.000359763177105486   Iteration 94 of 100, tot loss = 4.585352267356629, l1: 9.894240576342898e-05, l2: 0.00035959281918389644   Iteration 95 of 100, tot loss = 4.570523448994285, l1: 9.867713624292886e-05, l2: 0.00035837520661420727   Iteration 96 of 100, tot loss = 4.570090036839247, l1: 9.834739974697489e-05, l2: 0.0003586616018462034   Iteration 97 of 100, tot loss = 4.568581744567635, l1: 9.82469105438963e-05, l2: 0.0003586112618801597   Iteration 98 of 100, tot loss = 4.573414609140279, l1: 9.814020101045144e-05, l2: 0.00035920125766115605   Iteration 99 of 100, tot loss = 4.611886960087401, l1: 9.879665903457807e-05, l2: 0.00036239203487786303   Iteration 100 of 100, tot loss = 4.622523227930069, l1: 9.928584979206789e-05, l2: 0.0003629664710024372
   End of epoch 1118; saving model... 

Epoch 1119 of 2000
   Iteration 1 of 100, tot loss = 4.319756507873535, l1: 8.176064147846773e-05, l2: 0.00035021500661969185   Iteration 2 of 100, tot loss = 4.26698637008667, l1: 8.690751565154642e-05, l2: 0.00033979110594373196   Iteration 3 of 100, tot loss = 5.0896345774332685, l1: 0.00010096467546342562, l2: 0.00040799877024255693   Iteration 4 of 100, tot loss = 4.609096169471741, l1: 9.668664097262081e-05, l2: 0.00036422297125682235   Iteration 5 of 100, tot loss = 4.95189905166626, l1: 0.0001046071105520241, l2: 0.00039058278780430556   Iteration 6 of 100, tot loss = 5.078741232554118, l1: 0.00010680283715676826, l2: 0.00040107127764107037   Iteration 7 of 100, tot loss = 5.015599250793457, l1: 0.00010933848742362378, l2: 0.0003922214299174292   Iteration 8 of 100, tot loss = 4.970544278621674, l1: 0.00011138329591631191, l2: 0.0003856711227854248   Iteration 9 of 100, tot loss = 4.650762425528632, l1: 0.00010656950811001782, l2: 0.0003585067251050431   Iteration 10 of 100, tot loss = 4.638692831993103, l1: 0.00010430717084091157, l2: 0.0003595621048589237   Iteration 11 of 100, tot loss = 4.756475296887484, l1: 0.00010780945012811571, l2: 0.0003678380731302737   Iteration 12 of 100, tot loss = 4.622225224971771, l1: 0.00010582451795926318, l2: 0.0003563979977722435   Iteration 13 of 100, tot loss = 4.588330764036912, l1: 0.00010617026642788775, l2: 0.00035266280107092683   Iteration 14 of 100, tot loss = 4.690897107124329, l1: 0.00010861035012307443, l2: 0.0003604793507422853   Iteration 15 of 100, tot loss = 4.773726065953572, l1: 0.00010779972896367933, l2: 0.00036957286938559263   Iteration 16 of 100, tot loss = 4.7999497205019, l1: 0.00010883162076424924, l2: 0.0003711633426064509   Iteration 17 of 100, tot loss = 4.662739459206076, l1: 0.00010682803044533905, l2: 0.00035944590704542966   Iteration 18 of 100, tot loss = 4.589941581090291, l1: 0.00010553434387677246, l2: 0.0003534598064207886   Iteration 19 of 100, tot loss = 4.678620639600251, l1: 0.00010697001753693545, l2: 0.00036089203862693946   Iteration 20 of 100, tot loss = 4.696392011642456, l1: 0.00010818887240020559, l2: 0.00036145032281638124   Iteration 21 of 100, tot loss = 4.663595244998024, l1: 0.00010734037225899686, l2: 0.0003590191474705491   Iteration 22 of 100, tot loss = 4.660944765264338, l1: 0.00010674131342305124, l2: 0.00035935315761228344   Iteration 23 of 100, tot loss = 4.604029624358468, l1: 0.00010489972824554728, l2: 0.0003555032289232654   Iteration 24 of 100, tot loss = 4.624656528234482, l1: 0.00010538254688678232, l2: 0.0003570831013348652   Iteration 25 of 100, tot loss = 4.586870784759522, l1: 0.00010529791266890243, l2: 0.0003533891617553309   Iteration 26 of 100, tot loss = 4.644482704309317, l1: 0.0001054579146372047, l2: 0.00035899035160232766   Iteration 27 of 100, tot loss = 4.638324490299931, l1: 0.00010479964678072267, l2: 0.0003590327983989415   Iteration 28 of 100, tot loss = 4.68817891393389, l1: 0.00010565236568384404, l2: 0.0003631655225555213   Iteration 29 of 100, tot loss = 4.730996345651561, l1: 0.00010645642960925812, l2: 0.0003666432011382783   Iteration 30 of 100, tot loss = 4.653888972600301, l1: 0.00010492431974853388, l2: 0.00036046457389602435   Iteration 31 of 100, tot loss = 4.63392545330909, l1: 0.00010475549906034083, l2: 0.0003586370425651811   Iteration 32 of 100, tot loss = 4.612829387187958, l1: 0.00010398692938906606, l2: 0.0003572960063138453   Iteration 33 of 100, tot loss = 4.543547926527081, l1: 0.0001028625444054011, l2: 0.00035149224541700363   Iteration 34 of 100, tot loss = 4.525259186239803, l1: 0.00010227138409391046, l2: 0.0003502545317614396   Iteration 35 of 100, tot loss = 4.4953375339508055, l1: 0.00010175955727130973, l2: 0.0003477741935057566   Iteration 36 of 100, tot loss = 4.580000062783559, l1: 0.00010292314032590689, l2: 0.00035507686334312893   Iteration 37 of 100, tot loss = 4.56072025685697, l1: 0.00010260976371166573, l2: 0.0003534622598168868   Iteration 38 of 100, tot loss = 4.563781255169919, l1: 0.00010275222551794478, l2: 0.0003536258978113581   Iteration 39 of 100, tot loss = 4.561691082440889, l1: 0.00010337173443986103, l2: 0.00035279737195919437   Iteration 40 of 100, tot loss = 4.630756276845932, l1: 0.00010470147590240231, l2: 0.0003583741508919047   Iteration 41 of 100, tot loss = 4.667564886372264, l1: 0.00010558228855299559, l2: 0.00036117419914240246   Iteration 42 of 100, tot loss = 4.694222296987261, l1: 0.00010585666210058012, l2: 0.00036356556665850803   Iteration 43 of 100, tot loss = 4.66629118143126, l1: 0.00010489953477422946, l2: 0.0003617295821408503   Iteration 44 of 100, tot loss = 4.636974535205147, l1: 0.00010373320980182722, l2: 0.00035996424247224985   Iteration 45 of 100, tot loss = 4.623241567611695, l1: 0.000103544754609983, l2: 0.0003587794008328476   Iteration 46 of 100, tot loss = 4.640766034955564, l1: 0.0001037880932056831, l2: 0.000360288508881754   Iteration 47 of 100, tot loss = 4.676724002716389, l1: 0.00010386262921259758, l2: 0.00036380976962391287   Iteration 48 of 100, tot loss = 4.661097625891368, l1: 0.00010354871195280187, l2: 0.0003625610494661184   Iteration 49 of 100, tot loss = 4.668556845917994, l1: 0.0001035124556178094, l2: 0.00036334322809422273   Iteration 50 of 100, tot loss = 4.66208646774292, l1: 0.00010340720793465152, l2: 0.0003628014380228706   Iteration 51 of 100, tot loss = 4.681979141983331, l1: 0.00010380808859789635, l2: 0.0003643898255141525   Iteration 52 of 100, tot loss = 4.711770708744343, l1: 0.00010401231576487099, l2: 0.0003671647553766469   Iteration 53 of 100, tot loss = 4.691448895436413, l1: 0.00010366082045399005, l2: 0.00036548406905977863   Iteration 54 of 100, tot loss = 4.732211978347213, l1: 0.00010441181587305403, l2: 0.00036880938272655905   Iteration 55 of 100, tot loss = 4.759682698683306, l1: 0.0001049235211791132, l2: 0.00037104474957397375   Iteration 56 of 100, tot loss = 4.771580287388393, l1: 0.0001051193613810548, l2: 0.00037203866798206164   Iteration 57 of 100, tot loss = 4.743476859310217, l1: 0.0001045013943854556, l2: 0.0003698462921117122   Iteration 58 of 100, tot loss = 4.781556762498001, l1: 0.0001051284230799928, l2: 0.0003730272535527735   Iteration 59 of 100, tot loss = 4.78500984482846, l1: 0.00010508461226413216, l2: 0.0003734163728333473   Iteration 60 of 100, tot loss = 4.7573864459991455, l1: 0.00010450724488085447, l2: 0.00037123140039814945   Iteration 61 of 100, tot loss = 4.762779024780774, l1: 0.00010488994494004206, l2: 0.0003713879585895901   Iteration 62 of 100, tot loss = 4.760288007797733, l1: 0.00010515682644777811, l2: 0.0003708719753288484   Iteration 63 of 100, tot loss = 4.764559912303137, l1: 0.00010542913249024884, l2: 0.00037102685896642804   Iteration 64 of 100, tot loss = 4.808000311255455, l1: 0.00010616447150368913, l2: 0.0003746355594103079   Iteration 65 of 100, tot loss = 4.82375829403217, l1: 0.00010640331971584461, l2: 0.0003759725093214701   Iteration 66 of 100, tot loss = 4.835283113248421, l1: 0.00010647126019290282, l2: 0.0003770570511926163   Iteration 67 of 100, tot loss = 4.8297418053470444, l1: 0.00010635873766306704, l2: 0.0003766154428620809   Iteration 68 of 100, tot loss = 4.808093887918136, l1: 0.00010620455310238278, l2: 0.00037460483551566377   Iteration 69 of 100, tot loss = 4.787905022717904, l1: 0.00010599044269284882, l2: 0.00037280005926731974   Iteration 70 of 100, tot loss = 4.788051659720285, l1: 0.00010591928657959216, l2: 0.00037288587918737903   Iteration 71 of 100, tot loss = 4.762128296032758, l1: 0.00010549060653560651, l2: 0.00037072222285517033   Iteration 72 of 100, tot loss = 4.734711087412304, l1: 0.0001050595760211258, l2: 0.0003684115324884058   Iteration 73 of 100, tot loss = 4.735820688613473, l1: 0.00010519014216548036, l2: 0.00036839192612766455   Iteration 74 of 100, tot loss = 4.725746573628606, l1: 0.00010513079374701388, l2: 0.0003674438632662861   Iteration 75 of 100, tot loss = 4.742526048024495, l1: 0.000105326357006561, l2: 0.0003689262478534753   Iteration 76 of 100, tot loss = 4.781331369751378, l1: 0.00010592014443414213, l2: 0.0003722129919256794   Iteration 77 of 100, tot loss = 4.776836599622454, l1: 0.00010592610998209203, l2: 0.00037175754947714847   Iteration 78 of 100, tot loss = 4.775733061325856, l1: 0.00010595984536055953, l2: 0.00037161346009293664   Iteration 79 of 100, tot loss = 4.746305317818364, l1: 0.0001055208557761345, l2: 0.0003691096753294615   Iteration 80 of 100, tot loss = 4.731519770622254, l1: 0.00010513345805520657, l2: 0.0003680185185658047   Iteration 81 of 100, tot loss = 4.7161778667826715, l1: 0.00010484433616152218, l2: 0.0003667734498675499   Iteration 82 of 100, tot loss = 4.70511775772746, l1: 0.00010448035710596298, l2: 0.0003660314179752476   Iteration 83 of 100, tot loss = 4.716355366879199, l1: 0.00010462792966155762, l2: 0.00036700760646925186   Iteration 84 of 100, tot loss = 4.724344114462535, l1: 0.00010494837105008108, l2: 0.00036748604006355717   Iteration 85 of 100, tot loss = 4.710844578462488, l1: 0.00010475912294008166, l2: 0.0003663253343855853   Iteration 86 of 100, tot loss = 4.711980281874191, l1: 0.00010480837531653125, l2: 0.00036638965227825263   Iteration 87 of 100, tot loss = 4.715007792944196, l1: 0.00010500133133576326, l2: 0.00036649944745111224   Iteration 88 of 100, tot loss = 4.732659491625699, l1: 0.00010526498482993867, l2: 0.0003680009639620866   Iteration 89 of 100, tot loss = 4.70321817076608, l1: 0.00010474987169125986, l2: 0.0003655719449168092   Iteration 90 of 100, tot loss = 4.729215415318807, l1: 0.00010521214329249536, l2: 0.0003677093980109526   Iteration 91 of 100, tot loss = 4.700353803215446, l1: 0.00010469631400825405, l2: 0.00036533906621536095   Iteration 92 of 100, tot loss = 4.704978950645613, l1: 0.0001044491366132127, l2: 0.0003660487581446828   Iteration 93 of 100, tot loss = 4.6951728302945375, l1: 0.00010418565353154585, l2: 0.0003653316289469379   Iteration 94 of 100, tot loss = 4.702558864938452, l1: 0.000104245992259437, l2: 0.0003660098936574712   Iteration 95 of 100, tot loss = 4.680570225966604, l1: 0.00010396099092567486, l2: 0.00036409603107083395   Iteration 96 of 100, tot loss = 4.672488197684288, l1: 0.00010369243640676966, l2: 0.0003635563828841744   Iteration 97 of 100, tot loss = 4.661116514009299, l1: 0.00010314627164091493, l2: 0.00036296537944077294   Iteration 98 of 100, tot loss = 4.69254981498329, l1: 0.00010349400631361166, l2: 0.00036576097475944506   Iteration 99 of 100, tot loss = 4.675534905809345, l1: 0.00010321141650425177, l2: 0.00036434207368533906   Iteration 100 of 100, tot loss = 4.692839987277985, l1: 0.00010338422973291017, l2: 0.0003658997682214249
   End of epoch 1119; saving model... 

Epoch 1120 of 2000
   Iteration 1 of 100, tot loss = 6.926729202270508, l1: 0.00012926974159199744, l2: 0.0005634031840600073   Iteration 2 of 100, tot loss = 6.384563446044922, l1: 0.00012507112478488125, l2: 0.000513385224621743   Iteration 3 of 100, tot loss = 5.876390775044759, l1: 0.00012374418050361177, l2: 0.00046389489822710556   Iteration 4 of 100, tot loss = 5.649027347564697, l1: 0.000122183888379368, l2: 0.0004427188541740179   Iteration 5 of 100, tot loss = 5.3336989402771, l1: 0.00011638241994660348, l2: 0.00041698747663758694   Iteration 6 of 100, tot loss = 5.275016705195109, l1: 0.00011167538104928099, l2: 0.00041582629395027954   Iteration 7 of 100, tot loss = 5.175769601549421, l1: 0.0001117303327191621, l2: 0.00040584663344946293   Iteration 8 of 100, tot loss = 5.0485135316848755, l1: 0.00010981508967233822, l2: 0.0003950362661271356   Iteration 9 of 100, tot loss = 4.762253363927205, l1: 0.0001024237086918826, l2: 0.0003738016304042604   Iteration 10 of 100, tot loss = 4.964634490013123, l1: 0.00010604847484501079, l2: 0.00039041497511789204   Iteration 11 of 100, tot loss = 4.993460113351995, l1: 0.00010741184285672551, l2: 0.0003919341728429903   Iteration 12 of 100, tot loss = 5.053965349992116, l1: 0.00011009665953073029, l2: 0.0003952998837727743   Iteration 13 of 100, tot loss = 5.027211244289692, l1: 0.00010894605657085776, l2: 0.0003937750768203002   Iteration 14 of 100, tot loss = 4.927818502698626, l1: 0.00010795934420977054, l2: 0.00038482251485610116   Iteration 15 of 100, tot loss = 4.84593653678894, l1: 0.00010711914850010847, l2: 0.0003774745137585948   Iteration 16 of 100, tot loss = 4.85274301469326, l1: 0.00010666929665603675, l2: 0.00037860501470277086   Iteration 17 of 100, tot loss = 4.872188974829281, l1: 0.00010643452611368368, l2: 0.0003807843792909647   Iteration 18 of 100, tot loss = 4.813929478327434, l1: 0.0001060168680042908, l2: 0.0003753760878074293   Iteration 19 of 100, tot loss = 4.692484667426662, l1: 0.00010483322043192426, l2: 0.0003644152541375278   Iteration 20 of 100, tot loss = 4.694094979763031, l1: 0.0001042243660776876, l2: 0.0003651851395261474   Iteration 21 of 100, tot loss = 4.632416611626034, l1: 0.00010381510635072897, l2: 0.00035942656159888777   Iteration 22 of 100, tot loss = 4.589119531891563, l1: 0.00010229313093904321, l2: 0.00035661882826719773   Iteration 23 of 100, tot loss = 4.679002710010694, l1: 0.00010297093926372169, l2: 0.0003649293364066145   Iteration 24 of 100, tot loss = 4.659650971492131, l1: 0.00010375598837223758, l2: 0.0003622091135184746   Iteration 25 of 100, tot loss = 4.609871606826783, l1: 0.00010271343315253034, l2: 0.0003582737327087671   Iteration 26 of 100, tot loss = 4.659076002927927, l1: 0.00010333393416098821, l2: 0.0003625736694979983   Iteration 27 of 100, tot loss = 4.758928343101784, l1: 0.00010539542590440424, l2: 0.0003704974110910876   Iteration 28 of 100, tot loss = 4.689578924860273, l1: 0.00010393319774136347, l2: 0.0003650246970729703   Iteration 29 of 100, tot loss = 4.751167971512367, l1: 0.00010443870695749426, l2: 0.0003706780909600764   Iteration 30 of 100, tot loss = 4.68517415523529, l1: 0.00010326352761088249, l2: 0.00036525388810938845   Iteration 31 of 100, tot loss = 4.793851367888912, l1: 0.00010499289994420965, l2: 0.00037439223735653344   Iteration 32 of 100, tot loss = 4.82771947234869, l1: 0.00010559505358287424, l2: 0.00037717689519922715   Iteration 33 of 100, tot loss = 4.817991119442564, l1: 0.00010513810455890824, l2: 0.0003766610091340474   Iteration 34 of 100, tot loss = 4.779954131911783, l1: 0.0001049764573776081, l2: 0.00037301895688013993   Iteration 35 of 100, tot loss = 4.762663289478847, l1: 0.00010421614674019761, l2: 0.00037205018286061073   Iteration 36 of 100, tot loss = 4.732601437303755, l1: 0.00010397933571463606, l2: 0.00036928080872813653   Iteration 37 of 100, tot loss = 4.889990297523704, l1: 0.00010596498169753751, l2: 0.0003830340487970593   Iteration 38 of 100, tot loss = 4.857692737328379, l1: 0.0001056418856789701, l2: 0.00038012738897171065   Iteration 39 of 100, tot loss = 4.907899936040242, l1: 0.000106302808629922, l2: 0.000384487186331684   Iteration 40 of 100, tot loss = 4.87343642115593, l1: 0.00010603257833281532, l2: 0.00038131106484797785   Iteration 41 of 100, tot loss = 4.90601005205294, l1: 0.0001060135562111987, l2: 0.0003845874502652938   Iteration 42 of 100, tot loss = 4.946558254105704, l1: 0.0001060548477523428, l2: 0.0003886009792387042   Iteration 43 of 100, tot loss = 4.970069269801295, l1: 0.00010601863475532682, l2: 0.0003909882933691924   Iteration 44 of 100, tot loss = 4.96516871994192, l1: 0.00010590149874835994, l2: 0.000390615374437237   Iteration 45 of 100, tot loss = 4.95746636390686, l1: 0.00010606980132352975, l2: 0.0003896768360088269   Iteration 46 of 100, tot loss = 4.895737870879795, l1: 0.00010516207586751197, l2: 0.0003844117122555516   Iteration 47 of 100, tot loss = 4.890448149214399, l1: 0.00010478105290763159, l2: 0.0003842637627113095   Iteration 48 of 100, tot loss = 4.835771853725116, l1: 0.000103788212982181, l2: 0.000379788973077666   Iteration 49 of 100, tot loss = 4.874231168201992, l1: 0.00010435825643813884, l2: 0.0003830648611812871   Iteration 50 of 100, tot loss = 4.872555384635925, l1: 0.00010463395330589264, l2: 0.0003826215863227844   Iteration 51 of 100, tot loss = 4.895470352733836, l1: 0.00010494621129313885, l2: 0.0003846008256397338   Iteration 52 of 100, tot loss = 4.83748662013274, l1: 0.00010384765545705495, l2: 0.0003799010082730092   Iteration 53 of 100, tot loss = 4.807345115913535, l1: 0.00010343343174614381, l2: 0.0003773010816899532   Iteration 54 of 100, tot loss = 4.7994347016016645, l1: 0.0001035666857071057, l2: 0.0003763767863684161   Iteration 55 of 100, tot loss = 4.811540304530751, l1: 0.00010393657251949083, l2: 0.00037721745955588467   Iteration 56 of 100, tot loss = 4.7980799462114065, l1: 0.00010334309204803763, l2: 0.00037646490402819054   Iteration 57 of 100, tot loss = 4.789831558863322, l1: 0.0001030245147009274, l2: 0.0003759586425873925   Iteration 58 of 100, tot loss = 4.785847947515291, l1: 0.0001029735454282892, l2: 0.0003756112504373144   Iteration 59 of 100, tot loss = 4.76231359223188, l1: 0.00010307040416992711, l2: 0.0003731609558907606   Iteration 60 of 100, tot loss = 4.733046197891236, l1: 0.00010225652670972826, l2: 0.0003710480940450604   Iteration 61 of 100, tot loss = 4.7882134328123, l1: 0.00010275548850243422, l2: 0.0003760658551893029   Iteration 62 of 100, tot loss = 4.746919293557444, l1: 0.00010210283935227762, l2: 0.0003725890905292134   Iteration 63 of 100, tot loss = 4.761124444386316, l1: 0.00010254126483766687, l2: 0.0003735711805001345   Iteration 64 of 100, tot loss = 4.779729597270489, l1: 0.00010301589691152913, l2: 0.0003749570635136479   Iteration 65 of 100, tot loss = 4.776796825115497, l1: 0.00010286481466699535, l2: 0.0003748148686449545   Iteration 66 of 100, tot loss = 4.763450687581843, l1: 0.00010257520680721483, l2: 0.0003737698626178881   Iteration 67 of 100, tot loss = 4.775227454171252, l1: 0.00010267370849597587, l2: 0.0003748490371300019   Iteration 68 of 100, tot loss = 4.771941963364096, l1: 0.00010229617477949326, l2: 0.00037489802151171984   Iteration 69 of 100, tot loss = 4.797409251116324, l1: 0.00010257062693700141, l2: 0.0003771702984628011   Iteration 70 of 100, tot loss = 4.808626896994454, l1: 0.00010288841107727161, l2: 0.00037797427856795754   Iteration 71 of 100, tot loss = 4.800226748829156, l1: 0.00010292210362435148, l2: 0.000377100571198061   Iteration 72 of 100, tot loss = 4.807295044263204, l1: 0.00010315952436536059, l2: 0.0003775699804262129   Iteration 73 of 100, tot loss = 4.834679557852549, l1: 0.00010332484786998328, l2: 0.0003801431083869929   Iteration 74 of 100, tot loss = 4.835562171162786, l1: 0.00010374620025096512, l2: 0.00037981001750685627   Iteration 75 of 100, tot loss = 4.8412864303588865, l1: 0.00010384675047437971, l2: 0.0003802818934976434   Iteration 76 of 100, tot loss = 4.841553531194988, l1: 0.00010422337465187363, l2: 0.00037993197915248426   Iteration 77 of 100, tot loss = 4.816161929786979, l1: 0.00010373335000575073, l2: 0.00037788284387645104   Iteration 78 of 100, tot loss = 4.792640148065029, l1: 0.00010338619074154382, l2: 0.0003758778249716553   Iteration 79 of 100, tot loss = 4.772369574896897, l1: 0.0001031677762696319, l2: 0.00037406918226558526   Iteration 80 of 100, tot loss = 4.7775088280439375, l1: 0.00010340096177969826, l2: 0.00037434992173075444   Iteration 81 of 100, tot loss = 4.775387284196453, l1: 0.0001035426853096722, l2: 0.0003739960441713647   Iteration 82 of 100, tot loss = 4.820260667219395, l1: 0.00010434408729296836, l2: 0.00037768198033118986   Iteration 83 of 100, tot loss = 4.824774348592183, l1: 0.00010435761635245038, l2: 0.0003781198197484847   Iteration 84 of 100, tot loss = 4.81313750573567, l1: 0.00010409383901874425, l2: 0.0003772199126528687   Iteration 85 of 100, tot loss = 4.831864752488978, l1: 0.00010442560930695276, l2: 0.0003787608668276602   Iteration 86 of 100, tot loss = 4.8174293983814325, l1: 0.00010424237747386987, l2: 0.00037750056325485065   Iteration 87 of 100, tot loss = 4.828288445527527, l1: 0.0001045082793296743, l2: 0.00037832056615774614   Iteration 88 of 100, tot loss = 4.813973383470015, l1: 0.00010425147527050946, l2: 0.00037714586390145337   Iteration 89 of 100, tot loss = 4.808617720443212, l1: 0.00010442827730779693, l2: 0.0003764334955877949   Iteration 90 of 100, tot loss = 4.813987625969781, l1: 0.00010471749604524423, l2: 0.00037668126735499954   Iteration 91 of 100, tot loss = 4.808576190864647, l1: 0.00010459896365428794, l2: 0.0003762586564554802   Iteration 92 of 100, tot loss = 4.80953820373701, l1: 0.00010478244961553213, l2: 0.0003761713718552568   Iteration 93 of 100, tot loss = 4.822563309823313, l1: 0.00010493624323580955, l2: 0.00037732008880289695   Iteration 94 of 100, tot loss = 4.808196093173737, l1: 0.00010466288020982942, l2: 0.000376156730202096   Iteration 95 of 100, tot loss = 4.799543809890747, l1: 0.00010477738557567232, l2: 0.00037517699636641495   Iteration 96 of 100, tot loss = 4.789937451481819, l1: 0.00010444757610154436, l2: 0.00037454616979933536   Iteration 97 of 100, tot loss = 4.759734103360127, l1: 0.0001040001790766537, l2: 0.0003719732320457025   Iteration 98 of 100, tot loss = 4.758221727244708, l1: 0.0001041339847707066, l2: 0.00037168818885313193   Iteration 99 of 100, tot loss = 4.763325280613369, l1: 0.00010399307990670111, l2: 0.0003723394494054538   Iteration 100 of 100, tot loss = 4.763114746809006, l1: 0.00010389723414846231, l2: 0.00037241424186504443
   End of epoch 1120; saving model... 

Epoch 1121 of 2000
   Iteration 1 of 100, tot loss = 2.674569606781006, l1: 8.169082866515964e-05, l2: 0.00018576612637843937   Iteration 2 of 100, tot loss = 3.0332590341567993, l1: 7.196785372798331e-05, l2: 0.0002313580407644622   Iteration 3 of 100, tot loss = 3.3683103720347085, l1: 7.192434956474851e-05, l2: 0.0002649066785428052   Iteration 4 of 100, tot loss = 3.9791913628578186, l1: 7.815158096491359e-05, l2: 0.00031976754326024093   Iteration 5 of 100, tot loss = 4.068485498428345, l1: 8.439432422164827e-05, l2: 0.0003224542160751298   Iteration 6 of 100, tot loss = 3.958172837893168, l1: 8.485766253822173e-05, l2: 0.0003109596121551779   Iteration 7 of 100, tot loss = 4.142487287521362, l1: 8.750941820575722e-05, l2: 0.00032673930192166675   Iteration 8 of 100, tot loss = 4.119606524705887, l1: 8.686899218446342e-05, l2: 0.0003250916506658541   Iteration 9 of 100, tot loss = 4.152852508756849, l1: 8.840706868795678e-05, l2: 0.00032687817292753607   Iteration 10 of 100, tot loss = 4.2256656885147095, l1: 9.048282372532412e-05, l2: 0.00033208373497473076   Iteration 11 of 100, tot loss = 4.48685773936185, l1: 9.460665222088045e-05, l2: 0.0003540791129291227   Iteration 12 of 100, tot loss = 4.526945253213246, l1: 9.730484816827811e-05, l2: 0.0003553896688875587   Iteration 13 of 100, tot loss = 4.724272049390352, l1: 0.00010238375958019438, l2: 0.00037004343847421784   Iteration 14 of 100, tot loss = 4.804866909980774, l1: 0.0001025482062167222, l2: 0.00037793847877765074   Iteration 15 of 100, tot loss = 4.787691100438436, l1: 0.00010384628064154336, l2: 0.0003749228228116408   Iteration 16 of 100, tot loss = 4.725544318556786, l1: 0.00010285602547810413, l2: 0.0003696983994814218   Iteration 17 of 100, tot loss = 4.631419167799108, l1: 0.00010193370732903371, l2: 0.0003612082021872458   Iteration 18 of 100, tot loss = 4.584340506129795, l1: 0.00010001075699821943, l2: 0.00035842328684844286   Iteration 19 of 100, tot loss = 4.75964063092282, l1: 0.00010287155409490592, l2: 0.00037309250221465175   Iteration 20 of 100, tot loss = 4.871945416927337, l1: 0.0001047246994858142, l2: 0.0003824698338576127   Iteration 21 of 100, tot loss = 4.909171751567295, l1: 0.0001064118925049635, l2: 0.00038450527478874264   Iteration 22 of 100, tot loss = 4.9137360074303364, l1: 0.00010717131094266237, l2: 0.00038420228321973065   Iteration 23 of 100, tot loss = 4.935128947962886, l1: 0.00010749364889267346, l2: 0.0003860192392375725   Iteration 24 of 100, tot loss = 5.0382446348667145, l1: 0.00010930616978536516, l2: 0.0003945182876729329   Iteration 25 of 100, tot loss = 5.023369512557983, l1: 0.00010881245078053325, l2: 0.0003935244953026995   Iteration 26 of 100, tot loss = 5.020284936978267, l1: 0.00010768404424127836, l2: 0.00039434444345086097   Iteration 27 of 100, tot loss = 5.015897512435913, l1: 0.00010849469469610119, l2: 0.00039309504952850856   Iteration 28 of 100, tot loss = 4.987439147063664, l1: 0.00010881470032992573, l2: 0.0003899292075532555   Iteration 29 of 100, tot loss = 4.951528549194336, l1: 0.00010837040538498168, l2: 0.0003867824426410591   Iteration 30 of 100, tot loss = 4.8830939928690595, l1: 0.00010762182040101228, l2: 0.0003806875717903798   Iteration 31 of 100, tot loss = 4.828875072540775, l1: 0.00010635339347044788, l2: 0.00037653410747166604   Iteration 32 of 100, tot loss = 4.853342063724995, l1: 0.00010696184199332492, l2: 0.00037837235777260503   Iteration 33 of 100, tot loss = 4.845680374087709, l1: 0.00010668846141815072, l2: 0.00037787957064045423   Iteration 34 of 100, tot loss = 4.825781997512369, l1: 0.00010616670945583952, l2: 0.00037641148425309975   Iteration 35 of 100, tot loss = 4.817171335220337, l1: 0.00010569287592911028, l2: 0.00037602425103874077   Iteration 36 of 100, tot loss = 4.868484795093536, l1: 0.00010644466177230545, l2: 0.00038040381089861813   Iteration 37 of 100, tot loss = 4.930823603191891, l1: 0.0001075235940751972, l2: 0.0003855587604669602   Iteration 38 of 100, tot loss = 4.9700843974163655, l1: 0.00010824521354822394, l2: 0.00038876322076631417   Iteration 39 of 100, tot loss = 4.982092447769948, l1: 0.00010880853183036193, l2: 0.0003894007069249757   Iteration 40 of 100, tot loss = 4.952360886335373, l1: 0.00010810732128447853, l2: 0.0003871287619404029   Iteration 41 of 100, tot loss = 4.987716855072394, l1: 0.00010876758016997994, l2: 0.000390004100442705   Iteration 42 of 100, tot loss = 5.0141871599923995, l1: 0.00010892982321647218, l2: 0.00039248888788279146   Iteration 43 of 100, tot loss = 5.045149087905884, l1: 0.00010930159471679999, l2: 0.0003952133086195952   Iteration 44 of 100, tot loss = 5.01187524470416, l1: 0.00010900680618678135, l2: 0.00039218071271369064   Iteration 45 of 100, tot loss = 4.979770496156481, l1: 0.00010844514860461155, l2: 0.00038953189569939343   Iteration 46 of 100, tot loss = 4.987457187279411, l1: 0.00010879359362424229, l2: 0.0003899521201990707   Iteration 47 of 100, tot loss = 4.949551521463597, l1: 0.00010795259592494868, l2: 0.0003870025511206861   Iteration 48 of 100, tot loss = 4.988177319367726, l1: 0.0001085765478213337, l2: 0.0003902411790477345   Iteration 49 of 100, tot loss = 4.965275939630002, l1: 0.00010795967147937425, l2: 0.000388567917386298   Iteration 50 of 100, tot loss = 4.999386310577393, l1: 0.00010866659591556527, l2: 0.0003912720311200246   Iteration 51 of 100, tot loss = 5.001665489346373, l1: 0.00010862895906743063, l2: 0.0003915375866698941   Iteration 52 of 100, tot loss = 4.992279557081369, l1: 0.00010851727949137476, l2: 0.0003907106729457155   Iteration 53 of 100, tot loss = 5.033480023438075, l1: 0.00010898980690399185, l2: 0.00039435819314160154   Iteration 54 of 100, tot loss = 5.071955707338121, l1: 0.00010978030776540335, l2: 0.0003974152614655732   Iteration 55 of 100, tot loss = 5.051903308521617, l1: 0.00010905483134344897, l2: 0.0003961354980922558   Iteration 56 of 100, tot loss = 5.0737830485616415, l1: 0.00010957264514997535, l2: 0.00039780565878442885   Iteration 57 of 100, tot loss = 5.066464399036608, l1: 0.00010939197933399363, l2: 0.0003972544591146799   Iteration 58 of 100, tot loss = 5.0706803140969114, l1: 0.00010954671007970444, l2: 0.0003975213200239272   Iteration 59 of 100, tot loss = 5.105055623135324, l1: 0.00011030987180844423, l2: 0.00040019568947727903   Iteration 60 of 100, tot loss = 5.102102971076965, l1: 0.00011057724950660485, l2: 0.00039963304734556   Iteration 61 of 100, tot loss = 5.100284732756068, l1: 0.00011077198282468179, l2: 0.00039925649008316704   Iteration 62 of 100, tot loss = 5.062170686260346, l1: 0.0001102256721952149, l2: 0.0003959913961044813   Iteration 63 of 100, tot loss = 5.069411417794606, l1: 0.0001096831912281997, l2: 0.0003972579501033391   Iteration 64 of 100, tot loss = 5.0836390145123005, l1: 0.00010998525658578728, l2: 0.00039837864392211486   Iteration 65 of 100, tot loss = 5.067651913716243, l1: 0.00010967935690692124, l2: 0.00039708583297816895   Iteration 66 of 100, tot loss = 5.054513638669794, l1: 0.00010963658794273876, l2: 0.0003958147746302667   Iteration 67 of 100, tot loss = 5.059483752321841, l1: 0.0001096111344809491, l2: 0.00039633724003606266   Iteration 68 of 100, tot loss = 5.072220195742214, l1: 0.00010982783822007204, l2: 0.0003973941807891927   Iteration 69 of 100, tot loss = 5.02842206540315, l1: 0.00010909705699726845, l2: 0.0003937451490302287   Iteration 70 of 100, tot loss = 5.02945670400347, l1: 0.00010924264274113479, l2: 0.00039370302692987025   Iteration 71 of 100, tot loss = 4.9991763175373345, l1: 0.0001084875426309275, l2: 0.00039143008848120995   Iteration 72 of 100, tot loss = 4.97547173500061, l1: 0.00010775299420452534, l2: 0.00038979417897482764   Iteration 73 of 100, tot loss = 4.96128143349739, l1: 0.00010759168126491179, l2: 0.0003885364617230626   Iteration 74 of 100, tot loss = 4.954966335683255, l1: 0.00010745107455649477, l2: 0.0003880455590882087   Iteration 75 of 100, tot loss = 4.93122220993042, l1: 0.00010710818280737536, l2: 0.0003860140380371983   Iteration 76 of 100, tot loss = 4.928051107808163, l1: 0.00010705964685161904, l2: 0.0003857454639249849   Iteration 77 of 100, tot loss = 4.925905791195956, l1: 0.00010713383536702687, l2: 0.000385456744017527   Iteration 78 of 100, tot loss = 4.907768555176564, l1: 0.00010702394599950191, l2: 0.0003837529095621087   Iteration 79 of 100, tot loss = 4.890201429777507, l1: 0.00010668108103645057, l2: 0.0003823390622148762   Iteration 80 of 100, tot loss = 4.887894850969315, l1: 0.00010681347926038143, l2: 0.00038197600606508785   Iteration 81 of 100, tot loss = 4.868405247911995, l1: 0.00010614540349953681, l2: 0.0003806951214677229   Iteration 82 of 100, tot loss = 4.85537902611058, l1: 0.00010584140989344521, l2: 0.00037969649281699165   Iteration 83 of 100, tot loss = 4.848445688385561, l1: 0.0001057597523813237, l2: 0.00037908481685753286   Iteration 84 of 100, tot loss = 4.834937944298699, l1: 0.00010544800551586031, l2: 0.00037804578927794604   Iteration 85 of 100, tot loss = 4.8316106151132026, l1: 0.00010543063288102584, l2: 0.0003777304289328373   Iteration 86 of 100, tot loss = 4.820104016814121, l1: 0.00010537434363234138, l2: 0.00037663605833713093   Iteration 87 of 100, tot loss = 4.815196234604408, l1: 0.00010547746431508006, l2: 0.00037604215974389605   Iteration 88 of 100, tot loss = 4.824687063694, l1: 0.00010574504176474875, l2: 0.00037672366554695395   Iteration 89 of 100, tot loss = 4.805284269740072, l1: 0.00010555684962179093, l2: 0.0003749715783005434   Iteration 90 of 100, tot loss = 4.787036755349901, l1: 0.00010525940678134147, l2: 0.00037344426980578444   Iteration 91 of 100, tot loss = 4.786126369958396, l1: 0.00010530317792602415, l2: 0.0003733094600937585   Iteration 92 of 100, tot loss = 4.776098956232485, l1: 0.00010541999440845203, l2: 0.00037218990204881106   Iteration 93 of 100, tot loss = 4.748917225868471, l1: 0.00010494411636960082, l2: 0.0003699476072396983   Iteration 94 of 100, tot loss = 4.747376173100573, l1: 0.00010515643148113319, l2: 0.00036958118691976044   Iteration 95 of 100, tot loss = 4.7408467393172415, l1: 0.00010512771412705709, l2: 0.00036895696095215453   Iteration 96 of 100, tot loss = 4.741833835840225, l1: 0.00010511304208193906, l2: 0.00036907034260972676   Iteration 97 of 100, tot loss = 4.759722321303849, l1: 0.00010561087626613408, l2: 0.00037036135660114785   Iteration 98 of 100, tot loss = 4.751797513085968, l1: 0.00010537198873602652, l2: 0.000369807763342337   Iteration 99 of 100, tot loss = 4.758850875526968, l1: 0.00010544645721727812, l2: 0.0003704386312018308   Iteration 100 of 100, tot loss = 4.76565774679184, l1: 0.00010553200147114694, l2: 0.0003710337738448288
   End of epoch 1121; saving model... 

Epoch 1122 of 2000
   Iteration 1 of 100, tot loss = 5.6960649490356445, l1: 0.00013255739759188145, l2: 0.00043704910785891116   Iteration 2 of 100, tot loss = 3.95647931098938, l1: 9.410012353328057e-05, l2: 0.0003015478141605854   Iteration 3 of 100, tot loss = 4.806764284769694, l1: 9.513283051395167e-05, l2: 0.00038554360313961905   Iteration 4 of 100, tot loss = 5.517086148262024, l1: 0.00010762997408164665, l2: 0.0004440786433406174   Iteration 5 of 100, tot loss = 5.538425540924072, l1: 0.00010689527844078838, l2: 0.00044694727403111757   Iteration 6 of 100, tot loss = 5.227900385856628, l1: 0.0001061629506390697, l2: 0.00041662708584529656   Iteration 7 of 100, tot loss = 5.419541733605521, l1: 0.0001108049213403969, l2: 0.0004311492500294532   Iteration 8 of 100, tot loss = 5.299335211515427, l1: 0.00010935371847153874, l2: 0.00042057980317622423   Iteration 9 of 100, tot loss = 5.010359419716729, l1: 0.00010570574846092818, l2: 0.00039533019298687577   Iteration 10 of 100, tot loss = 4.9731196641921995, l1: 0.00010506318358238786, l2: 0.00039224878419190644   Iteration 11 of 100, tot loss = 5.020829959349199, l1: 0.0001064480159336447, l2: 0.00039563498152843255   Iteration 12 of 100, tot loss = 4.8703169624010725, l1: 0.0001049409578020762, l2: 0.0003820907392461474   Iteration 13 of 100, tot loss = 5.057502838281485, l1: 0.00010869336088385004, l2: 0.0003970569232478738   Iteration 14 of 100, tot loss = 5.084284867559161, l1: 0.00011018628668222976, l2: 0.00039824220168936463   Iteration 15 of 100, tot loss = 5.0576919714609785, l1: 0.00011125161011780923, l2: 0.000394517588817204   Iteration 16 of 100, tot loss = 5.040445491671562, l1: 0.00011240862068007118, l2: 0.00039163593100965954   Iteration 17 of 100, tot loss = 4.946678652482874, l1: 0.00011054484339718543, l2: 0.00038412302348981886   Iteration 18 of 100, tot loss = 4.887291855282253, l1: 0.00011024873795880315, l2: 0.0003784804494999763   Iteration 19 of 100, tot loss = 4.98514975999531, l1: 0.00011143585901376547, l2: 0.00038707911919214224   Iteration 20 of 100, tot loss = 4.969431471824646, l1: 0.00011204225920664612, l2: 0.00038490089063998313   Iteration 21 of 100, tot loss = 4.913562865484328, l1: 0.00011177098834499095, l2: 0.0003795853006609139   Iteration 22 of 100, tot loss = 4.963430036198009, l1: 0.00011286124109491621, l2: 0.00038348176431926816   Iteration 23 of 100, tot loss = 4.947292307148809, l1: 0.00011063755221624413, l2: 0.00038409168052527565   Iteration 24 of 100, tot loss = 4.942147433757782, l1: 0.00011063901638408424, l2: 0.00038357572824073333   Iteration 25 of 100, tot loss = 4.90194540977478, l1: 0.00010954265861073508, l2: 0.0003806518844794482   Iteration 26 of 100, tot loss = 4.901138956730183, l1: 0.00010966334425924848, l2: 0.00038045055407565087   Iteration 27 of 100, tot loss = 4.869091696209377, l1: 0.00010855452166180368, l2: 0.00037835464980970655   Iteration 28 of 100, tot loss = 4.8367698192596436, l1: 0.00010701095431743721, l2: 0.0003766660291668294   Iteration 29 of 100, tot loss = 4.894436606045427, l1: 0.00010803025787904987, l2: 0.0003814134036255037   Iteration 30 of 100, tot loss = 4.852747027079264, l1: 0.00010722455893604395, l2: 0.000378050144839411   Iteration 31 of 100, tot loss = 4.818752511855094, l1: 0.0001071333082959867, l2: 0.0003747419442891354   Iteration 32 of 100, tot loss = 4.805791862308979, l1: 0.00010725304309744388, l2: 0.00037332614465412917   Iteration 33 of 100, tot loss = 4.85499459324461, l1: 0.0001084226647490692, l2: 0.0003770767970391634   Iteration 34 of 100, tot loss = 4.851348337005167, l1: 0.00010839481440009823, l2: 0.00037674002198721557   Iteration 35 of 100, tot loss = 4.822045816693987, l1: 0.000107843718433287, l2: 0.0003743608665120389   Iteration 36 of 100, tot loss = 4.810575683911641, l1: 0.0001072944804844964, l2: 0.00037376309127365757   Iteration 37 of 100, tot loss = 4.849414309939823, l1: 0.00010797132012698598, l2: 0.00037697011312293644   Iteration 38 of 100, tot loss = 4.868311380085192, l1: 0.00010811868349647786, l2: 0.00037871245647118866   Iteration 39 of 100, tot loss = 4.867909027979924, l1: 0.00010730117878082614, l2: 0.00037948972581384273   Iteration 40 of 100, tot loss = 4.879475688934326, l1: 0.000107122934605286, l2: 0.0003808246365224477   Iteration 41 of 100, tot loss = 4.843187553126637, l1: 0.00010691332437734051, l2: 0.00037740543307225426   Iteration 42 of 100, tot loss = 4.870660009838286, l1: 0.00010702107378858186, l2: 0.00038004492948641113   Iteration 43 of 100, tot loss = 4.90005533085313, l1: 0.00010790509580591217, l2: 0.00038210043939491083   Iteration 44 of 100, tot loss = 4.89094335382635, l1: 0.00010754683303689076, l2: 0.0003815475042343182   Iteration 45 of 100, tot loss = 4.902771928575304, l1: 0.00010741911214103715, l2: 0.00038285808307894815   Iteration 46 of 100, tot loss = 4.90029093493586, l1: 0.00010661897597500406, l2: 0.00038341011974768225   Iteration 47 of 100, tot loss = 4.887523093122117, l1: 0.00010668964743207981, l2: 0.00038206266416385376   Iteration 48 of 100, tot loss = 4.879406124353409, l1: 0.00010648966569230349, l2: 0.0003814509488317223   Iteration 49 of 100, tot loss = 4.852083103997367, l1: 0.00010585061147362374, l2: 0.00037935770108431043   Iteration 50 of 100, tot loss = 4.8192114877700805, l1: 0.00010508140359888785, l2: 0.0003768397474777885   Iteration 51 of 100, tot loss = 4.832739039963367, l1: 0.00010536398550874426, l2: 0.00037790992092934673   Iteration 52 of 100, tot loss = 4.833325427312118, l1: 0.00010483057709419742, l2: 0.0003785019678336819   Iteration 53 of 100, tot loss = 4.791301920728864, l1: 0.00010416558081556534, l2: 0.00037496461327635807   Iteration 54 of 100, tot loss = 4.8212355022077205, l1: 0.00010363754120556189, l2: 0.00037848601075691276   Iteration 55 of 100, tot loss = 4.807009978727861, l1: 0.00010381497387160462, l2: 0.00037688602539922363   Iteration 56 of 100, tot loss = 4.7888569704123904, l1: 0.00010358453917953219, l2: 0.0003753011590431145   Iteration 57 of 100, tot loss = 4.784624321418896, l1: 0.00010383450627845728, l2: 0.00037462792687093546   Iteration 58 of 100, tot loss = 4.820058777414519, l1: 0.00010443089069730346, l2: 0.0003775749879925319   Iteration 59 of 100, tot loss = 4.851069632223097, l1: 0.0001050981499848619, l2: 0.0003800088141638375   Iteration 60 of 100, tot loss = 4.821913965543112, l1: 0.00010426228685294821, l2: 0.0003779291104971586   Iteration 61 of 100, tot loss = 4.853542593658948, l1: 0.00010472460864369421, l2: 0.00038062965141449   Iteration 62 of 100, tot loss = 4.837331118122224, l1: 0.00010438426896496, l2: 0.00037934884382975135   Iteration 63 of 100, tot loss = 4.82280705467103, l1: 0.00010415386748805066, l2: 0.00037812683865287534   Iteration 64 of 100, tot loss = 4.828636899590492, l1: 0.0001038542055198377, l2: 0.000379009485413917   Iteration 65 of 100, tot loss = 4.8253541066096375, l1: 0.00010369318350477932, l2: 0.0003788422280474781   Iteration 66 of 100, tot loss = 4.82024209427111, l1: 0.00010376557237936085, l2: 0.0003782586376694257   Iteration 67 of 100, tot loss = 4.794877721302545, l1: 0.00010332243130220892, l2: 0.0003761653412644293   Iteration 68 of 100, tot loss = 4.7634382773848145, l1: 0.00010278948839721339, l2: 0.0003735543396032881   Iteration 69 of 100, tot loss = 4.766099110893581, l1: 0.00010296105255543227, l2: 0.0003736488589851618   Iteration 70 of 100, tot loss = 4.790343935149057, l1: 0.00010362877266223742, l2: 0.00037540562064220596   Iteration 71 of 100, tot loss = 4.793649482055449, l1: 0.00010373594535926266, l2: 0.0003756290028268524   Iteration 72 of 100, tot loss = 4.7807672421137495, l1: 0.00010335634573897955, l2: 0.0003747203783051292   Iteration 73 of 100, tot loss = 4.794599663721372, l1: 0.00010376550433224335, l2: 0.0003756944619179767   Iteration 74 of 100, tot loss = 4.816847440358755, l1: 0.00010420429390324971, l2: 0.0003774804493799378   Iteration 75 of 100, tot loss = 4.805207007726033, l1: 0.00010398674615620014, l2: 0.0003765339538222179   Iteration 76 of 100, tot loss = 4.800949049623389, l1: 0.000104153216278447, l2: 0.00037594168813819505   Iteration 77 of 100, tot loss = 4.802206884730946, l1: 0.0001045254350835669, l2: 0.0003756952527592975   Iteration 78 of 100, tot loss = 4.799137699298369, l1: 0.00010477622144651086, l2: 0.00037513754814893054   Iteration 79 of 100, tot loss = 4.771110920966426, l1: 0.0001040694968409459, l2: 0.00037304159490350353   Iteration 80 of 100, tot loss = 4.771881818771362, l1: 0.00010390866846137215, l2: 0.00037327951304177986   Iteration 81 of 100, tot loss = 4.74644089628149, l1: 0.00010325853603447925, l2: 0.0003713855531029288   Iteration 82 of 100, tot loss = 4.721768274539855, l1: 0.00010286181443406372, l2: 0.0003693150125710839   Iteration 83 of 100, tot loss = 4.689696274608014, l1: 0.00010212018601287785, l2: 0.0003668494408260133   Iteration 84 of 100, tot loss = 4.678573699224563, l1: 0.00010211400175259249, l2: 0.0003657433674864781   Iteration 85 of 100, tot loss = 4.654094951293048, l1: 0.00010154696604278049, l2: 0.00036386252836296884   Iteration 86 of 100, tot loss = 4.6612614504126615, l1: 0.00010157466741200255, l2: 0.0003645514765128948   Iteration 87 of 100, tot loss = 4.717188037675003, l1: 0.0001022346032918538, l2: 0.00036948419931403566   Iteration 88 of 100, tot loss = 4.705104321241379, l1: 0.00010203862761541545, l2: 0.0003684718032507755   Iteration 89 of 100, tot loss = 4.703356665171934, l1: 0.00010203912702462311, l2: 0.00036829653810160337   Iteration 90 of 100, tot loss = 4.703833185301887, l1: 0.00010208818372727061, l2: 0.0003682951331332636   Iteration 91 of 100, tot loss = 4.7263653225951145, l1: 0.00010228697399245473, l2: 0.00037034955640082956   Iteration 92 of 100, tot loss = 4.71982518227204, l1: 0.00010219880420228709, l2: 0.00036978371218024023   Iteration 93 of 100, tot loss = 4.7190142575130665, l1: 0.00010217247308314507, l2: 0.0003697289511003101   Iteration 94 of 100, tot loss = 4.69281607485832, l1: 0.00010164207750231652, l2: 0.00036763952858622206   Iteration 95 of 100, tot loss = 4.69891105451082, l1: 0.00010187946007997533, l2: 0.0003680116443367871   Iteration 96 of 100, tot loss = 4.693332905570666, l1: 0.00010182676089698361, l2: 0.0003675065289219977   Iteration 97 of 100, tot loss = 4.6970778199815255, l1: 0.00010170892470416652, l2: 0.00036799885646425685   Iteration 98 of 100, tot loss = 4.690276861190796, l1: 0.00010145739713839341, l2: 0.0003675702878818087   Iteration 99 of 100, tot loss = 4.692049565941397, l1: 0.00010163597387027682, l2: 0.0003675689814656954   Iteration 100 of 100, tot loss = 4.735761051177978, l1: 0.00010222300104942406, l2: 0.00037135310223675334
   End of epoch 1122; saving model... 

Epoch 1123 of 2000
   Iteration 1 of 100, tot loss = 5.526342868804932, l1: 0.00011727355740731582, l2: 0.0004353607364464551   Iteration 2 of 100, tot loss = 3.533900022506714, l1: 7.763062421872746e-05, l2: 0.0002757593829301186   Iteration 3 of 100, tot loss = 4.221268018086751, l1: 9.065287910440627e-05, l2: 0.00033147393939240527   Iteration 4 of 100, tot loss = 4.2144997119903564, l1: 9.000290174299153e-05, l2: 0.0003314470850455109   Iteration 5 of 100, tot loss = 4.442518138885498, l1: 8.984622909338213e-05, l2: 0.00035440560022834686   Iteration 6 of 100, tot loss = 4.0838613112767534, l1: 8.223079203162342e-05, l2: 0.00032615535261963185   Iteration 7 of 100, tot loss = 3.9242909976414273, l1: 8.146450376703538e-05, l2: 0.0003109646079662655   Iteration 8 of 100, tot loss = 4.209236979484558, l1: 8.653333679831121e-05, l2: 0.0003343903663335368   Iteration 9 of 100, tot loss = 4.1433850394354925, l1: 8.678721577679325e-05, l2: 0.0003275512911689778   Iteration 10 of 100, tot loss = 4.280292510986328, l1: 9.23302250157576e-05, l2: 0.000335699028801173   Iteration 11 of 100, tot loss = 4.3334456357088955, l1: 9.305450657848269e-05, l2: 0.00034029006069018084   Iteration 12 of 100, tot loss = 4.2998369336128235, l1: 9.295146931738903e-05, l2: 0.00033703222531282034   Iteration 13 of 100, tot loss = 4.328717910326445, l1: 9.371435198073204e-05, l2: 0.00033915744055635657   Iteration 14 of 100, tot loss = 4.240992069244385, l1: 9.24836817180871e-05, l2: 0.00033161552710225806   Iteration 15 of 100, tot loss = 4.327480983734131, l1: 9.482890891376883e-05, l2: 0.00033791919025437286   Iteration 16 of 100, tot loss = 4.261621177196503, l1: 9.331228466180619e-05, l2: 0.00033284983419434866   Iteration 17 of 100, tot loss = 4.20578512023477, l1: 9.155352403039989e-05, l2: 0.0003290249895893366   Iteration 18 of 100, tot loss = 4.218798968527052, l1: 9.148911542272092e-05, l2: 0.00033039078294273675   Iteration 19 of 100, tot loss = 4.150590482511018, l1: 9.060679593331818e-05, l2: 0.00032445225454131633   Iteration 20 of 100, tot loss = 4.127544248104096, l1: 9.09310176211875e-05, l2: 0.0003218234094674699   Iteration 21 of 100, tot loss = 4.197765520640782, l1: 9.221702202921733e-05, l2: 0.0003275595330965838   Iteration 22 of 100, tot loss = 4.1599930849942295, l1: 9.036390772301026e-05, l2: 0.0003256354036486962   Iteration 23 of 100, tot loss = 4.196792436682659, l1: 9.120771352647115e-05, l2: 0.00032847153362008214   Iteration 24 of 100, tot loss = 4.242412249247233, l1: 9.312073598266579e-05, l2: 0.00033112049277406186   Iteration 25 of 100, tot loss = 4.196037397384644, l1: 9.202590212225914e-05, l2: 0.00032757784065324814   Iteration 26 of 100, tot loss = 4.3168982817576484, l1: 9.401966347538221e-05, l2: 0.0003376701682170208   Iteration 27 of 100, tot loss = 4.309434740631668, l1: 9.437831667363989e-05, l2: 0.0003365651619215323   Iteration 28 of 100, tot loss = 4.325748264789581, l1: 9.355168003821746e-05, l2: 0.00033902314992571646   Iteration 29 of 100, tot loss = 4.341449614228873, l1: 9.300271857024075e-05, l2: 0.0003411422472583644   Iteration 30 of 100, tot loss = 4.329690432548523, l1: 9.39261738191514e-05, l2: 0.00033904287411132825   Iteration 31 of 100, tot loss = 4.33557883385689, l1: 9.368423859761547e-05, l2: 0.0003398736500399067   Iteration 32 of 100, tot loss = 4.358438692986965, l1: 9.35657858462946e-05, l2: 0.0003422780887376575   Iteration 33 of 100, tot loss = 4.303981766556248, l1: 9.281537782182127e-05, l2: 0.00033758280341950893   Iteration 34 of 100, tot loss = 4.309820637983434, l1: 9.298699660575949e-05, l2: 0.00033799507133046384   Iteration 35 of 100, tot loss = 4.283236585344587, l1: 9.303724842279085e-05, l2: 0.0003352864142341007   Iteration 36 of 100, tot loss = 4.270085480478075, l1: 9.311796566180419e-05, l2: 0.00033389058605987683   Iteration 37 of 100, tot loss = 4.253125583803332, l1: 9.240790540069291e-05, l2: 0.0003329046566914012   Iteration 38 of 100, tot loss = 4.270857616474754, l1: 9.273572868551128e-05, l2: 0.00033435003646637167   Iteration 39 of 100, tot loss = 4.249684639466115, l1: 9.260304833356387e-05, l2: 0.0003323654188231255   Iteration 40 of 100, tot loss = 4.239454954862595, l1: 9.246897789125797e-05, l2: 0.0003314765210234327   Iteration 41 of 100, tot loss = 4.218985865755779, l1: 9.211556998041754e-05, l2: 0.00032978301985677697   Iteration 42 of 100, tot loss = 4.2452688955125355, l1: 9.315314621614692e-05, l2: 0.000331373745644842   Iteration 43 of 100, tot loss = 4.262487794077674, l1: 9.301703928856092e-05, l2: 0.00033323174216622097   Iteration 44 of 100, tot loss = 4.210348879749125, l1: 9.208616012818476e-05, l2: 0.0003289487298884937   Iteration 45 of 100, tot loss = 4.236213739713033, l1: 9.243766641399512e-05, l2: 0.0003311837092041969   Iteration 46 of 100, tot loss = 4.294034120829209, l1: 9.381822873483938e-05, l2: 0.00033558518549872804   Iteration 47 of 100, tot loss = 4.2870528976968, l1: 9.382468201789728e-05, l2: 0.0003348806101402783   Iteration 48 of 100, tot loss = 4.279175765812397, l1: 9.435039495050053e-05, l2: 0.0003335671838916217   Iteration 49 of 100, tot loss = 4.25341218101735, l1: 9.418123865580871e-05, l2: 0.00033115998192034585   Iteration 50 of 100, tot loss = 4.262034094333648, l1: 9.402252704603598e-05, l2: 0.0003321808853070252   Iteration 51 of 100, tot loss = 4.314889713829639, l1: 9.497197763766071e-05, l2: 0.0003365169963502672   Iteration 52 of 100, tot loss = 4.316078041608517, l1: 9.522529524894289e-05, l2: 0.0003363825114949857   Iteration 53 of 100, tot loss = 4.316671580638525, l1: 9.52491555955999e-05, l2: 0.0003364180049415291   Iteration 54 of 100, tot loss = 4.316841374944757, l1: 9.522807953611914e-05, l2: 0.0003364560605539871   Iteration 55 of 100, tot loss = 4.361259583993392, l1: 9.586917903189632e-05, l2: 0.00034025678257669576   Iteration 56 of 100, tot loss = 4.365823041115489, l1: 9.586175968122137e-05, l2: 0.00034072054742344856   Iteration 57 of 100, tot loss = 4.3353101935303, l1: 9.57568420409715e-05, l2: 0.0003377741803680628   Iteration 58 of 100, tot loss = 4.342724054024138, l1: 9.612917773634324e-05, l2: 0.0003381432307550908   Iteration 59 of 100, tot loss = 4.3346526117648105, l1: 9.632710479910218e-05, l2: 0.0003371381595971505   Iteration 60 of 100, tot loss = 4.358942176898321, l1: 9.707059095186802e-05, l2: 0.0003388236298633274   Iteration 61 of 100, tot loss = 4.359895035868785, l1: 9.744057704247992e-05, l2: 0.00033854893012712786   Iteration 62 of 100, tot loss = 4.399112141901447, l1: 9.804135040594115e-05, l2: 0.00034186986745372714   Iteration 63 of 100, tot loss = 4.388524046019902, l1: 9.81479389583766e-05, l2: 0.0003407044691451278   Iteration 64 of 100, tot loss = 4.431506199762225, l1: 9.909187735956948e-05, l2: 0.0003440587463501288   Iteration 65 of 100, tot loss = 4.418832835784325, l1: 9.891129593597725e-05, l2: 0.0003429719916312024   Iteration 66 of 100, tot loss = 4.472994426886241, l1: 9.961742266155356e-05, l2: 0.0003476820241882127   Iteration 67 of 100, tot loss = 4.453668097951519, l1: 9.969990538023134e-05, l2: 0.00034566690868162897   Iteration 68 of 100, tot loss = 4.444664897287593, l1: 9.946012238180949e-05, l2: 0.0003450063713364002   Iteration 69 of 100, tot loss = 4.469153181366298, l1: 9.958062920317836e-05, l2: 0.0003473346926815187   Iteration 70 of 100, tot loss = 4.454309965882983, l1: 9.967371941976515e-05, l2: 0.00034575728080069116   Iteration 71 of 100, tot loss = 4.479605343979849, l1: 0.00010022974259902159, l2: 0.000347730795267484   Iteration 72 of 100, tot loss = 4.469214368197653, l1: 0.00010007360570954107, l2: 0.0003468478347106914   Iteration 73 of 100, tot loss = 4.464235229034946, l1: 9.952227434315655e-05, l2: 0.0003469012518595164   Iteration 74 of 100, tot loss = 4.472421860372698, l1: 9.972380526911316e-05, l2: 0.00034751838432606056   Iteration 75 of 100, tot loss = 4.447107319831848, l1: 9.933853715968628e-05, l2: 0.0003453721983047823   Iteration 76 of 100, tot loss = 4.457833169322265, l1: 9.957382003209971e-05, l2: 0.000346209500429196   Iteration 77 of 100, tot loss = 4.469926764438679, l1: 0.0001000642150756903, l2: 0.0003469284645027735   Iteration 78 of 100, tot loss = 4.493255687065614, l1: 0.00010013746923038688, l2: 0.0003491881025883441   Iteration 79 of 100, tot loss = 4.476358774342114, l1: 9.981608788241172e-05, l2: 0.0003478197925031492   Iteration 80 of 100, tot loss = 4.4617622509598736, l1: 9.956935145964963e-05, l2: 0.0003466068765192176   Iteration 81 of 100, tot loss = 4.461686806914247, l1: 9.958766805752536e-05, l2: 0.0003465810159356391   Iteration 82 of 100, tot loss = 4.432488911035584, l1: 9.890267401493563e-05, l2: 0.00034434622045032817   Iteration 83 of 100, tot loss = 4.442686119711543, l1: 9.93532276079788e-05, l2: 0.0003449153882684186   Iteration 84 of 100, tot loss = 4.440853571607953, l1: 9.954993895787214e-05, l2: 0.00034453542199723114   Iteration 85 of 100, tot loss = 4.437065369942609, l1: 9.932756417102235e-05, l2: 0.00034437897659646456   Iteration 86 of 100, tot loss = 4.457039632076441, l1: 9.953094051398207e-05, l2: 0.000346173026205073   Iteration 87 of 100, tot loss = 4.458723483414485, l1: 9.947951259854217e-05, l2: 0.0003463928392036529   Iteration 88 of 100, tot loss = 4.470582743937319, l1: 9.972824359878855e-05, l2: 0.0003473300336984473   Iteration 89 of 100, tot loss = 4.4942243380493, l1: 0.00010014699086588755, l2: 0.0003492754458625058   Iteration 90 of 100, tot loss = 4.500348095099131, l1: 9.981635084841401e-05, l2: 0.0003502184617294309   Iteration 91 of 100, tot loss = 4.498284317634918, l1: 0.00010001771491031152, l2: 0.0003498107197139269   Iteration 92 of 100, tot loss = 4.4902598559856415, l1: 9.974292729368798e-05, l2: 0.0003492830613180084   Iteration 93 of 100, tot loss = 4.50656501323946, l1: 9.977949238056818e-05, l2: 0.0003508770122050097   Iteration 94 of 100, tot loss = 4.511210711712533, l1: 9.953214542660367e-05, l2: 0.0003515889289469379   Iteration 95 of 100, tot loss = 4.491726720960517, l1: 9.936824353644625e-05, l2: 0.0003498044318372482   Iteration 96 of 100, tot loss = 4.498039450496435, l1: 9.944826486692666e-05, l2: 0.0003503556833190184   Iteration 97 of 100, tot loss = 4.492381630484591, l1: 9.926446514280954e-05, l2: 0.00034997370106062476   Iteration 98 of 100, tot loss = 4.491310984504466, l1: 9.940537392121398e-05, l2: 0.00034972572757396847   Iteration 99 of 100, tot loss = 4.490020399141794, l1: 9.941982025706276e-05, l2: 0.00034958222290651516   Iteration 100 of 100, tot loss = 4.487671650648117, l1: 9.946767997462302e-05, l2: 0.00034929948829812927
   End of epoch 1123; saving model... 

Epoch 1124 of 2000
   Iteration 1 of 100, tot loss = 2.97739577293396, l1: 7.228930917335674e-05, l2: 0.00022545026149600744   Iteration 2 of 100, tot loss = 3.045855402946472, l1: 7.233869837364182e-05, l2: 0.0002322468426427804   Iteration 3 of 100, tot loss = 3.7121752897898355, l1: 9.017328072028856e-05, l2: 0.0002810442480646695   Iteration 4 of 100, tot loss = 3.3720319867134094, l1: 7.992393238964723e-05, l2: 0.0002572792691353243   Iteration 5 of 100, tot loss = 3.8008187770843507, l1: 9.2220935766818e-05, l2: 0.0002878609433537349   Iteration 6 of 100, tot loss = 3.8355437517166138, l1: 9.683591088105459e-05, l2: 0.00028671846909370896   Iteration 7 of 100, tot loss = 4.19453855923244, l1: 0.00010088468167980733, l2: 0.00031856917900898097   Iteration 8 of 100, tot loss = 4.28616800904274, l1: 0.00010030172006736393, l2: 0.00032831508906383533   Iteration 9 of 100, tot loss = 4.290302197138469, l1: 0.0001010759729575107, l2: 0.0003279542547210844   Iteration 10 of 100, tot loss = 4.220933818817139, l1: 9.872886912489775e-05, l2: 0.00032336452131858094   Iteration 11 of 100, tot loss = 4.503807024522261, l1: 0.00010330718975059095, l2: 0.00034707351667086846   Iteration 12 of 100, tot loss = 4.533761143684387, l1: 0.00010506728024968955, l2: 0.00034830883790467243   Iteration 13 of 100, tot loss = 4.448510023263784, l1: 0.00010102782611144133, l2: 0.0003438231797190383   Iteration 14 of 100, tot loss = 4.425169467926025, l1: 0.00010191614325906682, l2: 0.00034060080465029126   Iteration 15 of 100, tot loss = 4.246643495559693, l1: 9.905779297696427e-05, l2: 0.0003256065581808798   Iteration 16 of 100, tot loss = 4.0904544442892075, l1: 9.5147974207066e-05, l2: 0.0003138974711873743   Iteration 17 of 100, tot loss = 4.121245959225823, l1: 9.707842015332597e-05, l2: 0.00031504617543439106   Iteration 18 of 100, tot loss = 4.101079318258497, l1: 9.766991974578964e-05, l2: 0.0003124380111532648   Iteration 19 of 100, tot loss = 4.040243475060714, l1: 9.674371310546504e-05, l2: 0.0003072806326886884   Iteration 20 of 100, tot loss = 4.093822717666626, l1: 9.77973726548953e-05, l2: 0.000311584895462147   Iteration 21 of 100, tot loss = 4.074895711172195, l1: 9.774574088320757e-05, l2: 0.0003097438266628333   Iteration 22 of 100, tot loss = 4.091176238926974, l1: 9.74508187606592e-05, l2: 0.00031166680161153306   Iteration 23 of 100, tot loss = 4.169514438380366, l1: 9.873438102658838e-05, l2: 0.0003182170580460123   Iteration 24 of 100, tot loss = 4.154187162717183, l1: 9.747237830500428e-05, l2: 0.00031794633438645786   Iteration 25 of 100, tot loss = 4.130210304260254, l1: 9.589310910087079e-05, l2: 0.00031712791736936195   Iteration 26 of 100, tot loss = 4.100452239696796, l1: 9.447576947143086e-05, l2: 0.00031556945060191746   Iteration 27 of 100, tot loss = 4.08219889358238, l1: 9.422494272089184e-05, l2: 0.0003139949433346003   Iteration 28 of 100, tot loss = 4.055755129882267, l1: 9.390678444885583e-05, l2: 0.00031166872499202976   Iteration 29 of 100, tot loss = 4.127911000416197, l1: 9.379278068583266e-05, l2: 0.0003189983161001723   Iteration 30 of 100, tot loss = 4.201400176684062, l1: 9.505084284076778e-05, l2: 0.0003250891708982332   Iteration 31 of 100, tot loss = 4.245590617579799, l1: 9.614366295394458e-05, l2: 0.00032841539523947324   Iteration 32 of 100, tot loss = 4.224195554852486, l1: 9.496762004346238e-05, l2: 0.00032745193198024936   Iteration 33 of 100, tot loss = 4.224711259206136, l1: 9.544034401187673e-05, l2: 0.0003270307786420524   Iteration 34 of 100, tot loss = 4.226358399671667, l1: 9.618725893875201e-05, l2: 0.0003264485787761206   Iteration 35 of 100, tot loss = 4.1997662544250485, l1: 9.564860229147599e-05, l2: 0.0003243280215039184   Iteration 36 of 100, tot loss = 4.195353441768223, l1: 9.54016151000461e-05, l2: 0.000324133727594421   Iteration 37 of 100, tot loss = 4.210426601203713, l1: 9.565521287259872e-05, l2: 0.0003253874458243592   Iteration 38 of 100, tot loss = 4.224254369735718, l1: 9.648842521863499e-05, l2: 0.00032593701000319255   Iteration 39 of 100, tot loss = 4.20137148637038, l1: 9.602689440212905e-05, l2: 0.00032411025257225934   Iteration 40 of 100, tot loss = 4.139013338088989, l1: 9.453504062548746e-05, l2: 0.00031936629147821805   Iteration 41 of 100, tot loss = 4.141784609817877, l1: 9.511483328490768e-05, l2: 0.0003190636256831789   Iteration 42 of 100, tot loss = 4.190119368689401, l1: 9.612405933910937e-05, l2: 0.0003228878761417166   Iteration 43 of 100, tot loss = 4.249385944632596, l1: 9.760729001353084e-05, l2: 0.0003273313034929573   Iteration 44 of 100, tot loss = 4.230302588506178, l1: 9.684466997648336e-05, l2: 0.0003261855882473438   Iteration 45 of 100, tot loss = 4.284106694327461, l1: 9.8186697707408e-05, l2: 0.00033022397013458734   Iteration 46 of 100, tot loss = 4.273401275925014, l1: 9.771667986490723e-05, l2: 0.00032962344662139555   Iteration 47 of 100, tot loss = 4.290968727558218, l1: 9.78602389644019e-05, l2: 0.0003312366320739223   Iteration 48 of 100, tot loss = 4.310368334253629, l1: 9.801595130435696e-05, l2: 0.00033302088058917434   Iteration 49 of 100, tot loss = 4.3710934726559385, l1: 9.902969358503172e-05, l2: 0.00033807965237243404   Iteration 50 of 100, tot loss = 4.389413704872132, l1: 9.927166640409268e-05, l2: 0.0003396697020798456   Iteration 51 of 100, tot loss = 4.365380899578917, l1: 9.896519680064647e-05, l2: 0.0003375728914511901   Iteration 52 of 100, tot loss = 4.393285884306981, l1: 9.944916424641493e-05, l2: 0.0003398794230563978   Iteration 53 of 100, tot loss = 4.402416485660481, l1: 9.969991515390575e-05, l2: 0.0003405417323075147   Iteration 54 of 100, tot loss = 4.415871545120522, l1: 9.973539420537202e-05, l2: 0.0003418517588687354   Iteration 55 of 100, tot loss = 4.391779323057695, l1: 9.931053632912649e-05, l2: 0.00033986739462389697   Iteration 56 of 100, tot loss = 4.423256631408419, l1: 9.982291729621855e-05, l2: 0.0003425027445181123   Iteration 57 of 100, tot loss = 4.515372974830761, l1: 0.00010096680617017116, l2: 0.00035057049037851885   Iteration 58 of 100, tot loss = 4.519134106307194, l1: 0.00010099565331294234, l2: 0.00035091775611488567   Iteration 59 of 100, tot loss = 4.532061136375039, l1: 0.00010088104589435793, l2: 0.00035232506626802575   Iteration 60 of 100, tot loss = 4.564079113801321, l1: 0.00010144841932439401, l2: 0.0003549594911116098   Iteration 61 of 100, tot loss = 4.555672782366393, l1: 0.00010125832698979705, l2: 0.00035430895024815144   Iteration 62 of 100, tot loss = 4.545005390720982, l1: 0.00010099647601412969, l2: 0.0003535040623025833   Iteration 63 of 100, tot loss = 4.563715896909199, l1: 0.00010163832080757452, l2: 0.00035473326762070113   Iteration 64 of 100, tot loss = 4.564644396305084, l1: 0.00010164219133912411, l2: 0.0003548222470044493   Iteration 65 of 100, tot loss = 4.590232254908635, l1: 0.00010221352814159428, l2: 0.0003568096962855914   Iteration 66 of 100, tot loss = 4.5576935031197285, l1: 0.00010163378829020076, l2: 0.0003541355610209531   Iteration 67 of 100, tot loss = 4.5773496698977345, l1: 0.0001020541469133876, l2: 0.00035568081881110766   Iteration 68 of 100, tot loss = 4.576563533614664, l1: 0.00010155514920530978, l2: 0.00035610120315087606   Iteration 69 of 100, tot loss = 4.559346237044403, l1: 0.00010128001409559174, l2: 0.00035465460857359585   Iteration 70 of 100, tot loss = 4.547609611919948, l1: 0.00010110796824197418, l2: 0.00035365299211532277   Iteration 71 of 100, tot loss = 4.540565245588061, l1: 0.00010095269056069861, l2: 0.00035310383305251993   Iteration 72 of 100, tot loss = 4.542543560266495, l1: 0.00010039741624091726, l2: 0.00035385693900025217   Iteration 73 of 100, tot loss = 4.576583160112982, l1: 0.00010071141967846539, l2: 0.0003569468951342334   Iteration 74 of 100, tot loss = 4.577643049729837, l1: 0.00010108837562477895, l2: 0.000356675927882001   Iteration 75 of 100, tot loss = 4.599210151036581, l1: 0.00010151833014485115, l2: 0.0003584026839234866   Iteration 76 of 100, tot loss = 4.631822400971463, l1: 0.0001021466677660723, l2: 0.0003610355717300033   Iteration 77 of 100, tot loss = 4.648271508031077, l1: 0.00010256586732817572, l2: 0.0003622612830322514   Iteration 78 of 100, tot loss = 4.66324839530847, l1: 0.00010301295706318715, l2: 0.00036331188172110927   Iteration 79 of 100, tot loss = 4.669108520580243, l1: 0.00010333551417025325, l2: 0.0003635753376248788   Iteration 80 of 100, tot loss = 4.695779809355736, l1: 0.00010386458661741927, l2: 0.0003657133939668711   Iteration 81 of 100, tot loss = 4.681429091794991, l1: 0.00010373591022123295, l2: 0.00036440699863934565   Iteration 82 of 100, tot loss = 4.695734215945732, l1: 0.00010370277533827281, l2: 0.0003658706458426001   Iteration 83 of 100, tot loss = 4.68240294686283, l1: 0.00010354416911260236, l2: 0.00036469612511489204   Iteration 84 of 100, tot loss = 4.687006953216734, l1: 0.00010363000954038441, l2: 0.00036507068536940056   Iteration 85 of 100, tot loss = 4.677458033842199, l1: 0.00010359550874187228, l2: 0.0003641502941130003   Iteration 86 of 100, tot loss = 4.704567848249924, l1: 0.00010413757804916564, l2: 0.00036631920603361133   Iteration 87 of 100, tot loss = 4.7190215450593795, l1: 0.00010442737427716754, l2: 0.00036747477928259485   Iteration 88 of 100, tot loss = 4.730948962948539, l1: 0.00010444606364630586, l2: 0.00036864883135091407   Iteration 89 of 100, tot loss = 4.764131133476, l1: 0.00010486987414194291, l2: 0.0003715432375725725   Iteration 90 of 100, tot loss = 4.764623816808065, l1: 0.00010469665754094926, l2: 0.0003717657223104551   Iteration 91 of 100, tot loss = 4.7945622611831835, l1: 0.00010536598271286054, l2: 0.0003740902417460917   Iteration 92 of 100, tot loss = 4.792836137439894, l1: 0.0001053674606500072, l2: 0.0003739161512266551   Iteration 93 of 100, tot loss = 4.787324013248567, l1: 0.000105579728223083, l2: 0.000373152671028204   Iteration 94 of 100, tot loss = 4.773777418948234, l1: 0.0001053092224112256, l2: 0.00037206851738254894   Iteration 95 of 100, tot loss = 4.748440084959332, l1: 0.0001049888572245101, l2: 0.0003698551493519182   Iteration 96 of 100, tot loss = 4.743557487924893, l1: 0.00010484805541940052, l2: 0.0003695076917059244   Iteration 97 of 100, tot loss = 4.714264835279012, l1: 0.0001042790396034612, l2: 0.00036714744228166383   Iteration 98 of 100, tot loss = 4.739988424340073, l1: 0.00010449860884382731, l2: 0.000369500232149481   Iteration 99 of 100, tot loss = 4.729504505793254, l1: 0.00010422344811641696, l2: 0.00036872700119868504   Iteration 100 of 100, tot loss = 4.726753404140473, l1: 0.00010407953515823465, l2: 0.00036859580424788876
   End of epoch 1124; saving model... 

Epoch 1125 of 2000
   Iteration 1 of 100, tot loss = 6.510014533996582, l1: 0.0001160866959253326, l2: 0.0005349147832021117   Iteration 2 of 100, tot loss = 5.7855000495910645, l1: 0.00011315624578855932, l2: 0.00046539376489818096   Iteration 3 of 100, tot loss = 5.132833957672119, l1: 0.00011409254269286369, l2: 0.0003991908549020688   Iteration 4 of 100, tot loss = 5.012537240982056, l1: 0.00011783851914515253, l2: 0.0003834152012132108   Iteration 5 of 100, tot loss = 4.706974220275879, l1: 0.00011023974657291547, l2: 0.0003604576748330146   Iteration 6 of 100, tot loss = 4.824011564254761, l1: 0.000110427595548875, l2: 0.000371973563839371   Iteration 7 of 100, tot loss = 4.888854435511997, l1: 0.0001118643839228233, l2: 0.00037702106055803597   Iteration 8 of 100, tot loss = 4.622922986745834, l1: 0.00010631498207658296, l2: 0.00035597731584857684   Iteration 9 of 100, tot loss = 4.484412484698826, l1: 0.00010617379302857444, l2: 0.0003422674554814067   Iteration 10 of 100, tot loss = 4.3178609848022464, l1: 0.0001032142732583452, l2: 0.00032857182377483697   Iteration 11 of 100, tot loss = 4.1294801451943135, l1: 9.867231677741405e-05, l2: 0.00031427569708532906   Iteration 12 of 100, tot loss = 3.9897930224736533, l1: 9.576811711061357e-05, l2: 0.0003032111841700195   Iteration 13 of 100, tot loss = 3.9929063136761007, l1: 9.4853439245176e-05, l2: 0.0003044371920763157   Iteration 14 of 100, tot loss = 3.8633592299052646, l1: 9.211569574420406e-05, l2: 0.00029422022635117173   Iteration 15 of 100, tot loss = 3.92581893603007, l1: 9.420334123812305e-05, l2: 0.0002983785525429994   Iteration 16 of 100, tot loss = 3.9120047837495804, l1: 9.47773803545715e-05, l2: 0.0002964230989164207   Iteration 17 of 100, tot loss = 4.009792930939618, l1: 9.600163513609646e-05, l2: 0.00030497765949215084   Iteration 18 of 100, tot loss = 3.916531615787082, l1: 9.467670295432779e-05, l2: 0.000296976460958831   Iteration 19 of 100, tot loss = 3.9335695818850867, l1: 9.505636549683434e-05, l2: 0.0002983005956354502   Iteration 20 of 100, tot loss = 4.022201514244079, l1: 9.638630126573844e-05, l2: 0.0003058338537812233   Iteration 21 of 100, tot loss = 4.03267054330735, l1: 9.576470568260577e-05, l2: 0.0003075023525438848   Iteration 22 of 100, tot loss = 4.1195948123931885, l1: 9.722660665416201e-05, l2: 0.0003147328798976642   Iteration 23 of 100, tot loss = 4.292179625967274, l1: 9.818407646810596e-05, l2: 0.0003310338930611539   Iteration 24 of 100, tot loss = 4.284378270308177, l1: 9.762008918793678e-05, l2: 0.0003308177438157145   Iteration 25 of 100, tot loss = 4.2701489448547365, l1: 9.586789747118019e-05, l2: 0.00033114700228907166   Iteration 26 of 100, tot loss = 4.238622509516203, l1: 9.49468369351682e-05, l2: 0.0003289154186719455   Iteration 27 of 100, tot loss = 4.246878350222552, l1: 9.377064203295891e-05, l2: 0.0003309171976676832   Iteration 28 of 100, tot loss = 4.25356182881764, l1: 9.35578377071319e-05, l2: 0.00033179835008922964   Iteration 29 of 100, tot loss = 4.251518372831674, l1: 9.340918203916175e-05, l2: 0.0003317426589060703   Iteration 30 of 100, tot loss = 4.20560352007548, l1: 9.205407198654332e-05, l2: 0.0003285062834038399   Iteration 31 of 100, tot loss = 4.2413224404858, l1: 9.190553534990027e-05, l2: 0.000332226712470724   Iteration 32 of 100, tot loss = 4.165151242166758, l1: 9.089224295166787e-05, l2: 0.00032562288470217027   Iteration 33 of 100, tot loss = 4.1510915358861284, l1: 9.108031680983858e-05, l2: 0.00032402884045785123   Iteration 34 of 100, tot loss = 4.184310131213245, l1: 9.248303424957318e-05, l2: 0.00032594798283432335   Iteration 35 of 100, tot loss = 4.2374658209936955, l1: 9.364549332531169e-05, l2: 0.00033010109244579713   Iteration 36 of 100, tot loss = 4.247925920618905, l1: 9.381708453778022e-05, l2: 0.0003309755112342019   Iteration 37 of 100, tot loss = 4.201351645830515, l1: 9.33293582371285e-05, l2: 0.00032680581018605545   Iteration 38 of 100, tot loss = 4.20894845222172, l1: 9.367716330942992e-05, l2: 0.00032721768595939994   Iteration 39 of 100, tot loss = 4.186913456672277, l1: 9.323348785983399e-05, l2: 0.0003254578623455018   Iteration 40 of 100, tot loss = 4.187196078896522, l1: 9.312678721471457e-05, l2: 0.00032559282481088304   Iteration 41 of 100, tot loss = 4.15522729187477, l1: 9.233772386612212e-05, l2: 0.000323185009353726   Iteration 42 of 100, tot loss = 4.179606735706329, l1: 9.30361686533572e-05, l2: 0.00032492450832034505   Iteration 43 of 100, tot loss = 4.244879725367524, l1: 9.428611190023096e-05, l2: 0.000330201864545775   Iteration 44 of 100, tot loss = 4.237576850435951, l1: 9.39534290657982e-05, l2: 0.00032980425947409293   Iteration 45 of 100, tot loss = 4.282539661725362, l1: 9.440164778627352e-05, l2: 0.00033385232155625194   Iteration 46 of 100, tot loss = 4.2862555125485295, l1: 9.39594717593813e-05, l2: 0.00033466608210406065   Iteration 47 of 100, tot loss = 4.282809407153028, l1: 9.379020411870442e-05, l2: 0.0003344907389428625   Iteration 48 of 100, tot loss = 4.238932845493157, l1: 9.265392623092339e-05, l2: 0.0003312393606999346   Iteration 49 of 100, tot loss = 4.280585967764562, l1: 9.378385710784196e-05, l2: 0.00033427474186375583   Iteration 50 of 100, tot loss = 4.295936143398285, l1: 9.37481282016961e-05, l2: 0.000335845488589257   Iteration 51 of 100, tot loss = 4.3120298455743225, l1: 9.402947641016605e-05, l2: 0.0003371735105711017   Iteration 52 of 100, tot loss = 4.287644888346012, l1: 9.402206181291079e-05, l2: 0.0003347424292139924   Iteration 53 of 100, tot loss = 4.311660422469085, l1: 9.451094112317253e-05, l2: 0.0003366551042385048   Iteration 54 of 100, tot loss = 4.2901672411847995, l1: 9.417596009842344e-05, l2: 0.00033484076697344855   Iteration 55 of 100, tot loss = 4.313866977258162, l1: 9.498963237009858e-05, l2: 0.0003363970682998611   Iteration 56 of 100, tot loss = 4.33849428168365, l1: 9.552468684757644e-05, l2: 0.0003383247442668237   Iteration 57 of 100, tot loss = 4.312884454141583, l1: 9.537131349166055e-05, l2: 0.0003359171346426337   Iteration 58 of 100, tot loss = 4.344583040681378, l1: 9.551899491835402e-05, l2: 0.0003389393120947904   Iteration 59 of 100, tot loss = 4.325777181124283, l1: 9.474741874343589e-05, l2: 0.0003378303022068625   Iteration 60 of 100, tot loss = 4.356951353947322, l1: 9.545149763046841e-05, l2: 0.00034024364091843985   Iteration 61 of 100, tot loss = 4.348874543533951, l1: 9.514723414675425e-05, l2: 0.00033974022362152207   Iteration 62 of 100, tot loss = 4.374487252004685, l1: 9.568734863170258e-05, l2: 0.000341761379405075   Iteration 63 of 100, tot loss = 4.382191625852434, l1: 9.607672334476818e-05, l2: 0.0003421424425184904   Iteration 64 of 100, tot loss = 4.363439919427037, l1: 9.587374307784557e-05, l2: 0.00034047025201289216   Iteration 65 of 100, tot loss = 4.364061364760766, l1: 9.606297910347796e-05, l2: 0.00034034316060849683   Iteration 66 of 100, tot loss = 4.333390458063646, l1: 9.523119600392722e-05, l2: 0.0003381078530572157   Iteration 67 of 100, tot loss = 4.329466298444947, l1: 9.511247073750206e-05, l2: 0.00033783416221189357   Iteration 68 of 100, tot loss = 4.321397450040369, l1: 9.51295881296734e-05, l2: 0.0003370101598300286   Iteration 69 of 100, tot loss = 4.291629554568857, l1: 9.44621288394758e-05, l2: 0.00033470082954685813   Iteration 70 of 100, tot loss = 4.307426280634743, l1: 9.482622502088946e-05, l2: 0.00033591640551873883   Iteration 71 of 100, tot loss = 4.299833853479842, l1: 9.493620479627e-05, l2: 0.00033504718280902366   Iteration 72 of 100, tot loss = 4.31155172155963, l1: 9.458756913874013e-05, l2: 0.0003365676054980011   Iteration 73 of 100, tot loss = 4.293851867114028, l1: 9.431641109125076e-05, l2: 0.0003350687780168724   Iteration 74 of 100, tot loss = 4.285430687504846, l1: 9.38666143574173e-05, l2: 0.0003346764565936315   Iteration 75 of 100, tot loss = 4.299222823778789, l1: 9.440321999136358e-05, l2: 0.0003355190642954161   Iteration 76 of 100, tot loss = 4.3003870735042975, l1: 9.442291392540363e-05, l2: 0.0003356157949862168   Iteration 77 of 100, tot loss = 4.280657460163166, l1: 9.412572569101928e-05, l2: 0.0003339400217874435   Iteration 78 of 100, tot loss = 4.3553656079830265, l1: 9.534670999318433e-05, l2: 0.0003401898530473073   Iteration 79 of 100, tot loss = 4.351606630071809, l1: 9.544144231307356e-05, l2: 0.00033971922298233154   Iteration 80 of 100, tot loss = 4.324556465446949, l1: 9.501009590167087e-05, l2: 0.0003374455529410625   Iteration 81 of 100, tot loss = 4.34623525319276, l1: 9.550478267713369e-05, l2: 0.00033911874476118866   Iteration 82 of 100, tot loss = 4.341054176411977, l1: 9.550388317509759e-05, l2: 0.0003386015365632796   Iteration 83 of 100, tot loss = 4.331972327577063, l1: 9.553165079293638e-05, l2: 0.00033766558395238613   Iteration 84 of 100, tot loss = 4.319629997014999, l1: 9.553787865678758e-05, l2: 0.00033642512322070876   Iteration 85 of 100, tot loss = 4.364356118090012, l1: 9.637830037664732e-05, l2: 0.00034005731371614865   Iteration 86 of 100, tot loss = 4.369183128656343, l1: 9.61849155130547e-05, l2: 0.00034073339964717973   Iteration 87 of 100, tot loss = 4.361861474212559, l1: 9.619072004389982e-05, l2: 0.00033999542971998025   Iteration 88 of 100, tot loss = 4.359593238342892, l1: 9.611325136078945e-05, l2: 0.0003398460748113311   Iteration 89 of 100, tot loss = 4.361155353235395, l1: 9.622527113095386e-05, l2: 0.0003398902667817147   Iteration 90 of 100, tot loss = 4.345018441147275, l1: 9.597150597579053e-05, l2: 0.00033853034078169407   Iteration 91 of 100, tot loss = 4.387751433875534, l1: 9.642947303031438e-05, l2: 0.0003423456725943322   Iteration 92 of 100, tot loss = 4.412914324065913, l1: 9.657730310130358e-05, l2: 0.0003447141316428315   Iteration 93 of 100, tot loss = 4.4174526340218, l1: 9.655212997258591e-05, l2: 0.00034519313582374406   Iteration 94 of 100, tot loss = 4.441625931161515, l1: 9.703702473532448e-05, l2: 0.0003471255707715519   Iteration 95 of 100, tot loss = 4.431518450536226, l1: 9.676839116237764e-05, l2: 0.0003463834562449177   Iteration 96 of 100, tot loss = 4.409718979150057, l1: 9.631616338386569e-05, l2: 0.00034465573692917434   Iteration 97 of 100, tot loss = 4.412092497668316, l1: 9.615221477867363e-05, l2: 0.00034505703736135025   Iteration 98 of 100, tot loss = 4.4043989072040635, l1: 9.59849856705975e-05, l2: 0.00034445490746293217   Iteration 99 of 100, tot loss = 4.401897848254502, l1: 9.607904741477051e-05, l2: 0.0003441107395279099   Iteration 100 of 100, tot loss = 4.398463944196701, l1: 9.607046071323566e-05, l2: 0.0003437759360531345
   End of epoch 1125; saving model... 

Epoch 1126 of 2000
   Iteration 1 of 100, tot loss = 4.492249011993408, l1: 0.00012357452942524105, l2: 0.0003256503550801426   Iteration 2 of 100, tot loss = 3.5223039388656616, l1: 9.462684465688653e-05, l2: 0.0002576035403762944   Iteration 3 of 100, tot loss = 5.096282561620076, l1: 0.00011709852212031062, l2: 0.0003925297302581991   Iteration 4 of 100, tot loss = 5.171177804470062, l1: 0.00011825227011286188, l2: 0.00039886550439405255   Iteration 5 of 100, tot loss = 4.848022747039795, l1: 0.00011354752205079421, l2: 0.00037125474482309074   Iteration 6 of 100, tot loss = 4.743113040924072, l1: 0.00011334986023333234, l2: 0.0003609614383700925   Iteration 7 of 100, tot loss = 4.7713775634765625, l1: 0.00011135700749166842, l2: 0.0003657807433877939   Iteration 8 of 100, tot loss = 5.081230401992798, l1: 0.00011464910494396463, l2: 0.00039347393067146186   Iteration 9 of 100, tot loss = 5.1100490358140735, l1: 0.00011249032185231853, l2: 0.000398514577278143   Iteration 10 of 100, tot loss = 5.132460498809815, l1: 0.00011011712267645634, l2: 0.00040312892087968066   Iteration 11 of 100, tot loss = 5.0437915108420635, l1: 0.00011086031074889682, l2: 0.0003935188354014165   Iteration 12 of 100, tot loss = 5.137858589490254, l1: 0.00011143176379846409, l2: 0.00040235409323940985   Iteration 13 of 100, tot loss = 5.1049203139085035, l1: 0.00011174978751044434, l2: 0.00039874224211626616   Iteration 14 of 100, tot loss = 5.42858920778547, l1: 0.00011726198863470927, l2: 0.0004255969303942818   Iteration 15 of 100, tot loss = 5.409045060475667, l1: 0.00011517575427812213, l2: 0.00042572874711671225   Iteration 16 of 100, tot loss = 5.296060875058174, l1: 0.00011449348630776512, l2: 0.0004151125967837288   Iteration 17 of 100, tot loss = 5.3749126125784485, l1: 0.00011506484973805902, l2: 0.0004224264095468885   Iteration 18 of 100, tot loss = 5.327748311890496, l1: 0.0001143756372685958, l2: 0.0004183991930201753   Iteration 19 of 100, tot loss = 5.343179614920365, l1: 0.00011458603847514544, l2: 0.00041973192358119905   Iteration 20 of 100, tot loss = 5.2596065402030945, l1: 0.00011465421848697588, l2: 0.0004113064358534757   Iteration 21 of 100, tot loss = 5.269039278938656, l1: 0.0001145328857265191, l2: 0.00041237104215681377   Iteration 22 of 100, tot loss = 5.259199781851335, l1: 0.0001129512119487944, l2: 0.0004129687658860348   Iteration 23 of 100, tot loss = 5.202879636184029, l1: 0.00011109141727818339, l2: 0.00040919654534461546   Iteration 24 of 100, tot loss = 5.208533604939778, l1: 0.00011193799734125302, l2: 0.0004089153620346527   Iteration 25 of 100, tot loss = 5.352329368591309, l1: 0.00011345772916683928, l2: 0.00042177520517725495   Iteration 26 of 100, tot loss = 5.27048331957597, l1: 0.0001123734963095138, l2: 0.00041467483354678663   Iteration 27 of 100, tot loss = 5.268479108810425, l1: 0.00011202099375185315, l2: 0.0004148269163798196   Iteration 28 of 100, tot loss = 5.2395166754722595, l1: 0.00011196265274650483, l2: 0.00041198901297840554   Iteration 29 of 100, tot loss = 5.28613227811353, l1: 0.00011339249387423604, l2: 0.00041522073172081003   Iteration 30 of 100, tot loss = 5.234621326128642, l1: 0.0001126778578812567, l2: 0.00041078427311731503   Iteration 31 of 100, tot loss = 5.263819517627839, l1: 0.0001139882214990775, l2: 0.00041239372700557954   Iteration 32 of 100, tot loss = 5.239834137260914, l1: 0.00011337291812196781, l2: 0.00041061049205382005   Iteration 33 of 100, tot loss = 5.133435122894518, l1: 0.0001113476520748174, l2: 0.00040199585694636244   Iteration 34 of 100, tot loss = 5.124710409080281, l1: 0.00011109541864083011, l2: 0.00040137561882147565   Iteration 35 of 100, tot loss = 5.1247107676097325, l1: 0.00011066533386058706, l2: 0.00040180573996622114   Iteration 36 of 100, tot loss = 5.055339121156269, l1: 0.00010944449271240349, l2: 0.00039608941652760323   Iteration 37 of 100, tot loss = 5.054061280714499, l1: 0.0001099705408017365, l2: 0.0003954355842496797   Iteration 38 of 100, tot loss = 4.950758087007623, l1: 0.00010788107005952809, l2: 0.0003871947356904122   Iteration 39 of 100, tot loss = 4.969050523562309, l1: 0.00010767878195157465, l2: 0.0003892262667739907   Iteration 40 of 100, tot loss = 5.035210531949997, l1: 0.00010889691093325382, l2: 0.00039462413915316576   Iteration 41 of 100, tot loss = 5.0220986168559, l1: 0.00010875256933703473, l2: 0.0003934572898021831   Iteration 42 of 100, tot loss = 4.9641384454000566, l1: 0.00010765741985995278, l2: 0.00038875642245069945   Iteration 43 of 100, tot loss = 5.001637198204218, l1: 0.000107635990452552, l2: 0.0003925277272829526   Iteration 44 of 100, tot loss = 4.971033768220381, l1: 0.00010757477155659052, l2: 0.0003895286033432719   Iteration 45 of 100, tot loss = 5.015843211279975, l1: 0.00010782405087310407, l2: 0.00039376026866698845   Iteration 46 of 100, tot loss = 5.1086409921231475, l1: 0.00010864878498503695, l2: 0.0004022153136801258   Iteration 47 of 100, tot loss = 5.096200699501849, l1: 0.00010861782651231129, l2: 0.00040100224314881686   Iteration 48 of 100, tot loss = 5.0832472940286, l1: 0.00010841058883670485, l2: 0.0003999141399617656   Iteration 49 of 100, tot loss = 5.084605761936733, l1: 0.00010868411972092426, l2: 0.00039977645614345993   Iteration 50 of 100, tot loss = 5.056479811668396, l1: 0.00010833736625500024, l2: 0.0003973106146440841   Iteration 51 of 100, tot loss = 5.050777608273076, l1: 0.00010815683344844729, l2: 0.0003969209270401146   Iteration 52 of 100, tot loss = 5.053068908361288, l1: 0.0001078821479985401, l2: 0.00039742474221454287   Iteration 53 of 100, tot loss = 5.008020580939527, l1: 0.00010700691653358451, l2: 0.00039379514082783025   Iteration 54 of 100, tot loss = 4.964744276470608, l1: 0.00010643322107002691, l2: 0.0003900412056181166   Iteration 55 of 100, tot loss = 4.947930760817094, l1: 0.00010663886721224778, l2: 0.00038815420808863236   Iteration 56 of 100, tot loss = 4.988942129271371, l1: 0.00010733535197817088, l2: 0.00039155885990892   Iteration 57 of 100, tot loss = 4.989427683646219, l1: 0.00010738281248210881, l2: 0.0003915599546305378   Iteration 58 of 100, tot loss = 4.977099985911928, l1: 0.00010757086871810064, l2: 0.0003901391289124651   Iteration 59 of 100, tot loss = 4.97709062543966, l1: 0.00010740186109491703, l2: 0.00039030720037802803   Iteration 60 of 100, tot loss = 4.97187868754069, l1: 0.00010675740474349974, l2: 0.0003904304627212696   Iteration 61 of 100, tot loss = 4.943702983074501, l1: 0.00010670405515797864, l2: 0.00038766624195096615   Iteration 62 of 100, tot loss = 4.908727945819978, l1: 0.00010592706201996114, l2: 0.0003849457315218094   Iteration 63 of 100, tot loss = 4.930679147205655, l1: 0.00010625737564941306, l2: 0.0003868105383830086   Iteration 64 of 100, tot loss = 4.9063351564109325, l1: 0.00010587520955596119, l2: 0.00038475830547213263   Iteration 65 of 100, tot loss = 4.887676022602961, l1: 0.00010563949337945534, l2: 0.0003831281082686753   Iteration 66 of 100, tot loss = 4.870090119766466, l1: 0.00010539831407985537, l2: 0.0003816106973593405   Iteration 67 of 100, tot loss = 4.853477755589272, l1: 0.000105309188704757, l2: 0.0003800385863148371   Iteration 68 of 100, tot loss = 4.866642994039199, l1: 0.00010506470896550181, l2: 0.0003815995897799867   Iteration 69 of 100, tot loss = 4.862439266149549, l1: 0.00010517602303466909, l2: 0.00038106790284012055   Iteration 70 of 100, tot loss = 4.847545637403216, l1: 0.0001051456448164702, l2: 0.00037960891807285534   Iteration 71 of 100, tot loss = 4.825173408212796, l1: 0.0001044252404285608, l2: 0.00037809209935006383   Iteration 72 of 100, tot loss = 4.8094183339012995, l1: 0.00010435465093097364, l2: 0.00037658718166413665   Iteration 73 of 100, tot loss = 4.780464642668424, l1: 0.00010405075989507317, l2: 0.0003739957036275085   Iteration 74 of 100, tot loss = 4.774396155331586, l1: 0.00010378436422183154, l2: 0.00037365525035105794   Iteration 75 of 100, tot loss = 4.792208360036214, l1: 0.00010409482715961834, l2: 0.00037512600790554036   Iteration 76 of 100, tot loss = 4.788375735282898, l1: 0.00010422960871150473, l2: 0.00037460796413238553   Iteration 77 of 100, tot loss = 4.779196497681853, l1: 0.00010414555417101637, l2: 0.000373774094925891   Iteration 78 of 100, tot loss = 4.79164764820001, l1: 0.00010419989979336373, l2: 0.0003749648643021758   Iteration 79 of 100, tot loss = 4.766177409811865, l1: 0.00010349820375359983, l2: 0.0003731195364350783   Iteration 80 of 100, tot loss = 4.774437734484673, l1: 0.00010350564616601332, l2: 0.0003739381269042497   Iteration 81 of 100, tot loss = 4.751773513393638, l1: 0.00010306487568829461, l2: 0.00037211247534422136   Iteration 82 of 100, tot loss = 4.727103064699871, l1: 0.00010277479083104082, l2: 0.00036993551510954047   Iteration 83 of 100, tot loss = 4.71940161233925, l1: 0.00010285771538500663, l2: 0.00036908244554245147   Iteration 84 of 100, tot loss = 4.698651450020926, l1: 0.00010218967712543894, l2: 0.0003676754675411974   Iteration 85 of 100, tot loss = 4.693410099253935, l1: 0.0001021019925674944, l2: 0.00036723901729975993   Iteration 86 of 100, tot loss = 4.671716606894205, l1: 0.00010152690053875503, l2: 0.00036564476004271056   Iteration 87 of 100, tot loss = 4.669590966454868, l1: 0.00010157134961811879, l2: 0.00036538774702944205   Iteration 88 of 100, tot loss = 4.669891655445099, l1: 0.00010155986377536796, l2: 0.00036542930187847975   Iteration 89 of 100, tot loss = 4.667514265253303, l1: 0.00010189123807659405, l2: 0.00036486018883336463   Iteration 90 of 100, tot loss = 4.682804367277357, l1: 0.00010201996196378281, l2: 0.0003662604754531963   Iteration 91 of 100, tot loss = 4.672097656753037, l1: 0.00010194460272751902, l2: 0.0003652651634343623   Iteration 92 of 100, tot loss = 4.665687654329383, l1: 0.00010175661756638083, l2: 0.0003648121482515505   Iteration 93 of 100, tot loss = 4.671420466515325, l1: 0.00010177078862613198, l2: 0.000365371258506271   Iteration 94 of 100, tot loss = 4.647117784682741, l1: 0.00010139310406304232, l2: 0.0003633186749945533   Iteration 95 of 100, tot loss = 4.636019515991211, l1: 0.0001014683535399136, l2: 0.00036213359869036237   Iteration 96 of 100, tot loss = 4.636862536271413, l1: 0.00010176640584328804, l2: 0.00036191984842541086   Iteration 97 of 100, tot loss = 4.645246604054244, l1: 0.00010214158134761786, l2: 0.00036238307990229775   Iteration 98 of 100, tot loss = 4.645953718496829, l1: 0.00010206107020037183, l2: 0.0003625343028665045   Iteration 99 of 100, tot loss = 4.631025116853039, l1: 0.00010196302919395766, l2: 0.0003611394836252198   Iteration 100 of 100, tot loss = 4.6286234998703, l1: 0.00010195960374403512, l2: 0.00036090274748858066
   End of epoch 1126; saving model... 

Epoch 1127 of 2000
   Iteration 1 of 100, tot loss = 3.557957887649536, l1: 7.954738975968212e-05, l2: 0.00027624840731732547   Iteration 2 of 100, tot loss = 4.135163187980652, l1: 9.462212983635254e-05, l2: 0.00031889419187791646   Iteration 3 of 100, tot loss = 3.7374022801717124, l1: 8.549239282729104e-05, l2: 0.0002882478293031454   Iteration 4 of 100, tot loss = 4.706719756126404, l1: 9.573555689712521e-05, l2: 0.00037493641139008105   Iteration 5 of 100, tot loss = 4.6246637344360355, l1: 9.489076037425548e-05, l2: 0.00036757560446858405   Iteration 6 of 100, tot loss = 4.562150875727336, l1: 0.00010002234921557829, l2: 0.00035619273451933015   Iteration 7 of 100, tot loss = 4.454525027956281, l1: 9.694963020072984e-05, l2: 0.0003485028698508229   Iteration 8 of 100, tot loss = 4.214584767818451, l1: 9.030396495290915e-05, l2: 0.00033115450969489757   Iteration 9 of 100, tot loss = 4.051393985748291, l1: 8.880952413568998e-05, l2: 0.00031632987277892727   Iteration 10 of 100, tot loss = 4.313286542892456, l1: 9.390311497554648e-05, l2: 0.0003374255407834426   Iteration 11 of 100, tot loss = 4.1854001608761875, l1: 9.24462798469573e-05, l2: 0.0003260937387081371   Iteration 12 of 100, tot loss = 4.299003144105275, l1: 9.418184132906997e-05, l2: 0.00033571847598068416   Iteration 13 of 100, tot loss = 4.237479631717388, l1: 9.539037413420514e-05, l2: 0.0003283575922698499   Iteration 14 of 100, tot loss = 4.258008701460702, l1: 9.675257933849934e-05, l2: 0.0003290482927695848   Iteration 15 of 100, tot loss = 4.42287917137146, l1: 0.00010005521059307891, l2: 0.000342232709711728   Iteration 16 of 100, tot loss = 4.509555950760841, l1: 0.00010152794834539236, l2: 0.0003494276506899041   Iteration 17 of 100, tot loss = 4.5711697830873375, l1: 0.00010151687777525855, l2: 0.0003556001029210165   Iteration 18 of 100, tot loss = 4.4550628662109375, l1: 0.00010055115146517185, l2: 0.0003449551384417444   Iteration 19 of 100, tot loss = 4.506064138914409, l1: 0.00010185081325624579, l2: 0.00034875560499801253   Iteration 20 of 100, tot loss = 4.48604154586792, l1: 0.00010257804588036379, l2: 0.00034602611194713975   Iteration 21 of 100, tot loss = 4.472288744790213, l1: 0.00010228036383826596, l2: 0.0003449485139710651   Iteration 22 of 100, tot loss = 4.4092864556746045, l1: 0.00010085172222964253, l2: 0.00034007692606378856   Iteration 23 of 100, tot loss = 4.456296547599461, l1: 0.00010140930570069823, l2: 0.0003442203508355938   Iteration 24 of 100, tot loss = 4.49839460849762, l1: 0.00010123627377348991, l2: 0.0003486031885889436   Iteration 25 of 100, tot loss = 4.400181684494019, l1: 9.979588750866242e-05, l2: 0.0003402222826844081   Iteration 26 of 100, tot loss = 4.399547457695007, l1: 9.815035907824774e-05, l2: 0.00034180438860158366   Iteration 27 of 100, tot loss = 4.524362166722615, l1: 0.00010038613025379298, l2: 0.0003520500888246008   Iteration 28 of 100, tot loss = 4.495627675737653, l1: 9.923440705149136e-05, l2: 0.000350328362401342   Iteration 29 of 100, tot loss = 4.59989542796694, l1: 9.998751431402494e-05, l2: 0.0003600020311845081   Iteration 30 of 100, tot loss = 4.583395926157634, l1: 0.00010034847594700599, l2: 0.0003579911198661042   Iteration 31 of 100, tot loss = 4.638958515659455, l1: 0.00010159782041521413, l2: 0.0003622980349914982   Iteration 32 of 100, tot loss = 4.611677147448063, l1: 0.00010168434425850137, l2: 0.00035948337472291314   Iteration 33 of 100, tot loss = 4.604196194446448, l1: 0.00010178921376901792, l2: 0.0003586304096229882   Iteration 34 of 100, tot loss = 4.585808326216305, l1: 0.00010174182309984805, l2: 0.00035683901317979153   Iteration 35 of 100, tot loss = 4.710892915725708, l1: 0.00010327568608252997, l2: 0.00036781360769444813   Iteration 36 of 100, tot loss = 4.702122628688812, l1: 0.00010291725640046125, l2: 0.00036729500789078884   Iteration 37 of 100, tot loss = 4.684703356510884, l1: 0.00010283329479657491, l2: 0.0003656370418202887   Iteration 38 of 100, tot loss = 4.717312154016997, l1: 0.00010361143540649209, l2: 0.00036811978076768453   Iteration 39 of 100, tot loss = 4.700254715405977, l1: 0.00010335106577631683, l2: 0.0003666744066718727   Iteration 40 of 100, tot loss = 4.723637288808822, l1: 0.00010346076041969354, l2: 0.0003689029694214696   Iteration 41 of 100, tot loss = 4.76759934425354, l1: 0.00010375101209785675, l2: 0.0003730089239713668   Iteration 42 of 100, tot loss = 4.78822800658998, l1: 0.00010405824001022015, l2: 0.00037476456262603667   Iteration 43 of 100, tot loss = 4.788196680157683, l1: 0.00010363385546614069, l2: 0.00037518581463468024   Iteration 44 of 100, tot loss = 4.774630974639546, l1: 0.0001034289318175235, l2: 0.00037403416718007065   Iteration 45 of 100, tot loss = 4.7794630103641085, l1: 0.00010361641689087265, l2: 0.0003743298852997315   Iteration 46 of 100, tot loss = 4.783376118411189, l1: 0.00010361880405347937, l2: 0.00037471880932571366   Iteration 47 of 100, tot loss = 4.801240226055714, l1: 0.00010407583915289155, l2: 0.0003760481848346783   Iteration 48 of 100, tot loss = 4.78703332444032, l1: 0.00010347827014811628, l2: 0.0003752250634837158   Iteration 49 of 100, tot loss = 4.783062852158839, l1: 0.00010341957883555347, l2: 0.0003748867070962846   Iteration 50 of 100, tot loss = 4.814625849723816, l1: 0.0001040000392094953, l2: 0.0003774625467485748   Iteration 51 of 100, tot loss = 4.939131900375965, l1: 0.00010559232909787063, l2: 0.0003883208604141449   Iteration 52 of 100, tot loss = 4.951756316881913, l1: 0.00010564292971629094, l2: 0.00038953270072502515   Iteration 53 of 100, tot loss = 5.049468350860308, l1: 0.00010721373102981334, l2: 0.00039773310160150157   Iteration 54 of 100, tot loss = 4.988876592229913, l1: 0.00010611416824017565, l2: 0.0003927734885817497   Iteration 55 of 100, tot loss = 4.96733519597487, l1: 0.00010542418822296895, l2: 0.00039130932868415997   Iteration 56 of 100, tot loss = 4.97489837876388, l1: 0.0001056899682875415, l2: 0.0003917998663902316   Iteration 57 of 100, tot loss = 4.978515740026507, l1: 0.00010524436877683017, l2: 0.0003926072015913138   Iteration 58 of 100, tot loss = 4.950774108541423, l1: 0.00010494009990813131, l2: 0.0003901373076650057   Iteration 59 of 100, tot loss = 4.961938823683787, l1: 0.00010555014609886034, l2: 0.0003906437331490945   Iteration 60 of 100, tot loss = 4.948042597373327, l1: 0.00010561537098207434, l2: 0.00038918888531043195   Iteration 61 of 100, tot loss = 4.954660515316197, l1: 0.0001056657809564714, l2: 0.00038980026791429485   Iteration 62 of 100, tot loss = 4.938477237378398, l1: 0.00010546349333326948, l2: 0.00038838422798078445   Iteration 63 of 100, tot loss = 4.962342199825105, l1: 0.00010595156362840152, l2: 0.0003902826539983618   Iteration 64 of 100, tot loss = 4.931179082021117, l1: 0.00010568148496759022, l2: 0.0003874364210787462   Iteration 65 of 100, tot loss = 4.906149033399728, l1: 0.00010524483620359276, l2: 0.00038537006520737824   Iteration 66 of 100, tot loss = 4.936747346863602, l1: 0.000105899099335974, l2: 0.0003877756332230726   Iteration 67 of 100, tot loss = 4.949991074960623, l1: 0.00010625469673610877, l2: 0.00038874440795199863   Iteration 68 of 100, tot loss = 4.936785527888467, l1: 0.00010596679476293502, l2: 0.0003877117551917977   Iteration 69 of 100, tot loss = 4.943688169769619, l1: 0.0001060850080503387, l2: 0.00038828380638733506   Iteration 70 of 100, tot loss = 4.9497692908559525, l1: 0.00010630938107039714, l2: 0.00038866754621267317   Iteration 71 of 100, tot loss = 4.947204856805398, l1: 0.00010670755446099833, l2: 0.0003880129296588383   Iteration 72 of 100, tot loss = 4.923277869820595, l1: 0.00010624539406004867, l2: 0.00038608239143892814   Iteration 73 of 100, tot loss = 4.923648750945313, l1: 0.00010583221567324235, l2: 0.00038653265761390125   Iteration 74 of 100, tot loss = 4.902018461678479, l1: 0.00010545122991921694, l2: 0.0003847506143486228   Iteration 75 of 100, tot loss = 4.948318587938944, l1: 0.00010637138004919204, l2: 0.0003884604771155864   Iteration 76 of 100, tot loss = 4.948889178665061, l1: 0.00010620925350897443, l2: 0.0003886796629031826   Iteration 77 of 100, tot loss = 4.93130243134189, l1: 0.00010583366568059684, l2: 0.00038729657611035585   Iteration 78 of 100, tot loss = 4.9782234109365024, l1: 0.00010651700008915774, l2: 0.0003913053395734049   Iteration 79 of 100, tot loss = 4.971146627317501, l1: 0.00010665592722207177, l2: 0.0003904587341637551   Iteration 80 of 100, tot loss = 5.013878281414509, l1: 0.00010730900653470599, l2: 0.00039407882068189795   Iteration 81 of 100, tot loss = 5.008966476828964, l1: 0.0001072337924855629, l2: 0.0003936628546611762   Iteration 82 of 100, tot loss = 5.003679015287539, l1: 0.00010735638960152467, l2: 0.0003930115117549487   Iteration 83 of 100, tot loss = 5.020011932016855, l1: 0.00010736365303788502, l2: 0.00039463753970508204   Iteration 84 of 100, tot loss = 5.043289878538677, l1: 0.00010752158340481055, l2: 0.0003968074039827722   Iteration 85 of 100, tot loss = 5.01433184707866, l1: 0.00010725151561714216, l2: 0.0003941816688679597   Iteration 86 of 100, tot loss = 5.026550510595011, l1: 0.00010733608660514217, l2: 0.0003953189643229856   Iteration 87 of 100, tot loss = 4.995242503867752, l1: 0.00010693936280728232, l2: 0.00039258488753660657   Iteration 88 of 100, tot loss = 4.981533089821989, l1: 0.00010648459353681325, l2: 0.00039166871548249304   Iteration 89 of 100, tot loss = 4.984649316648419, l1: 0.00010645181877346281, l2: 0.00039201311236038134   Iteration 90 of 100, tot loss = 4.960797199938033, l1: 0.0001061350007350686, l2: 0.0003899447184974431   Iteration 91 of 100, tot loss = 4.952257262481438, l1: 0.00010554452179988672, l2: 0.0003896812039394198   Iteration 92 of 100, tot loss = 4.94690997833791, l1: 0.00010545553750094057, l2: 0.0003892354597274275   Iteration 93 of 100, tot loss = 4.9431351089990265, l1: 0.00010560339139755666, l2: 0.0003887101191393931   Iteration 94 of 100, tot loss = 4.929029394971564, l1: 0.00010555122126459808, l2: 0.0003873517179794788   Iteration 95 of 100, tot loss = 4.935309295905264, l1: 0.0001057131368725095, l2: 0.0003878177926373227   Iteration 96 of 100, tot loss = 4.972405539204677, l1: 0.00010637093233375101, l2: 0.0003908696221515129   Iteration 97 of 100, tot loss = 4.981173796752064, l1: 0.00010662843240274971, l2: 0.0003914889477771832   Iteration 98 of 100, tot loss = 4.96717186485018, l1: 0.0001064516840860775, l2: 0.0003902655029823833   Iteration 99 of 100, tot loss = 4.985254390071137, l1: 0.00010686163652207316, l2: 0.0003916638031086856   Iteration 100 of 100, tot loss = 4.981922250986099, l1: 0.0001064048274201923, l2: 0.00039178739811177363
   End of epoch 1127; saving model... 

Epoch 1128 of 2000
   Iteration 1 of 100, tot loss = 5.002552509307861, l1: 0.00010637084051268175, l2: 0.0003938844020012766   Iteration 2 of 100, tot loss = 4.1732800006866455, l1: 8.521725976606831e-05, l2: 0.000332110736053437   Iteration 3 of 100, tot loss = 4.204929669698079, l1: 7.678787126981963e-05, l2: 0.0003437050909269601   Iteration 4 of 100, tot loss = 3.809957265853882, l1: 7.467856812581886e-05, l2: 0.00030631715344497934   Iteration 5 of 100, tot loss = 4.004361820220947, l1: 8.268019446404651e-05, l2: 0.00031775598181411625   Iteration 6 of 100, tot loss = 4.825451135635376, l1: 9.429862378359151e-05, l2: 0.00038824649527668953   Iteration 7 of 100, tot loss = 4.8751175062997, l1: 9.703383797646634e-05, l2: 0.00039047791506163776   Iteration 8 of 100, tot loss = 4.925213038921356, l1: 9.794441575650126e-05, l2: 0.0003945768876292277   Iteration 9 of 100, tot loss = 4.684021552403768, l1: 9.255823776281129e-05, l2: 0.00037584391409634717   Iteration 10 of 100, tot loss = 4.8855855703353885, l1: 9.31103335460648e-05, l2: 0.000395448217750527   Iteration 11 of 100, tot loss = 4.809948509389704, l1: 9.246917893919586e-05, l2: 0.00038852566715583885   Iteration 12 of 100, tot loss = 4.96061497926712, l1: 9.590526375783763e-05, l2: 0.0004001562313836378   Iteration 13 of 100, tot loss = 4.823391694288987, l1: 9.397538591516562e-05, l2: 0.0003883637792359178   Iteration 14 of 100, tot loss = 4.846730266298566, l1: 9.718922904409868e-05, l2: 0.00038748379405920526   Iteration 15 of 100, tot loss = 4.986837546030681, l1: 0.00010025033698184416, l2: 0.0003984334121923894   Iteration 16 of 100, tot loss = 5.096019923686981, l1: 0.00010190724742642487, l2: 0.00040769473889668006   Iteration 17 of 100, tot loss = 4.984361985150506, l1: 0.00010093277481192832, l2: 0.0003975034170948407   Iteration 18 of 100, tot loss = 4.8685111204783125, l1: 9.817736827244516e-05, l2: 0.0003886737378909149   Iteration 19 of 100, tot loss = 4.836522805063348, l1: 9.745875458496525e-05, l2: 0.00038619352026695485   Iteration 20 of 100, tot loss = 4.958477807044983, l1: 0.00010068946121464251, l2: 0.00039515831449534746   Iteration 21 of 100, tot loss = 4.844525723230271, l1: 9.872973671354841e-05, l2: 0.00038572283007115837   Iteration 22 of 100, tot loss = 4.9343954433094375, l1: 0.00010159408752356698, l2: 0.0003918454511387443   Iteration 23 of 100, tot loss = 4.963238736857539, l1: 0.00010256258610752411, l2: 0.00039376128267537314   Iteration 24 of 100, tot loss = 4.897294183572133, l1: 0.00010248287147381537, l2: 0.00038724654223187827   Iteration 25 of 100, tot loss = 4.91859727859497, l1: 0.00010336493214708753, l2: 0.0003884947916958481   Iteration 26 of 100, tot loss = 4.826676350373488, l1: 0.00010170214689144184, l2: 0.0003809654836704095   Iteration 27 of 100, tot loss = 4.806312455071343, l1: 0.00010181979706868771, l2: 0.0003788114441300018   Iteration 28 of 100, tot loss = 4.878511990819659, l1: 0.00010285055590689549, l2: 0.000385000639653299   Iteration 29 of 100, tot loss = 4.847735980461383, l1: 0.00010204456577474926, l2: 0.0003827290292347557   Iteration 30 of 100, tot loss = 4.927145957946777, l1: 0.00010375367261682792, l2: 0.0003889609205846985   Iteration 31 of 100, tot loss = 4.876962977070963, l1: 0.00010289524265493626, l2: 0.00038480105239056773   Iteration 32 of 100, tot loss = 4.8819271847605705, l1: 0.00010306466299425665, l2: 0.0003851280525850598   Iteration 33 of 100, tot loss = 4.88077102285443, l1: 0.00010306659786499839, l2: 0.00038501050210360324   Iteration 34 of 100, tot loss = 4.878606592907625, l1: 0.00010326101330386283, l2: 0.0003845996432252886   Iteration 35 of 100, tot loss = 4.834874657222203, l1: 0.00010331948298179279, l2: 0.0003801679804122874   Iteration 36 of 100, tot loss = 4.786421782440609, l1: 0.00010296870510097101, l2: 0.0003756734707470362   Iteration 37 of 100, tot loss = 4.774321858947341, l1: 0.00010293751934974303, l2: 0.0003744946640123286   Iteration 38 of 100, tot loss = 4.774024643396077, l1: 0.00010267287497730362, l2: 0.0003747295873166111   Iteration 39 of 100, tot loss = 4.75717333646921, l1: 0.00010290598238964613, l2: 0.0003728113489416547   Iteration 40 of 100, tot loss = 4.764912456274033, l1: 0.00010296257050868007, l2: 0.00037352867366280407   Iteration 41 of 100, tot loss = 4.746519978453473, l1: 0.00010276158816229393, l2: 0.00037189040875357644   Iteration 42 of 100, tot loss = 4.789902272678557, l1: 0.00010343597984631612, l2: 0.0003755542462126219   Iteration 43 of 100, tot loss = 4.780308041461678, l1: 0.00010330855416954322, l2: 0.00037472224848474875   Iteration 44 of 100, tot loss = 4.768739824945277, l1: 0.00010300826951689405, l2: 0.000373865711232859   Iteration 45 of 100, tot loss = 4.791238398022122, l1: 0.00010331763001482209, l2: 0.000375806208467111   Iteration 46 of 100, tot loss = 4.79607211506885, l1: 0.0001034938991126736, l2: 0.0003761133108207065   Iteration 47 of 100, tot loss = 4.801720614128924, l1: 0.00010328570761943256, l2: 0.0003768863519306909   Iteration 48 of 100, tot loss = 4.80802068610986, l1: 0.00010369930464548816, l2: 0.00037710276228608564   Iteration 49 of 100, tot loss = 4.806047074648799, l1: 0.00010343125912334713, l2: 0.00037717344643244024   Iteration 50 of 100, tot loss = 4.765445046424865, l1: 0.00010288732235494536, l2: 0.0003736571804620326   Iteration 51 of 100, tot loss = 4.778221714730356, l1: 0.00010363988807440881, l2: 0.0003741822814952363   Iteration 52 of 100, tot loss = 4.799880967690394, l1: 0.000104570026381295, l2: 0.0003754180691500481   Iteration 53 of 100, tot loss = 4.749538700535612, l1: 0.00010390322852302079, l2: 0.0003710506403739652   Iteration 54 of 100, tot loss = 4.75287624641701, l1: 0.00010375706531501944, l2: 0.00037153055826719437   Iteration 55 of 100, tot loss = 4.786612892150879, l1: 0.00010462583681642585, l2: 0.0003740354515188797   Iteration 56 of 100, tot loss = 4.847196187291827, l1: 0.00010592436014381487, l2: 0.0003787952570876639   Iteration 57 of 100, tot loss = 4.842175441875792, l1: 0.00010604828052588313, l2: 0.0003781692618136539   Iteration 58 of 100, tot loss = 4.82125722539836, l1: 0.00010589708792144063, l2: 0.000376228632501744   Iteration 59 of 100, tot loss = 4.786935361765199, l1: 0.00010543249707059319, l2: 0.0003732610370577076   Iteration 60 of 100, tot loss = 4.813868753115336, l1: 0.00010598904549018092, l2: 0.0003753978278837167   Iteration 61 of 100, tot loss = 4.84891767189151, l1: 0.00010696448786250987, l2: 0.0003779272775005427   Iteration 62 of 100, tot loss = 4.839219854724023, l1: 0.00010635196755878672, l2: 0.000377570015599861   Iteration 63 of 100, tot loss = 4.8308335940043134, l1: 0.00010635726132369157, l2: 0.00037672609600815035   Iteration 64 of 100, tot loss = 4.848654106259346, l1: 0.00010690994889728245, l2: 0.00037795545995322755   Iteration 65 of 100, tot loss = 4.84488405080942, l1: 0.00010685050337297771, l2: 0.00037763790012552186   Iteration 66 of 100, tot loss = 4.849805181676691, l1: 0.00010696419579317356, l2: 0.00037801632027946073   Iteration 67 of 100, tot loss = 4.8632784387958585, l1: 0.00010750906759353507, l2: 0.0003788187741927469   Iteration 68 of 100, tot loss = 4.850701416240019, l1: 0.0001074075244640185, l2: 0.00037766261512468406   Iteration 69 of 100, tot loss = 4.8373034760571905, l1: 0.00010677011401191746, l2: 0.0003769602314434082   Iteration 70 of 100, tot loss = 4.853224226406643, l1: 0.00010675235716917086, l2: 0.00037857006370489087   Iteration 71 of 100, tot loss = 4.883671656460829, l1: 0.00010705098929334576, l2: 0.0003813161747530103   Iteration 72 of 100, tot loss = 4.871810108423233, l1: 0.0001069950923617095, l2: 0.00038018591698750644   Iteration 73 of 100, tot loss = 4.882634636473982, l1: 0.00010695075930203056, l2: 0.00038131270262616853   Iteration 74 of 100, tot loss = 4.881770014762878, l1: 0.00010715605708970247, l2: 0.00038102094274379214   Iteration 75 of 100, tot loss = 4.866890274683635, l1: 0.00010700993555171104, l2: 0.00037967909011058507   Iteration 76 of 100, tot loss = 4.835961470478459, l1: 0.00010654298770723951, l2: 0.0003770531574876881   Iteration 77 of 100, tot loss = 4.8472493809539, l1: 0.00010682046034504877, l2: 0.0003779044759598887   Iteration 78 of 100, tot loss = 4.854018685145256, l1: 0.00010669019493201259, l2: 0.00037871167147293303   Iteration 79 of 100, tot loss = 4.829250655596769, l1: 0.00010626136336904432, l2: 0.00037666369996570005   Iteration 80 of 100, tot loss = 4.81776096522808, l1: 0.00010612858573040284, l2: 0.0003756475087357103   Iteration 81 of 100, tot loss = 4.790128987512471, l1: 0.00010539004335756253, l2: 0.0003736228532988555   Iteration 82 of 100, tot loss = 4.811890157257638, l1: 0.00010601711579599906, l2: 0.0003751718977582641   Iteration 83 of 100, tot loss = 4.812612323875887, l1: 0.00010618878733079196, l2: 0.00037507244321960864   Iteration 84 of 100, tot loss = 4.799874722957611, l1: 0.00010595700825346977, l2: 0.0003740304623317483   Iteration 85 of 100, tot loss = 4.8041707908405975, l1: 0.00010597252690144266, l2: 0.00037444455032099915   Iteration 86 of 100, tot loss = 4.78754726121592, l1: 0.00010568358718659892, l2: 0.0003730711369128183   Iteration 87 of 100, tot loss = 4.825323954395865, l1: 0.00010622638452834257, l2: 0.00037630600871480787   Iteration 88 of 100, tot loss = 4.830486904491078, l1: 0.00010630476328349852, l2: 0.00037674392478947993   Iteration 89 of 100, tot loss = 4.811980882387482, l1: 0.00010606350549372447, l2: 0.0003751345805161936   Iteration 90 of 100, tot loss = 4.839220680130852, l1: 0.00010675095808336563, l2: 0.00037717110810465076   Iteration 91 of 100, tot loss = 4.843914165601626, l1: 0.00010664734525182592, l2: 0.0003777440692679342   Iteration 92 of 100, tot loss = 4.818632110305455, l1: 0.00010631176752687442, l2: 0.0003755514416021153   Iteration 93 of 100, tot loss = 4.806693856434156, l1: 0.0001061956085009946, l2: 0.00037447377549289094   Iteration 94 of 100, tot loss = 4.788295225894197, l1: 0.0001059881268837613, l2: 0.0003728413941373335   Iteration 95 of 100, tot loss = 4.8115553077898525, l1: 0.00010631239600020068, l2: 0.0003748431333071111   Iteration 96 of 100, tot loss = 4.792000529666741, l1: 0.00010597496994553997, l2: 0.00037322508160286816   Iteration 97 of 100, tot loss = 4.816487088645856, l1: 0.00010659699644035295, l2: 0.000375051711165087   Iteration 98 of 100, tot loss = 4.809228646511934, l1: 0.00010640965451691917, l2: 0.0003745132087306975   Iteration 99 of 100, tot loss = 4.804915372771446, l1: 0.0001061393987317452, l2: 0.00037435213729475786   Iteration 100 of 100, tot loss = 4.812814147472381, l1: 0.00010618459211400477, l2: 0.0003750968215172179
   End of epoch 1128; saving model... 

Epoch 1129 of 2000
   Iteration 1 of 100, tot loss = 3.3210642337799072, l1: 8.123984298435971e-05, l2: 0.00025086657842621207   Iteration 2 of 100, tot loss = 3.3235288858413696, l1: 6.463326280936599e-05, l2: 0.00026771963166538626   Iteration 3 of 100, tot loss = 5.090989192326863, l1: 0.00010002893880785753, l2: 0.0004090699852289011   Iteration 4 of 100, tot loss = 4.895922005176544, l1: 9.881614096229896e-05, l2: 0.00039077606197679415   Iteration 5 of 100, tot loss = 4.418720006942749, l1: 9.16758508537896e-05, l2: 0.0003501961502479389   Iteration 6 of 100, tot loss = 4.3358374039332075, l1: 9.339675671071745e-05, l2: 0.00034018698352156207   Iteration 7 of 100, tot loss = 4.252136128289359, l1: 9.37760591373912e-05, l2: 0.0003314375540607476   Iteration 8 of 100, tot loss = 4.130005717277527, l1: 9.407982179254759e-05, l2: 0.0003189207491232082   Iteration 9 of 100, tot loss = 4.19934876759847, l1: 9.598882105718885e-05, l2: 0.0003239460541711499   Iteration 10 of 100, tot loss = 4.168343329429627, l1: 9.669933351688086e-05, l2: 0.00032013499876484277   Iteration 11 of 100, tot loss = 4.1515442674810235, l1: 9.564764332026243e-05, l2: 0.00031950678253038365   Iteration 12 of 100, tot loss = 4.111964325110118, l1: 9.26381195919627e-05, l2: 0.0003185583118465729   Iteration 13 of 100, tot loss = 4.178533682456384, l1: 9.410674689346566e-05, l2: 0.0003237466220385753   Iteration 14 of 100, tot loss = 4.160923906735012, l1: 9.412565252984808e-05, l2: 0.00032196673730920466   Iteration 15 of 100, tot loss = 4.169949738184611, l1: 9.424156281359805e-05, l2: 0.0003227534102431188   Iteration 16 of 100, tot loss = 4.290568009018898, l1: 9.67132220921485e-05, l2: 0.0003323435794300167   Iteration 17 of 100, tot loss = 4.38803573215709, l1: 9.747809642302694e-05, l2: 0.0003413254761493162   Iteration 18 of 100, tot loss = 4.343516574965583, l1: 9.625926132786035e-05, l2: 0.00033809239458706643   Iteration 19 of 100, tot loss = 4.498936841362401, l1: 9.87870974457014e-05, l2: 0.0003511065856161478   Iteration 20 of 100, tot loss = 4.504381811618805, l1: 9.814002787607023e-05, l2: 0.00035229815257480367   Iteration 21 of 100, tot loss = 4.519867567788987, l1: 9.953981723464538e-05, l2: 0.0003524469391864148   Iteration 22 of 100, tot loss = 4.505992271683433, l1: 9.863041569505268e-05, l2: 0.00035196881138042295   Iteration 23 of 100, tot loss = 4.533625343571538, l1: 9.844738799628153e-05, l2: 0.00035491514616929317   Iteration 24 of 100, tot loss = 4.609959314266841, l1: 9.981386074287002e-05, l2: 0.00036118206965814653   Iteration 25 of 100, tot loss = 4.5869929981231685, l1: 9.900399527396075e-05, l2: 0.0003596953034866601   Iteration 26 of 100, tot loss = 4.626197493993319, l1: 9.826287812477228e-05, l2: 0.0003643568701451071   Iteration 27 of 100, tot loss = 4.587106881318269, l1: 9.751860098158768e-05, l2: 0.000361192086495942   Iteration 28 of 100, tot loss = 4.592398592403957, l1: 9.725505632169578e-05, l2: 0.0003619848025014757   Iteration 29 of 100, tot loss = 4.649026262349095, l1: 9.73273956082392e-05, l2: 0.0003675752297316774   Iteration 30 of 100, tot loss = 4.829080375035604, l1: 0.00010039061126008165, l2: 0.00038251742468370744   Iteration 31 of 100, tot loss = 4.831136365090647, l1: 0.00010089396633819917, l2: 0.0003822196684327097   Iteration 32 of 100, tot loss = 4.874887630343437, l1: 0.00010192626984917297, l2: 0.00038556249091925565   Iteration 33 of 100, tot loss = 4.91655732646133, l1: 0.00010300595099441566, l2: 0.00038864977911792016   Iteration 34 of 100, tot loss = 4.954671817667344, l1: 0.00010397855518984965, l2: 0.0003914886251817841   Iteration 35 of 100, tot loss = 4.9389361381530765, l1: 0.00010377133741193184, l2: 0.0003901222745688366   Iteration 36 of 100, tot loss = 4.941812634468079, l1: 0.0001037979926776542, l2: 0.00039038326940499246   Iteration 37 of 100, tot loss = 4.9393123936008765, l1: 0.00010417222811893962, l2: 0.0003897590095466758   Iteration 38 of 100, tot loss = 4.979565582777324, l1: 0.00010503076277688563, l2: 0.0003929257930538274   Iteration 39 of 100, tot loss = 5.028032645201072, l1: 0.00010620324228045483, l2: 0.0003966000207210294   Iteration 40 of 100, tot loss = 5.060391342639923, l1: 0.00010731547308751033, l2: 0.00039872365960036404   Iteration 41 of 100, tot loss = 5.143403716203643, l1: 0.00010878438239692846, l2: 0.00040555598779308907   Iteration 42 of 100, tot loss = 5.163419280733381, l1: 0.00010902826337163181, l2: 0.0004073136626088637   Iteration 43 of 100, tot loss = 5.115750944891642, l1: 0.00010842192323464185, l2: 0.00040315316961590894   Iteration 44 of 100, tot loss = 5.058140689676458, l1: 0.00010740144625742687, l2: 0.0003984126210112167   Iteration 45 of 100, tot loss = 5.071177630954319, l1: 0.00010731978796280196, l2: 0.0003997979740638079   Iteration 46 of 100, tot loss = 5.026531245397485, l1: 0.00010667031457832427, l2: 0.00039598280842335004   Iteration 47 of 100, tot loss = 4.977929997951426, l1: 0.00010581729270114088, l2: 0.0003919757056134218   Iteration 48 of 100, tot loss = 4.910325817763805, l1: 0.00010459503710080753, l2: 0.0003864375433598373   Iteration 49 of 100, tot loss = 4.844954605005225, l1: 0.0001032592712251745, l2: 0.00038123618792362357   Iteration 50 of 100, tot loss = 4.854568612575531, l1: 0.00010391294497821946, l2: 0.00038154391513671725   Iteration 51 of 100, tot loss = 4.859479892487619, l1: 0.00010422490260109542, l2: 0.0003817230862492294   Iteration 52 of 100, tot loss = 4.927616988237087, l1: 0.00010538433154859097, l2: 0.000387377366468382   Iteration 53 of 100, tot loss = 4.931320745989962, l1: 0.00010495089764107142, l2: 0.00038818117584769597   Iteration 54 of 100, tot loss = 4.925093489664572, l1: 0.00010536182923749072, l2: 0.00038714751846984855   Iteration 55 of 100, tot loss = 4.9018126812848175, l1: 0.0001049273778996642, l2: 0.0003852538891475309   Iteration 56 of 100, tot loss = 4.8817539449249, l1: 0.00010462359263588692, l2: 0.000383551801080882   Iteration 57 of 100, tot loss = 4.932051547786646, l1: 0.00010570715618852869, l2: 0.0003874979978590681   Iteration 58 of 100, tot loss = 4.901566528040787, l1: 0.00010553942455113304, l2: 0.0003846172277980227   Iteration 59 of 100, tot loss = 4.890906574362416, l1: 0.00010551907517010014, l2: 0.0003835715822603206   Iteration 60 of 100, tot loss = 4.8910888731479645, l1: 0.0001056220828104415, l2: 0.00038348680463968777   Iteration 61 of 100, tot loss = 4.915720148164718, l1: 0.00010564790858045511, l2: 0.0003859241062214171   Iteration 62 of 100, tot loss = 4.9020812569126, l1: 0.00010504042468035609, l2: 0.0003851677008037035   Iteration 63 of 100, tot loss = 4.89661547305092, l1: 0.00010530951742169684, l2: 0.000384352029262421   Iteration 64 of 100, tot loss = 4.907925171777606, l1: 0.00010594177723532994, l2: 0.0003848507392376632   Iteration 65 of 100, tot loss = 4.94051363834968, l1: 0.00010669029876365899, l2: 0.00038736106429356507   Iteration 66 of 100, tot loss = 4.91541091781674, l1: 0.00010656368012308975, l2: 0.00038497741082789037   Iteration 67 of 100, tot loss = 4.896614531972515, l1: 0.00010636197302078675, l2: 0.00038329947938788126   Iteration 68 of 100, tot loss = 4.923011487021165, l1: 0.00010685162589438464, l2: 0.0003854495223683497   Iteration 69 of 100, tot loss = 4.894344711649245, l1: 0.00010676741916593501, l2: 0.000382667051609673   Iteration 70 of 100, tot loss = 4.853473358494895, l1: 0.0001061099117318268, l2: 0.0003792374234762974   Iteration 71 of 100, tot loss = 4.866534395956657, l1: 0.00010643906055738113, l2: 0.00038021437854106   Iteration 72 of 100, tot loss = 4.837050260768996, l1: 0.00010562456403325389, l2: 0.000378080461612424   Iteration 73 of 100, tot loss = 4.81393484710014, l1: 0.00010527748186364515, l2: 0.00037611600219699466   Iteration 74 of 100, tot loss = 4.782471088138786, l1: 0.00010473866623706763, l2: 0.00037350844217439155   Iteration 75 of 100, tot loss = 4.762306354840597, l1: 0.00010473069521443297, l2: 0.0003714999397440503   Iteration 76 of 100, tot loss = 4.759143216045279, l1: 0.00010473499739753989, l2: 0.0003711793236250646   Iteration 77 of 100, tot loss = 4.743851610592434, l1: 0.00010465158284913942, l2: 0.0003697335778269917   Iteration 78 of 100, tot loss = 4.732010989617079, l1: 0.0001047627576134311, l2: 0.0003684383409563452   Iteration 79 of 100, tot loss = 4.712998587873917, l1: 0.00010463406804866523, l2: 0.0003666657903983222   Iteration 80 of 100, tot loss = 4.7094691202044485, l1: 0.00010450624440636603, l2: 0.00036644066731241767   Iteration 81 of 100, tot loss = 4.702996752880238, l1: 0.00010450751977731232, l2: 0.0003657921551177738   Iteration 82 of 100, tot loss = 4.706558191194767, l1: 0.00010447350324262161, l2: 0.0003661823151249825   Iteration 83 of 100, tot loss = 4.722362068762262, l1: 0.00010457625414500103, l2: 0.00036765995147183583   Iteration 84 of 100, tot loss = 4.74400157445953, l1: 0.00010439592915645335, l2: 0.00037000422699271613   Iteration 85 of 100, tot loss = 4.71465483974008, l1: 0.00010367354561287143, l2: 0.000367791937006747   Iteration 86 of 100, tot loss = 4.70930533215057, l1: 0.0001037820146065801, l2: 0.00036714851705974704   Iteration 87 of 100, tot loss = 4.733164624236096, l1: 0.0001040963585998317, l2: 0.00036922010262603817   Iteration 88 of 100, tot loss = 4.724430088292468, l1: 0.00010394576251697303, l2: 0.00036849724512483755   Iteration 89 of 100, tot loss = 4.751146261611681, l1: 0.00010441107481843635, l2: 0.0003707035499614444   Iteration 90 of 100, tot loss = 4.744816021124522, l1: 0.00010433797967076922, l2: 0.0003701436210475448   Iteration 91 of 100, tot loss = 4.744266472019992, l1: 0.00010446111152235132, l2: 0.0003699655340764426   Iteration 92 of 100, tot loss = 4.74905649345854, l1: 0.00010435452065354659, l2: 0.0003705511272014321   Iteration 93 of 100, tot loss = 4.720856114100385, l1: 0.00010368074709799902, l2: 0.0003684048629453486   Iteration 94 of 100, tot loss = 4.716536949289606, l1: 0.00010381029897736028, l2: 0.0003678433944166519   Iteration 95 of 100, tot loss = 4.727732258093985, l1: 0.0001040996423015665, l2: 0.00036867358185669507   Iteration 96 of 100, tot loss = 4.747697642693917, l1: 0.0001043297587178434, l2: 0.0003704400037349842   Iteration 97 of 100, tot loss = 4.743821681160288, l1: 0.00010436988435985674, l2: 0.0003700122816581759   Iteration 98 of 100, tot loss = 4.7281438538006375, l1: 0.00010419555637410518, l2: 0.00036861882687603333   Iteration 99 of 100, tot loss = 4.7482357085353195, l1: 0.00010474391706109122, l2: 0.0003700796514749527   Iteration 100 of 100, tot loss = 4.741271449327469, l1: 0.00010459588149387855, l2: 0.00036953126109438015
   End of epoch 1129; saving model... 

Epoch 1130 of 2000
   Iteration 1 of 100, tot loss = 6.057549476623535, l1: 0.00012852784129790962, l2: 0.0004772271204274148   Iteration 2 of 100, tot loss = 6.282907247543335, l1: 0.00011999297203146853, l2: 0.0005082977440906689   Iteration 3 of 100, tot loss = 5.2051927248636884, l1: 0.00010236608068225905, l2: 0.0004181531791497643   Iteration 4 of 100, tot loss = 4.967183589935303, l1: 0.00010246586316497996, l2: 0.00039425248178304173   Iteration 5 of 100, tot loss = 4.808384704589844, l1: 0.00010017294262070209, l2: 0.00038066550914663824   Iteration 6 of 100, tot loss = 5.520702838897705, l1: 0.00011248828377574682, l2: 0.00043958198754504946   Iteration 7 of 100, tot loss = 5.206219468797956, l1: 0.00010346290114934423, l2: 0.00041715903249236623   Iteration 8 of 100, tot loss = 4.877530783414841, l1: 9.972679526981665e-05, l2: 0.00038802627204859164   Iteration 9 of 100, tot loss = 4.675981865988837, l1: 9.771568228542391e-05, l2: 0.0003698824956599209   Iteration 10 of 100, tot loss = 4.565672421455384, l1: 9.773078927537427e-05, l2: 0.00035883644595742227   Iteration 11 of 100, tot loss = 4.513194062493064, l1: 9.71802358187481e-05, l2: 0.0003541391633916646   Iteration 12 of 100, tot loss = 4.738518138726552, l1: 0.00010130361442861613, l2: 0.00037254819229322794   Iteration 13 of 100, tot loss = 4.9128250525547905, l1: 0.00010541722552331451, l2: 0.00038586527136011195   Iteration 14 of 100, tot loss = 5.286446758678982, l1: 0.00011165968161159461, l2: 0.00041698498742854487   Iteration 15 of 100, tot loss = 5.373667542139689, l1: 0.00011196580453542992, l2: 0.00042540094388338425   Iteration 16 of 100, tot loss = 5.44639091193676, l1: 0.00011358974506947561, l2: 0.0004310493404773297   Iteration 17 of 100, tot loss = 5.435344794217278, l1: 0.00011308305607492323, l2: 0.000430451420149492   Iteration 18 of 100, tot loss = 5.269207768970066, l1: 0.00011069160488356526, l2: 0.0004162291685740153   Iteration 19 of 100, tot loss = 5.150322788640072, l1: 0.00010978887286527377, l2: 0.00040524340338857944   Iteration 20 of 100, tot loss = 5.082260060310364, l1: 0.00010963347631331999, l2: 0.0003985925271990709   Iteration 21 of 100, tot loss = 5.001612924394154, l1: 0.0001084182499436706, l2: 0.00039174303916903835   Iteration 22 of 100, tot loss = 5.0547602501782505, l1: 0.00010902496135465547, l2: 0.00039645106219474906   Iteration 23 of 100, tot loss = 5.076660394668579, l1: 0.00010932032898341751, l2: 0.00039834570919123035   Iteration 24 of 100, tot loss = 5.027360330025355, l1: 0.00010840452462919832, l2: 0.00039433150717134896   Iteration 25 of 100, tot loss = 4.991634626388549, l1: 0.00010701937368139625, l2: 0.0003921440884005278   Iteration 26 of 100, tot loss = 5.008339047431946, l1: 0.00010646865037135565, l2: 0.00039436525212994847   Iteration 27 of 100, tot loss = 4.95733822716607, l1: 0.00010611534849481864, l2: 0.00038961847248073254   Iteration 28 of 100, tot loss = 4.9133762036051065, l1: 0.00010559339404738109, l2: 0.0003857442248512858   Iteration 29 of 100, tot loss = 4.9183281454546695, l1: 0.00010544945865243287, l2: 0.00038638335435876045   Iteration 30 of 100, tot loss = 4.815419201056162, l1: 0.00010352574063290376, l2: 0.00037801617775888493   Iteration 31 of 100, tot loss = 4.880292142591169, l1: 0.00010437267302486655, l2: 0.0003836565405025237   Iteration 32 of 100, tot loss = 4.884222511202097, l1: 0.00010347869772431295, l2: 0.0003849435515803634   Iteration 33 of 100, tot loss = 4.8779242725083325, l1: 0.00010404040321273814, l2: 0.0003837520224888894   Iteration 34 of 100, tot loss = 4.8185269026195305, l1: 0.0001035623288985602, l2: 0.0003782903599237804   Iteration 35 of 100, tot loss = 4.799685740470887, l1: 0.00010390552374052017, l2: 0.00037606304977089167   Iteration 36 of 100, tot loss = 4.762586226065953, l1: 0.00010348351103554403, l2: 0.00037277511081710044   Iteration 37 of 100, tot loss = 4.829418623769605, l1: 0.0001047509652601769, l2: 0.0003781908968891445   Iteration 38 of 100, tot loss = 4.896439291928944, l1: 0.00010599588102578357, l2: 0.0003836480471746702   Iteration 39 of 100, tot loss = 4.925403622480539, l1: 0.00010717771240310051, l2: 0.00038536264895437617   Iteration 40 of 100, tot loss = 4.911355975270271, l1: 0.0001070298177182849, l2: 0.00038410577908507546   Iteration 41 of 100, tot loss = 4.916603509972735, l1: 0.00010741715524956107, l2: 0.00038424319525181156   Iteration 42 of 100, tot loss = 4.932566316354842, l1: 0.00010777205543057616, l2: 0.00038548457572081435   Iteration 43 of 100, tot loss = 4.9320170075394385, l1: 0.00010791042239911486, l2: 0.0003852912781186142   Iteration 44 of 100, tot loss = 4.9094194146719845, l1: 0.0001081401705960161, l2: 0.00038280177051836455   Iteration 45 of 100, tot loss = 4.962076216273838, l1: 0.00010891671934384957, l2: 0.0003872909026944803   Iteration 46 of 100, tot loss = 4.925303378830785, l1: 0.00010857827882042012, l2: 0.0003839520596438492   Iteration 47 of 100, tot loss = 4.9780336973514965, l1: 0.00010959508445205544, l2: 0.0003882082865915281   Iteration 48 of 100, tot loss = 4.973190737267335, l1: 0.00010957012811256088, l2: 0.00038774894680197275   Iteration 49 of 100, tot loss = 4.955502512503643, l1: 0.00010914752048699242, l2: 0.0003864027316829342   Iteration 50 of 100, tot loss = 4.953193609714508, l1: 0.00010943755078187677, l2: 0.00038588181108934806   Iteration 51 of 100, tot loss = 4.943449546309078, l1: 0.00010886236001776221, l2: 0.0003854825955190166   Iteration 52 of 100, tot loss = 4.93438678750625, l1: 0.00010873139364356085, l2: 0.0003847072861949877   Iteration 53 of 100, tot loss = 4.930392033648941, l1: 0.00010912983236507126, l2: 0.0003839093713498376   Iteration 54 of 100, tot loss = 4.892353466263524, l1: 0.00010832456724461668, l2: 0.000380910779683661   Iteration 55 of 100, tot loss = 4.888347731937062, l1: 0.00010788879729013636, l2: 0.0003809459764108231   Iteration 56 of 100, tot loss = 4.896276382463319, l1: 0.0001075158721895215, l2: 0.00038211176615732256   Iteration 57 of 100, tot loss = 4.977891765142742, l1: 0.00010895422319503778, l2: 0.0003888349530283002   Iteration 58 of 100, tot loss = 4.946933822385196, l1: 0.0001083961526781987, l2: 0.00038629722884848136   Iteration 59 of 100, tot loss = 4.908166252960593, l1: 0.00010756386374419365, l2: 0.0003832527608965855   Iteration 60 of 100, tot loss = 4.933684362967809, l1: 0.00010775344405071034, l2: 0.0003856149914402825   Iteration 61 of 100, tot loss = 4.936754592129441, l1: 0.00010790243925198485, l2: 0.0003857730194109446   Iteration 62 of 100, tot loss = 4.96183749744969, l1: 0.00010824346780634275, l2: 0.0003879402812022055   Iteration 63 of 100, tot loss = 4.984780922768608, l1: 0.00010893183773280018, l2: 0.0003895462543282303   Iteration 64 of 100, tot loss = 5.0212436225265265, l1: 0.0001097349652354751, l2: 0.00039238939689312247   Iteration 65 of 100, tot loss = 5.027731761565575, l1: 0.00010986322987054547, l2: 0.00039290994620667057   Iteration 66 of 100, tot loss = 5.012908355756239, l1: 0.00011011187829849818, l2: 0.00039117895710550135   Iteration 67 of 100, tot loss = 4.994442539428597, l1: 0.00010977184387799631, l2: 0.00038967241008362887   Iteration 68 of 100, tot loss = 4.980384698685477, l1: 0.00010902433838483001, l2: 0.0003890141310608562   Iteration 69 of 100, tot loss = 4.991960330285888, l1: 0.0001095411750703942, l2: 0.0003896548574709374   Iteration 70 of 100, tot loss = 4.980858242511749, l1: 0.00010915648568438233, l2: 0.0003889293380780146   Iteration 71 of 100, tot loss = 4.96915439652725, l1: 0.00010923537344534055, l2: 0.00038768006587059983   Iteration 72 of 100, tot loss = 4.9964689132240085, l1: 0.0001097974656861172, l2: 0.0003898494251188822   Iteration 73 of 100, tot loss = 4.976276859845201, l1: 0.0001093506408785151, l2: 0.0003882770447302865   Iteration 74 of 100, tot loss = 4.983939188557702, l1: 0.00010927304587361554, l2: 0.00038912087266736135   Iteration 75 of 100, tot loss = 4.959712831179301, l1: 0.0001088282973311531, l2: 0.00038714298532189183   Iteration 76 of 100, tot loss = 4.983842280350234, l1: 0.00010934996364659655, l2: 0.00038903426416494   Iteration 77 of 100, tot loss = 4.958297997326046, l1: 0.00010899308193535484, l2: 0.0003868367174464919   Iteration 78 of 100, tot loss = 4.9441903202961655, l1: 0.00010889309739398483, l2: 0.0003855259345026496   Iteration 79 of 100, tot loss = 4.962405544293078, l1: 0.00010902833125754436, l2: 0.0003872122228752727   Iteration 80 of 100, tot loss = 4.972542299330234, l1: 0.00010915198245129432, l2: 0.00038810224759799894   Iteration 81 of 100, tot loss = 5.034253498654307, l1: 0.00011026782311001838, l2: 0.00039315752700879526   Iteration 82 of 100, tot loss = 5.032141541562429, l1: 0.00011034079509000748, l2: 0.00039287335942101825   Iteration 83 of 100, tot loss = 5.025157183049673, l1: 0.0001103010341379208, l2: 0.00039221468439090056   Iteration 84 of 100, tot loss = 5.046646210409346, l1: 0.00011064635986085272, l2: 0.0003940182612582465   Iteration 85 of 100, tot loss = 5.059120370359982, l1: 0.00011070480421253973, l2: 0.00039520723272717614   Iteration 86 of 100, tot loss = 5.04724352442941, l1: 0.0001104185251088618, l2: 0.0003943058274967899   Iteration 87 of 100, tot loss = 5.037174308437041, l1: 0.0001105074494764016, l2: 0.0003932099812797902   Iteration 88 of 100, tot loss = 5.039925376122648, l1: 0.00011069601086174688, l2: 0.00039329652645392343   Iteration 89 of 100, tot loss = 5.057317772608124, l1: 0.00011098180182037477, l2: 0.00039474997509877837   Iteration 90 of 100, tot loss = 5.05483760966195, l1: 0.0001110907146842995, l2: 0.00039439304576565823   Iteration 91 of 100, tot loss = 5.056035519956232, l1: 0.00011129615242160591, l2: 0.00039430739867224636   Iteration 92 of 100, tot loss = 5.073363497205403, l1: 0.00011111279910318959, l2: 0.00039622354961466044   Iteration 93 of 100, tot loss = 5.068863634140261, l1: 0.00011099996998147558, l2: 0.00039588639253540145   Iteration 94 of 100, tot loss = 5.051007236572022, l1: 0.00011091042231666816, l2: 0.0003941903004430215   Iteration 95 of 100, tot loss = 5.028341091306586, l1: 0.00011060510457211517, l2: 0.0003922290036969475   Iteration 96 of 100, tot loss = 5.045736788461606, l1: 0.00011066596221098735, l2: 0.000393907715685297   Iteration 97 of 100, tot loss = 5.039685046549925, l1: 0.00011046457172793903, l2: 0.00039350393212630807   Iteration 98 of 100, tot loss = 5.044241748294052, l1: 0.00011045818350144796, l2: 0.00039396599102622774   Iteration 99 of 100, tot loss = 5.043575926260515, l1: 0.00011045499522511781, l2: 0.00039390259689293955   Iteration 100 of 100, tot loss = 5.031429599523545, l1: 0.00011028631204681006, l2: 0.0003928566473769024
   End of epoch 1130; saving model... 

Epoch 1131 of 2000
   Iteration 1 of 100, tot loss = 6.630420207977295, l1: 0.00012353749480098486, l2: 0.0005395045154727995   Iteration 2 of 100, tot loss = 6.225627422332764, l1: 0.00013166648568585515, l2: 0.000490896258270368   Iteration 3 of 100, tot loss = 7.4516175587972, l1: 0.0001514011867887651, l2: 0.0005937605941047271   Iteration 4 of 100, tot loss = 6.46740859746933, l1: 0.00012881400471087545, l2: 0.0005179268700885586   Iteration 5 of 100, tot loss = 5.852013778686524, l1: 0.0001175068799057044, l2: 0.00046769451000727713   Iteration 6 of 100, tot loss = 5.6870189507802325, l1: 0.00011476373886883569, l2: 0.0004539381673869987   Iteration 7 of 100, tot loss = 5.739592279706683, l1: 0.00011771161163259032, l2: 0.0004562476200849882   Iteration 8 of 100, tot loss = 5.399601250886917, l1: 0.00010890702696997323, l2: 0.0004310531039664056   Iteration 9 of 100, tot loss = 5.320780303743151, l1: 0.0001060543982021045, l2: 0.00042602363909180794   Iteration 10 of 100, tot loss = 5.211615777015686, l1: 0.00010792003558890428, l2: 0.00041324154881294817   Iteration 11 of 100, tot loss = 5.05013732476668, l1: 0.00010646197750413029, l2: 0.00039855175947939807   Iteration 12 of 100, tot loss = 4.843307415644328, l1: 0.00010158473166181163, l2: 0.00038274601320154034   Iteration 13 of 100, tot loss = 4.840982620532696, l1: 0.00010013917450963233, l2: 0.0003839590924774082   Iteration 14 of 100, tot loss = 4.736066664968218, l1: 9.768309584095343e-05, l2: 0.00037592357486054037   Iteration 15 of 100, tot loss = 4.852083698908488, l1: 9.932249304256402e-05, l2: 0.0003858858826182162   Iteration 16 of 100, tot loss = 4.960846230387688, l1: 0.000101852612942821, l2: 0.00039423201724275714   Iteration 17 of 100, tot loss = 4.973727885414572, l1: 0.00010323416469926836, l2: 0.0003941386333762613   Iteration 18 of 100, tot loss = 5.025251507759094, l1: 0.00010537820930443963, l2: 0.00039714695028831356   Iteration 19 of 100, tot loss = 5.061982142297845, l1: 0.00010644992385335315, l2: 0.0003997483007352505   Iteration 20 of 100, tot loss = 5.044811236858368, l1: 0.00010601696867524879, l2: 0.00039846416402724574   Iteration 21 of 100, tot loss = 5.039265939167568, l1: 0.00010610020174118247, l2: 0.0003978264001142677   Iteration 22 of 100, tot loss = 4.941348997029391, l1: 0.00010526716819599228, l2: 0.00038886773902711206   Iteration 23 of 100, tot loss = 4.887070313743923, l1: 0.00010448509593792868, l2: 0.00038422194294348037   Iteration 24 of 100, tot loss = 4.911168207724889, l1: 0.00010541364144955878, l2: 0.00038570318671797094   Iteration 25 of 100, tot loss = 4.892198133468628, l1: 0.00010476230949279852, l2: 0.00038445751182734965   Iteration 26 of 100, tot loss = 4.8360441739742575, l1: 0.00010360021615969554, l2: 0.00038000420877674164   Iteration 27 of 100, tot loss = 4.839312844806248, l1: 0.00010338603317871241, l2: 0.00038054525955683656   Iteration 28 of 100, tot loss = 4.92660392182214, l1: 0.00010523736052294095, l2: 0.00038742303981312683   Iteration 29 of 100, tot loss = 4.9297680608157455, l1: 0.00010536408821092758, l2: 0.0003876127272539226   Iteration 30 of 100, tot loss = 5.02692744731903, l1: 0.00010640934500164198, l2: 0.0003962834103731439   Iteration 31 of 100, tot loss = 5.03987899134236, l1: 0.00010731556669211826, l2: 0.000396672343354552   Iteration 32 of 100, tot loss = 5.0918097868561745, l1: 0.00010826749496573029, l2: 0.0004009134954685578   Iteration 33 of 100, tot loss = 5.094329306573579, l1: 0.00010845489889492676, l2: 0.000400978042638268   Iteration 34 of 100, tot loss = 5.048537212259629, l1: 0.00010743675043426371, l2: 0.00039741698128636926   Iteration 35 of 100, tot loss = 4.996904448100499, l1: 0.0001065527528615868, l2: 0.00039313770248554646   Iteration 36 of 100, tot loss = 4.9556304878658715, l1: 0.00010630666201905115, l2: 0.0003892563965766587   Iteration 37 of 100, tot loss = 4.963134520762676, l1: 0.00010630219883357829, l2: 0.00039001126188432445   Iteration 38 of 100, tot loss = 4.9957109376003865, l1: 0.0001067004583379266, l2: 0.0003928706435927827   Iteration 39 of 100, tot loss = 4.983599650554168, l1: 0.00010605959248958896, l2: 0.0003923003812535451   Iteration 40 of 100, tot loss = 5.026511037349701, l1: 0.00010669947560018044, l2: 0.00039595163689227776   Iteration 41 of 100, tot loss = 5.039018584460747, l1: 0.00010641845943698682, l2: 0.00039748340723013917   Iteration 42 of 100, tot loss = 5.109585557665143, l1: 0.00010774385797655366, l2: 0.0004032147054593744   Iteration 43 of 100, tot loss = 5.095217460809752, l1: 0.00010743330655481427, l2: 0.00040208844692706197   Iteration 44 of 100, tot loss = 5.138888402418657, l1: 0.00010844537882803706, l2: 0.00040544346914182165   Iteration 45 of 100, tot loss = 5.111166678534614, l1: 0.00010848027296661813, l2: 0.0004026364024159395   Iteration 46 of 100, tot loss = 5.059964345849079, l1: 0.0001077236456299231, l2: 0.0003982727965036326   Iteration 47 of 100, tot loss = 5.08248877018056, l1: 0.00010856130880865942, l2: 0.0003996875751367275   Iteration 48 of 100, tot loss = 5.0625945925712585, l1: 0.00010875807364148689, l2: 0.0003975013927023004   Iteration 49 of 100, tot loss = 5.065391598915567, l1: 0.00010904998627517429, l2: 0.0003974891804652859   Iteration 50 of 100, tot loss = 5.056041393280029, l1: 0.00010858803310838994, l2: 0.0003970161126926541   Iteration 51 of 100, tot loss = 5.049530543533026, l1: 0.00010847532301376501, l2: 0.00039647773818989447   Iteration 52 of 100, tot loss = 5.01092155621602, l1: 0.00010781611539501822, l2: 0.00039327604659215116   Iteration 53 of 100, tot loss = 5.028865854695158, l1: 0.00010822741291430814, l2: 0.000394659178641203   Iteration 54 of 100, tot loss = 5.061175756984287, l1: 0.00010790628861017197, l2: 0.00039821129245899136   Iteration 55 of 100, tot loss = 5.026981700550426, l1: 0.00010754631214447066, l2: 0.00039515186349903654   Iteration 56 of 100, tot loss = 5.04005230324609, l1: 0.00010795496362853945, l2: 0.00039605027242422305   Iteration 57 of 100, tot loss = 5.0478359356261135, l1: 0.00010819642889066272, l2: 0.00039658717031941927   Iteration 58 of 100, tot loss = 5.042516905685951, l1: 0.00010813738751086679, l2: 0.0003961143091397264   Iteration 59 of 100, tot loss = 5.028116937411034, l1: 0.0001081965555524714, l2: 0.0003946151446760237   Iteration 60 of 100, tot loss = 5.00864870150884, l1: 0.0001084223210758258, l2: 0.0003924425553123001   Iteration 61 of 100, tot loss = 5.044872921021258, l1: 0.00010867723003535562, l2: 0.00039581006815557783   Iteration 62 of 100, tot loss = 5.05684639946107, l1: 0.00010872751338484381, l2: 0.000396957132550362   Iteration 63 of 100, tot loss = 5.061042214196826, l1: 0.00010890847214983614, l2: 0.0003971957547556875   Iteration 64 of 100, tot loss = 5.023316863924265, l1: 0.000108409394613318, l2: 0.00039392229723489436   Iteration 65 of 100, tot loss = 5.016916865568895, l1: 0.00010839943542333248, l2: 0.0003932922561384308   Iteration 66 of 100, tot loss = 5.019624215183836, l1: 0.00010813624507908102, l2: 0.00039382618133183286   Iteration 67 of 100, tot loss = 4.992746168108129, l1: 0.0001077501695892433, l2: 0.0003915244520992847   Iteration 68 of 100, tot loss = 5.003620294963612, l1: 0.00010816425476471469, l2: 0.0003921977791726343   Iteration 69 of 100, tot loss = 4.980304759481679, l1: 0.00010816415448536408, l2: 0.000389866325903036   Iteration 70 of 100, tot loss = 4.963373453276498, l1: 0.0001079546826596405, l2: 0.00038838266711016853   Iteration 71 of 100, tot loss = 5.0356156758859125, l1: 0.00010885364560674364, l2: 0.0003947079268572721   Iteration 72 of 100, tot loss = 5.0204792684978905, l1: 0.00010887680961483663, l2: 0.000393171122090684   Iteration 73 of 100, tot loss = 5.001399980832453, l1: 0.00010875125999892551, l2: 0.00039138874255937255   Iteration 74 of 100, tot loss = 5.002927799482603, l1: 0.00010882783351769683, l2: 0.00039146495102023755   Iteration 75 of 100, tot loss = 4.981342252095541, l1: 0.00010832617859705351, l2: 0.0003898080512105177   Iteration 76 of 100, tot loss = 4.985722127713655, l1: 0.00010862871611032843, l2: 0.00038994350095316276   Iteration 77 of 100, tot loss = 4.958866243238573, l1: 0.00010818650815809786, l2: 0.0003877001203741733   Iteration 78 of 100, tot loss = 4.969086824319302, l1: 0.000108047572118663, l2: 0.0003888611148189132   Iteration 79 of 100, tot loss = 4.972027965738803, l1: 0.00010832938181621, l2: 0.0003888734194149464   Iteration 80 of 100, tot loss = 4.987052172422409, l1: 0.00010788426893668657, l2: 0.00039082095281628426   Iteration 81 of 100, tot loss = 4.96922140945623, l1: 0.00010763350188808321, l2: 0.0003892886434411515   Iteration 82 of 100, tot loss = 4.947742409822418, l1: 0.00010724348803483824, l2: 0.00038753075719663376   Iteration 83 of 100, tot loss = 4.933640379503549, l1: 0.00010708764791369427, l2: 0.00038627639422698104   Iteration 84 of 100, tot loss = 4.929943995816367, l1: 0.00010718134316924815, l2: 0.0003858130606074285   Iteration 85 of 100, tot loss = 4.896645907794728, l1: 0.00010641307235346176, l2: 0.0003832515226403142   Iteration 86 of 100, tot loss = 4.9085616594137145, l1: 0.00010678943705665174, l2: 0.0003840667330984806   Iteration 87 of 100, tot loss = 4.890925325196365, l1: 0.00010676053557551638, l2: 0.00038233200113567384   Iteration 88 of 100, tot loss = 4.872990264133974, l1: 0.00010625690616837925, l2: 0.0003810421244865707   Iteration 89 of 100, tot loss = 4.867660246538312, l1: 0.00010643865533985375, l2: 0.0003803273730942707   Iteration 90 of 100, tot loss = 4.85991723007626, l1: 0.00010654621134259893, l2: 0.0003794455153789992   Iteration 91 of 100, tot loss = 4.8400559818351665, l1: 0.00010605478038976819, l2: 0.0003779508214402985   Iteration 92 of 100, tot loss = 4.8264811894168025, l1: 0.00010585992834461175, l2: 0.00037678819448363197   Iteration 93 of 100, tot loss = 4.834664531933364, l1: 0.00010589103816704253, l2: 0.0003775754187416325   Iteration 94 of 100, tot loss = 4.859567730984789, l1: 0.0001061962288695987, l2: 0.0003797605477689587   Iteration 95 of 100, tot loss = 4.8608073109074645, l1: 0.00010639589106032968, l2: 0.00037968484347833224   Iteration 96 of 100, tot loss = 4.853456415235996, l1: 0.00010626431643837957, l2: 0.0003790813285983556   Iteration 97 of 100, tot loss = 4.854979163592624, l1: 0.00010613653618757152, l2: 0.0003793613838761584   Iteration 98 of 100, tot loss = 4.863034567054437, l1: 0.0001060363576508113, l2: 0.00038026710283201265   Iteration 99 of 100, tot loss = 4.843562658387001, l1: 0.00010570518266335553, l2: 0.00037865108716880167   Iteration 100 of 100, tot loss = 4.817889437675476, l1: 0.00010535275945585454, l2: 0.0003764361883804668
   End of epoch 1131; saving model... 

Epoch 1132 of 2000
   Iteration 1 of 100, tot loss = 3.0087828636169434, l1: 0.00010223303979728371, l2: 0.00019864524074364454   Iteration 2 of 100, tot loss = 4.323806047439575, l1: 0.00010808818842633627, l2: 0.0003242924067308195   Iteration 3 of 100, tot loss = 4.742484410603841, l1: 0.00010378438309999183, l2: 0.00037046405971826363   Iteration 4 of 100, tot loss = 4.408131659030914, l1: 9.839519043453038e-05, l2: 0.00034241797766298987   Iteration 5 of 100, tot loss = 5.082310533523559, l1: 0.0001071060571121052, l2: 0.00040112500137183814   Iteration 6 of 100, tot loss = 4.511903345584869, l1: 9.490720185567625e-05, l2: 0.0003562831382926864   Iteration 7 of 100, tot loss = 4.40238174370357, l1: 9.473747403327642e-05, l2: 0.0003455007037181141   Iteration 8 of 100, tot loss = 4.589607402682304, l1: 9.358621537103318e-05, l2: 0.00036537452433549333   Iteration 9 of 100, tot loss = 4.691854516665141, l1: 9.874876478635188e-05, l2: 0.0003704366827150807   Iteration 10 of 100, tot loss = 4.781957280635834, l1: 9.883426828309894e-05, l2: 0.00037936145527055486   Iteration 11 of 100, tot loss = 4.8144767392765395, l1: 0.0001010822359768843, l2: 0.000380365435954776   Iteration 12 of 100, tot loss = 4.8104254702727, l1: 0.00010136504958306129, l2: 0.0003796774944930803   Iteration 13 of 100, tot loss = 4.729608141458952, l1: 0.00010009985159670647, l2: 0.00037286096015192854   Iteration 14 of 100, tot loss = 4.894676285130637, l1: 0.00010127107244833107, l2: 0.0003881965562218933   Iteration 15 of 100, tot loss = 4.922607429822286, l1: 0.00010398898690861339, l2: 0.00038827175837165365   Iteration 16 of 100, tot loss = 4.854657106101513, l1: 0.0001035418272294919, l2: 0.0003819238854703144   Iteration 17 of 100, tot loss = 4.953557287945467, l1: 0.00010511970150914481, l2: 0.00039023602713474673   Iteration 18 of 100, tot loss = 4.973471211062537, l1: 0.00010644687346131023, l2: 0.00039090024640447355   Iteration 19 of 100, tot loss = 4.932197564526608, l1: 0.00010520728851827841, l2: 0.000388012466014755   Iteration 20 of 100, tot loss = 4.98844787478447, l1: 0.00010624604583426844, l2: 0.00039259873956325463   Iteration 21 of 100, tot loss = 5.026549265498207, l1: 0.00010714723785418929, l2: 0.0003955076882405029   Iteration 22 of 100, tot loss = 5.020641169764779, l1: 0.00010715064126998186, l2: 0.0003949134757435373   Iteration 23 of 100, tot loss = 4.961060798686484, l1: 0.00010697133573399776, l2: 0.00038913474354953706   Iteration 24 of 100, tot loss = 4.90171102186044, l1: 0.0001066335113743359, l2: 0.0003835375894899092   Iteration 25 of 100, tot loss = 4.970825037956238, l1: 0.00010790569911478087, l2: 0.0003891768044559285   Iteration 26 of 100, tot loss = 4.948006295240843, l1: 0.00010848226100698902, l2: 0.00038631836818799807   Iteration 27 of 100, tot loss = 4.947831926522432, l1: 0.00010884784731186098, l2: 0.0003859353444702854   Iteration 28 of 100, tot loss = 4.887328066996166, l1: 0.00010733350453457595, l2: 0.00038139930100961853   Iteration 29 of 100, tot loss = 5.00410338516893, l1: 0.00010760766474112612, l2: 0.0003928026713326509   Iteration 30 of 100, tot loss = 5.047976593176524, l1: 0.00010915341505703206, l2: 0.00039564424223499374   Iteration 31 of 100, tot loss = 4.977825460895415, l1: 0.00010795698284299203, l2: 0.0003898255606820326   Iteration 32 of 100, tot loss = 4.970404226332903, l1: 0.00010880403419832874, l2: 0.00038823638487883727   Iteration 33 of 100, tot loss = 4.979373155218182, l1: 0.00010923915705819981, l2: 0.0003886981545347777   Iteration 34 of 100, tot loss = 4.963064351502587, l1: 0.00010852857196689857, l2: 0.0003877778598347076   Iteration 35 of 100, tot loss = 4.931086216654096, l1: 0.00010790290938790089, l2: 0.0003852057091925027   Iteration 36 of 100, tot loss = 4.918700420194202, l1: 0.00010763964423353577, l2: 0.0003842303950578854   Iteration 37 of 100, tot loss = 4.914312417442734, l1: 0.00010731047990014524, l2: 0.0003841207586889583   Iteration 38 of 100, tot loss = 4.898126266504589, l1: 0.00010744871944648606, l2: 0.0003823639046076048   Iteration 39 of 100, tot loss = 4.8341407378514605, l1: 0.00010669402138610634, l2: 0.00037672005008715083   Iteration 40 of 100, tot loss = 4.879769799113274, l1: 0.00010756704377854476, l2: 0.00038040993385948243   Iteration 41 of 100, tot loss = 4.9026032046573915, l1: 0.0001081390224365949, l2: 0.00038212129673011967   Iteration 42 of 100, tot loss = 4.871349808715639, l1: 0.00010771566234152055, l2: 0.00037941931735812907   Iteration 43 of 100, tot loss = 4.804673835288646, l1: 0.00010614041313474868, l2: 0.0003743269690583178   Iteration 44 of 100, tot loss = 4.850436793132261, l1: 0.00010707034810118124, l2: 0.0003779733292917213   Iteration 45 of 100, tot loss = 4.836286934216817, l1: 0.00010690210684616533, l2: 0.0003767265854144676   Iteration 46 of 100, tot loss = 4.873878859955331, l1: 0.00010776433697648589, l2: 0.00037962354848707986   Iteration 47 of 100, tot loss = 4.8979702071940645, l1: 0.00010765841456149962, l2: 0.00038213860503635665   Iteration 48 of 100, tot loss = 4.883043684065342, l1: 0.00010720529326135875, l2: 0.00038109907412338845   Iteration 49 of 100, tot loss = 4.919915946162477, l1: 0.00010763863365911898, l2: 0.00038435295929809156   Iteration 50 of 100, tot loss = 4.866196072101593, l1: 0.00010641009335813578, l2: 0.0003802095120772719   Iteration 51 of 100, tot loss = 4.851904413279365, l1: 0.00010651013023258808, l2: 0.00037868030930814496   Iteration 52 of 100, tot loss = 4.827424143369381, l1: 0.00010605725940247514, l2: 0.0003766851531019291   Iteration 53 of 100, tot loss = 4.776256568026993, l1: 0.00010513252284068105, l2: 0.0003724931319187097   Iteration 54 of 100, tot loss = 4.779222132983031, l1: 0.00010502285773260087, l2: 0.0003728993542608805   Iteration 55 of 100, tot loss = 4.791350717978044, l1: 0.00010545420406164009, l2: 0.00037368086658799173   Iteration 56 of 100, tot loss = 4.7917416244745255, l1: 0.00010496592631170643, l2: 0.0003742082348513317   Iteration 57 of 100, tot loss = 4.765708130702638, l1: 0.00010448611085354187, l2: 0.00037208470090071817   Iteration 58 of 100, tot loss = 4.82256157028264, l1: 0.00010497198202536875, l2: 0.0003772841739881752   Iteration 59 of 100, tot loss = 4.780660243357643, l1: 0.00010396656752919179, l2: 0.00037409945576593787   Iteration 60 of 100, tot loss = 4.809290156761805, l1: 0.00010456105398285824, l2: 0.00037636796090131003   Iteration 61 of 100, tot loss = 4.807060282738482, l1: 0.00010473305811784917, l2: 0.0003759729691216203   Iteration 62 of 100, tot loss = 4.786096313307362, l1: 0.00010434754292345456, l2: 0.000374262087373814   Iteration 63 of 100, tot loss = 4.837227263147869, l1: 0.00010523988333131586, l2: 0.0003784828424815916   Iteration 64 of 100, tot loss = 4.820110699161887, l1: 0.00010476293311967311, l2: 0.00037724813637396437   Iteration 65 of 100, tot loss = 4.797462839346665, l1: 0.00010448174020544124, l2: 0.0003752645436459436   Iteration 66 of 100, tot loss = 4.830137297962651, l1: 0.00010541438027620174, l2: 0.00037759934925953996   Iteration 67 of 100, tot loss = 4.810883285394356, l1: 0.0001051203324285739, l2: 0.00037596799579184894   Iteration 68 of 100, tot loss = 4.816750461564345, l1: 0.00010519891657168046, l2: 0.00037647612942451174   Iteration 69 of 100, tot loss = 4.823789981828219, l1: 0.00010539127317209984, l2: 0.00037698772486191297   Iteration 70 of 100, tot loss = 4.814180433750153, l1: 0.00010515655844106472, l2: 0.00037626148509194276   Iteration 71 of 100, tot loss = 4.790477039108814, l1: 0.00010475510508756638, l2: 0.0003742925992028945   Iteration 72 of 100, tot loss = 4.784326949053341, l1: 0.0001048877611538046, l2: 0.0003735449344175221   Iteration 73 of 100, tot loss = 4.76767817752002, l1: 0.00010478158378957656, l2: 0.00037198623483649724   Iteration 74 of 100, tot loss = 4.766839215884337, l1: 0.00010476822963273908, l2: 0.0003719156927734337   Iteration 75 of 100, tot loss = 4.770668571790059, l1: 0.00010465593804838136, l2: 0.00037241092009935526   Iteration 76 of 100, tot loss = 4.766168495542125, l1: 0.00010474271268896318, l2: 0.00037187413755945223   Iteration 77 of 100, tot loss = 4.754185811265723, l1: 0.00010475280256972153, l2: 0.0003706657793750841   Iteration 78 of 100, tot loss = 4.784919761694395, l1: 0.00010548915489403627, l2: 0.00037300282248254056   Iteration 79 of 100, tot loss = 4.804085041903242, l1: 0.00010519727948121727, l2: 0.0003752112257827804   Iteration 80 of 100, tot loss = 4.864250527322293, l1: 0.00010610577246552566, l2: 0.00038031928070267893   Iteration 81 of 100, tot loss = 4.834753608997957, l1: 0.00010573205212562311, l2: 0.00037774330929904763   Iteration 82 of 100, tot loss = 4.827666057319176, l1: 0.00010554708871229483, l2: 0.00037721951727090946   Iteration 83 of 100, tot loss = 4.849625715290207, l1: 0.00010563913620208918, l2: 0.00037932343600761054   Iteration 84 of 100, tot loss = 4.8448020574592405, l1: 0.00010567243303479959, l2: 0.00037880777364036823   Iteration 85 of 100, tot loss = 4.8494726756039785, l1: 0.00010583264854229878, l2: 0.0003791146204628817   Iteration 86 of 100, tot loss = 4.883493358312651, l1: 0.00010627870091580959, l2: 0.00038207063588806495   Iteration 87 of 100, tot loss = 4.868126253972108, l1: 0.00010605029117492638, l2: 0.00038076233504816416   Iteration 88 of 100, tot loss = 4.879426765170964, l1: 0.00010607593148134501, l2: 0.00038186674630031285   Iteration 89 of 100, tot loss = 4.884988124450941, l1: 0.00010601397449401825, l2: 0.0003824848392285051   Iteration 90 of 100, tot loss = 4.8919423143068945, l1: 0.00010630688888745176, l2: 0.0003828873432616496   Iteration 91 of 100, tot loss = 4.905110740399623, l1: 0.00010636647717686943, l2: 0.0003841445977341609   Iteration 92 of 100, tot loss = 4.925465214511623, l1: 0.00010677284167879564, l2: 0.0003857736802551106   Iteration 93 of 100, tot loss = 4.898053693514998, l1: 0.00010625458021569366, l2: 0.00038355078954394826   Iteration 94 of 100, tot loss = 4.910326289369705, l1: 0.00010661869604751081, l2: 0.0003844139330582   Iteration 95 of 100, tot loss = 4.903078040323759, l1: 0.00010636732350624362, l2: 0.00038394048077814086   Iteration 96 of 100, tot loss = 4.894005920737982, l1: 0.00010623161176681606, l2: 0.0003831689805338101   Iteration 97 of 100, tot loss = 4.877788574425216, l1: 0.00010583162330692278, l2: 0.00038194723445898136   Iteration 98 of 100, tot loss = 4.883851524518461, l1: 0.00010588495276852901, l2: 0.00038250020009641326   Iteration 99 of 100, tot loss = 4.882678086107427, l1: 0.00010559205548641404, l2: 0.0003826757532843116   Iteration 100 of 100, tot loss = 4.8857681620121, l1: 0.00010562884406681405, l2: 0.0003829479722480755
   End of epoch 1132; saving model... 

Epoch 1133 of 2000
   Iteration 1 of 100, tot loss = 4.6941609382629395, l1: 7.465963426511735e-05, l2: 0.0003947564400732517   Iteration 2 of 100, tot loss = 6.822141885757446, l1: 0.00011536954116309062, l2: 0.0005668446247000247   Iteration 3 of 100, tot loss = 6.31544844309489, l1: 0.00011786483810283244, l2: 0.0005136799882166088   Iteration 4 of 100, tot loss = 6.181622266769409, l1: 0.0001152667773567373, l2: 0.0005028954255976714   Iteration 5 of 100, tot loss = 5.913025951385498, l1: 0.00011352562287356705, l2: 0.00047777695581316946   Iteration 6 of 100, tot loss = 5.555106163024902, l1: 0.00010959007583248119, l2: 0.0004459205277574559   Iteration 7 of 100, tot loss = 4.978217686925616, l1: 9.994550107096854e-05, l2: 0.00039787625636173677   Iteration 8 of 100, tot loss = 5.323142543435097, l1: 0.00010534638522585738, l2: 0.00042696786294982303   Iteration 9 of 100, tot loss = 5.159309638871087, l1: 0.00010265174164992964, l2: 0.00041327921563707705   Iteration 10 of 100, tot loss = 5.2002902865409855, l1: 0.00010632642479322386, l2: 0.00041370259859832006   Iteration 11 of 100, tot loss = 5.325948400930925, l1: 0.00010918758760619147, l2: 0.0004234072469196028   Iteration 12 of 100, tot loss = 5.18792250752449, l1: 0.00010843017025763402, l2: 0.00041036207524787943   Iteration 13 of 100, tot loss = 5.151275552236116, l1: 0.00010726175493730877, l2: 0.0004078657959605782   Iteration 14 of 100, tot loss = 4.884788470608847, l1: 0.00010247903004970535, l2: 0.00038599981313122304   Iteration 15 of 100, tot loss = 4.896369481086731, l1: 0.00010235461425812294, l2: 0.00038728232854433976   Iteration 16 of 100, tot loss = 4.965086869895458, l1: 0.00010371898042649264, l2: 0.0003927897000721714   Iteration 17 of 100, tot loss = 4.937039508539088, l1: 0.0001041961252614034, l2: 0.0003895078180529013   Iteration 18 of 100, tot loss = 4.8740491138564215, l1: 0.0001042722736504705, l2: 0.0003831326309106468   Iteration 19 of 100, tot loss = 4.886711478233337, l1: 0.00010505793455338694, l2: 0.00038361320414543644   Iteration 20 of 100, tot loss = 4.857776266336441, l1: 0.00010416751574666705, l2: 0.0003816101019765483   Iteration 21 of 100, tot loss = 4.855239737601507, l1: 0.00010362346317075814, l2: 0.0003819005024048411   Iteration 22 of 100, tot loss = 4.777495086193085, l1: 0.00010234325104234317, l2: 0.0003754062494077847   Iteration 23 of 100, tot loss = 4.738799711932307, l1: 0.00010208496172234173, l2: 0.0003717950011892042   Iteration 24 of 100, tot loss = 4.881108755866687, l1: 0.00010521557639246264, l2: 0.00038289529311441584   Iteration 25 of 100, tot loss = 4.849884219169617, l1: 0.00010505158425075934, l2: 0.00037993683043168857   Iteration 26 of 100, tot loss = 4.813781174329611, l1: 0.00010478540850901761, l2: 0.0003765927017061935   Iteration 27 of 100, tot loss = 4.8356483644909325, l1: 0.00010496795477121379, l2: 0.00037859687479256   Iteration 28 of 100, tot loss = 4.9124875111239295, l1: 0.00010676978178837868, l2: 0.00038447896310702035   Iteration 29 of 100, tot loss = 4.919949289025931, l1: 0.00010659676225823832, l2: 0.0003853981608448646   Iteration 30 of 100, tot loss = 4.912536672751108, l1: 0.00010677733201494752, l2: 0.0003844763290544506   Iteration 31 of 100, tot loss = 4.9567459052608855, l1: 0.00010688768769238865, l2: 0.00038878689731298494   Iteration 32 of 100, tot loss = 5.082573879510164, l1: 0.00010917060126303113, l2: 0.00039908677922539937   Iteration 33 of 100, tot loss = 4.999298670075157, l1: 0.00010839298810231301, l2: 0.00039153687138230344   Iteration 34 of 100, tot loss = 4.965097557095921, l1: 0.0001083994545350673, l2: 0.00038811029384412584   Iteration 35 of 100, tot loss = 4.993247553280422, l1: 0.0001089376534634669, l2: 0.000390387094687737   Iteration 36 of 100, tot loss = 4.953216397100025, l1: 0.00010825179055650046, l2: 0.00038706984216534894   Iteration 37 of 100, tot loss = 4.9193960299363, l1: 0.00010747026071594273, l2: 0.0003844693358562811   Iteration 38 of 100, tot loss = 4.876894709311034, l1: 0.00010601606692561243, l2: 0.0003816733976571796   Iteration 39 of 100, tot loss = 4.892140324299152, l1: 0.00010674958432970855, l2: 0.00038246444119767356   Iteration 40 of 100, tot loss = 4.876942428946495, l1: 0.00010645242400642019, l2: 0.0003812418117377092   Iteration 41 of 100, tot loss = 4.863586853190166, l1: 0.0001067290387454261, l2: 0.0003796296398987335   Iteration 42 of 100, tot loss = 4.867092016197386, l1: 0.00010729470124628971, l2: 0.00037941449391794215   Iteration 43 of 100, tot loss = 4.815256104912868, l1: 0.00010699448399083299, l2: 0.00037453112041518135   Iteration 44 of 100, tot loss = 4.796921510588039, l1: 0.00010689149795242966, l2: 0.0003728006468008971   Iteration 45 of 100, tot loss = 4.819455557399326, l1: 0.00010771977168688964, l2: 0.00037422577685598905   Iteration 46 of 100, tot loss = 4.812138046907342, l1: 0.00010784034400231853, l2: 0.0003733734536027718   Iteration 47 of 100, tot loss = 4.8279927106613805, l1: 0.00010779225502777448, l2: 0.00037500700938758105   Iteration 48 of 100, tot loss = 4.821346916258335, l1: 0.00010732117122339939, l2: 0.0003748135139479321   Iteration 49 of 100, tot loss = 4.820008664715047, l1: 0.00010734120557887708, l2: 0.00037465965430542105   Iteration 50 of 100, tot loss = 4.832184092998505, l1: 0.00010767858868348412, l2: 0.0003755398136854637   Iteration 51 of 100, tot loss = 4.824830417539559, l1: 0.00010774221471579308, l2: 0.0003747408206668208   Iteration 52 of 100, tot loss = 4.838622512725683, l1: 0.00010753009323357568, l2: 0.0003763321523225526   Iteration 53 of 100, tot loss = 4.8577035125696435, l1: 0.00010811677435962131, l2: 0.00037765357039873225   Iteration 54 of 100, tot loss = 4.870933221446143, l1: 0.00010821684770268837, l2: 0.0003788764676115803   Iteration 55 of 100, tot loss = 4.87150381044908, l1: 0.00010764172197510066, l2: 0.00037950865186179395   Iteration 56 of 100, tot loss = 4.965863634433065, l1: 0.0001090903958486576, l2: 0.00038749596010997526   Iteration 57 of 100, tot loss = 4.975706399532786, l1: 0.00010945415366829063, l2: 0.00038811647940300436   Iteration 58 of 100, tot loss = 4.971684996424051, l1: 0.00010926907296003453, l2: 0.000387899419679186   Iteration 59 of 100, tot loss = 4.9333155660306, l1: 0.00010816331854646521, l2: 0.0003851682308546299   Iteration 60 of 100, tot loss = 4.9227239271005, l1: 0.00010794506039625654, l2: 0.00038432732483973574   Iteration 61 of 100, tot loss = 4.873284101486206, l1: 0.00010681574194230231, l2: 0.0003805126610534556   Iteration 62 of 100, tot loss = 4.857439333392728, l1: 0.0001066947000752759, l2: 0.00037904922634967783   Iteration 63 of 100, tot loss = 4.879414914146302, l1: 0.00010683275040863673, l2: 0.00038110873431414515   Iteration 64 of 100, tot loss = 4.88371517509222, l1: 0.00010728526865477761, l2: 0.00038108624232791044   Iteration 65 of 100, tot loss = 4.8826073939983665, l1: 0.00010737157840944397, l2: 0.0003808891545094621   Iteration 66 of 100, tot loss = 4.90033406199831, l1: 0.00010789322693722859, l2: 0.0003821401735353213   Iteration 67 of 100, tot loss = 4.921132237163942, l1: 0.00010837922700239457, l2: 0.00038373399121467884   Iteration 68 of 100, tot loss = 4.909237658276277, l1: 0.00010813254619493177, l2: 0.0003827912143392228   Iteration 69 of 100, tot loss = 4.879731257756551, l1: 0.00010744924103441325, l2: 0.0003805238795947567   Iteration 70 of 100, tot loss = 4.894629761150905, l1: 0.00010762333284739206, l2: 0.00038183963843039235   Iteration 71 of 100, tot loss = 4.88539806889816, l1: 0.00010759155000568183, l2: 0.0003809482520192035   Iteration 72 of 100, tot loss = 4.896741804149416, l1: 0.00010767652823536385, l2: 0.0003819976475117275   Iteration 73 of 100, tot loss = 4.877686663849713, l1: 0.0001070083761153175, l2: 0.00038076028531203556   Iteration 74 of 100, tot loss = 4.874450574050078, l1: 0.00010697279980672033, l2: 0.0003804722526527601   Iteration 75 of 100, tot loss = 4.858919035593669, l1: 0.00010670627821430875, l2: 0.0003791856204043143   Iteration 76 of 100, tot loss = 4.862365710107904, l1: 0.00010689919216881543, l2: 0.0003793373736315468   Iteration 77 of 100, tot loss = 4.823322214089431, l1: 0.00010600561044948454, l2: 0.00037632660583862066   Iteration 78 of 100, tot loss = 4.828869408521896, l1: 0.00010600709389016115, l2: 0.00037687984214967285   Iteration 79 of 100, tot loss = 4.822749335554581, l1: 0.00010577026726803271, l2: 0.0003765046614394641   Iteration 80 of 100, tot loss = 4.844546435773372, l1: 0.00010600460614114126, l2: 0.0003784500328947615   Iteration 81 of 100, tot loss = 4.836313046055076, l1: 0.00010584860852018378, l2: 0.0003777826917221206   Iteration 82 of 100, tot loss = 4.813046128284641, l1: 0.00010553637730481569, l2: 0.000375768231251419   Iteration 83 of 100, tot loss = 4.8159111996731125, l1: 0.00010537874550209381, l2: 0.00037621237004122483   Iteration 84 of 100, tot loss = 4.859298477570216, l1: 0.00010593029803983103, l2: 0.0003799995452677   Iteration 85 of 100, tot loss = 4.846127716232749, l1: 0.00010580949879915673, l2: 0.0003788032682838521   Iteration 86 of 100, tot loss = 4.87878000874852, l1: 0.00010611815517666739, l2: 0.00038175984141267946   Iteration 87 of 100, tot loss = 4.858507916845125, l1: 0.00010582112865930626, l2: 0.0003800296589120655   Iteration 88 of 100, tot loss = 4.848778051408854, l1: 0.00010553924795947916, l2: 0.0003793385528545266   Iteration 89 of 100, tot loss = 4.844679948988925, l1: 0.00010530186716167602, l2: 0.0003791661236371081   Iteration 90 of 100, tot loss = 4.857928783363766, l1: 0.00010528032786775536, l2: 0.0003805125467705592   Iteration 91 of 100, tot loss = 4.849926784798339, l1: 0.00010508827472222038, l2: 0.0003799043999886355   Iteration 92 of 100, tot loss = 4.856762993594875, l1: 0.00010477465900254649, l2: 0.0003809016361628892   Iteration 93 of 100, tot loss = 4.8525511052018855, l1: 0.00010495101761539787, l2: 0.00038030408865769684   Iteration 94 of 100, tot loss = 4.846754580102068, l1: 0.00010489297188116247, l2: 0.0003797824820720323   Iteration 95 of 100, tot loss = 4.87307349129727, l1: 0.00010517743024760612, l2: 0.0003821299147124304   Iteration 96 of 100, tot loss = 4.8557197240491705, l1: 0.00010511674527909538, l2: 0.00038045522304249363   Iteration 97 of 100, tot loss = 4.848386178311613, l1: 0.00010504708680133431, l2: 0.00037979152701379847   Iteration 98 of 100, tot loss = 4.846265747839091, l1: 0.00010493198273512203, l2: 0.00037969458824182783   Iteration 99 of 100, tot loss = 4.835658792293433, l1: 0.00010481409716326978, l2: 0.0003787517782642901   Iteration 100 of 100, tot loss = 4.803101449012757, l1: 0.00010413908206828637, l2: 0.00037617105910612734
   End of epoch 1133; saving model... 

Epoch 1134 of 2000
   Iteration 1 of 100, tot loss = 2.704184055328369, l1: 7.780499436194077e-05, l2: 0.0001926134282257408   Iteration 2 of 100, tot loss = 4.943131685256958, l1: 0.00011171886217198335, l2: 0.0003825943131232634   Iteration 3 of 100, tot loss = 4.648784478505452, l1: 0.00010949235002044588, l2: 0.00035538609760502976   Iteration 4 of 100, tot loss = 4.354483366012573, l1: 0.0001036499634210486, l2: 0.0003317983719171025   Iteration 5 of 100, tot loss = 4.003140497207641, l1: 9.969778184313328e-05, l2: 0.00030061626748647543   Iteration 6 of 100, tot loss = 4.402715643246968, l1: 0.00010154544603816855, l2: 0.00033872611190114793   Iteration 7 of 100, tot loss = 4.411239794322422, l1: 0.0001028655907638105, l2: 0.00033825838181655854   Iteration 8 of 100, tot loss = 4.4379062950611115, l1: 9.961735577235231e-05, l2: 0.00034417326423863415   Iteration 9 of 100, tot loss = 4.607934766345554, l1: 0.0001028402111842297, l2: 0.0003579532543274884   Iteration 10 of 100, tot loss = 4.517295074462891, l1: 0.0001020975600113161, l2: 0.00034963193611474707   Iteration 11 of 100, tot loss = 4.626609672199596, l1: 9.940695467362688e-05, l2: 0.0003632540002317083   Iteration 12 of 100, tot loss = 4.763953407605489, l1: 0.00010369783437151152, l2: 0.0003726974985814498   Iteration 13 of 100, tot loss = 4.626238951316247, l1: 0.00010219401771722075, l2: 0.0003604298701527743   Iteration 14 of 100, tot loss = 4.63982846055712, l1: 0.00010362139281434273, l2: 0.00036036144696741497   Iteration 15 of 100, tot loss = 4.527630313237508, l1: 0.00010106243620005747, l2: 0.0003517005893324191   Iteration 16 of 100, tot loss = 4.654375687241554, l1: 0.0001030433040796197, l2: 0.00036239426026440924   Iteration 17 of 100, tot loss = 4.628374814987183, l1: 0.00010356059090768481, l2: 0.0003592768874060472   Iteration 18 of 100, tot loss = 4.639300227165222, l1: 0.00010480630549459925, l2: 0.00035912371418412984   Iteration 19 of 100, tot loss = 4.692047156785664, l1: 0.0001051145039750018, l2: 0.00036409020800707176   Iteration 20 of 100, tot loss = 4.616090130805969, l1: 0.00010311830337741412, l2: 0.0003584907062759157   Iteration 21 of 100, tot loss = 4.57689585004534, l1: 0.00010223498940745013, l2: 0.00035545459208411297   Iteration 22 of 100, tot loss = 4.433494307778099, l1: 9.950378758234861e-05, l2: 0.0003438456399387426   Iteration 23 of 100, tot loss = 4.478736587192701, l1: 9.994833548578596e-05, l2: 0.00034792532096616924   Iteration 24 of 100, tot loss = 4.498303413391113, l1: 0.00010101294734946957, l2: 0.0003488173909621158   Iteration 25 of 100, tot loss = 4.4212542915344235, l1: 9.911968852975406e-05, l2: 0.000343005737522617   Iteration 26 of 100, tot loss = 4.441014821712788, l1: 0.0001002713751614255, l2: 0.00034383010419193085   Iteration 27 of 100, tot loss = 4.5066004329257545, l1: 0.00010213843254152582, l2: 0.00034852160671208467   Iteration 28 of 100, tot loss = 4.4808944037982394, l1: 0.0001012145555380682, l2: 0.00034687488057118444   Iteration 29 of 100, tot loss = 4.483872520512548, l1: 0.00010156305218296093, l2: 0.0003468241956052852   Iteration 30 of 100, tot loss = 4.539782277743021, l1: 0.00010235968426665446, l2: 0.00035161853884346783   Iteration 31 of 100, tot loss = 4.521772776880572, l1: 0.0001016132222092335, l2: 0.00035056405073602594   Iteration 32 of 100, tot loss = 4.4491294622421265, l1: 0.000100663611078744, l2: 0.00034424933073751163   Iteration 33 of 100, tot loss = 4.436749516111432, l1: 0.00010098709257859313, l2: 0.00034268785472677064   Iteration 34 of 100, tot loss = 4.430788278579712, l1: 0.00010134362491953652, l2: 0.0003417351990184911   Iteration 35 of 100, tot loss = 4.370597655432565, l1: 0.00010037279519435418, l2: 0.0003366869662256379   Iteration 36 of 100, tot loss = 4.362597088019053, l1: 0.00010093983019032748, l2: 0.0003353198743651673   Iteration 37 of 100, tot loss = 4.334769371393564, l1: 0.00010061410687961364, l2: 0.00033286282583454475   Iteration 38 of 100, tot loss = 4.344758905862507, l1: 0.00010105150364612008, l2: 0.00033342438301157306   Iteration 39 of 100, tot loss = 4.354583428456233, l1: 0.00010107436733400951, l2: 0.0003343839724458611   Iteration 40 of 100, tot loss = 4.391914910078048, l1: 0.00010209157871940989, l2: 0.0003370999085746007   Iteration 41 of 100, tot loss = 4.430634411369882, l1: 0.00010297218649633933, l2: 0.0003400912516176837   Iteration 42 of 100, tot loss = 4.397737230573382, l1: 0.00010259797316047341, l2: 0.0003371757470415018   Iteration 43 of 100, tot loss = 4.433524364648863, l1: 0.00010261172251058418, l2: 0.000340740711476957   Iteration 44 of 100, tot loss = 4.465390237894925, l1: 0.00010311891897303708, l2: 0.0003434201029118743   Iteration 45 of 100, tot loss = 4.531714248657226, l1: 0.00010381675537145283, l2: 0.0003493546669940568   Iteration 46 of 100, tot loss = 4.482152695241182, l1: 0.00010306593487811092, l2: 0.0003451493318172415   Iteration 47 of 100, tot loss = 4.496576020058165, l1: 0.00010309349957598432, l2: 0.00034656410013880344   Iteration 48 of 100, tot loss = 4.462814852595329, l1: 0.00010249891715830017, l2: 0.00034378256592996576   Iteration 49 of 100, tot loss = 4.463014179346513, l1: 0.00010284173623979453, l2: 0.0003434596791131688   Iteration 50 of 100, tot loss = 4.493146872520446, l1: 0.00010312611535482574, l2: 0.00034618856967426836   Iteration 51 of 100, tot loss = 4.466131813385907, l1: 0.00010277985076719037, l2: 0.0003438333281105859   Iteration 52 of 100, tot loss = 4.4880815881949205, l1: 0.00010353396835573501, l2: 0.00034527418756624684   Iteration 53 of 100, tot loss = 4.5198269295242595, l1: 0.00010356627233636143, l2: 0.0003484164179030384   Iteration 54 of 100, tot loss = 4.550461455627724, l1: 0.00010375936896404407, l2: 0.00035128677403985486   Iteration 55 of 100, tot loss = 4.569391905177723, l1: 0.0001042317073204851, l2: 0.00035270748035558924   Iteration 56 of 100, tot loss = 4.540545689208167, l1: 0.00010390983613563418, l2: 0.00035014473022394147   Iteration 57 of 100, tot loss = 4.54607331543638, l1: 0.00010395904010102528, l2: 0.00035064828861913267   Iteration 58 of 100, tot loss = 4.559681074372653, l1: 0.00010461045913588947, l2: 0.00035135764608577153   Iteration 59 of 100, tot loss = 4.5634950904522915, l1: 0.00010422917650923749, l2: 0.0003521203308097892   Iteration 60 of 100, tot loss = 4.542609643936157, l1: 0.0001038336433339282, l2: 0.0003504273195479376   Iteration 61 of 100, tot loss = 4.556029648077293, l1: 0.00010438053659974505, l2: 0.00035122242712293613   Iteration 62 of 100, tot loss = 4.5518841358923146, l1: 0.00010416694573406505, l2: 0.00035102146672039863   Iteration 63 of 100, tot loss = 4.524156017908974, l1: 0.00010355449893044323, l2: 0.0003488611015084658   Iteration 64 of 100, tot loss = 4.494201239198446, l1: 0.00010287068715797432, l2: 0.00034654943556233775   Iteration 65 of 100, tot loss = 4.494597563376794, l1: 0.00010224456955956366, l2: 0.00034721518526426877   Iteration 66 of 100, tot loss = 4.5209334510745425, l1: 0.00010265896041043258, l2: 0.00034943438367918134   Iteration 67 of 100, tot loss = 4.534332172194524, l1: 0.00010307852230721098, l2: 0.00035035469414624595   Iteration 68 of 100, tot loss = 4.522500564070309, l1: 0.00010295787171130273, l2: 0.00034929218407555977   Iteration 69 of 100, tot loss = 4.520753922669784, l1: 0.0001029973258403555, l2: 0.0003490780659046942   Iteration 70 of 100, tot loss = 4.533566999435425, l1: 0.00010328793479337556, l2: 0.0003500687648608748   Iteration 71 of 100, tot loss = 4.527904356029672, l1: 0.00010338875182869871, l2: 0.0003494016836810542   Iteration 72 of 100, tot loss = 4.554366654819912, l1: 0.00010401655703794354, l2: 0.000351420108523194   Iteration 73 of 100, tot loss = 4.524195279160591, l1: 0.00010326922357270545, l2: 0.000349150304379554   Iteration 74 of 100, tot loss = 4.535746883701634, l1: 0.00010319306395988836, l2: 0.00035038162486917163   Iteration 75 of 100, tot loss = 4.5417256164550786, l1: 0.00010328497902567808, l2: 0.0003508875829478105   Iteration 76 of 100, tot loss = 4.532762649812196, l1: 0.00010267072713376317, l2: 0.00035060553828340123   Iteration 77 of 100, tot loss = 4.534546062543795, l1: 0.00010303976339312247, l2: 0.0003504148433468752   Iteration 78 of 100, tot loss = 4.526706331815475, l1: 0.00010271784907299429, l2: 0.000349952784465411   Iteration 79 of 100, tot loss = 4.538596204564541, l1: 0.0001030171746307029, l2: 0.0003508424461536298   Iteration 80 of 100, tot loss = 4.537121888995171, l1: 0.00010306738959116046, l2: 0.0003506447996187489   Iteration 81 of 100, tot loss = 4.543018832618808, l1: 0.00010331753143008775, l2: 0.0003509843523648602   Iteration 82 of 100, tot loss = 4.513664809668937, l1: 0.00010264599355961276, l2: 0.000348720487836101   Iteration 83 of 100, tot loss = 4.506982326507568, l1: 0.00010271507887070135, l2: 0.0003479831543097452   Iteration 84 of 100, tot loss = 4.511546628815787, l1: 0.00010287714953197533, l2: 0.0003482775135212467   Iteration 85 of 100, tot loss = 4.49803490077748, l1: 0.00010251002048094319, l2: 0.0003472934697877944   Iteration 86 of 100, tot loss = 4.465474638828011, l1: 0.00010184352089878345, l2: 0.0003447039431870677   Iteration 87 of 100, tot loss = 4.475977486577527, l1: 0.000101895336260425, l2: 0.00034570241254625907   Iteration 88 of 100, tot loss = 4.496803191575137, l1: 0.0001020360573007069, l2: 0.00034764426163746975   Iteration 89 of 100, tot loss = 4.493379898285598, l1: 0.00010207523326938867, l2: 0.0003472627565693738   Iteration 90 of 100, tot loss = 4.489914004007975, l1: 0.0001021362337471348, l2: 0.00034685516665275727   Iteration 91 of 100, tot loss = 4.5143411709712105, l1: 0.00010232435194596362, l2: 0.00034910976531106854   Iteration 92 of 100, tot loss = 4.511284403179003, l1: 0.00010219396743037657, l2: 0.0003489344728776537   Iteration 93 of 100, tot loss = 4.495339129560737, l1: 0.00010186108768046383, l2: 0.000347672825521459   Iteration 94 of 100, tot loss = 4.482523410878283, l1: 0.00010150117668672465, l2: 0.00034675116469944886   Iteration 95 of 100, tot loss = 4.469144881399054, l1: 0.00010115520968898444, l2: 0.0003457592788005346   Iteration 96 of 100, tot loss = 4.449571164945762, l1: 0.00010066210415971, l2: 0.0003442950125342274   Iteration 97 of 100, tot loss = 4.45338874256488, l1: 0.0001006597728341261, l2: 0.0003446791013198684   Iteration 98 of 100, tot loss = 4.481714625747836, l1: 0.00010109006020962736, l2: 0.00034708140224303896   Iteration 99 of 100, tot loss = 4.466165340307987, l1: 0.00010076841429720228, l2: 0.0003458481195860194   Iteration 100 of 100, tot loss = 4.454036855697632, l1: 0.00010080426451168022, l2: 0.0003445994209323544
   End of epoch 1134; saving model... 

Epoch 1135 of 2000
   Iteration 1 of 100, tot loss = 2.1923129558563232, l1: 6.634753663092852e-05, l2: 0.00015288376016542315   Iteration 2 of 100, tot loss = 2.4335094690322876, l1: 6.888097414048389e-05, l2: 0.00017446996935177594   Iteration 3 of 100, tot loss = 2.6720030307769775, l1: 6.90007655066438e-05, l2: 0.0001981995398333917   Iteration 4 of 100, tot loss = 2.7979930639266968, l1: 7.207472117443103e-05, l2: 0.0002077245844702702   Iteration 5 of 100, tot loss = 2.662936210632324, l1: 6.578426691703499e-05, l2: 0.00020050935272593052   Iteration 6 of 100, tot loss = 2.8017125129699707, l1: 6.88289655954577e-05, l2: 0.00021134228397083157   Iteration 7 of 100, tot loss = 2.7849386078970775, l1: 6.848522649046831e-05, l2: 0.00021000863177635307   Iteration 8 of 100, tot loss = 2.9333780705928802, l1: 7.076382917148294e-05, l2: 0.00022257397540670354   Iteration 9 of 100, tot loss = 3.1685342523786755, l1: 7.620119948923174e-05, l2: 0.00024065222209578173   Iteration 10 of 100, tot loss = 3.207956004142761, l1: 7.612033514305949e-05, l2: 0.00024467526382068174   Iteration 11 of 100, tot loss = 3.1762857653877954, l1: 7.43622417344754e-05, l2: 0.00024326633402696726   Iteration 12 of 100, tot loss = 3.316580275694529, l1: 7.673827925221606e-05, l2: 0.00025491974884062074   Iteration 13 of 100, tot loss = 3.4987840835864725, l1: 8.032257895907745e-05, l2: 0.00026955582591024443   Iteration 14 of 100, tot loss = 3.8338125262941634, l1: 8.600339450432719e-05, l2: 0.00029737785371253267   Iteration 15 of 100, tot loss = 3.8749324639638263, l1: 8.680873167274209e-05, l2: 0.00030068451014813035   Iteration 16 of 100, tot loss = 3.827174499630928, l1: 8.752365170039411e-05, l2: 0.00029519379495468456   Iteration 17 of 100, tot loss = 3.805972884683048, l1: 8.752798898700242e-05, l2: 0.0002930692965438699   Iteration 18 of 100, tot loss = 3.963211456934611, l1: 9.10637320177759e-05, l2: 0.00030525741102691326   Iteration 19 of 100, tot loss = 4.052711687589946, l1: 9.312848411163462e-05, l2: 0.0003121426837567828   Iteration 20 of 100, tot loss = 3.967370533943176, l1: 9.060877291631187e-05, l2: 0.00030612827904406   Iteration 21 of 100, tot loss = 3.9522642408098494, l1: 8.998686808765688e-05, l2: 0.0003052395539790658   Iteration 22 of 100, tot loss = 3.894478754563765, l1: 8.909804736586838e-05, l2: 0.000300349825755058   Iteration 23 of 100, tot loss = 3.9278510549794072, l1: 9.013869551025614e-05, l2: 0.00030264640720459914   Iteration 24 of 100, tot loss = 4.025582830111186, l1: 9.14186643967696e-05, l2: 0.00031113961631490383   Iteration 25 of 100, tot loss = 4.053084239959717, l1: 9.076949514565058e-05, l2: 0.0003145389264682308   Iteration 26 of 100, tot loss = 4.106418554599468, l1: 9.191955611570917e-05, l2: 0.00031872229849865945   Iteration 27 of 100, tot loss = 4.112998856438531, l1: 9.227327501759175e-05, l2: 0.0003190266094558562   Iteration 28 of 100, tot loss = 4.135533349854605, l1: 9.287902204440408e-05, l2: 0.00032067431259617606   Iteration 29 of 100, tot loss = 4.0950122126217545, l1: 9.165564200696375e-05, l2: 0.0003178455788593996   Iteration 30 of 100, tot loss = 4.07162872950236, l1: 9.196304769526857e-05, l2: 0.0003151998248843787   Iteration 31 of 100, tot loss = 4.066301914953416, l1: 9.27606304157375e-05, l2: 0.00031386956059929706   Iteration 32 of 100, tot loss = 4.053439103066921, l1: 9.246223476111481e-05, l2: 0.0003128816747448582   Iteration 33 of 100, tot loss = 4.021628611015551, l1: 9.16449293568307e-05, l2: 0.0003105179308807816   Iteration 34 of 100, tot loss = 4.017493584576775, l1: 9.17434819498072e-05, l2: 0.0003100058756364674   Iteration 35 of 100, tot loss = 4.039809499468122, l1: 9.196959290420636e-05, l2: 0.0003120113567482414   Iteration 36 of 100, tot loss = 4.069877478811476, l1: 9.250375700907575e-05, l2: 0.00031448399103181955   Iteration 37 of 100, tot loss = 4.078395650193498, l1: 9.28006971859046e-05, l2: 0.0003150388676314489   Iteration 38 of 100, tot loss = 4.113652881823088, l1: 9.356677458670579e-05, l2: 0.00031779851346264447   Iteration 39 of 100, tot loss = 4.108249511474218, l1: 9.329693887430507e-05, l2: 0.0003175280121817755   Iteration 40 of 100, tot loss = 4.113802641630173, l1: 9.300148376496508e-05, l2: 0.0003183787801390281   Iteration 41 of 100, tot loss = 4.149316747014115, l1: 9.384376874261694e-05, l2: 0.00032108790623080894   Iteration 42 of 100, tot loss = 4.1406309604644775, l1: 9.347541050547512e-05, l2: 0.00032058768591100703   Iteration 43 of 100, tot loss = 4.138943882875664, l1: 9.395326653544163e-05, l2: 0.00031994112265068867   Iteration 44 of 100, tot loss = 4.164632092822682, l1: 9.466204392083455e-05, l2: 0.0003218011667988983   Iteration 45 of 100, tot loss = 4.151027022467719, l1: 9.431705564363964e-05, l2: 0.0003207856482024201   Iteration 46 of 100, tot loss = 4.1993402294490645, l1: 9.504920846033518e-05, l2: 0.0003248848158324583   Iteration 47 of 100, tot loss = 4.235826411145799, l1: 9.500906920986885e-05, l2: 0.00032857357322466264   Iteration 48 of 100, tot loss = 4.206555197636287, l1: 9.447519551031291e-05, l2: 0.000326180325828318   Iteration 49 of 100, tot loss = 4.163122289034785, l1: 9.381447619359408e-05, l2: 0.00032249775421758166   Iteration 50 of 100, tot loss = 4.190970511436462, l1: 9.424283620319329e-05, l2: 0.00032485421717865393   Iteration 51 of 100, tot loss = 4.1679988842384486, l1: 9.351568409257714e-05, l2: 0.0003232842065307184   Iteration 52 of 100, tot loss = 4.193619434650127, l1: 9.38569661369771e-05, l2: 0.0003255049800477886   Iteration 53 of 100, tot loss = 4.171544938717249, l1: 9.345754184529519e-05, l2: 0.0003236969548523567   Iteration 54 of 100, tot loss = 4.169431209564209, l1: 9.279574226605257e-05, l2: 0.000324147380996254   Iteration 55 of 100, tot loss = 4.1700516700744625, l1: 9.286253453782676e-05, l2: 0.0003241426348474554   Iteration 56 of 100, tot loss = 4.171963802405766, l1: 9.242961446683953e-05, l2: 0.0003247667679845888   Iteration 57 of 100, tot loss = 4.172624320314641, l1: 9.277470899018736e-05, l2: 0.0003244877254729274   Iteration 58 of 100, tot loss = 4.1892525245403425, l1: 9.296682150432326e-05, l2: 0.0003259584331756522   Iteration 59 of 100, tot loss = 4.158715817887904, l1: 9.235786533050271e-05, l2: 0.0003235137188250704   Iteration 60 of 100, tot loss = 4.21982661485672, l1: 9.324155920088136e-05, l2: 0.00032874110402190124   Iteration 61 of 100, tot loss = 4.250970039211336, l1: 9.362869819656747e-05, l2: 0.0003314683074036186   Iteration 62 of 100, tot loss = 4.246904334714336, l1: 9.353067822127243e-05, l2: 0.00033115975689255604   Iteration 63 of 100, tot loss = 4.214464509297931, l1: 9.265071766715246e-05, l2: 0.00032879573492599387   Iteration 64 of 100, tot loss = 4.213236462324858, l1: 9.259295831043346e-05, l2: 0.0003287306899437681   Iteration 65 of 100, tot loss = 4.235447535148034, l1: 9.30777639531208e-05, l2: 0.00033046699155910085   Iteration 66 of 100, tot loss = 4.249435746308529, l1: 9.359063711378963e-05, l2: 0.00033135293977631426   Iteration 67 of 100, tot loss = 4.278433369166815, l1: 9.42680625172867e-05, l2: 0.00033357527685601876   Iteration 68 of 100, tot loss = 4.259976313394659, l1: 9.43540955952759e-05, l2: 0.00033164353804108137   Iteration 69 of 100, tot loss = 4.273863477983337, l1: 9.48026469774524e-05, l2: 0.0003325837026130867   Iteration 70 of 100, tot loss = 4.277997388158526, l1: 9.444584798724723e-05, l2: 0.0003333538922431347   Iteration 71 of 100, tot loss = 4.266304348556089, l1: 9.457027757069765e-05, l2: 0.000332060158720017   Iteration 72 of 100, tot loss = 4.2735561894045935, l1: 9.461996114623616e-05, l2: 0.00033273565936219203   Iteration 73 of 100, tot loss = 4.27085016524955, l1: 9.440631277369628e-05, l2: 0.000332678705079111   Iteration 74 of 100, tot loss = 4.279496608553706, l1: 9.440157427397444e-05, l2: 0.0003335480883047362   Iteration 75 of 100, tot loss = 4.286948709487915, l1: 9.430343319157448e-05, l2: 0.000334391439294753   Iteration 76 of 100, tot loss = 4.3100957086211755, l1: 9.508887481053152e-05, l2: 0.00033592069724507284   Iteration 77 of 100, tot loss = 4.3272020073680135, l1: 9.560607619963091e-05, l2: 0.0003371141257910542   Iteration 78 of 100, tot loss = 4.377559530429351, l1: 9.645468502966627e-05, l2: 0.00034130126956509997   Iteration 79 of 100, tot loss = 4.370244681080686, l1: 9.620468994444885e-05, l2: 0.00034081977972601646   Iteration 80 of 100, tot loss = 4.36712136566639, l1: 9.630179015402973e-05, l2: 0.00034041034796246096   Iteration 81 of 100, tot loss = 4.378630223097624, l1: 9.671744627678296e-05, l2: 0.0003411455777327151   Iteration 82 of 100, tot loss = 4.396533468874489, l1: 9.7018494737468e-05, l2: 0.00034263485381748847   Iteration 83 of 100, tot loss = 4.418140391269362, l1: 9.70673153274384e-05, l2: 0.0003447467252592478   Iteration 84 of 100, tot loss = 4.435611977463677, l1: 9.723326656101216e-05, l2: 0.0003463279331515965   Iteration 85 of 100, tot loss = 4.4166357236749985, l1: 9.667522056534997e-05, l2: 0.00034498835362576166   Iteration 86 of 100, tot loss = 4.4302476311838905, l1: 9.681176455815994e-05, l2: 0.00034621300031620534   Iteration 87 of 100, tot loss = 4.4359618959755736, l1: 9.679169574385809e-05, l2: 0.00034680449549007045   Iteration 88 of 100, tot loss = 4.437362397258932, l1: 9.675516056714432e-05, l2: 0.0003469810804248978   Iteration 89 of 100, tot loss = 4.444020043598132, l1: 9.682743796709862e-05, l2: 0.00034757456765117733   Iteration 90 of 100, tot loss = 4.4174876477983265, l1: 9.640449497965165e-05, l2: 0.0003453442710451782   Iteration 91 of 100, tot loss = 4.407705196967492, l1: 9.636773405133512e-05, l2: 0.0003444027868201854   Iteration 92 of 100, tot loss = 4.41177993753682, l1: 9.651033099583836e-05, l2: 0.00034466766405587447   Iteration 93 of 100, tot loss = 4.417876105154714, l1: 9.643278916814534e-05, l2: 0.0003453548224667908   Iteration 94 of 100, tot loss = 4.45664563584835, l1: 9.73347662867572e-05, l2: 0.0003483297982614765   Iteration 95 of 100, tot loss = 4.451854690752532, l1: 9.75035095746678e-05, l2: 0.0003476819605566561   Iteration 96 of 100, tot loss = 4.454892545938492, l1: 9.756124791238108e-05, l2: 0.00034792800791668316   Iteration 97 of 100, tot loss = 4.428275881354342, l1: 9.712684066508915e-05, l2: 0.00034570074868244455   Iteration 98 of 100, tot loss = 4.408123280320849, l1: 9.662544697657352e-05, l2: 0.0003441868822698538   Iteration 99 of 100, tot loss = 4.445820626586374, l1: 9.741672233712501e-05, l2: 0.0003471653420487513   Iteration 100 of 100, tot loss = 4.447085911035538, l1: 9.738145130540943e-05, l2: 0.00034732714120764283
   End of epoch 1135; saving model... 

Epoch 1136 of 2000
   Iteration 1 of 100, tot loss = 3.7278456687927246, l1: 6.226362165762112e-05, l2: 0.0003105209325440228   Iteration 2 of 100, tot loss = 4.798272609710693, l1: 9.129146201303229e-05, l2: 0.0003885358019033447   Iteration 3 of 100, tot loss = 5.41190751393636, l1: 0.00011057548302536209, l2: 0.00043061527928027016   Iteration 4 of 100, tot loss = 5.063466787338257, l1: 0.00010441597078170162, l2: 0.0004019307089038193   Iteration 5 of 100, tot loss = 4.956468105316162, l1: 0.00010313615493942052, l2: 0.00039251065463759006   Iteration 6 of 100, tot loss = 5.262391567230225, l1: 0.00010963118741832052, l2: 0.0004166079597780481   Iteration 7 of 100, tot loss = 4.996664660317557, l1: 0.00010629274454134117, l2: 0.0003933737129305622   Iteration 8 of 100, tot loss = 5.243493318557739, l1: 0.00011194779381185072, l2: 0.00041240152859245427   Iteration 9 of 100, tot loss = 5.246354050106472, l1: 0.0001130941277046481, l2: 0.00041154126908319694   Iteration 10 of 100, tot loss = 5.0857356786727905, l1: 0.00011169093559146859, l2: 0.00039688262331765144   Iteration 11 of 100, tot loss = 5.259671059521762, l1: 0.00011318735712186687, l2: 0.00041277974113737315   Iteration 12 of 100, tot loss = 5.151128749052684, l1: 0.00011108846714099248, l2: 0.0004040244020870887   Iteration 13 of 100, tot loss = 5.417088783704317, l1: 0.00011346716648684099, l2: 0.0004282417066860944   Iteration 14 of 100, tot loss = 5.261660643986294, l1: 0.00010960112584663355, l2: 0.0004165649339224079   Iteration 15 of 100, tot loss = 5.161368783315023, l1: 0.00010982096403798399, l2: 0.00040631590915533405   Iteration 16 of 100, tot loss = 5.1793365478515625, l1: 0.00011154849357808416, l2: 0.0004063851538376184   Iteration 17 of 100, tot loss = 5.134538061478558, l1: 0.00010872780716026147, l2: 0.0004047259914360064   Iteration 18 of 100, tot loss = 4.972919596566094, l1: 0.0001060207828737071, l2: 0.0003912711690645665   Iteration 19 of 100, tot loss = 5.0334372771413705, l1: 0.00010602563615018306, l2: 0.000397318085704587   Iteration 20 of 100, tot loss = 5.1278728723526, l1: 0.0001070144910045201, l2: 0.0004057727899635211   Iteration 21 of 100, tot loss = 5.152727581205822, l1: 0.00010734353348934313, l2: 0.0004079292176313521   Iteration 22 of 100, tot loss = 5.09461353041909, l1: 0.00010755938803247938, l2: 0.00040190195828803223   Iteration 23 of 100, tot loss = 5.110419377036717, l1: 0.00010820487944368759, l2: 0.0004028370515342154   Iteration 24 of 100, tot loss = 5.04367995262146, l1: 0.00010785959451216816, l2: 0.0003965083936539789   Iteration 25 of 100, tot loss = 5.089847240447998, l1: 0.00010948469600407406, l2: 0.0003995000210124999   Iteration 26 of 100, tot loss = 4.976075704281147, l1: 0.00010742210934740097, l2: 0.00039018545458720136   Iteration 27 of 100, tot loss = 4.960964061595775, l1: 0.00010777387325544152, l2: 0.0003883225279558381   Iteration 28 of 100, tot loss = 4.989175472940717, l1: 0.00010815344713981696, l2: 0.0003907640956250751   Iteration 29 of 100, tot loss = 4.960867865332242, l1: 0.00010766837219527024, l2: 0.00038841840909424656   Iteration 30 of 100, tot loss = 4.957383314768474, l1: 0.00010733527148355885, l2: 0.00038840305496705695   Iteration 31 of 100, tot loss = 4.936293063625213, l1: 0.00010773447713525516, l2: 0.0003858948255055434   Iteration 32 of 100, tot loss = 4.958860173821449, l1: 0.00010717023712913942, l2: 0.0003887157768076577   Iteration 33 of 100, tot loss = 4.9784798911123564, l1: 0.0001075379794091339, l2: 0.00039031000668535745   Iteration 34 of 100, tot loss = 4.988622917848475, l1: 0.0001071535352821229, l2: 0.0003917087524314411   Iteration 35 of 100, tot loss = 4.963216495513916, l1: 0.0001068210783289812, l2: 0.000389500567273769   Iteration 36 of 100, tot loss = 4.920666065480974, l1: 0.00010633538810604175, l2: 0.0003857312141109206   Iteration 37 of 100, tot loss = 4.962620509637369, l1: 0.00010731330582308835, l2: 0.00038894874199507505   Iteration 38 of 100, tot loss = 4.941000380014119, l1: 0.00010679055594664533, l2: 0.00038730947869082327   Iteration 39 of 100, tot loss = 4.9084450954046, l1: 0.0001063853354329088, l2: 0.00038445917090986116   Iteration 40 of 100, tot loss = 4.925510889291763, l1: 0.00010587605929686106, l2: 0.0003866750263114227   Iteration 41 of 100, tot loss = 4.904177718046235, l1: 0.00010560336336422507, l2: 0.0003848144051295183   Iteration 42 of 100, tot loss = 4.858893207141331, l1: 0.00010514146067310191, l2: 0.000380747856979724   Iteration 43 of 100, tot loss = 4.845862449601639, l1: 0.00010561521683188784, l2: 0.0003789710249822197   Iteration 44 of 100, tot loss = 4.818565227768638, l1: 0.00010575632197949083, l2: 0.0003761001978339416   Iteration 45 of 100, tot loss = 4.821959326002333, l1: 0.00010612126166557169, l2: 0.0003760746683433859   Iteration 46 of 100, tot loss = 4.760483612184939, l1: 0.00010486828582740955, l2: 0.00037118007268240353   Iteration 47 of 100, tot loss = 4.720961124339002, l1: 0.0001041854317163791, l2: 0.000367910677673632   Iteration 48 of 100, tot loss = 4.797926604747772, l1: 0.00010539740302798843, l2: 0.00037439525446340366   Iteration 49 of 100, tot loss = 4.795148032052176, l1: 0.00010535167084712231, l2: 0.00037416312944533645   Iteration 50 of 100, tot loss = 4.763961734771729, l1: 0.00010503247889573686, l2: 0.00037136369151994584   Iteration 51 of 100, tot loss = 4.773247195225136, l1: 0.00010528286357465036, l2: 0.00037204185226822603   Iteration 52 of 100, tot loss = 4.773313183050889, l1: 0.00010499887293218098, l2: 0.00037233244186679187   Iteration 53 of 100, tot loss = 4.76226155263073, l1: 0.00010501685797210502, l2: 0.00037120929413984687   Iteration 54 of 100, tot loss = 4.70166234837638, l1: 0.00010380169523058512, l2: 0.0003663645366032142   Iteration 55 of 100, tot loss = 4.711858846924522, l1: 0.00010409272197813897, l2: 0.0003670931596637026   Iteration 56 of 100, tot loss = 4.705458174858775, l1: 0.00010414419187197512, l2: 0.0003664016230686684   Iteration 57 of 100, tot loss = 4.692300200462341, l1: 0.00010386626551432225, l2: 0.0003653637519292125   Iteration 58 of 100, tot loss = 4.715566361772603, l1: 0.00010414818192155759, l2: 0.0003674084521129583   Iteration 59 of 100, tot loss = 4.698180216853902, l1: 0.00010409713267709792, l2: 0.0003657208870794482   Iteration 60 of 100, tot loss = 4.7524609188238776, l1: 0.00010467765381084367, l2: 0.00037056843563429236   Iteration 61 of 100, tot loss = 4.734594436942554, l1: 0.00010441926449439946, l2: 0.0003690401770386341   Iteration 62 of 100, tot loss = 4.775764509554832, l1: 0.00010487271855151536, l2: 0.00037270372969590336   Iteration 63 of 100, tot loss = 4.7372883974559725, l1: 0.00010425648655481858, l2: 0.00036947235050666636   Iteration 64 of 100, tot loss = 4.728624938055873, l1: 0.00010372863761176632, l2: 0.00036913385326897696   Iteration 65 of 100, tot loss = 4.77098465516017, l1: 0.00010433550739365343, l2: 0.00037276295468193264   Iteration 66 of 100, tot loss = 4.764652850049915, l1: 0.00010375299621735007, l2: 0.00037271228553215747   Iteration 67 of 100, tot loss = 4.7476056422760236, l1: 0.0001033380594075859, l2: 0.0003714225016692445   Iteration 68 of 100, tot loss = 4.7409862402607414, l1: 0.00010324528102501063, l2: 0.0003708533394648625   Iteration 69 of 100, tot loss = 4.739464198333629, l1: 0.00010359123516009541, l2: 0.0003703551815503844   Iteration 70 of 100, tot loss = 4.733819374016353, l1: 0.00010336015896297925, l2: 0.0003700217754937642   Iteration 71 of 100, tot loss = 4.744856859596682, l1: 0.000103474410755386, l2: 0.00037101127195391726   Iteration 72 of 100, tot loss = 4.7345441828171415, l1: 0.00010335595212988362, l2: 0.0003700984629580893   Iteration 73 of 100, tot loss = 4.731637143108943, l1: 0.00010359365438638982, l2: 0.00036957005685919335   Iteration 74 of 100, tot loss = 4.709664539710896, l1: 0.00010328527123665726, l2: 0.0003676811797344566   Iteration 75 of 100, tot loss = 4.708158424695333, l1: 0.00010359830431601344, l2: 0.0003672175350948237   Iteration 76 of 100, tot loss = 4.707422835262198, l1: 0.0001037696110391775, l2: 0.0003669726694696644   Iteration 77 of 100, tot loss = 4.698917508125305, l1: 0.00010354598375020682, l2: 0.00036634576430184427   Iteration 78 of 100, tot loss = 4.721278357200133, l1: 0.00010402950717364617, l2: 0.0003680983258490391   Iteration 79 of 100, tot loss = 4.713098599940916, l1: 0.0001042390884989961, l2: 0.0003670707690018182   Iteration 80 of 100, tot loss = 4.7175635054707525, l1: 0.00010439820566716662, l2: 0.00036735814255735023   Iteration 81 of 100, tot loss = 4.744308873459145, l1: 0.00010505417150044296, l2: 0.0003693767132152757   Iteration 82 of 100, tot loss = 4.716842809828316, l1: 0.00010450980335008353, l2: 0.0003671744752642642   Iteration 83 of 100, tot loss = 4.6838112983358915, l1: 0.00010379708950801932, l2: 0.000364584038023562   Iteration 84 of 100, tot loss = 4.682402265923364, l1: 0.00010398342796439206, l2: 0.00036425679657744087   Iteration 85 of 100, tot loss = 4.675211773199194, l1: 0.0001037556019368554, l2: 0.00036376557314617776   Iteration 86 of 100, tot loss = 4.688993616159572, l1: 0.0001039048374666383, l2: 0.0003649945220088337   Iteration 87 of 100, tot loss = 4.709410252242253, l1: 0.00010426817419196093, l2: 0.00036667284882565876   Iteration 88 of 100, tot loss = 4.735615536570549, l1: 0.000104656250062941, l2: 0.00036890530141549374   Iteration 89 of 100, tot loss = 4.7496617737780795, l1: 0.00010483834353436486, l2: 0.0003701278318034935   Iteration 90 of 100, tot loss = 4.73887519174152, l1: 0.00010476630345187408, l2: 0.0003691212135890964   Iteration 91 of 100, tot loss = 4.756125616503286, l1: 0.00010476011064403772, l2: 0.0003708524487632491   Iteration 92 of 100, tot loss = 4.734761052805444, l1: 0.00010420188877817608, l2: 0.00036927421443460446   Iteration 93 of 100, tot loss = 4.722734660230657, l1: 0.00010414330538497456, l2: 0.00036813015869620845   Iteration 94 of 100, tot loss = 4.704114780781117, l1: 0.00010375905138913522, l2: 0.0003666524245729422   Iteration 95 of 100, tot loss = 4.717759246575205, l1: 0.00010413130949018523, l2: 0.00036764461325028126   Iteration 96 of 100, tot loss = 4.688004220525424, l1: 0.00010355098515901773, l2: 0.0003652494349353219   Iteration 97 of 100, tot loss = 4.6855317243595715, l1: 0.00010354994079422148, l2: 0.0003650032297243633   Iteration 98 of 100, tot loss = 4.689665453774588, l1: 0.0001037162824383015, l2: 0.0003652502608168348   Iteration 99 of 100, tot loss = 4.700174635106867, l1: 0.0001034306988738609, l2: 0.0003665867623916564   Iteration 100 of 100, tot loss = 4.70321626663208, l1: 0.0001033966217073612, l2: 0.00036692500267236026
   End of epoch 1136; saving model... 

Epoch 1137 of 2000
   Iteration 1 of 100, tot loss = 6.299291133880615, l1: 0.00013743223098572344, l2: 0.000492496881633997   Iteration 2 of 100, tot loss = 5.242180347442627, l1: 0.00011657572031253949, l2: 0.00040764230652712286   Iteration 3 of 100, tot loss = 4.4764790534973145, l1: 0.00010340407607145607, l2: 0.00034424382223126787   Iteration 4 of 100, tot loss = 4.589670777320862, l1: 0.00010352380559197627, l2: 0.0003554432696546428   Iteration 5 of 100, tot loss = 4.392471075057983, l1: 9.489930598647334e-05, l2: 0.00034434780245646835   Iteration 6 of 100, tot loss = 4.30197544892629, l1: 9.827978707714162e-05, l2: 0.00033191775825495523   Iteration 7 of 100, tot loss = 4.432891402925764, l1: 9.334160599142447e-05, l2: 0.00034994753410241434   Iteration 8 of 100, tot loss = 4.148351937532425, l1: 9.002187152873375e-05, l2: 0.00032481332345923875   Iteration 9 of 100, tot loss = 4.073614093992445, l1: 8.983943674441737e-05, l2: 0.00031752197375883244   Iteration 10 of 100, tot loss = 4.367809700965881, l1: 9.652825210650917e-05, l2: 0.00034025272325379776   Iteration 11 of 100, tot loss = 4.3842599825425586, l1: 9.624450169874102e-05, l2: 0.00034218150143384594   Iteration 12 of 100, tot loss = 4.466881215572357, l1: 9.957658130588243e-05, l2: 0.0003471115475501089   Iteration 13 of 100, tot loss = 4.597866920324472, l1: 0.00010141738787821781, l2: 0.00035836931224016903   Iteration 14 of 100, tot loss = 4.5199017354420254, l1: 0.0001005467578839411, l2: 0.0003514434242138772   Iteration 15 of 100, tot loss = 4.591636768976847, l1: 0.0001017914723585515, l2: 0.00035737221187446264   Iteration 16 of 100, tot loss = 4.540317326784134, l1: 9.995930645345652e-05, l2: 0.00035407243285590084   Iteration 17 of 100, tot loss = 4.544239577125101, l1: 9.919210959410789e-05, l2: 0.00035523185557822753   Iteration 18 of 100, tot loss = 4.486727886729771, l1: 9.80800854853846e-05, l2: 0.0003505927104722812   Iteration 19 of 100, tot loss = 4.428818112925479, l1: 9.698851135908626e-05, l2: 0.0003458933076073759   Iteration 20 of 100, tot loss = 4.420162165164948, l1: 9.502040829829639e-05, l2: 0.00034699581519817004   Iteration 21 of 100, tot loss = 4.3760529699779696, l1: 9.409820441284129e-05, l2: 0.00034350709964720797   Iteration 22 of 100, tot loss = 4.421696988019076, l1: 9.500158772756218e-05, l2: 0.000347168116687416   Iteration 23 of 100, tot loss = 4.448105936465056, l1: 9.520069627045734e-05, l2: 0.0003496099031864382   Iteration 24 of 100, tot loss = 4.339933296044667, l1: 9.394108640966199e-05, l2: 0.000340052249157452   Iteration 25 of 100, tot loss = 4.2886511325836185, l1: 9.35329265485052e-05, l2: 0.00033533219248056413   Iteration 26 of 100, tot loss = 4.301543373327989, l1: 9.405024306593426e-05, l2: 0.0003361040994954797   Iteration 27 of 100, tot loss = 4.341239372889201, l1: 9.430268942701837e-05, l2: 0.00033982125266144675   Iteration 28 of 100, tot loss = 4.265580339091165, l1: 9.309307071297164e-05, l2: 0.00033346496794755306   Iteration 29 of 100, tot loss = 4.3170370973389725, l1: 9.412414711450452e-05, l2: 0.0003375795660821464   Iteration 30 of 100, tot loss = 4.319940034548441, l1: 9.42010376699424e-05, l2: 0.00033779296936700123   Iteration 31 of 100, tot loss = 4.292728547127016, l1: 9.355601204949761e-05, l2: 0.000335716845696762   Iteration 32 of 100, tot loss = 4.333687990903854, l1: 9.404843694937881e-05, l2: 0.0003393203655832622   Iteration 33 of 100, tot loss = 4.287452900048458, l1: 9.357226107650521e-05, l2: 0.00033517303227475196   Iteration 34 of 100, tot loss = 4.272303756545572, l1: 9.382965928285986e-05, l2: 0.00033340071968268603   Iteration 35 of 100, tot loss = 4.317573962892805, l1: 9.527455882302352e-05, l2: 0.00033648284068996353   Iteration 36 of 100, tot loss = 4.291208114888933, l1: 9.545532763392355e-05, l2: 0.0003336654865254079   Iteration 37 of 100, tot loss = 4.295303286732854, l1: 9.553987970198127e-05, l2: 0.00033399045217241083   Iteration 38 of 100, tot loss = 4.246055151286878, l1: 9.474287760799358e-05, l2: 0.0003298626409224725   Iteration 39 of 100, tot loss = 4.257081838754507, l1: 9.437145602602798e-05, l2: 0.0003313367319955992   Iteration 40 of 100, tot loss = 4.28332153558731, l1: 9.3788930644223e-05, l2: 0.0003345432265632553   Iteration 41 of 100, tot loss = 4.274536388676341, l1: 9.391433364149502e-05, l2: 0.0003335393085740734   Iteration 42 of 100, tot loss = 4.257595084962391, l1: 9.351626864545757e-05, l2: 0.000332243242675239   Iteration 43 of 100, tot loss = 4.267747047335603, l1: 9.375746645504435e-05, l2: 0.00033301724084425547   Iteration 44 of 100, tot loss = 4.324103420430964, l1: 9.484378708955113e-05, l2: 0.0003375665576392996   Iteration 45 of 100, tot loss = 4.341225899590386, l1: 9.484312888364204e-05, l2: 0.0003392794644848133   Iteration 46 of 100, tot loss = 4.403242297794508, l1: 9.635544736975926e-05, l2: 0.0003439687852337754   Iteration 47 of 100, tot loss = 4.423779589064578, l1: 9.719694842599293e-05, l2: 0.0003451810133559233   Iteration 48 of 100, tot loss = 4.492927193641663, l1: 9.795069202785574e-05, l2: 0.00035134203032309114   Iteration 49 of 100, tot loss = 4.521858964647565, l1: 9.851965133210037e-05, l2: 0.00035366624782612663   Iteration 50 of 100, tot loss = 4.529449806213379, l1: 9.840883751166985e-05, l2: 0.0003545361457508989   Iteration 51 of 100, tot loss = 4.557608164992987, l1: 9.903815910494064e-05, l2: 0.0003567226604882664   Iteration 52 of 100, tot loss = 4.513238260379205, l1: 9.84246346958501e-05, l2: 0.0003528991941353664   Iteration 53 of 100, tot loss = 4.486368062361231, l1: 9.808465681930582e-05, l2: 0.00035055215198124916   Iteration 54 of 100, tot loss = 4.523021309464066, l1: 9.857401016267465e-05, l2: 0.0003537281235266063   Iteration 55 of 100, tot loss = 4.527931542830034, l1: 9.855355800192973e-05, l2: 0.00035423959912308915   Iteration 56 of 100, tot loss = 4.564869863646371, l1: 9.920636291229e-05, l2: 0.00035728062617376314   Iteration 57 of 100, tot loss = 4.577097733815511, l1: 9.924129686592016e-05, l2: 0.0003584684791827672   Iteration 58 of 100, tot loss = 4.595039564987709, l1: 9.926161736326196e-05, l2: 0.00036024234151258935   Iteration 59 of 100, tot loss = 4.553288031432588, l1: 9.850814949150861e-05, l2: 0.00035682065615635683   Iteration 60 of 100, tot loss = 4.563585591316223, l1: 9.864999404574822e-05, l2: 0.000357708568238498   Iteration 61 of 100, tot loss = 4.527657567477617, l1: 9.842051834118033e-05, l2: 0.0003543452414126731   Iteration 62 of 100, tot loss = 4.545279722059926, l1: 9.929876784641942e-05, l2: 0.00035522920793042546   Iteration 63 of 100, tot loss = 4.528030842069596, l1: 9.908556580379058e-05, l2: 0.00035371752187890546   Iteration 64 of 100, tot loss = 4.524786777794361, l1: 9.919277187009357e-05, l2: 0.0003532859095685126   Iteration 65 of 100, tot loss = 4.545954176095816, l1: 9.96016356041834e-05, l2: 0.0003549937852837432   Iteration 66 of 100, tot loss = 4.514394167697791, l1: 9.912034219076193e-05, l2: 0.00035231907774383825   Iteration 67 of 100, tot loss = 4.530081670675704, l1: 9.96151576865527e-05, l2: 0.00035339301198359524   Iteration 68 of 100, tot loss = 4.595372249098385, l1: 0.00010046424663682153, l2: 0.0003590729816898923   Iteration 69 of 100, tot loss = 4.58495803501295, l1: 0.00010012149389207269, l2: 0.00035837431315103197   Iteration 70 of 100, tot loss = 4.583681440353393, l1: 0.00010034800948882808, l2: 0.00035802013797885075   Iteration 71 of 100, tot loss = 4.582268439548116, l1: 0.00010037710917843046, l2: 0.0003578497382195693   Iteration 72 of 100, tot loss = 4.587959044509464, l1: 0.00010032930827542764, l2: 0.00035846659981568035   Iteration 73 of 100, tot loss = 4.583483689451871, l1: 0.00010059945420274345, l2: 0.00035774891826081766   Iteration 74 of 100, tot loss = 4.5982545195399105, l1: 0.00010101296932073121, l2: 0.0003588124861896693   Iteration 75 of 100, tot loss = 4.66225793838501, l1: 0.00010187797874095849, l2: 0.0003643478185404092   Iteration 76 of 100, tot loss = 4.660973197535465, l1: 0.00010162483105982813, l2: 0.00036447249179840773   Iteration 77 of 100, tot loss = 4.662524464842561, l1: 0.00010186667638557189, l2: 0.0003643857727927918   Iteration 78 of 100, tot loss = 4.671353609133989, l1: 0.00010208896440953817, l2: 0.0003650463995887922   Iteration 79 of 100, tot loss = 4.664956527420237, l1: 0.00010195795062205849, l2: 0.00036453770523702233   Iteration 80 of 100, tot loss = 4.678361827135086, l1: 0.00010218946158602193, l2: 0.0003656467244582018   Iteration 81 of 100, tot loss = 4.6550940378212635, l1: 0.00010192971729585774, l2: 0.0003635796896515437   Iteration 82 of 100, tot loss = 4.67165451805766, l1: 0.00010171170274921546, l2: 0.00036545375237105094   Iteration 83 of 100, tot loss = 4.666336751845946, l1: 0.00010183751764559995, l2: 0.00036479616067821094   Iteration 84 of 100, tot loss = 4.650887239547003, l1: 0.00010159825255652235, l2: 0.00036349047461823957   Iteration 85 of 100, tot loss = 4.656854994156781, l1: 0.00010167671695372144, l2: 0.00036400878555415306   Iteration 86 of 100, tot loss = 4.643860830817112, l1: 0.00010156539593840852, l2: 0.00036282069015933937   Iteration 87 of 100, tot loss = 4.627262466255275, l1: 0.00010121498425356005, l2: 0.0003615112653424985   Iteration 88 of 100, tot loss = 4.6401114084503865, l1: 0.0001017073781109833, l2: 0.0003623037657896268   Iteration 89 of 100, tot loss = 4.605661946736025, l1: 0.00010114319037496939, l2: 0.000359423007385078   Iteration 90 of 100, tot loss = 4.59450773662991, l1: 0.00010073853401182633, l2: 0.0003587122425314091   Iteration 91 of 100, tot loss = 4.604593318897289, l1: 0.00010100249114707317, l2: 0.00035945684330827035   Iteration 92 of 100, tot loss = 4.593049868293431, l1: 0.00010076529329323776, l2: 0.0003585396961684835   Iteration 93 of 100, tot loss = 4.591256367262973, l1: 0.00010105663464875311, l2: 0.000358069004877145   Iteration 94 of 100, tot loss = 4.602197175330304, l1: 0.00010137721720310126, l2: 0.00035884250347078975   Iteration 95 of 100, tot loss = 4.619060506318744, l1: 0.00010168945147214752, l2: 0.0003602166023675205   Iteration 96 of 100, tot loss = 4.619755387306213, l1: 0.0001018336323947248, l2: 0.0003601419094441856   Iteration 97 of 100, tot loss = 4.622369859636445, l1: 0.00010188995619285276, l2: 0.0003603470330060575   Iteration 98 of 100, tot loss = 4.621834394883137, l1: 0.0001020719210661197, l2: 0.0003601115216098355   Iteration 99 of 100, tot loss = 4.6313052996240485, l1: 0.00010211827001778992, l2: 0.00036101226291837963   Iteration 100 of 100, tot loss = 4.630371518135071, l1: 0.00010185197948885616, l2: 0.00036118517520662864
   End of epoch 1137; saving model... 

Epoch 1138 of 2000
   Iteration 1 of 100, tot loss = 8.157562255859375, l1: 0.00016506941756233573, l2: 0.0006506868521682918   Iteration 2 of 100, tot loss = 7.767315626144409, l1: 0.00014439514779951423, l2: 0.0006323364505078644   Iteration 3 of 100, tot loss = 6.48016619682312, l1: 0.00012853062556435665, l2: 0.0005194860180684676   Iteration 4 of 100, tot loss = 5.5445640087127686, l1: 0.00011424470903875772, l2: 0.0004402117083373014   Iteration 5 of 100, tot loss = 4.968057632446289, l1: 0.00010778079158626496, l2: 0.00038902498490642754   Iteration 6 of 100, tot loss = 4.805097858111064, l1: 0.00010877627816322881, l2: 0.00037173351787108305   Iteration 7 of 100, tot loss = 4.4805238246917725, l1: 0.0001036548284381362, l2: 0.00034439756252270726   Iteration 8 of 100, tot loss = 4.391987323760986, l1: 0.0001018326438497752, l2: 0.00033736609839252196   Iteration 9 of 100, tot loss = 4.225543075137669, l1: 9.542767010215257e-05, l2: 0.00032712664688006043   Iteration 10 of 100, tot loss = 4.128235721588135, l1: 9.463387395953759e-05, l2: 0.0003181897060130723   Iteration 11 of 100, tot loss = 4.192317529158159, l1: 9.578183116073805e-05, l2: 0.00032344992955172944   Iteration 12 of 100, tot loss = 4.238258083661397, l1: 9.684130782261491e-05, l2: 0.00032698450741008855   Iteration 13 of 100, tot loss = 4.353482796595647, l1: 9.578096460945044e-05, l2: 0.0003395673198750816   Iteration 14 of 100, tot loss = 4.565703051430838, l1: 0.00010013423473407914, l2: 0.0003564360740710981   Iteration 15 of 100, tot loss = 4.381076200803121, l1: 9.596199815860018e-05, l2: 0.00034214562523023534   Iteration 16 of 100, tot loss = 4.3374408558011055, l1: 9.314915064351226e-05, l2: 0.000340594938279537   Iteration 17 of 100, tot loss = 4.318860145176158, l1: 9.245139453388915e-05, l2: 0.0003394346233964076   Iteration 18 of 100, tot loss = 4.351947499646081, l1: 9.433385902310774e-05, l2: 0.000340860894033944   Iteration 19 of 100, tot loss = 4.236142428297746, l1: 9.247212744268931e-05, l2: 0.0003311421184518718   Iteration 20 of 100, tot loss = 4.216497856378555, l1: 9.079661740543087e-05, l2: 0.0003308531719085295   Iteration 21 of 100, tot loss = 4.2448341108503795, l1: 9.169270145191279e-05, l2: 0.00033279071309758973   Iteration 22 of 100, tot loss = 4.247460023923353, l1: 9.217872618385378e-05, l2: 0.0003325672792429528   Iteration 23 of 100, tot loss = 4.355343761651413, l1: 9.434746781429884e-05, l2: 0.0003411869092024458   Iteration 24 of 100, tot loss = 4.387709801395734, l1: 9.386991723658866e-05, l2: 0.0003449010637268657   Iteration 25 of 100, tot loss = 4.35502393245697, l1: 9.294294504798017e-05, l2: 0.00034255944832693784   Iteration 26 of 100, tot loss = 4.351222492181337, l1: 9.39247291591108e-05, l2: 0.00034119752000641223   Iteration 27 of 100, tot loss = 4.375570010255884, l1: 9.553039597920831e-05, l2: 0.00034202660565454235   Iteration 28 of 100, tot loss = 4.352462278945105, l1: 9.607539959688438e-05, l2: 0.00033917082795856653   Iteration 29 of 100, tot loss = 4.309410863909228, l1: 9.570737604698134e-05, l2: 0.0003352337100358423   Iteration 30 of 100, tot loss = 4.335149331887563, l1: 9.612888813232227e-05, l2: 0.00033738604543032125   Iteration 31 of 100, tot loss = 4.329779421129534, l1: 9.673309590828965e-05, l2: 0.00033624484551124156   Iteration 32 of 100, tot loss = 4.400145586580038, l1: 9.754123254879232e-05, l2: 0.0003424733245083189   Iteration 33 of 100, tot loss = 4.412739070979032, l1: 9.790464526194035e-05, l2: 0.0003433692605044185   Iteration 34 of 100, tot loss = 4.48864815866246, l1: 9.900105375409647e-05, l2: 0.0003498637614778572   Iteration 35 of 100, tot loss = 4.485518057005746, l1: 9.877451427330795e-05, l2: 0.00034977729041461966   Iteration 36 of 100, tot loss = 4.517893006404241, l1: 9.970863286273218e-05, l2: 0.0003520806676533539   Iteration 37 of 100, tot loss = 4.5409955430675195, l1: 0.00010016230550497091, l2: 0.0003539372484885609   Iteration 38 of 100, tot loss = 4.538702058164697, l1: 0.00010020001575053596, l2: 0.0003536701892459716   Iteration 39 of 100, tot loss = 4.582995631755927, l1: 0.00010122340207976193, l2: 0.0003570761596515345   Iteration 40 of 100, tot loss = 4.606833824515343, l1: 0.00010199892485616147, l2: 0.0003586844566598302   Iteration 41 of 100, tot loss = 4.623908540097679, l1: 0.00010254278041851117, l2: 0.0003598480724598985   Iteration 42 of 100, tot loss = 4.577355796382541, l1: 0.0001020401772880827, l2: 0.00035569540098852787   Iteration 43 of 100, tot loss = 4.639604837395424, l1: 0.00010344632899559232, l2: 0.0003605141540648156   Iteration 44 of 100, tot loss = 4.606275089762428, l1: 0.0001029214276555153, l2: 0.0003577060808972667   Iteration 45 of 100, tot loss = 4.623478738466899, l1: 0.00010351977762184106, l2: 0.0003588280954217124   Iteration 46 of 100, tot loss = 4.6836205228515295, l1: 0.00010388574818258032, l2: 0.00036447630396730307   Iteration 47 of 100, tot loss = 4.716136077617077, l1: 0.00010433288542194628, l2: 0.0003672807227864743   Iteration 48 of 100, tot loss = 4.734735367198785, l1: 0.0001042628246826401, l2: 0.00036921071265775635   Iteration 49 of 100, tot loss = 4.74175183383786, l1: 0.00010411333693939793, l2: 0.0003700618473551597   Iteration 50 of 100, tot loss = 4.749208905696869, l1: 0.00010404726584965829, l2: 0.00037087362579768524   Iteration 51 of 100, tot loss = 4.735778609911601, l1: 0.00010333872976879516, l2: 0.00037023913219123714   Iteration 52 of 100, tot loss = 4.738841173740534, l1: 0.00010323657142390193, l2: 0.0003706475469628528   Iteration 53 of 100, tot loss = 4.697703390751245, l1: 0.00010224172063248741, l2: 0.00036752861946434624   Iteration 54 of 100, tot loss = 4.725653103104344, l1: 0.00010281134685404443, l2: 0.0003697539645212668   Iteration 55 of 100, tot loss = 4.6969881772995, l1: 0.00010247591079942966, l2: 0.00036722290811171246   Iteration 56 of 100, tot loss = 4.7041392007044385, l1: 0.00010255696861349861, l2: 0.0003678569532244832   Iteration 57 of 100, tot loss = 4.668363573258383, l1: 0.0001019797114407227, l2: 0.000364856647680873   Iteration 58 of 100, tot loss = 4.65357538132832, l1: 0.00010111394326015906, l2: 0.00036424359658949784   Iteration 59 of 100, tot loss = 4.665548585229001, l1: 0.00010145142393520149, l2: 0.00036510343636757047   Iteration 60 of 100, tot loss = 4.704956922928492, l1: 0.00010153537726485714, l2: 0.00036896031645786327   Iteration 61 of 100, tot loss = 4.723580385817856, l1: 0.00010174484540599581, l2: 0.00037061319410701697   Iteration 62 of 100, tot loss = 4.737633276370264, l1: 0.00010229325861502965, l2: 0.00037147006984131653   Iteration 63 of 100, tot loss = 4.707282719157991, l1: 0.000101462513171894, l2: 0.0003692657596224712   Iteration 64 of 100, tot loss = 4.753614889457822, l1: 0.00010191247116608793, l2: 0.0003734490182978334   Iteration 65 of 100, tot loss = 4.738021049132714, l1: 0.00010195024879067205, l2: 0.00037185185669491496   Iteration 66 of 100, tot loss = 4.732717801224101, l1: 0.00010204667204093174, l2: 0.0003712251085689235   Iteration 67 of 100, tot loss = 4.733741431093928, l1: 0.00010246096812874955, l2: 0.0003709131756227519   Iteration 68 of 100, tot loss = 4.717806148178437, l1: 0.0001026039239792119, l2: 0.0003691766914391123   Iteration 69 of 100, tot loss = 4.750841892283896, l1: 0.00010297544239606857, l2: 0.0003721087477574854   Iteration 70 of 100, tot loss = 4.809541911738259, l1: 0.00010348301496248626, l2: 0.0003774711762421897   Iteration 71 of 100, tot loss = 4.797064440351137, l1: 0.00010369069279707693, l2: 0.0003760157511274422   Iteration 72 of 100, tot loss = 4.790017427669631, l1: 0.00010396748878823321, l2: 0.00037503425402165804   Iteration 73 of 100, tot loss = 4.835562230789498, l1: 0.00010454264697291863, l2: 0.00037901357576978226   Iteration 74 of 100, tot loss = 4.830810542042191, l1: 0.00010471306710757278, l2: 0.00037836798661816364   Iteration 75 of 100, tot loss = 4.805781009991963, l1: 0.00010449757503617245, l2: 0.0003760805252629022   Iteration 76 of 100, tot loss = 4.790507719704979, l1: 0.00010423812536349655, l2: 0.0003748126458192832   Iteration 77 of 100, tot loss = 4.793688075883048, l1: 0.00010426520053773525, l2: 0.00037510360647053395   Iteration 78 of 100, tot loss = 4.77091400592755, l1: 0.00010408048617066994, l2: 0.00037301091372053354   Iteration 79 of 100, tot loss = 4.787310951872717, l1: 0.000104582749660454, l2: 0.0003741483446844276   Iteration 80 of 100, tot loss = 4.8229425445199015, l1: 0.00010507186029826699, l2: 0.0003772223933992791   Iteration 81 of 100, tot loss = 4.812281645374534, l1: 0.00010499656163580015, l2: 0.000376231602089836   Iteration 82 of 100, tot loss = 4.820021136504848, l1: 0.00010509890928848862, l2: 0.00037690320335436475   Iteration 83 of 100, tot loss = 4.82966224400394, l1: 0.00010525589762010095, l2: 0.0003777103256981214   Iteration 84 of 100, tot loss = 4.807873114233925, l1: 0.00010497115047092804, l2: 0.0003758161598982822   Iteration 85 of 100, tot loss = 4.797489922186908, l1: 0.00010469693776993903, l2: 0.00037505205323481387   Iteration 86 of 100, tot loss = 4.805760576281437, l1: 0.00010482350199010905, l2: 0.0003757525545404141   Iteration 87 of 100, tot loss = 4.808299013938027, l1: 0.00010479281522800622, l2: 0.00037603708480527605   Iteration 88 of 100, tot loss = 4.839437819340012, l1: 0.00010511991889540117, l2: 0.0003788238619480663   Iteration 89 of 100, tot loss = 4.851145453667373, l1: 0.00010543522802528172, l2: 0.000379679316466444   Iteration 90 of 100, tot loss = 4.830685569180383, l1: 0.00010516447562420378, l2: 0.0003779040804753701   Iteration 91 of 100, tot loss = 4.845467233395839, l1: 0.00010518771979240138, l2: 0.00037935900292103436   Iteration 92 of 100, tot loss = 4.872311178756797, l1: 0.00010558720465348375, l2: 0.00038164391298778355   Iteration 93 of 100, tot loss = 4.879630382343005, l1: 0.00010577340995090493, l2: 0.0003821896276466789   Iteration 94 of 100, tot loss = 4.874896649350512, l1: 0.00010585358088180928, l2: 0.0003816360834381603   Iteration 95 of 100, tot loss = 4.90569605701848, l1: 0.00010645075583995907, l2: 0.00038411884973021715   Iteration 96 of 100, tot loss = 4.914284702390432, l1: 0.00010649672947238287, l2: 0.0003849317405790013   Iteration 97 of 100, tot loss = 4.913852864933997, l1: 0.00010646142464727596, l2: 0.0003849238615732678   Iteration 98 of 100, tot loss = 4.91577924149377, l1: 0.00010665830279544842, l2: 0.00038491962127130936   Iteration 99 of 100, tot loss = 4.922602462046074, l1: 0.00010703780531508123, l2: 0.00038522244090063854   Iteration 100 of 100, tot loss = 4.915152045488358, l1: 0.00010682334537705174, l2: 0.00038469185907160866
   End of epoch 1138; saving model... 

Epoch 1139 of 2000
   Iteration 1 of 100, tot loss = 4.2739739418029785, l1: 9.91625856840983e-05, l2: 0.000328234804328531   Iteration 2 of 100, tot loss = 4.798557996749878, l1: 0.00011520188127178699, l2: 0.0003646539116743952   Iteration 3 of 100, tot loss = 4.016234000523885, l1: 9.677570293812703e-05, l2: 0.00030484768891862285   Iteration 4 of 100, tot loss = 3.5404248237609863, l1: 8.253229771071346e-05, l2: 0.0002715101800276898   Iteration 5 of 100, tot loss = 3.563837099075317, l1: 7.951237130328081e-05, l2: 0.0002768713340628892   Iteration 6 of 100, tot loss = 3.3930267492930093, l1: 7.989854627036645e-05, l2: 0.0002594041240323956   Iteration 7 of 100, tot loss = 3.5001217978341237, l1: 8.181423358369752e-05, l2: 0.00026819793856702745   Iteration 8 of 100, tot loss = 3.620960772037506, l1: 8.684573413120233e-05, l2: 0.0002752503387455363   Iteration 9 of 100, tot loss = 3.8624922964307995, l1: 9.176079482939612e-05, l2: 0.00029448842785010737   Iteration 10 of 100, tot loss = 4.0360743522644045, l1: 9.251682022295426e-05, l2: 0.00031109060801099987   Iteration 11 of 100, tot loss = 4.067078286951238, l1: 9.165623255698434e-05, l2: 0.000315051589330489   Iteration 12 of 100, tot loss = 4.057214538256328, l1: 9.251871142623713e-05, l2: 0.0003132027365306082   Iteration 13 of 100, tot loss = 4.140862428225004, l1: 9.319615296240394e-05, l2: 0.0003208900833180031   Iteration 14 of 100, tot loss = 4.153745515005929, l1: 9.388541392192045e-05, l2: 0.0003214891309783395   Iteration 15 of 100, tot loss = 4.265478038787842, l1: 9.58616023126524e-05, l2: 0.00033068619474458196   Iteration 16 of 100, tot loss = 4.294205367565155, l1: 9.751687798598141e-05, l2: 0.0003319036513858009   Iteration 17 of 100, tot loss = 4.25627052082735, l1: 9.679920335389291e-05, l2: 0.0003288278410978177   Iteration 18 of 100, tot loss = 4.576633400387234, l1: 0.00010280102792926805, l2: 0.0003548623062670231   Iteration 19 of 100, tot loss = 4.542289344887984, l1: 0.00010149561249387549, l2: 0.0003527333151156965   Iteration 20 of 100, tot loss = 4.665013802051544, l1: 0.00010311277601431357, l2: 0.0003633885964518413   Iteration 21 of 100, tot loss = 4.659728606541951, l1: 0.00010292688117867026, l2: 0.00036304597077625137   Iteration 22 of 100, tot loss = 4.675078901377591, l1: 0.00010238870047694284, l2: 0.00036511918138289315   Iteration 23 of 100, tot loss = 4.5821021743442705, l1: 0.00010028236710754952, l2: 0.00035792784309824526   Iteration 24 of 100, tot loss = 4.562755266825358, l1: 0.00010023437744166586, l2: 0.00035604114236775786   Iteration 25 of 100, tot loss = 4.570146369934082, l1: 0.00010085182220791467, l2: 0.0003561628074385226   Iteration 26 of 100, tot loss = 4.505979794722337, l1: 9.934585376378472e-05, l2: 0.00035125211825988325   Iteration 27 of 100, tot loss = 4.471061644730745, l1: 9.907690926209195e-05, l2: 0.0003480292485888909   Iteration 28 of 100, tot loss = 4.393554219177791, l1: 9.773102868036534e-05, l2: 0.00034162438688716587   Iteration 29 of 100, tot loss = 4.385751995547064, l1: 9.802269339380818e-05, l2: 0.0003405524992809535   Iteration 30 of 100, tot loss = 4.400922656059265, l1: 9.864892429807999e-05, l2: 0.0003414433347643353   Iteration 31 of 100, tot loss = 4.471648162411105, l1: 9.998026566350862e-05, l2: 0.00034718454445547035   Iteration 32 of 100, tot loss = 4.490679450333118, l1: 9.991937679387775e-05, l2: 0.000349148561781476   Iteration 33 of 100, tot loss = 4.454905365452622, l1: 9.934984645400563e-05, l2: 0.00034614068422480625   Iteration 34 of 100, tot loss = 4.4069001324036545, l1: 9.870463808885474e-05, l2: 0.0003419853695531321   Iteration 35 of 100, tot loss = 4.481698192868914, l1: 0.0001003315746077403, l2: 0.00034783823981082864   Iteration 36 of 100, tot loss = 4.516292313734691, l1: 0.0001009589204841177, l2: 0.00035067030613491725   Iteration 37 of 100, tot loss = 4.542113117269568, l1: 0.00010075354333133185, l2: 0.00035345776406167125   Iteration 38 of 100, tot loss = 4.5321771282898755, l1: 0.00010103069306703554, l2: 0.00035218701630487646   Iteration 39 of 100, tot loss = 4.480002427712465, l1: 9.967429221619685e-05, l2: 0.0003483259471324392   Iteration 40 of 100, tot loss = 4.553020334243774, l1: 0.00010074449601233937, l2: 0.0003545575338648632   Iteration 41 of 100, tot loss = 4.596692143416986, l1: 0.00010138486544658407, l2: 0.0003582843460097182   Iteration 42 of 100, tot loss = 4.582955973488944, l1: 0.00010033630176275481, l2: 0.0003579592926793599   Iteration 43 of 100, tot loss = 4.554055358088294, l1: 9.963069236999767e-05, l2: 0.0003557748406269956   Iteration 44 of 100, tot loss = 4.530749597332695, l1: 9.942629648404281e-05, l2: 0.0003536486606621607   Iteration 45 of 100, tot loss = 4.598926485909356, l1: 0.00010072326921443973, l2: 0.0003591693769623008   Iteration 46 of 100, tot loss = 4.536478097024172, l1: 9.976460419041004e-05, l2: 0.00035388320311144724   Iteration 47 of 100, tot loss = 4.469124274050936, l1: 9.830228497021574e-05, l2: 0.00034861014028912056   Iteration 48 of 100, tot loss = 4.444711568454902, l1: 9.817168969069219e-05, l2: 0.0003462994651878641   Iteration 49 of 100, tot loss = 4.4839445741809145, l1: 9.904192328186972e-05, l2: 0.00034935253242277827   Iteration 50 of 100, tot loss = 4.480857956409454, l1: 9.916151451761834e-05, l2: 0.000348924279678613   Iteration 51 of 100, tot loss = 4.490931316918018, l1: 9.934298634784772e-05, l2: 0.0003497501431197366   Iteration 52 of 100, tot loss = 4.482576592610433, l1: 9.867665917576793e-05, l2: 0.00034958099758324143   Iteration 53 of 100, tot loss = 4.484607194954494, l1: 9.906758309368325e-05, l2: 0.0003493931336671044   Iteration 54 of 100, tot loss = 4.494357879515047, l1: 9.951306667601414e-05, l2: 0.0003499227190062542   Iteration 55 of 100, tot loss = 4.4655185981230305, l1: 9.895233604104513e-05, l2: 0.00034759952157566496   Iteration 56 of 100, tot loss = 4.490794873663357, l1: 9.979149672290077e-05, l2: 0.00034928798881342767   Iteration 57 of 100, tot loss = 4.525045137656362, l1: 0.0001005306981632633, l2: 0.0003519738138971948   Iteration 58 of 100, tot loss = 4.530920260939105, l1: 0.00010076835666229178, l2: 0.0003523236679531441   Iteration 59 of 100, tot loss = 4.545878840705096, l1: 0.00010140671151082458, l2: 0.0003531811718426455   Iteration 60 of 100, tot loss = 4.525665611028671, l1: 0.0001012043827358866, l2: 0.0003513621777528897   Iteration 61 of 100, tot loss = 4.537479394772014, l1: 0.00010124650952257368, l2: 0.0003525014293044195   Iteration 62 of 100, tot loss = 4.543862844667127, l1: 0.00010120101069333032, l2: 0.00035318527314361303   Iteration 63 of 100, tot loss = 4.544936832927522, l1: 0.00010112439832439468, l2: 0.00035336928438424827   Iteration 64 of 100, tot loss = 4.539453553035855, l1: 0.00010116554233263741, l2: 0.000352779812146764   Iteration 65 of 100, tot loss = 4.54330041041741, l1: 0.00010142637978648194, l2: 0.00035290366011815003   Iteration 66 of 100, tot loss = 4.518504281838735, l1: 0.00010068851412198097, l2: 0.00035116191297261554   Iteration 67 of 100, tot loss = 4.499667304665295, l1: 0.00010032529886004015, l2: 0.00034964143050097816   Iteration 68 of 100, tot loss = 4.46934902142076, l1: 9.995320659286603e-05, l2: 0.0003469816943192307   Iteration 69 of 100, tot loss = 4.476166505744492, l1: 0.00010021858775028217, l2: 0.0003473980612108025   Iteration 70 of 100, tot loss = 4.486988879953112, l1: 0.00010029275545093696, l2: 0.0003484061303814607   Iteration 71 of 100, tot loss = 4.519159943285123, l1: 0.00010061341639760633, l2: 0.0003513025757725495   Iteration 72 of 100, tot loss = 4.501792753736178, l1: 0.00010005640438571895, l2: 0.0003501228687431042   Iteration 73 of 100, tot loss = 4.5024229320761275, l1: 9.963741163804502e-05, l2: 0.0003506048795390772   Iteration 74 of 100, tot loss = 4.523913704060219, l1: 0.00010001975400931819, l2: 0.00035237161461515606   Iteration 75 of 100, tot loss = 4.517555807431539, l1: 0.00010016571812836142, l2: 0.000351589861093089   Iteration 76 of 100, tot loss = 4.497832657475221, l1: 0.00010009494991735617, l2: 0.0003496883142169712   Iteration 77 of 100, tot loss = 4.478302446278659, l1: 9.980801863908804e-05, l2: 0.0003480222244375059   Iteration 78 of 100, tot loss = 4.506639839747013, l1: 0.00010008032661108956, l2: 0.00035058365607162553   Iteration 79 of 100, tot loss = 4.534630532506146, l1: 0.00010037007998591113, l2: 0.000353092971866467   Iteration 80 of 100, tot loss = 4.5163939341902735, l1: 9.968825997930253e-05, l2: 0.0003519511321428581   Iteration 81 of 100, tot loss = 4.53940185352608, l1: 0.00010018218790157986, l2: 0.0003537579960934243   Iteration 82 of 100, tot loss = 4.525362147063744, l1: 9.986281062393995e-05, l2: 0.0003526734027418126   Iteration 83 of 100, tot loss = 4.530996137354747, l1: 9.975452630020431e-05, l2: 0.0003533450858905934   Iteration 84 of 100, tot loss = 4.545008737416494, l1: 9.993996068910096e-05, l2: 0.00035456091167621447   Iteration 85 of 100, tot loss = 4.523256261208478, l1: 9.95651657028836e-05, l2: 0.00035276045896085525   Iteration 86 of 100, tot loss = 4.514890788599502, l1: 9.958504436559283e-05, l2: 0.0003519040330375415   Iteration 87 of 100, tot loss = 4.536604773039105, l1: 9.980725133510563e-05, l2: 0.00035385322478069954   Iteration 88 of 100, tot loss = 4.526677875356241, l1: 9.981325654900337e-05, l2: 0.00035285452974097677   Iteration 89 of 100, tot loss = 4.51335322053245, l1: 9.963867194712518e-05, l2: 0.0003516966487827784   Iteration 90 of 100, tot loss = 4.510807298289405, l1: 9.99329303643511e-05, l2: 0.00035114779812930565   Iteration 91 of 100, tot loss = 4.566514744863405, l1: 0.00010083385771664965, l2: 0.00035581761503535844   Iteration 92 of 100, tot loss = 4.5627895295619965, l1: 0.00010053702056251021, l2: 0.00035574193079019494   Iteration 93 of 100, tot loss = 4.5510890163401125, l1: 0.0001004007493955664, l2: 0.00035470815063128747   Iteration 94 of 100, tot loss = 4.546558979977953, l1: 0.00010043518883089456, l2: 0.0003542207075255685   Iteration 95 of 100, tot loss = 4.558207643659491, l1: 0.00010076364644766344, l2: 0.00035505711643245855   Iteration 96 of 100, tot loss = 4.5612833214302855, l1: 0.00010101275446080156, l2: 0.00035511557643985725   Iteration 97 of 100, tot loss = 4.5725733189238715, l1: 0.0001013660879197204, l2: 0.0003558912432390102   Iteration 98 of 100, tot loss = 4.581429949828556, l1: 0.0001015169404201181, l2: 0.00035662605377907237   Iteration 99 of 100, tot loss = 4.5886012339832805, l1: 0.00010182156080246028, l2: 0.0003570385617991872   Iteration 100 of 100, tot loss = 4.584964514970779, l1: 0.00010184735845541581, l2: 0.0003566490921366494
   End of epoch 1139; saving model... 

Epoch 1140 of 2000
   Iteration 1 of 100, tot loss = 6.096573829650879, l1: 0.00012155050353612751, l2: 0.00048810685984790325   Iteration 2 of 100, tot loss = 4.627948999404907, l1: 0.0001093337959900964, l2: 0.0003534610877977684   Iteration 3 of 100, tot loss = 4.220673720041911, l1: 9.694926120573655e-05, l2: 0.00032511810422874987   Iteration 4 of 100, tot loss = 4.20094358921051, l1: 9.328795022156555e-05, l2: 0.00032680640288162977   Iteration 5 of 100, tot loss = 4.413954734802246, l1: 9.796433587325737e-05, l2: 0.0003434311365708709   Iteration 6 of 100, tot loss = 4.150866866111755, l1: 9.577261516824365e-05, l2: 0.00031931407283991575   Iteration 7 of 100, tot loss = 4.050555058888027, l1: 9.723330211792407e-05, l2: 0.000307822207105346   Iteration 8 of 100, tot loss = 4.061023265123367, l1: 9.773372767085675e-05, l2: 0.00030836860059935134   Iteration 9 of 100, tot loss = 4.06199034055074, l1: 9.770809346163232e-05, l2: 0.000308490945220304   Iteration 10 of 100, tot loss = 3.904586601257324, l1: 9.363142235088162e-05, l2: 0.00029682724125450476   Iteration 11 of 100, tot loss = 4.02005351673473, l1: 9.550903169755739e-05, l2: 0.0003064963265470314   Iteration 12 of 100, tot loss = 3.992628733317057, l1: 9.449327141434576e-05, l2: 0.0003047696094048054   Iteration 13 of 100, tot loss = 3.867740172606248, l1: 9.138270755871557e-05, l2: 0.00029539131635549263   Iteration 14 of 100, tot loss = 3.992608666419983, l1: 9.301076456072874e-05, l2: 0.00030625010653498715   Iteration 15 of 100, tot loss = 3.990531651178996, l1: 9.312063145140806e-05, l2: 0.0003059325380794083   Iteration 16 of 100, tot loss = 4.266328260302544, l1: 9.734876493894262e-05, l2: 0.0003292840656285989   Iteration 17 of 100, tot loss = 4.270046164007748, l1: 9.862726480658988e-05, l2: 0.0003283773557461031   Iteration 18 of 100, tot loss = 4.265556852022807, l1: 9.925133619819664e-05, l2: 0.00032730435244351003   Iteration 19 of 100, tot loss = 4.365661834415636, l1: 0.00010001762137490962, l2: 0.0003365485654763975   Iteration 20 of 100, tot loss = 4.432358705997467, l1: 0.00010174503513553646, l2: 0.00034149083876400255   Iteration 21 of 100, tot loss = 4.573265927178519, l1: 0.00010449376817892439, l2: 0.0003528328261814923   Iteration 22 of 100, tot loss = 4.562932935628024, l1: 0.00010380521350519054, l2: 0.0003524880812619813   Iteration 23 of 100, tot loss = 4.538544167643008, l1: 0.00010358275745681527, l2: 0.0003502716610218277   Iteration 24 of 100, tot loss = 4.605934153000514, l1: 0.00010237966277297043, l2: 0.00035821375361895963   Iteration 25 of 100, tot loss = 4.545465002059936, l1: 0.00010008675468270667, l2: 0.000354459747322835   Iteration 26 of 100, tot loss = 4.510621868647062, l1: 9.940320286169971e-05, l2: 0.0003516589858918451   Iteration 27 of 100, tot loss = 4.495293414151227, l1: 9.891756067022302e-05, l2: 0.00035061178183510764   Iteration 28 of 100, tot loss = 4.489224391324179, l1: 9.915611800741837e-05, l2: 0.0003497663217006318   Iteration 29 of 100, tot loss = 4.51901887203085, l1: 0.00010065314099533837, l2: 0.0003512487470790551   Iteration 30 of 100, tot loss = 4.455658801396688, l1: 9.976729573584938e-05, l2: 0.00034579858523405466   Iteration 31 of 100, tot loss = 4.460755179005284, l1: 0.00010045559693319393, l2: 0.00034561992065244986   Iteration 32 of 100, tot loss = 4.45929554104805, l1: 0.0001000353937570253, l2: 0.00034589415963637293   Iteration 33 of 100, tot loss = 4.534534179803097, l1: 0.00010154616305071889, l2: 0.000351907253692237   Iteration 34 of 100, tot loss = 4.535799363080193, l1: 0.00010113703844099826, l2: 0.0003524428967092022   Iteration 35 of 100, tot loss = 4.514093685150146, l1: 0.0001010603149812336, l2: 0.00035034905262624046   Iteration 36 of 100, tot loss = 4.5161739985148115, l1: 0.00010137073624921161, l2: 0.00035024666229017184   Iteration 37 of 100, tot loss = 4.558777113218565, l1: 0.00010161891170994479, l2: 0.0003542587986197071   Iteration 38 of 100, tot loss = 4.607466998853181, l1: 0.00010197044407265678, l2: 0.00035877625563635344   Iteration 39 of 100, tot loss = 4.5367522942714205, l1: 0.00010028057005957891, l2: 0.000353394659424129   Iteration 40 of 100, tot loss = 4.530672904849053, l1: 0.00010024209523180616, l2: 0.00035282519565953406   Iteration 41 of 100, tot loss = 4.5478321720914145, l1: 0.00010052321486887749, l2: 0.00035426000214079623   Iteration 42 of 100, tot loss = 4.515590528647105, l1: 9.98093369811873e-05, l2: 0.00035174971569739723   Iteration 43 of 100, tot loss = 4.528779381929442, l1: 0.00010059256085294819, l2: 0.0003522853769746414   Iteration 44 of 100, tot loss = 4.485505962913686, l1: 0.00010004951077875872, l2: 0.00034850108535134825   Iteration 45 of 100, tot loss = 4.497593839963277, l1: 9.987945563302168e-05, l2: 0.0003498799277521256   Iteration 46 of 100, tot loss = 4.480194177316583, l1: 9.946892714651767e-05, l2: 0.00034855049022513885   Iteration 47 of 100, tot loss = 4.499603162420557, l1: 9.973712850354632e-05, l2: 0.0003502231878922024   Iteration 48 of 100, tot loss = 4.531666847566764, l1: 9.964453564255867e-05, l2: 0.0003535221488467262   Iteration 49 of 100, tot loss = 4.612300106457302, l1: 0.00010109499887722469, l2: 0.00036013501040561466   Iteration 50 of 100, tot loss = 4.616340801715851, l1: 0.0001014320218382636, l2: 0.0003602020570542663   Iteration 51 of 100, tot loss = 4.624798412416496, l1: 0.00010211517705922197, l2: 0.00036036466275725295   Iteration 52 of 100, tot loss = 4.60938596037718, l1: 0.0001018530801257629, l2: 0.00035908551497134165   Iteration 53 of 100, tot loss = 4.60204764357153, l1: 0.00010141089989942111, l2: 0.00035879386364886503   Iteration 54 of 100, tot loss = 4.590760480474542, l1: 0.00010160234867523993, l2: 0.00035747369865593673   Iteration 55 of 100, tot loss = 4.585338273915378, l1: 0.00010149723179479638, l2: 0.0003570365947036242   Iteration 56 of 100, tot loss = 4.558028653264046, l1: 0.00010091985359395039, l2: 0.00035488301078398114   Iteration 57 of 100, tot loss = 4.611453832241526, l1: 0.00010153203364002126, l2: 0.00035961334856039143   Iteration 58 of 100, tot loss = 4.61641899265092, l1: 0.00010190956521094856, l2: 0.000359732332761833   Iteration 59 of 100, tot loss = 4.608895324044308, l1: 0.00010220377669734801, l2: 0.00035868575416655296   Iteration 60 of 100, tot loss = 4.612634100516638, l1: 0.00010240061219519702, l2: 0.00035886279632298584   Iteration 61 of 100, tot loss = 4.670464795143878, l1: 0.00010333148592970426, l2: 0.0003637149918072506   Iteration 62 of 100, tot loss = 4.664714142199485, l1: 0.00010297369584394244, l2: 0.0003634977167107976   Iteration 63 of 100, tot loss = 4.687859699839637, l1: 0.00010348867946295142, l2: 0.0003652972885729035   Iteration 64 of 100, tot loss = 4.667846893891692, l1: 0.0001031939573863383, l2: 0.000363590730330543   Iteration 65 of 100, tot loss = 4.645511687718905, l1: 0.00010275520275732001, l2: 0.0003617959645523045   Iteration 66 of 100, tot loss = 4.624127165837721, l1: 0.00010267930619275748, l2: 0.0003597334088584775   Iteration 67 of 100, tot loss = 4.639588258159694, l1: 0.00010272951872885547, l2: 0.0003612293058627549   Iteration 68 of 100, tot loss = 4.688892801018322, l1: 0.00010344236981087735, l2: 0.00036544690866482113   Iteration 69 of 100, tot loss = 4.6778568409491275, l1: 0.00010332155706355874, l2: 0.0003644641253850895   Iteration 70 of 100, tot loss = 4.672536756311144, l1: 0.00010285798650459453, l2: 0.00036439568773078334   Iteration 71 of 100, tot loss = 4.684699490036763, l1: 0.00010320787746615013, l2: 0.00036526207084944603   Iteration 72 of 100, tot loss = 4.662644099858072, l1: 0.00010279834739574451, l2: 0.0003634660616727261   Iteration 73 of 100, tot loss = 4.65253168099547, l1: 0.00010253572556443754, l2: 0.0003627174418684325   Iteration 74 of 100, tot loss = 4.673887354296607, l1: 0.00010249089447480967, l2: 0.00036489784011988575   Iteration 75 of 100, tot loss = 4.684975768725077, l1: 0.00010276576120910856, l2: 0.00036573181491500386   Iteration 76 of 100, tot loss = 4.6624402984192495, l1: 0.00010232654451675971, l2: 0.00036391748462368635   Iteration 77 of 100, tot loss = 4.6409514275464145, l1: 0.00010191042114362157, l2: 0.000362184720965066   Iteration 78 of 100, tot loss = 4.621653659221454, l1: 0.00010187374951223879, l2: 0.0003602916157643239   Iteration 79 of 100, tot loss = 4.623318266265, l1: 0.00010174918820148724, l2: 0.00036058263761828404   Iteration 80 of 100, tot loss = 4.5995366558432575, l1: 0.00010102947962877806, l2: 0.0003589241849113023   Iteration 81 of 100, tot loss = 4.588986327618729, l1: 0.00010089147582658809, l2: 0.0003580071558790672   Iteration 82 of 100, tot loss = 4.609246918341008, l1: 0.00010135093774496032, l2: 0.0003595737528862295   Iteration 83 of 100, tot loss = 4.603489239531827, l1: 0.00010119082251176552, l2: 0.0003591581003695547   Iteration 84 of 100, tot loss = 4.624875688836688, l1: 0.00010162756214932804, l2: 0.0003608600056308898   Iteration 85 of 100, tot loss = 4.621009048293619, l1: 0.0001016147419303546, l2: 0.00036048616166226564   Iteration 86 of 100, tot loss = 4.609852933606436, l1: 0.0001013119404507411, l2: 0.00035967335150966987   Iteration 87 of 100, tot loss = 4.637665692417101, l1: 0.00010182288486708554, l2: 0.00036194368303036213   Iteration 88 of 100, tot loss = 4.632452827962962, l1: 0.00010135211695424832, l2: 0.0003618931643193884   Iteration 89 of 100, tot loss = 4.615449716535847, l1: 0.00010091806954855958, l2: 0.000360626900657551   Iteration 90 of 100, tot loss = 4.595003974437714, l1: 0.0001004564522898161, l2: 0.00035904394349523097   Iteration 91 of 100, tot loss = 4.58907954116444, l1: 0.00010042602634012628, l2: 0.0003584819264738094   Iteration 92 of 100, tot loss = 4.583481818437576, l1: 0.00010036780800557766, l2: 0.0003579803728232788   Iteration 93 of 100, tot loss = 4.5862619376951645, l1: 0.00010061906978471695, l2: 0.00035800712301410853   Iteration 94 of 100, tot loss = 4.591888746048542, l1: 0.00010100508537087739, l2: 0.00035818378881308585   Iteration 95 of 100, tot loss = 4.604812219268397, l1: 0.00010146513948137382, l2: 0.0003590160823047259   Iteration 96 of 100, tot loss = 4.59327066813906, l1: 0.00010123891649982397, l2: 0.00035808815012690803   Iteration 97 of 100, tot loss = 4.592196822166443, l1: 0.00010119736892652468, l2: 0.0003580223130165436   Iteration 98 of 100, tot loss = 4.612461979291877, l1: 0.00010120997040405423, l2: 0.0003600362271049098   Iteration 99 of 100, tot loss = 4.613802479975151, l1: 0.00010126552201608769, l2: 0.0003601147258487257   Iteration 100 of 100, tot loss = 4.615290359258652, l1: 0.00010131707316759276, l2: 0.00036021196268848143
   End of epoch 1140; saving model... 

Epoch 1141 of 2000
   Iteration 1 of 100, tot loss = 5.702168941497803, l1: 0.00015026349865365773, l2: 0.0004199534305371344   Iteration 2 of 100, tot loss = 5.002653360366821, l1: 0.00011927868399652652, l2: 0.00038098667573649436   Iteration 3 of 100, tot loss = 5.378872235616048, l1: 0.00012351575181431448, l2: 0.0004143714807772388   Iteration 4 of 100, tot loss = 5.0935609340667725, l1: 0.00011717414417944383, l2: 0.00039218195161083713   Iteration 5 of 100, tot loss = 5.369765281677246, l1: 0.00012404595472617074, l2: 0.000412930577294901   Iteration 6 of 100, tot loss = 5.142024834950765, l1: 0.00011966266659631704, l2: 0.00039453981541252386   Iteration 7 of 100, tot loss = 5.135976791381836, l1: 0.00012063653723868941, l2: 0.00039296113703000756   Iteration 8 of 100, tot loss = 4.999644339084625, l1: 0.00011567011915758485, l2: 0.0003842943115159869   Iteration 9 of 100, tot loss = 4.699260976579454, l1: 0.00010806342025817785, l2: 0.0003618626748599733   Iteration 10 of 100, tot loss = 4.869219732284546, l1: 0.00011216087332286407, l2: 0.00037476110010175033   Iteration 11 of 100, tot loss = 4.888343334197998, l1: 0.00011332562908964147, l2: 0.0003755087051434781   Iteration 12 of 100, tot loss = 4.898129781087239, l1: 0.00011132675141804309, l2: 0.0003784862262061021   Iteration 13 of 100, tot loss = 4.743269865329449, l1: 0.00010833384826233109, l2: 0.00036599313898477703   Iteration 14 of 100, tot loss = 4.855370232037136, l1: 0.0001108437934038063, l2: 0.0003746932291375872   Iteration 15 of 100, tot loss = 4.797845458984375, l1: 0.00010804079089818211, l2: 0.0003717437551434462   Iteration 16 of 100, tot loss = 4.669277101755142, l1: 0.00010632480575623049, l2: 0.00036060290403838735   Iteration 17 of 100, tot loss = 4.752573434044333, l1: 0.00010650133845047094, l2: 0.00036875600412981035   Iteration 18 of 100, tot loss = 4.649174491564433, l1: 0.00010475785388229674, l2: 0.0003601595946949803   Iteration 19 of 100, tot loss = 4.576094803057219, l1: 0.00010358284041438693, l2: 0.00035402663955832586   Iteration 20 of 100, tot loss = 4.804356789588928, l1: 0.00010508285795367556, l2: 0.0003753528195375111   Iteration 21 of 100, tot loss = 4.885270527430943, l1: 0.00010661837465401429, l2: 0.0003819086781558802   Iteration 22 of 100, tot loss = 4.886300433765758, l1: 0.00010650389644566035, l2: 0.0003821261482450857   Iteration 23 of 100, tot loss = 4.8817830085754395, l1: 0.00010628553070052041, l2: 0.0003818927716170235   Iteration 24 of 100, tot loss = 4.904535452524821, l1: 0.00010592467560854857, l2: 0.000384528870805904   Iteration 25 of 100, tot loss = 4.889378719329834, l1: 0.0001053811410383787, l2: 0.0003835567313944921   Iteration 26 of 100, tot loss = 4.912712408946111, l1: 0.00010675774221733893, l2: 0.0003845134997391142   Iteration 27 of 100, tot loss = 4.846229332464713, l1: 0.00010624430878023204, l2: 0.00037837862543744485   Iteration 28 of 100, tot loss = 4.843484018530164, l1: 0.00010591157076435463, l2: 0.0003784368325016528   Iteration 29 of 100, tot loss = 4.761835690202384, l1: 0.00010394195136353213, l2: 0.00037224161898283736   Iteration 30 of 100, tot loss = 4.77224645614624, l1: 0.00010494567529046132, l2: 0.0003722789721602264   Iteration 31 of 100, tot loss = 4.774466437678183, l1: 0.00010477085262180245, l2: 0.0003726757924831022   Iteration 32 of 100, tot loss = 4.77419076859951, l1: 0.00010508820571430988, l2: 0.0003723308732332953   Iteration 33 of 100, tot loss = 4.831505399761778, l1: 0.00010605432169430776, l2: 0.0003770962203182562   Iteration 34 of 100, tot loss = 4.834953672745648, l1: 0.00010561145641344517, l2: 0.00037788391337551945   Iteration 35 of 100, tot loss = 4.817963545663016, l1: 0.00010539773616723583, l2: 0.0003763986215095169   Iteration 36 of 100, tot loss = 4.742502974139319, l1: 0.00010421772630151181, l2: 0.00037003257441230945   Iteration 37 of 100, tot loss = 4.7623057816479655, l1: 0.00010480480970629189, l2: 0.0003714257728534977   Iteration 38 of 100, tot loss = 4.7838752708937, l1: 0.00010522834963921923, l2: 0.000373159181387899   Iteration 39 of 100, tot loss = 4.804119519698314, l1: 0.00010535702938049768, l2: 0.00037505492666521325   Iteration 40 of 100, tot loss = 4.8088331639766695, l1: 0.00010533628019402386, l2: 0.00037554704103968104   Iteration 41 of 100, tot loss = 4.835503851495138, l1: 0.0001060763820643798, l2: 0.00037747400844038076   Iteration 42 of 100, tot loss = 4.893016230492365, l1: 0.00010717782086950527, l2: 0.00038212380812702965   Iteration 43 of 100, tot loss = 4.860776418863341, l1: 0.00010709071419875917, l2: 0.0003789869329328974   Iteration 44 of 100, tot loss = 4.856359248811549, l1: 0.000107248149933267, l2: 0.00037838778040498835   Iteration 45 of 100, tot loss = 4.82055221663581, l1: 0.0001067538254978394, l2: 0.0003753014012343354   Iteration 46 of 100, tot loss = 4.803821226824885, l1: 0.00010693805811372485, l2: 0.0003734440694603583   Iteration 47 of 100, tot loss = 4.836197787142814, l1: 0.00010744550835373415, l2: 0.0003761742759416712   Iteration 48 of 100, tot loss = 4.823154533902804, l1: 0.0001070949152411534, l2: 0.0003752205429918831   Iteration 49 of 100, tot loss = 4.771904531790286, l1: 0.00010595844584286725, l2: 0.00037123201229628556   Iteration 50 of 100, tot loss = 4.795341029167175, l1: 0.00010676164114556741, l2: 0.0003727724659256637   Iteration 51 of 100, tot loss = 4.805554460076725, l1: 0.00010715232294351867, l2: 0.0003734031274421688   Iteration 52 of 100, tot loss = 4.7691196249081536, l1: 0.00010652901234537309, l2: 0.0003703829542456123   Iteration 53 of 100, tot loss = 4.737421638560745, l1: 0.00010548161120617428, l2: 0.00036826055663866255   Iteration 54 of 100, tot loss = 4.71860776565693, l1: 0.00010534843702906208, l2: 0.0003665123432357278   Iteration 55 of 100, tot loss = 4.6885692639784375, l1: 0.00010505476152915931, l2: 0.00036380216823255813   Iteration 56 of 100, tot loss = 4.727359878165381, l1: 0.00010603256623653579, l2: 0.00036670342550288685   Iteration 57 of 100, tot loss = 4.72820354344552, l1: 0.00010626615499189172, l2: 0.00036655420313891547   Iteration 58 of 100, tot loss = 4.7386460509793515, l1: 0.00010606546625083489, l2: 0.0003677991423093521   Iteration 59 of 100, tot loss = 4.816253746970225, l1: 0.00010730028989817553, l2: 0.0003743250891309768   Iteration 60 of 100, tot loss = 4.822957464059194, l1: 0.00010788541882599627, l2: 0.0003744103322484686   Iteration 61 of 100, tot loss = 4.8184573494020055, l1: 0.00010757772226915237, l2: 0.0003742680173523755   Iteration 62 of 100, tot loss = 4.830719059513461, l1: 0.0001077925680874614, l2: 0.00037527934205906104   Iteration 63 of 100, tot loss = 4.802786524333651, l1: 0.00010690362816805848, l2: 0.0003733750280743051   Iteration 64 of 100, tot loss = 4.821307733654976, l1: 0.00010699995584673161, l2: 0.0003751308215669269   Iteration 65 of 100, tot loss = 4.820949121621939, l1: 0.00010710710966332744, l2: 0.0003749878062920358   Iteration 66 of 100, tot loss = 4.8139510732708555, l1: 0.00010721672471404555, l2: 0.00037417838603053525   Iteration 67 of 100, tot loss = 4.81732509385294, l1: 0.000107375362370191, l2: 0.00037435715068513946   Iteration 68 of 100, tot loss = 4.790154457092285, l1: 0.00010685369435283493, l2: 0.00037216175469973893   Iteration 69 of 100, tot loss = 4.817828613778819, l1: 0.00010712850285083245, l2: 0.0003746543617402811   Iteration 70 of 100, tot loss = 4.826690394537789, l1: 0.00010745221796761533, l2: 0.00037521682409403313   Iteration 71 of 100, tot loss = 4.8004860575769985, l1: 0.00010704164230100758, l2: 0.0003730069659814291   Iteration 72 of 100, tot loss = 4.79369243979454, l1: 0.00010672246915217127, l2: 0.0003726467774767015   Iteration 73 of 100, tot loss = 4.782174561121693, l1: 0.00010634008055466725, l2: 0.00037187737793891297   Iteration 74 of 100, tot loss = 4.779011139998564, l1: 0.00010605205062421265, l2: 0.00037184906525638054   Iteration 75 of 100, tot loss = 4.7647285016377765, l1: 0.00010584565073562165, l2: 0.00037062720123988885   Iteration 76 of 100, tot loss = 4.780315919926292, l1: 0.00010627001852538485, l2: 0.0003717615756230723   Iteration 77 of 100, tot loss = 4.7727581432887485, l1: 0.00010625773329475242, l2: 0.00037101808332160795   Iteration 78 of 100, tot loss = 4.77833060729198, l1: 0.00010598162985750091, l2: 0.0003718514329729936   Iteration 79 of 100, tot loss = 4.810031860689573, l1: 0.00010661804881698416, l2: 0.0003743851395276717   Iteration 80 of 100, tot loss = 4.867023450136185, l1: 0.00010724021931309835, l2: 0.000379462127602892   Iteration 81 of 100, tot loss = 4.880414144492444, l1: 0.00010772553507172514, l2: 0.00038031588148981055   Iteration 82 of 100, tot loss = 4.859190684993092, l1: 0.00010725453111950717, l2: 0.00037866453966693724   Iteration 83 of 100, tot loss = 4.842470315565546, l1: 0.00010704883791194914, l2: 0.0003771981959377068   Iteration 84 of 100, tot loss = 4.850807925065358, l1: 0.00010699004546733617, l2: 0.0003780907496721262   Iteration 85 of 100, tot loss = 4.8661181309643915, l1: 0.00010720026004812955, l2: 0.0003794115557170966   Iteration 86 of 100, tot loss = 4.881522381028463, l1: 0.00010754889470178547, l2: 0.00038060334613979904   Iteration 87 of 100, tot loss = 4.864271747654882, l1: 0.0001070013032435311, l2: 0.0003794258742743097   Iteration 88 of 100, tot loss = 4.852017559788444, l1: 0.00010705947972217638, l2: 0.0003781422786811494   Iteration 89 of 100, tot loss = 4.849538610222634, l1: 0.00010698431377819729, l2: 0.00037796954948748093   Iteration 90 of 100, tot loss = 4.838520736164517, l1: 0.00010673235665308312, l2: 0.00037711971898614946   Iteration 91 of 100, tot loss = 4.840902304911351, l1: 0.00010693236941773608, l2: 0.000377157863273009   Iteration 92 of 100, tot loss = 4.843448553396308, l1: 0.00010702624936327439, l2: 0.00037731860831390014   Iteration 93 of 100, tot loss = 4.829533541074363, l1: 0.0001067800928653288, l2: 0.000376173263473276   Iteration 94 of 100, tot loss = 4.832497703268173, l1: 0.00010703967749809925, l2: 0.0003762100947069678   Iteration 95 of 100, tot loss = 4.825608459271883, l1: 0.00010688037115247234, l2: 0.0003756804768559768   Iteration 96 of 100, tot loss = 4.817888746658961, l1: 0.00010657826093544524, l2: 0.0003752106155540484   Iteration 97 of 100, tot loss = 4.823380711152382, l1: 0.00010679887287251498, l2: 0.0003755392000251978   Iteration 98 of 100, tot loss = 4.816983349469243, l1: 0.00010661486529136951, l2: 0.00037508347126351173   Iteration 99 of 100, tot loss = 4.82204183424362, l1: 0.0001066098269104553, l2: 0.0003755943578635013   Iteration 100 of 100, tot loss = 4.83000876903534, l1: 0.00010685029417800251, l2: 0.00037615058448864147
   End of epoch 1141; saving model... 

Epoch 1142 of 2000
   Iteration 1 of 100, tot loss = 4.023694038391113, l1: 9.468018834013492e-05, l2: 0.00030768924625590444   Iteration 2 of 100, tot loss = 4.212686777114868, l1: 9.627830877434462e-05, l2: 0.0003249903966207057   Iteration 3 of 100, tot loss = 3.8449588616689048, l1: 9.586470211312796e-05, l2: 0.00028863119950983673   Iteration 4 of 100, tot loss = 4.145807445049286, l1: 0.00010395342360425275, l2: 0.0003106273325101938   Iteration 5 of 100, tot loss = 4.153619813919067, l1: 0.00010165706626139582, l2: 0.0003137049294309691   Iteration 6 of 100, tot loss = 4.181283672650655, l1: 0.00010447981064013827, l2: 0.0003136485635574597   Iteration 7 of 100, tot loss = 4.5068539551326205, l1: 0.00011152806629460039, l2: 0.00033915733677401604   Iteration 8 of 100, tot loss = 4.457531839609146, l1: 0.00011038468801416457, l2: 0.00033536850060045253   Iteration 9 of 100, tot loss = 4.34082113371955, l1: 0.00010810597531316389, l2: 0.0003259761417413958   Iteration 10 of 100, tot loss = 4.46850426197052, l1: 0.00011220614760532044, l2: 0.000334644278336782   Iteration 11 of 100, tot loss = 4.435071013190529, l1: 0.00011245383343934505, l2: 0.00033105326821731234   Iteration 12 of 100, tot loss = 4.5534228682518005, l1: 0.00011324012848490383, l2: 0.0003421021574467886   Iteration 13 of 100, tot loss = 4.63920364013085, l1: 0.00011441569110432353, l2: 0.00034950467054911243   Iteration 14 of 100, tot loss = 4.68421471118927, l1: 0.00011447815580010814, l2: 0.00035394331228287356   Iteration 15 of 100, tot loss = 4.558343203862508, l1: 0.0001117511162495551, l2: 0.00034408320110136025   Iteration 16 of 100, tot loss = 4.691273674368858, l1: 0.00011334997407175251, l2: 0.0003557773916327278   Iteration 17 of 100, tot loss = 4.634674913742963, l1: 0.0001121132167206858, l2: 0.00035135427349493564   Iteration 18 of 100, tot loss = 4.5347646077473955, l1: 0.0001108078686229419, l2: 0.00034266859034283296   Iteration 19 of 100, tot loss = 4.6540670896831315, l1: 0.00011174049340249774, l2: 0.00035366621236071775   Iteration 20 of 100, tot loss = 4.681601738929748, l1: 0.00011261795916652772, l2: 0.0003555422139470465   Iteration 21 of 100, tot loss = 4.634732870828538, l1: 0.00011154685558895359, l2: 0.000351926431037663   Iteration 22 of 100, tot loss = 4.655329216610301, l1: 0.00011112398186179979, l2: 0.00035440894002518195   Iteration 23 of 100, tot loss = 4.5579991547957714, l1: 0.00010931924771284685, l2: 0.00034648066756072575   Iteration 24 of 100, tot loss = 4.593199769655864, l1: 0.00010938547347905114, l2: 0.00034993450390174985   Iteration 25 of 100, tot loss = 4.633802280426026, l1: 0.00010922190849669278, l2: 0.0003541583195328712   Iteration 26 of 100, tot loss = 4.6391733793111944, l1: 0.00010892574237704348, l2: 0.00035499159555631474   Iteration 27 of 100, tot loss = 4.617626896610966, l1: 0.0001092883119579508, l2: 0.00035247437793899463   Iteration 28 of 100, tot loss = 4.562120616436005, l1: 0.00010807887364145634, l2: 0.0003481331883189601   Iteration 29 of 100, tot loss = 4.5898229582556365, l1: 0.00010679616865798317, l2: 0.000352186127264727   Iteration 30 of 100, tot loss = 4.607744320233663, l1: 0.00010655354587167191, l2: 0.000354220887432651   Iteration 31 of 100, tot loss = 4.603774263012793, l1: 0.00010706152416758179, l2: 0.0003533159038089516   Iteration 32 of 100, tot loss = 4.6443255469202995, l1: 0.00010824424248312425, l2: 0.0003561883127076726   Iteration 33 of 100, tot loss = 4.663779424898552, l1: 0.00010808120339678487, l2: 0.0003582967386164731   Iteration 34 of 100, tot loss = 4.635197029394262, l1: 0.00010776011763750027, l2: 0.0003557595846359618   Iteration 35 of 100, tot loss = 4.612871217727661, l1: 0.00010737643107339474, l2: 0.0003539106895914301   Iteration 36 of 100, tot loss = 4.559874229960972, l1: 0.00010651522845566635, l2: 0.00034947219319292344   Iteration 37 of 100, tot loss = 4.572321015435296, l1: 0.00010682240944997584, l2: 0.0003504096919739921   Iteration 38 of 100, tot loss = 4.51357428651107, l1: 0.00010535171263894115, l2: 0.0003460057161742282   Iteration 39 of 100, tot loss = 4.485842362428323, l1: 0.00010480574938134314, l2: 0.0003437784874292377   Iteration 40 of 100, tot loss = 4.477787172794342, l1: 0.00010491332996025449, l2: 0.0003428653875744203   Iteration 41 of 100, tot loss = 4.448179849764196, l1: 0.00010419200407341123, l2: 0.0003406259812636119   Iteration 42 of 100, tot loss = 4.469626824061076, l1: 0.00010428980455063062, l2: 0.00034267287888464386   Iteration 43 of 100, tot loss = 4.513991677483847, l1: 0.00010540429587329759, l2: 0.00034599487370605647   Iteration 44 of 100, tot loss = 4.621487238190391, l1: 0.00010665295766640074, l2: 0.00035549576800390656   Iteration 45 of 100, tot loss = 4.710534826914469, l1: 0.00010825441228613878, l2: 0.00036279907184911685   Iteration 46 of 100, tot loss = 4.740864276885986, l1: 0.00010877832913968672, l2: 0.00036530810029944405   Iteration 47 of 100, tot loss = 4.791922031564916, l1: 0.00010891365502871811, l2: 0.00037027854983883097   Iteration 48 of 100, tot loss = 4.761975293358167, l1: 0.000107901804009695, l2: 0.0003682957270333039   Iteration 49 of 100, tot loss = 4.733145820851228, l1: 0.00010685807093086994, l2: 0.00036645651294859314   Iteration 50 of 100, tot loss = 4.68664400100708, l1: 0.00010567092082055751, l2: 0.0003629934808122925   Iteration 51 of 100, tot loss = 4.658042472951553, l1: 0.0001051345610832476, l2: 0.0003606696877350081   Iteration 52 of 100, tot loss = 4.676998445620904, l1: 0.00010536548807767058, l2: 0.0003623343584947109   Iteration 53 of 100, tot loss = 4.670162187432343, l1: 0.00010544333651163263, l2: 0.0003615728843592564   Iteration 54 of 100, tot loss = 4.683905067267241, l1: 0.00010575786243996979, l2: 0.00036263264606801657   Iteration 55 of 100, tot loss = 4.702823461185802, l1: 0.00010635253122679636, l2: 0.00036392981657462024   Iteration 56 of 100, tot loss = 4.719098308256695, l1: 0.0001067604467997236, l2: 0.00036514938593817145   Iteration 57 of 100, tot loss = 4.738086629332158, l1: 0.00010697123830220989, l2: 0.0003668374262991149   Iteration 58 of 100, tot loss = 4.729854037021768, l1: 0.00010712562144707872, l2: 0.000365859784480939   Iteration 59 of 100, tot loss = 4.685551990896968, l1: 0.00010606139822827525, l2: 0.00036249380318005965   Iteration 60 of 100, tot loss = 4.680745967229208, l1: 0.00010614581915433519, l2: 0.0003619287798452812   Iteration 61 of 100, tot loss = 4.6606221238120655, l1: 0.00010586358782392545, l2: 0.00036019862669550614   Iteration 62 of 100, tot loss = 4.652505286278263, l1: 0.00010558672217866256, l2: 0.00035966380880469637   Iteration 63 of 100, tot loss = 4.667659400001405, l1: 0.00010616138209274689, l2: 0.00036060456038703993   Iteration 64 of 100, tot loss = 4.681318666785955, l1: 0.00010600165342111723, l2: 0.0003621302157625905   Iteration 65 of 100, tot loss = 4.706379791406484, l1: 0.00010676139634317504, l2: 0.0003638765855262486   Iteration 66 of 100, tot loss = 4.725721622958328, l1: 0.00010707424217164798, l2: 0.0003654979227576405   Iteration 67 of 100, tot loss = 4.774760107495892, l1: 0.00010768181462806942, l2: 0.0003697941984995199   Iteration 68 of 100, tot loss = 4.733380868154414, l1: 0.00010684825819600305, l2: 0.00036648983094731677   Iteration 69 of 100, tot loss = 4.750584882238637, l1: 0.00010739885302441046, l2: 0.0003676596376032609   Iteration 70 of 100, tot loss = 4.75106646333422, l1: 0.00010748859718400387, l2: 0.0003676180515737672   Iteration 71 of 100, tot loss = 4.7362443695605645, l1: 0.00010697348107883877, l2: 0.00036665095831267536   Iteration 72 of 100, tot loss = 4.732616378201379, l1: 0.00010654386970701226, l2: 0.00036671777060190734   Iteration 73 of 100, tot loss = 4.741421516627481, l1: 0.0001067827035188962, l2: 0.0003673594500008358   Iteration 74 of 100, tot loss = 4.73421565262047, l1: 0.00010678018092270626, l2: 0.0003666413859055201   Iteration 75 of 100, tot loss = 4.728088150024414, l1: 0.00010635709724738263, l2: 0.0003664517189220836   Iteration 76 of 100, tot loss = 4.70903815407502, l1: 0.00010606675770855203, l2: 0.00036483705882442576   Iteration 77 of 100, tot loss = 4.674474880292818, l1: 0.00010555631827144254, l2: 0.00036189117084874155   Iteration 78 of 100, tot loss = 4.680780860093924, l1: 0.00010598064802298979, l2: 0.0003620974394158484   Iteration 79 of 100, tot loss = 4.679001847399941, l1: 0.00010600371692068816, l2: 0.00036189646925777197   Iteration 80 of 100, tot loss = 4.705591878294944, l1: 0.00010622161112223693, l2: 0.0003643375777755864   Iteration 81 of 100, tot loss = 4.72091504379555, l1: 0.00010630879963476521, l2: 0.0003657827055351924   Iteration 82 of 100, tot loss = 4.696650362596279, l1: 0.00010580750051481229, l2: 0.00036385753646311236   Iteration 83 of 100, tot loss = 4.699595678283508, l1: 0.00010575939686751521, l2: 0.0003642001717368208   Iteration 84 of 100, tot loss = 4.680514329955692, l1: 0.00010541234271277928, l2: 0.0003626390910843232   Iteration 85 of 100, tot loss = 4.6814714263467225, l1: 0.0001054217833049787, l2: 0.00036272535987390096   Iteration 86 of 100, tot loss = 4.668835903322974, l1: 0.00010511099529583928, l2: 0.00036177259540255723   Iteration 87 of 100, tot loss = 4.669777341272639, l1: 0.00010504779631195686, l2: 0.00036192993782648025   Iteration 88 of 100, tot loss = 4.677191005511717, l1: 0.00010533591685915863, l2: 0.000362383183693799   Iteration 89 of 100, tot loss = 4.67572338393565, l1: 0.00010531876389731213, l2: 0.00036225357440200875   Iteration 90 of 100, tot loss = 4.687694568104214, l1: 0.00010575331313399753, l2: 0.000363016143900394   Iteration 91 of 100, tot loss = 4.6981270889659505, l1: 0.00010598152356988566, l2: 0.00036383118556561664   Iteration 92 of 100, tot loss = 4.708650695241016, l1: 0.00010628219305268114, l2: 0.0003645828763420349   Iteration 93 of 100, tot loss = 4.71378275399567, l1: 0.00010634311719738718, l2: 0.00036503515855324084   Iteration 94 of 100, tot loss = 4.705832849157617, l1: 0.00010623240825922, l2: 0.0003643508771734134   Iteration 95 of 100, tot loss = 4.694167746995625, l1: 0.00010588551702216807, l2: 0.0003635312582206863   Iteration 96 of 100, tot loss = 4.682786653439204, l1: 0.00010590702341535992, l2: 0.0003623716424954182   Iteration 97 of 100, tot loss = 4.657661651827626, l1: 0.0001052379541095471, l2: 0.00036052821162660826   Iteration 98 of 100, tot loss = 4.661449488328428, l1: 0.00010516607638849991, l2: 0.00036097887314841796   Iteration 99 of 100, tot loss = 4.676748976562962, l1: 0.00010548266568961976, l2: 0.0003621922325251172   Iteration 100 of 100, tot loss = 4.662507638931275, l1: 0.0001053391086315969, l2: 0.00036091165588004515
   End of epoch 1142; saving model... 

Epoch 1143 of 2000
   Iteration 1 of 100, tot loss = 7.0377702713012695, l1: 0.0001431245618732646, l2: 0.0005606524646282196   Iteration 2 of 100, tot loss = 4.744791030883789, l1: 0.00010650247349985875, l2: 0.00036797663051402196   Iteration 3 of 100, tot loss = 4.950128396352132, l1: 0.00010581439710222185, l2: 0.0003891984573177372   Iteration 4 of 100, tot loss = 5.438353776931763, l1: 0.00010979606122418772, l2: 0.00043403933159424923   Iteration 5 of 100, tot loss = 5.433253574371338, l1: 0.00011230718664592131, l2: 0.0004310181800974533   Iteration 6 of 100, tot loss = 4.881182710329692, l1: 0.00010156264882728767, l2: 0.00038655562821077183   Iteration 7 of 100, tot loss = 4.705684661865234, l1: 9.906705859715917e-05, l2: 0.00037150141309081974   Iteration 8 of 100, tot loss = 4.8025736808776855, l1: 9.9480896096793e-05, l2: 0.00038077647514001   Iteration 9 of 100, tot loss = 4.894699732462565, l1: 0.00010396483250790172, l2: 0.0003855051456614294   Iteration 10 of 100, tot loss = 4.797807598114014, l1: 0.00010415743236080743, l2: 0.00037562333309324456   Iteration 11 of 100, tot loss = 4.9377641677856445, l1: 0.00010784138033738022, l2: 0.00038593504392668944   Iteration 12 of 100, tot loss = 4.893387873967488, l1: 0.00010745374250594371, l2: 0.00038188505156237323   Iteration 13 of 100, tot loss = 4.992987082554744, l1: 0.00010840055452158245, l2: 0.0003908981575827616   Iteration 14 of 100, tot loss = 4.9049815109797885, l1: 0.00010505202798023154, l2: 0.0003854461257495651   Iteration 15 of 100, tot loss = 4.823050594329834, l1: 0.00010452530453524863, l2: 0.00037777975764280805   Iteration 16 of 100, tot loss = 4.6898427456617355, l1: 0.00010182922460444388, l2: 0.0003671550530270906   Iteration 17 of 100, tot loss = 4.780750400879803, l1: 0.0001029186139074976, l2: 0.0003751564295003738   Iteration 18 of 100, tot loss = 4.767870651351081, l1: 0.00010436890200556566, l2: 0.00037241816527158435   Iteration 19 of 100, tot loss = 4.878192035775435, l1: 0.000106796518881119, l2: 0.00038102268713134294   Iteration 20 of 100, tot loss = 4.839591324329376, l1: 0.00010680309133022092, l2: 0.00037715604266850276   Iteration 21 of 100, tot loss = 4.8131072748275034, l1: 0.00010708441723614843, l2: 0.0003742263111884573   Iteration 22 of 100, tot loss = 4.752181443301114, l1: 0.00010576062215312214, l2: 0.000369457522703504   Iteration 23 of 100, tot loss = 4.8039957544078, l1: 0.00010592872327517556, l2: 0.0003744708525988719   Iteration 24 of 100, tot loss = 4.753217875957489, l1: 0.00010501711858523777, l2: 0.00037030467016544816   Iteration 25 of 100, tot loss = 4.84593132019043, l1: 0.00010702818894060328, l2: 0.0003775649447925389   Iteration 26 of 100, tot loss = 5.059707421522874, l1: 0.00011093069462311597, l2: 0.00039504004570727167   Iteration 27 of 100, tot loss = 5.05146853129069, l1: 0.00011068755810373221, l2: 0.0003944592921201278   Iteration 28 of 100, tot loss = 5.0541627407073975, l1: 0.0001104933482045973, l2: 0.00039492292229884436   Iteration 29 of 100, tot loss = 5.032766177736479, l1: 0.00010977681387007108, l2: 0.00039349979989969266   Iteration 30 of 100, tot loss = 5.0143921852111815, l1: 0.00011002604951499961, l2: 0.00039141316471310954   Iteration 31 of 100, tot loss = 5.017541162429318, l1: 0.00011002057163427855, l2: 0.00039173353974136614   Iteration 32 of 100, tot loss = 5.040445983409882, l1: 0.00011096759476458828, l2: 0.0003930769998987671   Iteration 33 of 100, tot loss = 4.983605305353801, l1: 0.00011069787984579858, l2: 0.00038766264730260116   Iteration 34 of 100, tot loss = 5.019543234039755, l1: 0.00011127192577559446, l2: 0.0003906823947224492   Iteration 35 of 100, tot loss = 5.0036094052451, l1: 0.00011091697233496234, l2: 0.0003894439653127587   Iteration 36 of 100, tot loss = 5.012639065583547, l1: 0.0001104259301630211, l2: 0.0003908379735043531   Iteration 37 of 100, tot loss = 5.021640487619348, l1: 0.00011062683542321965, l2: 0.00039153721071842656   Iteration 38 of 100, tot loss = 4.989927373434368, l1: 0.0001100498229789082, l2: 0.00038894291202795053   Iteration 39 of 100, tot loss = 5.004218633358295, l1: 0.00011034716552762219, l2: 0.00039007469604257494   Iteration 40 of 100, tot loss = 4.982487243413925, l1: 0.00011013263811037177, l2: 0.0003881160835589981   Iteration 41 of 100, tot loss = 5.040379588196918, l1: 0.00011115343854393492, l2: 0.00039288451825552506   Iteration 42 of 100, tot loss = 5.0011633804866245, l1: 0.00010991091377426139, l2: 0.0003902054224800806   Iteration 43 of 100, tot loss = 4.955698085385699, l1: 0.00010898823199758501, l2: 0.000386581574783249   Iteration 44 of 100, tot loss = 4.917927557771856, l1: 0.00010846997206086102, l2: 0.00038332278199959546   Iteration 45 of 100, tot loss = 4.884103361765543, l1: 0.00010805183790378376, l2: 0.000380358496396285   Iteration 46 of 100, tot loss = 4.927036326864491, l1: 0.00010865082771013714, l2: 0.00038405280223662925   Iteration 47 of 100, tot loss = 4.9708823244622415, l1: 0.00010948463200970474, l2: 0.00038760359847343507   Iteration 48 of 100, tot loss = 4.922625750303268, l1: 0.0001089064843805924, l2: 0.0003833560886050691   Iteration 49 of 100, tot loss = 4.937437904124358, l1: 0.00010941388007973282, l2: 0.000384329908767392   Iteration 50 of 100, tot loss = 4.921543846130371, l1: 0.00010909530836215708, l2: 0.00038305907481117174   Iteration 51 of 100, tot loss = 4.911528867833755, l1: 0.00010880575877415729, l2: 0.0003823471263019076   Iteration 52 of 100, tot loss = 4.922087137515728, l1: 0.0001085851746001236, l2: 0.00038362353734555654   Iteration 53 of 100, tot loss = 4.965066540916011, l1: 0.0001095602804886063, l2: 0.00038694637198273513   Iteration 54 of 100, tot loss = 4.976471565387867, l1: 0.00010945053125194843, l2: 0.0003881966234378827   Iteration 55 of 100, tot loss = 4.96736327084628, l1: 0.00010962388617124155, l2: 0.0003871124393878166   Iteration 56 of 100, tot loss = 4.924261885029929, l1: 0.00010846377965987943, l2: 0.00038396240727576824   Iteration 57 of 100, tot loss = 4.904772996902466, l1: 0.00010816506874745533, l2: 0.0003823122293524967   Iteration 58 of 100, tot loss = 4.891177452843765, l1: 0.0001076812637495158, l2: 0.00038143648045410497   Iteration 59 of 100, tot loss = 4.8570412902508755, l1: 0.00010716298579532353, l2: 0.0003785411420305875   Iteration 60 of 100, tot loss = 4.82958881855011, l1: 0.00010685730479356911, l2: 0.0003761015754813949   Iteration 61 of 100, tot loss = 4.876017304717517, l1: 0.00010745941913451953, l2: 0.0003801423103976079   Iteration 62 of 100, tot loss = 4.826215013380973, l1: 0.00010639632872029793, l2: 0.00037622517171961763   Iteration 63 of 100, tot loss = 4.827972116924467, l1: 0.00010674923506153319, l2: 0.00037604797583457733   Iteration 64 of 100, tot loss = 4.837531439960003, l1: 0.00010700876629243794, l2: 0.0003767443777178414   Iteration 65 of 100, tot loss = 4.841308491046612, l1: 0.00010740169074359493, l2: 0.0003767291585413309   Iteration 66 of 100, tot loss = 4.822225393670978, l1: 0.00010701528161448412, l2: 0.0003752072578098512   Iteration 67 of 100, tot loss = 4.824685285340494, l1: 0.00010709329646162397, l2: 0.0003753752319955392   Iteration 68 of 100, tot loss = 4.815449248341953, l1: 0.00010718227847118054, l2: 0.0003743626464925268   Iteration 69 of 100, tot loss = 4.797156696734221, l1: 0.000106460885549693, l2: 0.00037325478405219275   Iteration 70 of 100, tot loss = 4.773704351697649, l1: 0.00010610713891635117, l2: 0.0003712632963600169   Iteration 71 of 100, tot loss = 4.754713259952169, l1: 0.00010590396239832175, l2: 0.00036956736380168773   Iteration 72 of 100, tot loss = 4.766089896361033, l1: 0.00010590700245908617, l2: 0.00037070198757444613   Iteration 73 of 100, tot loss = 4.778424073572028, l1: 0.00010614641041061139, l2: 0.00037169599728595007   Iteration 74 of 100, tot loss = 4.801467824626613, l1: 0.0001066492048856752, l2: 0.0003734975781864361   Iteration 75 of 100, tot loss = 4.795044720967611, l1: 0.00010618500295095145, l2: 0.0003733194697027405   Iteration 76 of 100, tot loss = 4.773269534111023, l1: 0.00010615483839309921, l2: 0.0003711721156402123   Iteration 77 of 100, tot loss = 4.7739522116524835, l1: 0.00010627642967103219, l2: 0.0003711187925912416   Iteration 78 of 100, tot loss = 4.777224247272198, l1: 0.00010622926073175712, l2: 0.00037149316537976026   Iteration 79 of 100, tot loss = 4.784056536759002, l1: 0.00010633207255270025, l2: 0.0003720735823455609   Iteration 80 of 100, tot loss = 4.801180136203766, l1: 0.00010684069120543427, l2: 0.00037327732334233585   Iteration 81 of 100, tot loss = 4.784356808956758, l1: 0.00010646602441053722, l2: 0.0003719696576382252   Iteration 82 of 100, tot loss = 4.798491963526097, l1: 0.00010651158521707147, l2: 0.0003733376121622647   Iteration 83 of 100, tot loss = 4.7866749878389285, l1: 0.00010655368935998462, l2: 0.0003721138105814693   Iteration 84 of 100, tot loss = 4.803208589553833, l1: 0.00010694109117875563, l2: 0.0003733797685916735   Iteration 85 of 100, tot loss = 4.857591314876781, l1: 0.00010776228009490296, l2: 0.00037799685258719634   Iteration 86 of 100, tot loss = 4.864169852678166, l1: 0.00010808084580064535, l2: 0.0003783361406571795   Iteration 87 of 100, tot loss = 4.8540190499404385, l1: 0.00010781927501258237, l2: 0.0003775826311769264   Iteration 88 of 100, tot loss = 4.879349838603627, l1: 0.00010812874709451783, l2: 0.0003798062377675458   Iteration 89 of 100, tot loss = 4.893693538194292, l1: 0.00010870652387096557, l2: 0.00038066283087481565   Iteration 90 of 100, tot loss = 4.882618874973721, l1: 0.00010831054621828824, l2: 0.0003799513420542806   Iteration 91 of 100, tot loss = 4.8904092364258815, l1: 0.0001083421683198629, l2: 0.0003806987560192983   Iteration 92 of 100, tot loss = 4.884898836198061, l1: 0.0001081094849961, l2: 0.0003803803991341619   Iteration 93 of 100, tot loss = 4.882371171828239, l1: 0.00010777523693245565, l2: 0.0003804618807857536   Iteration 94 of 100, tot loss = 4.8643365388220925, l1: 0.00010749792528123238, l2: 0.00037893572895901593   Iteration 95 of 100, tot loss = 4.86238290636163, l1: 0.00010721712475826375, l2: 0.00037902116662160936   Iteration 96 of 100, tot loss = 4.836976180473964, l1: 0.0001065995656214606, l2: 0.00037709805322568474   Iteration 97 of 100, tot loss = 4.840660719527412, l1: 0.00010649909059212245, l2: 0.0003775669821086772   Iteration 98 of 100, tot loss = 4.846442081490341, l1: 0.0001068481869136734, l2: 0.00037779602250654476   Iteration 99 of 100, tot loss = 4.828714960753316, l1: 0.00010645038657292554, l2: 0.00037642111066926384   Iteration 100 of 100, tot loss = 4.801598987579346, l1: 0.00010583720682916465, l2: 0.00037432269295095466
   End of epoch 1143; saving model... 

Epoch 1144 of 2000
   Iteration 1 of 100, tot loss = 5.799966812133789, l1: 0.00011861126404255629, l2: 0.00046138541074469686   Iteration 2 of 100, tot loss = 5.471534490585327, l1: 0.00011237988655921072, l2: 0.00043477353756316006   Iteration 3 of 100, tot loss = 4.949596563975017, l1: 0.00010374478976397465, l2: 0.0003912148531526327   Iteration 4 of 100, tot loss = 4.5371310114860535, l1: 0.00010065741298603825, l2: 0.00035305567871546373   Iteration 5 of 100, tot loss = 4.49240460395813, l1: 9.340688557131216e-05, l2: 0.0003558335651177913   Iteration 6 of 100, tot loss = 4.4177645444869995, l1: 9.168221125340399e-05, l2: 0.00035009423542457324   Iteration 7 of 100, tot loss = 4.904406241008213, l1: 9.854440057616947e-05, l2: 0.0003918962154005255   Iteration 8 of 100, tot loss = 4.6531244814395905, l1: 9.61286586971255e-05, l2: 0.00036918378100381233   Iteration 9 of 100, tot loss = 4.817126936382717, l1: 9.718894903522191e-05, l2: 0.00038452373701147735   Iteration 10 of 100, tot loss = 4.681568717956543, l1: 9.655230896896683e-05, l2: 0.0003716045554028824   Iteration 11 of 100, tot loss = 4.650457165457985, l1: 9.701467140323736e-05, l2: 0.00036803103814070875   Iteration 12 of 100, tot loss = 4.580735564231873, l1: 9.556614895700477e-05, l2: 0.00036250739988948527   Iteration 13 of 100, tot loss = 4.518295948322002, l1: 9.342900515408613e-05, l2: 0.0003584005820672386   Iteration 14 of 100, tot loss = 4.628421374729702, l1: 9.774059175729885e-05, l2: 0.00036510154105989   Iteration 15 of 100, tot loss = 4.738300768534343, l1: 9.818205823345731e-05, l2: 0.00037564801362653574   Iteration 16 of 100, tot loss = 4.675290361046791, l1: 9.796086669666693e-05, l2: 0.00036956816620659083   Iteration 17 of 100, tot loss = 4.6569681027356316, l1: 9.615873183493557e-05, l2: 0.0003695380754138836   Iteration 18 of 100, tot loss = 4.487654215759701, l1: 9.266175685398694e-05, l2: 0.00035610366143777757   Iteration 19 of 100, tot loss = 4.523183603035776, l1: 9.473074628897991e-05, l2: 0.0003575876111618096   Iteration 20 of 100, tot loss = 4.548222380876541, l1: 9.456662010052242e-05, l2: 0.0003602556163968984   Iteration 21 of 100, tot loss = 4.641892688614981, l1: 9.776046153135775e-05, l2: 0.0003664288039241607   Iteration 22 of 100, tot loss = 4.708005715500224, l1: 0.00010026684520363978, l2: 0.00037053372480758384   Iteration 23 of 100, tot loss = 4.601505678632985, l1: 9.811095261956443e-05, l2: 0.00036203961454979753   Iteration 24 of 100, tot loss = 4.705882633725802, l1: 0.00010008747055204974, l2: 0.00037050079117761925   Iteration 25 of 100, tot loss = 4.6996663331985475, l1: 0.0001011102375923656, l2: 0.0003688563941977918   Iteration 26 of 100, tot loss = 4.691048406637632, l1: 0.00010193011229813028, l2: 0.00036717472763624613   Iteration 27 of 100, tot loss = 4.655977174087807, l1: 0.00010177452700359195, l2: 0.00036382319026247216   Iteration 28 of 100, tot loss = 4.572561225720814, l1: 0.00010048621879832353, l2: 0.000356769903321817   Iteration 29 of 100, tot loss = 4.5935565644297105, l1: 0.00010121012044315836, l2: 0.00035814553447035625   Iteration 30 of 100, tot loss = 4.572926684220632, l1: 0.00010071389066676298, l2: 0.0003565787764576574   Iteration 31 of 100, tot loss = 4.557989416583892, l1: 0.00010036872540255107, l2: 0.00035543021494372477   Iteration 32 of 100, tot loss = 4.499293219298124, l1: 9.963337583940302e-05, l2: 0.000350295944372192   Iteration 33 of 100, tot loss = 4.552628282344703, l1: 0.00010025256602658015, l2: 0.00035501026132172257   Iteration 34 of 100, tot loss = 4.519948661327362, l1: 9.960119603099028e-05, l2: 0.00035239366971997213   Iteration 35 of 100, tot loss = 4.432725214958191, l1: 9.770583585902517e-05, l2: 0.0003455666852200271   Iteration 36 of 100, tot loss = 4.427545166677898, l1: 9.773360640489652e-05, l2: 0.00034502090925848787   Iteration 37 of 100, tot loss = 4.4421607900310205, l1: 9.75286659652468e-05, l2: 0.0003466874109722699   Iteration 38 of 100, tot loss = 4.421913162658089, l1: 9.720362600331243e-05, l2: 0.0003449876885018679   Iteration 39 of 100, tot loss = 4.442470492460789, l1: 9.792165768046219e-05, l2: 0.00034632539031101775   Iteration 40 of 100, tot loss = 4.445196726918221, l1: 9.74704636973911e-05, l2: 0.0003470492085398291   Iteration 41 of 100, tot loss = 4.3980687914825065, l1: 9.667387224376066e-05, l2: 0.00034313300633717826   Iteration 42 of 100, tot loss = 4.3728776063237875, l1: 9.659582671398918e-05, l2: 0.0003406919334208526   Iteration 43 of 100, tot loss = 4.329156889471897, l1: 9.597493467587154e-05, l2: 0.00033694075411700017   Iteration 44 of 100, tot loss = 4.311408912593668, l1: 9.598841312584806e-05, l2: 0.0003351524777496128   Iteration 45 of 100, tot loss = 4.320576257175869, l1: 9.669201649052816e-05, l2: 0.0003353656086902548   Iteration 46 of 100, tot loss = 4.2984005912490515, l1: 9.603081391263834e-05, l2: 0.0003338092449310985   Iteration 47 of 100, tot loss = 4.302656034205822, l1: 9.612355875396268e-05, l2: 0.00033414204393766145   Iteration 48 of 100, tot loss = 4.307488851249218, l1: 9.613697996731692e-05, l2: 0.00033461190393306123   Iteration 49 of 100, tot loss = 4.342789263141398, l1: 9.712910550475425e-05, l2: 0.00033714981943877337   Iteration 50 of 100, tot loss = 4.354192364215851, l1: 9.7350844880566e-05, l2: 0.00033806838953751137   Iteration 51 of 100, tot loss = 4.352227447079677, l1: 9.734889503235143e-05, l2: 0.0003378738473139831   Iteration 52 of 100, tot loss = 4.369656849365968, l1: 9.794357188534028e-05, l2: 0.00033902211096294475   Iteration 53 of 100, tot loss = 4.351511908027361, l1: 9.73385890488917e-05, l2: 0.00033781259968099274   Iteration 54 of 100, tot loss = 4.4074353884767605, l1: 9.866113611712569e-05, l2: 0.0003420824007965469   Iteration 55 of 100, tot loss = 4.388995844667608, l1: 9.834068292878906e-05, l2: 0.0003405588995189067   Iteration 56 of 100, tot loss = 4.373208478093147, l1: 9.806773024528021e-05, l2: 0.0003392531157909876   Iteration 57 of 100, tot loss = 4.405842419256244, l1: 9.87863844656794e-05, l2: 0.00034179785551825993   Iteration 58 of 100, tot loss = 4.391098982301251, l1: 9.814780599393496e-05, l2: 0.00034096209040806823   Iteration 59 of 100, tot loss = 4.4001496343289395, l1: 9.813764521268086e-05, l2: 0.0003418773169761476   Iteration 60 of 100, tot loss = 4.449871506293615, l1: 9.869506684481166e-05, l2: 0.00034629208227367295   Iteration 61 of 100, tot loss = 4.4627715075602294, l1: 9.908358834599924e-05, l2: 0.0003471935608521577   Iteration 62 of 100, tot loss = 4.500811836411876, l1: 9.968065710235087e-05, l2: 0.0003504005251142512   Iteration 63 of 100, tot loss = 4.5399046909241445, l1: 9.98827461097833e-05, l2: 0.0003541077218999687   Iteration 64 of 100, tot loss = 4.57875151745975, l1: 0.00010033617400040384, l2: 0.0003575389768002424   Iteration 65 of 100, tot loss = 4.5686003519938545, l1: 0.00010031730164952863, l2: 0.00035654273222182663   Iteration 66 of 100, tot loss = 4.547456110968734, l1: 0.00010011039965761346, l2: 0.0003546352098496617   Iteration 67 of 100, tot loss = 4.538973148189374, l1: 9.989636926738018e-05, l2: 0.00035400094397960286   Iteration 68 of 100, tot loss = 4.529249964391484, l1: 9.977055742638186e-05, l2: 0.0003531544377491467   Iteration 69 of 100, tot loss = 4.542262887609178, l1: 9.990562535096905e-05, l2: 0.00035432066152165845   Iteration 70 of 100, tot loss = 4.547706435407911, l1: 9.96341672102322e-05, l2: 0.00035513647422443943   Iteration 71 of 100, tot loss = 4.541432650995926, l1: 9.974097410007052e-05, l2: 0.00035440228856086707   Iteration 72 of 100, tot loss = 4.564245801832941, l1: 0.00010040046941463111, l2: 0.00035602410844148835   Iteration 73 of 100, tot loss = 4.5791811044902015, l1: 0.00010099600899282026, l2: 0.0003569220990693669   Iteration 74 of 100, tot loss = 4.573820296171549, l1: 0.00010080981834058498, l2: 0.0003565722093028583   Iteration 75 of 100, tot loss = 4.588548763593038, l1: 0.00010127514629857614, l2: 0.000357579727715347   Iteration 76 of 100, tot loss = 4.620878597623424, l1: 0.00010169402106073808, l2: 0.0003603938360978837   Iteration 77 of 100, tot loss = 4.628048762098535, l1: 0.00010190632331484397, l2: 0.0003608985502210857   Iteration 78 of 100, tot loss = 4.631820533520136, l1: 0.00010220913781011358, l2: 0.00036097291320822056   Iteration 79 of 100, tot loss = 4.634674580791328, l1: 0.00010227701190897394, l2: 0.0003611904437634563   Iteration 80 of 100, tot loss = 4.624497856199741, l1: 0.0001018762739477097, l2: 0.00036057350944247447   Iteration 81 of 100, tot loss = 4.601636543686007, l1: 0.00010154857258901667, l2: 0.00035861507965392653   Iteration 82 of 100, tot loss = 4.608732431400113, l1: 0.00010183523240479322, l2: 0.0003590380089540637   Iteration 83 of 100, tot loss = 4.58333515977285, l1: 0.00010110641820377289, l2: 0.0003572270958452957   Iteration 84 of 100, tot loss = 4.581257628543036, l1: 0.00010138899800776493, l2: 0.0003567367629806367   Iteration 85 of 100, tot loss = 4.583453157368829, l1: 0.00010142433632555528, l2: 0.00035692097727031284   Iteration 86 of 100, tot loss = 4.6215605971425076, l1: 0.00010211435949523273, l2: 0.00036004169796374194   Iteration 87 of 100, tot loss = 4.606375638095812, l1: 0.00010180901400657938, l2: 0.0003588285477632582   Iteration 88 of 100, tot loss = 4.633727782151916, l1: 0.00010233190927134605, l2: 0.00036104086716064063   Iteration 89 of 100, tot loss = 4.649852478102352, l1: 0.00010254735540267698, l2: 0.0003624378903500809   Iteration 90 of 100, tot loss = 4.64252271519767, l1: 0.00010237362168684033, l2: 0.00036187864793141164   Iteration 91 of 100, tot loss = 4.642281950175107, l1: 0.00010229410750920639, l2: 0.00036193408539894823   Iteration 92 of 100, tot loss = 4.641220179588898, l1: 0.00010211862919949571, l2: 0.00036200338684505806   Iteration 93 of 100, tot loss = 4.661656516854481, l1: 0.00010239541214065857, l2: 0.0003637702373120563   Iteration 94 of 100, tot loss = 4.636957583275247, l1: 0.00010181052439680144, l2: 0.00036188523169604445   Iteration 95 of 100, tot loss = 4.623552913414804, l1: 0.00010178508076167322, l2: 0.0003605702085463379   Iteration 96 of 100, tot loss = 4.609191004186869, l1: 0.00010128430104335469, l2: 0.0003596347974810972   Iteration 97 of 100, tot loss = 4.603283482728545, l1: 0.00010142464576422276, l2: 0.00035890370069405295   Iteration 98 of 100, tot loss = 4.581197379803171, l1: 0.000100843318863071, l2: 0.00035727641740350565   Iteration 99 of 100, tot loss = 4.554796621052906, l1: 0.00010043747754411235, l2: 0.00035504218285863353   Iteration 100 of 100, tot loss = 4.557529237270355, l1: 0.00010065052108984674, l2: 0.0003551024008629611
   End of epoch 1144; saving model... 

Epoch 1145 of 2000
   Iteration 1 of 100, tot loss = 5.431641101837158, l1: 0.00013425384531728923, l2: 0.00040891021490097046   Iteration 2 of 100, tot loss = 4.529736876487732, l1: 9.916077397065237e-05, l2: 0.0003538128803484142   Iteration 3 of 100, tot loss = 5.16832439104716, l1: 0.00011244306127385546, l2: 0.0004043893616956969   Iteration 4 of 100, tot loss = 5.415554583072662, l1: 0.00012166598025942221, l2: 0.00041988947486970574   Iteration 5 of 100, tot loss = 5.347463369369507, l1: 0.00011871827155118808, l2: 0.0004160280688665807   Iteration 6 of 100, tot loss = 5.224178433418274, l1: 0.00011976418682024814, l2: 0.0004026536626042798   Iteration 7 of 100, tot loss = 4.948703799928937, l1: 0.00011417659490169691, l2: 0.000380693789338693   Iteration 8 of 100, tot loss = 5.1561711728572845, l1: 0.0001158749309979612, l2: 0.0003997421918029431   Iteration 9 of 100, tot loss = 5.305675321155125, l1: 0.0001181663214487748, l2: 0.0004124012161304967   Iteration 10 of 100, tot loss = 5.091229033470154, l1: 0.00011315787778585218, l2: 0.00039596502901986244   Iteration 11 of 100, tot loss = 4.808991941538724, l1: 0.00010849535663675687, l2: 0.00037240384104238314   Iteration 12 of 100, tot loss = 4.893166571855545, l1: 0.0001088742592401104, l2: 0.00038044239772716537   Iteration 13 of 100, tot loss = 4.873508590918321, l1: 0.00010732815504665129, l2: 0.00038002270202224073   Iteration 14 of 100, tot loss = 4.878575146198273, l1: 0.00010676118888243633, l2: 0.0003810963264965851   Iteration 15 of 100, tot loss = 4.775529885292054, l1: 0.00010527443325069423, l2: 0.0003722785564605147   Iteration 16 of 100, tot loss = 4.677334167063236, l1: 0.0001042165345097601, l2: 0.00036351688322611153   Iteration 17 of 100, tot loss = 4.707922353464014, l1: 0.00010383417590192574, l2: 0.0003669580619316548   Iteration 18 of 100, tot loss = 4.807368470562829, l1: 0.00010503385096348615, l2: 0.00037570299819991406   Iteration 19 of 100, tot loss = 4.818364927643223, l1: 0.00010500276632803051, l2: 0.0003768337283920693   Iteration 20 of 100, tot loss = 4.788239675760269, l1: 0.00010493705958651845, l2: 0.00037388691125670446   Iteration 21 of 100, tot loss = 4.8352730217434114, l1: 0.00010441879324692611, l2: 0.0003791085106231982   Iteration 22 of 100, tot loss = 4.8903087323362175, l1: 0.00010467412986062382, l2: 0.00038435674716972494   Iteration 23 of 100, tot loss = 4.859569679135862, l1: 0.00010420195695073546, l2: 0.00038175501496247625   Iteration 24 of 100, tot loss = 4.901871666312218, l1: 0.00010505570874859889, l2: 0.00038513146015854244   Iteration 25 of 100, tot loss = 4.903141808509827, l1: 0.00010491721623111517, l2: 0.0003853969683405012   Iteration 26 of 100, tot loss = 4.8983560021107015, l1: 0.00010600062299860068, l2: 0.0003838349803118035   Iteration 27 of 100, tot loss = 4.855775440180743, l1: 0.0001050805671925277, l2: 0.00038049697953586775   Iteration 28 of 100, tot loss = 4.880541865314756, l1: 0.00010604861329608996, l2: 0.00038200557693406675   Iteration 29 of 100, tot loss = 4.899789674528714, l1: 0.00010675660149868706, l2: 0.0003832223692684081   Iteration 30 of 100, tot loss = 4.896487502257029, l1: 0.00010685649055327909, l2: 0.0003827922618559872   Iteration 31 of 100, tot loss = 4.880792152497076, l1: 0.00010672648486140514, l2: 0.0003813527329193969   Iteration 32 of 100, tot loss = 4.944775190204382, l1: 0.00010665520539987483, l2: 0.00038782231604272965   Iteration 33 of 100, tot loss = 4.979678179278518, l1: 0.00010713947951003458, l2: 0.00039082834105515343   Iteration 34 of 100, tot loss = 4.966854400494519, l1: 0.00010693245584501282, l2: 0.0003897529874366763   Iteration 35 of 100, tot loss = 5.019335770606995, l1: 0.00010813407633187515, l2: 0.0003937995052962963   Iteration 36 of 100, tot loss = 5.131954083840053, l1: 0.00010907032831002855, l2: 0.0004041250835547948   Iteration 37 of 100, tot loss = 5.102552681355863, l1: 0.00010784119021499882, l2: 0.000402414081051845   Iteration 38 of 100, tot loss = 5.023449486807773, l1: 0.00010665110695347386, l2: 0.0003956938449654875   Iteration 39 of 100, tot loss = 4.958948217905485, l1: 0.00010561783636102262, l2: 0.000390276988559307   Iteration 40 of 100, tot loss = 4.981450709700584, l1: 0.00010683674245228758, l2: 0.00039130833210947457   Iteration 41 of 100, tot loss = 4.98458653833808, l1: 0.00010734205769528293, l2: 0.0003911165996857851   Iteration 42 of 100, tot loss = 4.9777294879867915, l1: 0.00010736070925347685, l2: 0.0003904122426146863   Iteration 43 of 100, tot loss = 4.903222125630046, l1: 0.0001058519524650724, l2: 0.0003844702631224293   Iteration 44 of 100, tot loss = 4.865826533599333, l1: 0.00010527384628486206, l2: 0.00038130881016100335   Iteration 45 of 100, tot loss = 4.82949746714698, l1: 0.00010497893874546409, l2: 0.0003779708110313449   Iteration 46 of 100, tot loss = 4.813863663569741, l1: 0.00010478680546246166, l2: 0.0003765995643334463   Iteration 47 of 100, tot loss = 4.7725320800821835, l1: 0.00010427393050470509, l2: 0.00037297928034565707   Iteration 48 of 100, tot loss = 4.753385546306769, l1: 0.00010381803948196951, l2: 0.00037152051784990664   Iteration 49 of 100, tot loss = 4.729452164805665, l1: 0.00010339627750704958, l2: 0.00036954894134470697   Iteration 50 of 100, tot loss = 4.706572434902191, l1: 0.00010304657414963004, l2: 0.0003676106716739014   Iteration 51 of 100, tot loss = 4.70942773772221, l1: 0.00010329604771743784, l2: 0.0003676467281811889   Iteration 52 of 100, tot loss = 4.713278924043362, l1: 0.00010339995599049592, l2: 0.0003679279381266007   Iteration 53 of 100, tot loss = 4.755013463632116, l1: 0.00010401614616846859, l2: 0.00037148520174734996   Iteration 54 of 100, tot loss = 4.784344503173122, l1: 0.00010423036164192363, l2: 0.0003742040900289322   Iteration 55 of 100, tot loss = 4.795153147524053, l1: 0.00010443090160895902, l2: 0.00037508441442200407   Iteration 56 of 100, tot loss = 4.777499920555523, l1: 0.0001039971933615951, l2: 0.0003737527996625951   Iteration 57 of 100, tot loss = 4.789665479409067, l1: 0.00010415470859969214, l2: 0.00037481184086860403   Iteration 58 of 100, tot loss = 4.774528271165387, l1: 0.00010378009812547103, l2: 0.0003736727305985024   Iteration 59 of 100, tot loss = 4.77571457321361, l1: 0.00010389844834917003, l2: 0.0003736730106569574   Iteration 60 of 100, tot loss = 4.769591361284256, l1: 0.00010402084535598988, l2: 0.0003729382927607124   Iteration 61 of 100, tot loss = 4.782220455466724, l1: 0.00010460583161759252, l2: 0.00037361621536070205   Iteration 62 of 100, tot loss = 4.752131037173733, l1: 0.00010427686984551275, l2: 0.000370936235178247   Iteration 63 of 100, tot loss = 4.7215487294726906, l1: 0.00010380382428549984, l2: 0.00036835104979145974   Iteration 64 of 100, tot loss = 4.712985148653388, l1: 0.00010360640379758479, l2: 0.00036769211237697164   Iteration 65 of 100, tot loss = 4.756196342981779, l1: 0.00010449588942440012, l2: 0.0003711237464673244   Iteration 66 of 100, tot loss = 4.73766639918992, l1: 0.00010397317275805328, l2: 0.00036979346865943324   Iteration 67 of 100, tot loss = 4.753217827028303, l1: 0.0001042680644908913, l2: 0.0003710537197575696   Iteration 68 of 100, tot loss = 4.740582239978454, l1: 0.000104057739918062, l2: 0.0003700004856321303   Iteration 69 of 100, tot loss = 4.727230011553004, l1: 0.0001042345309384552, l2: 0.00036848847167161495   Iteration 70 of 100, tot loss = 4.751603060109275, l1: 0.00010470167996702782, l2: 0.00037045862741901404   Iteration 71 of 100, tot loss = 4.729441066862831, l1: 0.00010431012878412525, l2: 0.000368633979364154   Iteration 72 of 100, tot loss = 4.700275949305958, l1: 0.00010392574995421455, l2: 0.0003661018466421713   Iteration 73 of 100, tot loss = 4.713409774923978, l1: 0.00010423102817372164, l2: 0.00036710995123466505   Iteration 74 of 100, tot loss = 4.740729233703098, l1: 0.0001048730931428066, l2: 0.00036919983206128046   Iteration 75 of 100, tot loss = 4.749700493812561, l1: 0.00010513076194911264, l2: 0.0003698392891480277   Iteration 76 of 100, tot loss = 4.749075536665163, l1: 0.00010526128756426165, l2: 0.00036964626786740204   Iteration 77 of 100, tot loss = 4.7583680508972765, l1: 0.00010556937190636012, l2: 0.00037026743505863693   Iteration 78 of 100, tot loss = 4.76713797220817, l1: 0.00010550921615504194, l2: 0.0003712045831772952   Iteration 79 of 100, tot loss = 4.757349981537348, l1: 0.00010524064616876348, l2: 0.0003704943537971453   Iteration 80 of 100, tot loss = 4.746777661144733, l1: 0.00010502346754037717, l2: 0.0003696543004480191   Iteration 81 of 100, tot loss = 4.750449841405138, l1: 0.00010502681479808, l2: 0.00037001817124999233   Iteration 82 of 100, tot loss = 4.717169882320777, l1: 0.00010454837138240691, l2: 0.0003671686187395031   Iteration 83 of 100, tot loss = 4.7343757971223575, l1: 0.00010469031043481138, l2: 0.0003687472716296451   Iteration 84 of 100, tot loss = 4.708377621003559, l1: 0.00010421385130549814, l2: 0.00036662391305851216   Iteration 85 of 100, tot loss = 4.696736101543202, l1: 0.00010402758639162022, l2: 0.0003656460258164717   Iteration 86 of 100, tot loss = 4.700729560020358, l1: 0.00010414408778922015, l2: 0.00036592887045117095   Iteration 87 of 100, tot loss = 4.6780055596910675, l1: 0.00010374424703607881, l2: 0.0003640563113497163   Iteration 88 of 100, tot loss = 4.718864648179575, l1: 0.00010447591082331861, l2: 0.00036741055572003296   Iteration 89 of 100, tot loss = 4.719682898414269, l1: 0.0001044989196211492, l2: 0.00036746937208651995   Iteration 90 of 100, tot loss = 4.7211640742090015, l1: 0.0001046780806872347, l2: 0.00036743832849121344   Iteration 91 of 100, tot loss = 4.719382827098553, l1: 0.0001046558361769792, l2: 0.00036728244810876854   Iteration 92 of 100, tot loss = 4.758428113616032, l1: 0.00010540932892225733, l2: 0.0003704334841408443   Iteration 93 of 100, tot loss = 4.735530041879223, l1: 0.00010508298392749331, l2: 0.00036847002188404723   Iteration 94 of 100, tot loss = 4.7612073560978505, l1: 0.00010555097419165442, l2: 0.0003705697630759963   Iteration 95 of 100, tot loss = 4.794234625916732, l1: 0.00010594530080867882, l2: 0.0003734781639650464   Iteration 96 of 100, tot loss = 4.791566862414281, l1: 0.00010610909881355231, l2: 0.0003730475897706735   Iteration 97 of 100, tot loss = 4.781346582874809, l1: 0.00010590846447025656, l2: 0.0003722261960891834   Iteration 98 of 100, tot loss = 4.764910319629981, l1: 0.0001057725129408491, l2: 0.0003707185212394451   Iteration 99 of 100, tot loss = 4.7703049652504195, l1: 0.00010597361603750602, l2: 0.00037105688284329996   Iteration 100 of 100, tot loss = 4.791239672899247, l1: 0.00010610081302729668, l2: 0.0003730231564259157
   End of epoch 1145; saving model... 

Epoch 1146 of 2000
   Iteration 1 of 100, tot loss = 6.254512310028076, l1: 0.00012090384552720934, l2: 0.0005045473808422685   Iteration 2 of 100, tot loss = 4.980402112007141, l1: 9.825249799177982e-05, l2: 0.00039978770655579865   Iteration 3 of 100, tot loss = 4.87311593691508, l1: 9.923699690261856e-05, l2: 0.0003880745983527352   Iteration 4 of 100, tot loss = 5.024875223636627, l1: 0.00010491215289221145, l2: 0.0003975753716076724   Iteration 5 of 100, tot loss = 4.792896890640259, l1: 0.00010630144970491529, l2: 0.0003729882359039038   Iteration 6 of 100, tot loss = 4.959962407747905, l1: 0.00011138262925669551, l2: 0.0003846135999386509   Iteration 7 of 100, tot loss = 4.586066654750279, l1: 0.00010518789037762742, l2: 0.00035341876667059423   Iteration 8 of 100, tot loss = 4.489066392183304, l1: 0.00010583338462311076, l2: 0.0003430732485867338   Iteration 9 of 100, tot loss = 4.635493410958184, l1: 0.00011063808778999373, l2: 0.00035291124853150296   Iteration 10 of 100, tot loss = 4.507789969444275, l1: 0.00010645864240359515, l2: 0.00034432035026839004   Iteration 11 of 100, tot loss = 4.5861216675151475, l1: 0.00010804766762620685, l2: 0.000350564494676126   Iteration 12 of 100, tot loss = 4.445078551769257, l1: 0.00010604364373042093, l2: 0.00033846420774352737   Iteration 13 of 100, tot loss = 4.38764999463008, l1: 0.0001038460843399382, l2: 0.0003349189120434368   Iteration 14 of 100, tot loss = 4.395720532962254, l1: 0.00010420379424301376, l2: 0.00033536825607630556   Iteration 15 of 100, tot loss = 4.317530870437622, l1: 0.00010318340646335855, l2: 0.0003285696778524046   Iteration 16 of 100, tot loss = 4.5155226439237595, l1: 0.00010577414695944753, l2: 0.00034577811402414227   Iteration 17 of 100, tot loss = 4.548185755224789, l1: 0.00010729760833783075, l2: 0.00034752096347373854   Iteration 18 of 100, tot loss = 4.674683504634434, l1: 0.00010776256426146979, l2: 0.0003597057810919877   Iteration 19 of 100, tot loss = 4.604024661214728, l1: 0.00010641552025366476, l2: 0.0003539869404018023   Iteration 20 of 100, tot loss = 4.597493290901184, l1: 0.00010476439638296142, l2: 0.00035498492725309915   Iteration 21 of 100, tot loss = 4.705684253147671, l1: 0.00010617841395460779, l2: 0.0003643900057622453   Iteration 22 of 100, tot loss = 4.653274167667735, l1: 0.00010556092919697139, l2: 0.0003597664822750217   Iteration 23 of 100, tot loss = 4.714341744132664, l1: 0.00010706213399079506, l2: 0.0003643720341168101   Iteration 24 of 100, tot loss = 4.739045838514964, l1: 0.00010707534117197308, l2: 0.00036682923685778707   Iteration 25 of 100, tot loss = 4.739304504394531, l1: 0.00010726463719038293, l2: 0.00036666580650489776   Iteration 26 of 100, tot loss = 4.775137497828557, l1: 0.0001064950036099897, l2: 0.0003710187397350987   Iteration 27 of 100, tot loss = 4.768634637196858, l1: 0.00010716601833269965, l2: 0.00036969743895396177   Iteration 28 of 100, tot loss = 4.739161763872419, l1: 0.00010655522653126224, l2: 0.0003673609436581111   Iteration 29 of 100, tot loss = 4.737455384484653, l1: 0.00010588785316313392, l2: 0.00036785767889372875   Iteration 30 of 100, tot loss = 4.722313515345255, l1: 0.00010592795151751489, l2: 0.00036630339309340343   Iteration 31 of 100, tot loss = 4.714957560262373, l1: 0.00010510643750744601, l2: 0.00036638931065211976   Iteration 32 of 100, tot loss = 4.708944633603096, l1: 0.0001049212855832593, l2: 0.00036597317057385226   Iteration 33 of 100, tot loss = 4.72365817156705, l1: 0.00010564229821532287, l2: 0.0003667235110752341   Iteration 34 of 100, tot loss = 4.759967313093298, l1: 0.00010604973474127131, l2: 0.0003699469880837783   Iteration 35 of 100, tot loss = 4.7391418320792065, l1: 0.00010604670366904299, l2: 0.0003678674714007814   Iteration 36 of 100, tot loss = 4.813855595058865, l1: 0.00010631978087379441, l2: 0.00037506577023628377   Iteration 37 of 100, tot loss = 4.754603547018927, l1: 0.00010496795427595341, l2: 0.00037049239255940994   Iteration 38 of 100, tot loss = 4.73734566412474, l1: 0.00010491145344291765, l2: 0.0003688231050833373   Iteration 39 of 100, tot loss = 4.742863844602536, l1: 0.00010550574836834597, l2: 0.00036878062737508654   Iteration 40 of 100, tot loss = 4.715488564968109, l1: 0.00010496726317796857, l2: 0.00036658158496720715   Iteration 41 of 100, tot loss = 4.691263065105531, l1: 0.00010460576382550842, l2: 0.0003645205344982082   Iteration 42 of 100, tot loss = 4.681352371261234, l1: 0.0001043734659365977, l2: 0.00036376176354852285   Iteration 43 of 100, tot loss = 4.669134178826975, l1: 0.00010422566316095939, l2: 0.00036268774758439596   Iteration 44 of 100, tot loss = 4.705197339708155, l1: 0.00010493324705087368, l2: 0.0003655864797606641   Iteration 45 of 100, tot loss = 4.693637450536092, l1: 0.00010451572112894307, l2: 0.00036484801676124335   Iteration 46 of 100, tot loss = 4.692514455836752, l1: 0.00010471212305634728, l2: 0.00036453931470927984   Iteration 47 of 100, tot loss = 4.650201543848565, l1: 0.00010412897725163543, l2: 0.0003608911693903637   Iteration 48 of 100, tot loss = 4.690835078557332, l1: 0.00010442198329959258, l2: 0.0003646615171722563   Iteration 49 of 100, tot loss = 4.67502837278405, l1: 0.00010389863048342759, l2: 0.00036360419976610536   Iteration 50 of 100, tot loss = 4.683695640563965, l1: 0.00010387289497884922, l2: 0.0003644966622232459   Iteration 51 of 100, tot loss = 4.69515864054362, l1: 0.00010445397635396825, l2: 0.00036506188089511924   Iteration 52 of 100, tot loss = 4.716998604627756, l1: 0.00010461913678647342, l2: 0.0003670807163871359   Iteration 53 of 100, tot loss = 4.711001819034792, l1: 0.0001045649777603652, l2: 0.0003665351972779048   Iteration 54 of 100, tot loss = 4.739176820825647, l1: 0.00010511692771278494, l2: 0.0003688007475122706   Iteration 55 of 100, tot loss = 4.798901263150301, l1: 0.00010611857009776445, l2: 0.0003737715488320894   Iteration 56 of 100, tot loss = 4.801604492323739, l1: 0.00010600515608010548, l2: 0.00037415528562892826   Iteration 57 of 100, tot loss = 4.786358063681083, l1: 0.00010545764502909917, l2: 0.0003731781542770924   Iteration 58 of 100, tot loss = 4.76811475178291, l1: 0.00010530508445989308, l2: 0.0003715063842186094   Iteration 59 of 100, tot loss = 4.767203844199746, l1: 0.00010561452617659628, l2: 0.00037110585179692926   Iteration 60 of 100, tot loss = 4.74126607577006, l1: 0.00010542801658933362, l2: 0.0003686985844979063   Iteration 61 of 100, tot loss = 4.7611333424927755, l1: 0.00010583496420667126, l2: 0.0003702783638412957   Iteration 62 of 100, tot loss = 4.728943705558777, l1: 0.00010525630686090388, l2: 0.00036763805767423626   Iteration 63 of 100, tot loss = 4.741032354415409, l1: 0.00010527938046300459, l2: 0.0003688238486776956   Iteration 64 of 100, tot loss = 4.732163440436125, l1: 0.00010528555026212416, l2: 0.0003679307872062054   Iteration 65 of 100, tot loss = 4.727349284979013, l1: 0.00010544349934207275, l2: 0.0003672914228025967   Iteration 66 of 100, tot loss = 4.753957983219262, l1: 0.00010581010616133977, l2: 0.0003695856860510751   Iteration 67 of 100, tot loss = 4.743605873477993, l1: 0.00010565120855836309, l2: 0.00036870937254940116   Iteration 68 of 100, tot loss = 4.751356920775245, l1: 0.00010563792196027113, l2: 0.00036949776382671725   Iteration 69 of 100, tot loss = 4.782041580780692, l1: 0.00010643116589905559, l2: 0.00037177298551229626   Iteration 70 of 100, tot loss = 4.775128763062614, l1: 0.00010607882483912232, l2: 0.00037143404499927   Iteration 71 of 100, tot loss = 4.738756495462337, l1: 0.00010548194753117597, l2: 0.000368393695475468   Iteration 72 of 100, tot loss = 4.736964974138472, l1: 0.00010544560326606088, l2: 0.000368250887428682   Iteration 73 of 100, tot loss = 4.724362824061146, l1: 0.00010560541744630988, l2: 0.0003668308584577697   Iteration 74 of 100, tot loss = 4.741895527453036, l1: 0.00010578353986430384, l2: 0.0003684060060107653   Iteration 75 of 100, tot loss = 4.7631039174397785, l1: 0.00010604737413814291, l2: 0.0003702630112335707   Iteration 76 of 100, tot loss = 4.732105650399861, l1: 0.00010563747309788596, l2: 0.00036757308560864707   Iteration 77 of 100, tot loss = 4.715039092224914, l1: 0.00010521226133939557, l2: 0.0003662916416659129   Iteration 78 of 100, tot loss = 4.706146038495577, l1: 0.00010531932760689718, l2: 0.000365295270174288   Iteration 79 of 100, tot loss = 4.706792632235756, l1: 0.0001051207984830076, l2: 0.0003655584587249905   Iteration 80 of 100, tot loss = 4.704826527833939, l1: 0.00010503655685170087, l2: 0.00036544608992699067   Iteration 81 of 100, tot loss = 4.6907933906272605, l1: 0.00010501318589646232, l2: 0.00036406614724259227   Iteration 82 of 100, tot loss = 4.661315196897926, l1: 0.00010442771067240888, l2: 0.00036170380314238507   Iteration 83 of 100, tot loss = 4.65822042901832, l1: 0.00010452137799605345, l2: 0.000361300658971823   Iteration 84 of 100, tot loss = 4.661811675344195, l1: 0.00010463723022742973, l2: 0.00036154393196089325   Iteration 85 of 100, tot loss = 4.697020917780259, l1: 0.00010522420197958126, l2: 0.00036447788468863375   Iteration 86 of 100, tot loss = 4.701125510903292, l1: 0.0001053872894222285, l2: 0.0003647252561613852   Iteration 87 of 100, tot loss = 4.711063845404263, l1: 0.00010569902751113063, l2: 0.0003654073518045225   Iteration 88 of 100, tot loss = 4.6906568733128635, l1: 0.00010552875615262151, l2: 0.00036353692617426117   Iteration 89 of 100, tot loss = 4.6982749606786145, l1: 0.00010575805887190122, l2: 0.0003640694322006965   Iteration 90 of 100, tot loss = 4.679127648141649, l1: 0.00010542061305992926, l2: 0.00036249214673363085   Iteration 91 of 100, tot loss = 4.661662513083154, l1: 0.00010526555674764646, l2: 0.0003609006895343415   Iteration 92 of 100, tot loss = 4.6548144169475725, l1: 0.00010515504626433223, l2: 0.00036032639052238034   Iteration 93 of 100, tot loss = 4.642782711213635, l1: 0.00010459226690218985, l2: 0.0003596859993801142   Iteration 94 of 100, tot loss = 4.628701149149144, l1: 0.00010454513868991969, l2: 0.0003583249713398239   Iteration 95 of 100, tot loss = 4.660846574682939, l1: 0.00010482264030877942, l2: 0.0003612620121872935   Iteration 96 of 100, tot loss = 4.659551123778026, l1: 0.000104838644612452, l2: 0.00036111646295466926   Iteration 97 of 100, tot loss = 4.650580398815194, l1: 0.00010485909488406177, l2: 0.0003601989403400647   Iteration 98 of 100, tot loss = 4.672393791529597, l1: 0.00010511958379891216, l2: 0.000362119790373313   Iteration 99 of 100, tot loss = 4.673549668957489, l1: 0.00010507864818932968, l2: 0.00036227631393227387   Iteration 100 of 100, tot loss = 4.6455977296829225, l1: 0.000104663914644334, l2: 0.0003598958536167629
   End of epoch 1146; saving model... 

Epoch 1147 of 2000
   Iteration 1 of 100, tot loss = 3.5581352710723877, l1: 8.350635471288115e-05, l2: 0.00027230719570070505   Iteration 2 of 100, tot loss = 3.3854715824127197, l1: 6.597736319235992e-05, l2: 0.0002725698141148314   Iteration 3 of 100, tot loss = 3.4579381942749023, l1: 7.093681900490385e-05, l2: 0.00027485701139084995   Iteration 4 of 100, tot loss = 3.5060386061668396, l1: 7.661633117095334e-05, l2: 0.00027398753445595503   Iteration 5 of 100, tot loss = 4.250012826919556, l1: 8.687120498507284e-05, l2: 0.0003381300834007561   Iteration 6 of 100, tot loss = 4.3110694487889605, l1: 8.977192859068357e-05, l2: 0.000341335020493716   Iteration 7 of 100, tot loss = 4.179171766553607, l1: 8.487578036562939e-05, l2: 0.0003330414017130222   Iteration 8 of 100, tot loss = 4.374546051025391, l1: 8.652073302073404e-05, l2: 0.000350933874869952   Iteration 9 of 100, tot loss = 4.437939643859863, l1: 8.950759931596824e-05, l2: 0.0003542863673323558   Iteration 10 of 100, tot loss = 4.247567582130432, l1: 8.855876076268032e-05, l2: 0.00033619799942243846   Iteration 11 of 100, tot loss = 4.5869551355188545, l1: 9.253293154126203e-05, l2: 0.0003661625854544003   Iteration 12 of 100, tot loss = 4.641174574693044, l1: 9.656127925457743e-05, l2: 0.00036755617717669037   Iteration 13 of 100, tot loss = 4.521371603012085, l1: 9.580190929297644e-05, l2: 0.0003563352488876822   Iteration 14 of 100, tot loss = 4.440538014684405, l1: 9.565658027505768e-05, l2: 0.00034839721878857484   Iteration 15 of 100, tot loss = 4.480356582005819, l1: 9.697766799945384e-05, l2: 0.00035105798742733895   Iteration 16 of 100, tot loss = 4.609580382704735, l1: 9.89377394944313e-05, l2: 0.00036202029696141835   Iteration 17 of 100, tot loss = 4.686319281073177, l1: 0.00010038848267868161, l2: 0.00036824344458770663   Iteration 18 of 100, tot loss = 4.6716274287965565, l1: 0.00010075704024833006, l2: 0.00036640570033341646   Iteration 19 of 100, tot loss = 4.780198661904586, l1: 0.00010198786185355857, l2: 0.00037603200057913596   Iteration 20 of 100, tot loss = 4.798841106891632, l1: 0.00010279981979692821, l2: 0.0003770842871745117   Iteration 21 of 100, tot loss = 4.814966440200806, l1: 0.00010290009993782622, l2: 0.0003785965423698404   Iteration 22 of 100, tot loss = 4.830347613854841, l1: 0.00010334497148340398, l2: 0.00037968978920782155   Iteration 23 of 100, tot loss = 4.8157007694244385, l1: 0.00010302356721418302, l2: 0.0003785465087275952   Iteration 24 of 100, tot loss = 4.863875975211461, l1: 0.0001044552570116745, l2: 0.000381932340436227   Iteration 25 of 100, tot loss = 4.971672163009644, l1: 0.00010683584987418726, l2: 0.00039033136679790916   Iteration 26 of 100, tot loss = 5.014643421539893, l1: 0.00010684023884375795, l2: 0.0003946241038367869   Iteration 27 of 100, tot loss = 4.990601036283705, l1: 0.00010685923377793558, l2: 0.0003922008704480336   Iteration 28 of 100, tot loss = 4.970019536358969, l1: 0.00010739075176908435, l2: 0.0003896112019512137   Iteration 29 of 100, tot loss = 4.9040886533671415, l1: 0.0001064048952559523, l2: 0.00038400396969618983   Iteration 30 of 100, tot loss = 4.945729597409566, l1: 0.0001079025188422141, l2: 0.0003866704403966044   Iteration 31 of 100, tot loss = 4.968110707498366, l1: 0.00010851327885663317, l2: 0.0003882977921861194   Iteration 32 of 100, tot loss = 5.070227362215519, l1: 0.0001097857116292289, l2: 0.00039723702593619237   Iteration 33 of 100, tot loss = 5.061870654424031, l1: 0.00010934755229038384, l2: 0.00039683951410662496   Iteration 34 of 100, tot loss = 5.001958475393407, l1: 0.00010774545765290146, l2: 0.0003924503912190523   Iteration 35 of 100, tot loss = 5.013419893809727, l1: 0.00010761391749838367, l2: 0.0003937280728548233   Iteration 36 of 100, tot loss = 4.966219431824154, l1: 0.00010644463004104586, l2: 0.0003901773145318859   Iteration 37 of 100, tot loss = 4.955598360783345, l1: 0.00010639074969583669, l2: 0.000389169087488758   Iteration 38 of 100, tot loss = 4.962884131230806, l1: 0.00010711624705244934, l2: 0.00038917216711285477   Iteration 39 of 100, tot loss = 4.893469614860339, l1: 0.0001059867331955152, l2: 0.0003833602293012425   Iteration 40 of 100, tot loss = 4.9185715675354, l1: 0.00010615682040224784, l2: 0.0003857003372104373   Iteration 41 of 100, tot loss = 4.8901644625314855, l1: 0.00010583744218953454, l2: 0.0003831790044631173   Iteration 42 of 100, tot loss = 4.88495257354918, l1: 0.00010581732411602778, l2: 0.000382677933278804   Iteration 43 of 100, tot loss = 4.946066252020902, l1: 0.00010680560317691849, l2: 0.000387801022572053   Iteration 44 of 100, tot loss = 5.057557555762204, l1: 0.00010863304718937302, l2: 0.00039712270981081844   Iteration 45 of 100, tot loss = 5.071984773212009, l1: 0.00010868982563907694, l2: 0.0003985086520616379   Iteration 46 of 100, tot loss = 5.04552746337393, l1: 0.00010815546635928078, l2: 0.0003963972798690362   Iteration 47 of 100, tot loss = 5.050166368484497, l1: 0.00010807585389789273, l2: 0.0003969407828643601   Iteration 48 of 100, tot loss = 5.043614303072293, l1: 0.00010825421334933101, l2: 0.00039610721614735667   Iteration 49 of 100, tot loss = 5.030327928309538, l1: 0.00010842363954740291, l2: 0.00039460915212082313   Iteration 50 of 100, tot loss = 5.060577673912048, l1: 0.0001091128762345761, l2: 0.00039694488979876043   Iteration 51 of 100, tot loss = 5.030776968189314, l1: 0.00010821008389957213, l2: 0.00039486761156962635   Iteration 52 of 100, tot loss = 5.013906267973093, l1: 0.00010745384329661298, l2: 0.0003939367827395192   Iteration 53 of 100, tot loss = 4.991872715500166, l1: 0.00010700857778571827, l2: 0.0003921786929428015   Iteration 54 of 100, tot loss = 4.978342003292507, l1: 0.00010693139510867359, l2: 0.00039090280457727475   Iteration 55 of 100, tot loss = 4.961770456487482, l1: 0.00010703953475141051, l2: 0.0003891375106336041   Iteration 56 of 100, tot loss = 4.991692806993212, l1: 0.0001077303155138257, l2: 0.0003914389651202198   Iteration 57 of 100, tot loss = 4.9690063459831375, l1: 0.00010678922307418186, l2: 0.00039011141154588315   Iteration 58 of 100, tot loss = 4.993674639997812, l1: 0.0001074174185838462, l2: 0.0003919500460173806   Iteration 59 of 100, tot loss = 4.999594914711128, l1: 0.00010771582686683652, l2: 0.0003922436646801435   Iteration 60 of 100, tot loss = 4.981275832653045, l1: 0.00010747389612030626, l2: 0.00039065368716061734   Iteration 61 of 100, tot loss = 4.951181650161743, l1: 0.00010703689091672861, l2: 0.0003880812740239666   Iteration 62 of 100, tot loss = 4.919142938429309, l1: 0.00010644982203009403, l2: 0.00038546447162403753   Iteration 63 of 100, tot loss = 4.899454241707211, l1: 0.00010645337349001599, l2: 0.0003834920503394974   Iteration 64 of 100, tot loss = 4.912509087473154, l1: 0.00010628133765067105, l2: 0.0003849695706321654   Iteration 65 of 100, tot loss = 4.900739475396963, l1: 0.00010641139406988468, l2: 0.00038366255265338204   Iteration 66 of 100, tot loss = 4.87862042947249, l1: 0.00010592306898615789, l2: 0.000381938973133043   Iteration 67 of 100, tot loss = 4.890571245506628, l1: 0.00010578971135268447, l2: 0.0003832674121793443   Iteration 68 of 100, tot loss = 4.85274338722229, l1: 0.00010509652367866639, l2: 0.0003801778139484882   Iteration 69 of 100, tot loss = 4.864844709202863, l1: 0.00010538343912744692, l2: 0.0003811010308023812   Iteration 70 of 100, tot loss = 4.923482309068953, l1: 0.00010654646149695119, l2: 0.00038580176769755783   Iteration 71 of 100, tot loss = 4.892734930548869, l1: 0.00010606896815350948, l2: 0.00038320452329853525   Iteration 72 of 100, tot loss = 4.880377921793196, l1: 0.00010578874202514676, l2: 0.000382249048925587   Iteration 73 of 100, tot loss = 4.903766011538571, l1: 0.00010656700077847807, l2: 0.000383809598980867   Iteration 74 of 100, tot loss = 4.934426919834034, l1: 0.00010696015350412061, l2: 0.0003864825374802348   Iteration 75 of 100, tot loss = 4.912181797027588, l1: 0.0001066033010041186, l2: 0.00038461487759680797   Iteration 76 of 100, tot loss = 4.893441407304061, l1: 0.0001063250138988798, l2: 0.00038301912584421717   Iteration 77 of 100, tot loss = 4.950009215961803, l1: 0.00010734398826819167, l2: 0.0003876569319906487   Iteration 78 of 100, tot loss = 4.938836433948615, l1: 0.00010707133104659032, l2: 0.00038681231093598914   Iteration 79 of 100, tot loss = 4.924910819983181, l1: 0.00010686725685837137, l2: 0.00038562382367303996   Iteration 80 of 100, tot loss = 4.910824647545814, l1: 0.00010630826332089783, l2: 0.0003847742000289145   Iteration 81 of 100, tot loss = 4.910976860258314, l1: 0.00010624660879127785, l2: 0.00038485107623632814   Iteration 82 of 100, tot loss = 4.936102439717549, l1: 0.00010675326772864692, l2: 0.0003868569753050418   Iteration 83 of 100, tot loss = 4.943968965346555, l1: 0.00010667228532137338, l2: 0.00038772461047334347   Iteration 84 of 100, tot loss = 4.935476391088395, l1: 0.00010609147617894148, l2: 0.00038745616222426874   Iteration 85 of 100, tot loss = 4.957700934129603, l1: 0.00010649552264915067, l2: 0.00038927457011167835   Iteration 86 of 100, tot loss = 4.961102654767591, l1: 0.00010680327677407551, l2: 0.0003893069879268296   Iteration 87 of 100, tot loss = 4.976389608163943, l1: 0.0001069352443643776, l2: 0.00039070371612665597   Iteration 88 of 100, tot loss = 4.96151804653081, l1: 0.00010637619776720054, l2: 0.0003897756063948195   Iteration 89 of 100, tot loss = 4.9320881661404385, l1: 0.000105832281528666, l2: 0.0003873765345314894   Iteration 90 of 100, tot loss = 4.914737508032057, l1: 0.00010545884114334411, l2: 0.00038601490911484385   Iteration 91 of 100, tot loss = 4.917650455956931, l1: 0.00010568610202056703, l2: 0.0003860789435278563   Iteration 92 of 100, tot loss = 4.920394920784494, l1: 0.0001058975823743031, l2: 0.0003861419095103528   Iteration 93 of 100, tot loss = 4.906279810013309, l1: 0.00010595608843579357, l2: 0.0003846718922510283   Iteration 94 of 100, tot loss = 4.918257256771656, l1: 0.00010640698851506037, l2: 0.00038541873723164477   Iteration 95 of 100, tot loss = 4.932491714075992, l1: 0.00010668289457833836, l2: 0.00038656627707256885   Iteration 96 of 100, tot loss = 4.942019070188205, l1: 0.00010685571877881254, l2: 0.00038734618813881144   Iteration 97 of 100, tot loss = 4.954401139131527, l1: 0.00010737521838986065, l2: 0.00038806489562572547   Iteration 98 of 100, tot loss = 4.958688589991356, l1: 0.00010750846020676605, l2: 0.00038836039890825027   Iteration 99 of 100, tot loss = 4.978420782570887, l1: 0.00010771238617099248, l2: 0.00039012969199290516   Iteration 100 of 100, tot loss = 4.9719240093231205, l1: 0.00010741073961980873, l2: 0.0003897816613607574
   End of epoch 1147; saving model... 

Epoch 1148 of 2000
   Iteration 1 of 100, tot loss = 5.000288963317871, l1: 0.00013346665946301073, l2: 0.0003665622207336128   Iteration 2 of 100, tot loss = 5.976954460144043, l1: 0.00014324502262752503, l2: 0.00045445040450431406   Iteration 3 of 100, tot loss = 5.484533627827962, l1: 0.00012719694495899603, l2: 0.0004212564090266824   Iteration 4 of 100, tot loss = 5.611068964004517, l1: 0.0001232112535944907, l2: 0.00043789563642349094   Iteration 5 of 100, tot loss = 5.289932155609131, l1: 0.00011553443764569237, l2: 0.0004134587768930942   Iteration 6 of 100, tot loss = 4.900813182195027, l1: 0.00010790324934835856, l2: 0.000382178067714752   Iteration 7 of 100, tot loss = 4.718348400933402, l1: 0.00010756691751469458, l2: 0.00036426792212296277   Iteration 8 of 100, tot loss = 4.613258630037308, l1: 0.00010649013984220801, l2: 0.00035483572173689026   Iteration 9 of 100, tot loss = 4.776385810640123, l1: 0.00010826699387204523, l2: 0.00036937158276689134   Iteration 10 of 100, tot loss = 4.770009684562683, l1: 0.00010982564199366607, l2: 0.00036717532348120587   Iteration 11 of 100, tot loss = 4.696374979886142, l1: 0.00010698972949864005, l2: 0.0003626477647974918   Iteration 12 of 100, tot loss = 4.897406578063965, l1: 0.00011134393146979467, l2: 0.00037839672465149005   Iteration 13 of 100, tot loss = 4.885733090914213, l1: 0.00010905826471011656, l2: 0.0003795150427882058   Iteration 14 of 100, tot loss = 4.846504654203143, l1: 0.00010902968954594274, l2: 0.0003756207744507784   Iteration 15 of 100, tot loss = 4.80796267191569, l1: 0.00010936310524508977, l2: 0.00037143315906481195   Iteration 16 of 100, tot loss = 4.844295144081116, l1: 0.00011006769500454539, l2: 0.00037436181355587905   Iteration 17 of 100, tot loss = 4.79471144956701, l1: 0.0001092743212425643, l2: 0.0003701968175044064   Iteration 18 of 100, tot loss = 4.696981297598945, l1: 0.00010712519401244613, l2: 0.00036257293039751757   Iteration 19 of 100, tot loss = 4.626184563887747, l1: 0.00010543732404537303, l2: 0.00035718112715250373   Iteration 20 of 100, tot loss = 4.665317416191101, l1: 0.00010564074837020599, l2: 0.00036089098794036547   Iteration 21 of 100, tot loss = 4.638499464307513, l1: 0.00010496338100416498, l2: 0.0003588865615061617   Iteration 22 of 100, tot loss = 4.617812871932983, l1: 0.00010540805934314531, l2: 0.00035637322302632543   Iteration 23 of 100, tot loss = 4.565057453901871, l1: 0.00010488466195175019, l2: 0.0003516210791017131   Iteration 24 of 100, tot loss = 4.589904099702835, l1: 0.00010320452535476458, l2: 0.00035578587934044964   Iteration 25 of 100, tot loss = 4.563086338043213, l1: 0.00010264519281918184, l2: 0.0003536634350894019   Iteration 26 of 100, tot loss = 4.525592959844149, l1: 0.00010204592875267666, l2: 0.00035051336061979574   Iteration 27 of 100, tot loss = 4.568426882779157, l1: 0.00010183008459905438, l2: 0.00035501259790854183   Iteration 28 of 100, tot loss = 4.623878078801291, l1: 0.00010335096236043942, l2: 0.00035903683965443633   Iteration 29 of 100, tot loss = 4.630044945355119, l1: 0.00010240310326355092, l2: 0.0003606013844295501   Iteration 30 of 100, tot loss = 4.660138726234436, l1: 0.0001031620192710155, l2: 0.0003628518466333238   Iteration 31 of 100, tot loss = 4.651867751152285, l1: 0.00010379683484526111, l2: 0.0003613899335182542   Iteration 32 of 100, tot loss = 4.6039276868104935, l1: 0.00010280364676873432, l2: 0.0003575891150831012   Iteration 33 of 100, tot loss = 4.56934759833596, l1: 0.00010217120253593859, l2: 0.0003547635507967436   Iteration 34 of 100, tot loss = 4.524163722991943, l1: 0.00010081832474765285, l2: 0.0003515980411868762   Iteration 35 of 100, tot loss = 4.484715516226633, l1: 0.00010066620047187566, l2: 0.0003478053449985704   Iteration 36 of 100, tot loss = 4.42446834511227, l1: 9.944123495289305e-05, l2: 0.00034300559329696826   Iteration 37 of 100, tot loss = 4.395332549069379, l1: 9.88106925482547e-05, l2: 0.0003407225563392244   Iteration 38 of 100, tot loss = 4.403692402337727, l1: 9.876253161086137e-05, l2: 0.00034160670321924905   Iteration 39 of 100, tot loss = 4.402864425610273, l1: 9.913202246445088e-05, l2: 0.0003411544140619345   Iteration 40 of 100, tot loss = 4.4372738063335415, l1: 0.00010007140472225728, l2: 0.000343655970209511   Iteration 41 of 100, tot loss = 4.394213816014732, l1: 9.892719851738615e-05, l2: 0.0003404941778238181   Iteration 42 of 100, tot loss = 4.352266788482666, l1: 9.812259183187659e-05, l2: 0.0003371040812843213   Iteration 43 of 100, tot loss = 4.412389222965684, l1: 9.889815504890109e-05, l2: 0.000342340761901226   Iteration 44 of 100, tot loss = 4.4386592019688, l1: 9.914230610196351e-05, l2: 0.0003447236089083493   Iteration 45 of 100, tot loss = 4.484984800550673, l1: 9.97927820814463e-05, l2: 0.00034870569264361015   Iteration 46 of 100, tot loss = 4.483557649280714, l1: 0.00010004247135087928, l2: 0.0003483132886290348   Iteration 47 of 100, tot loss = 4.4794144224613275, l1: 0.00010029487980608927, l2: 0.0003476465579761351   Iteration 48 of 100, tot loss = 4.440400277574857, l1: 9.943451694501466e-05, l2: 0.0003446055064462901   Iteration 49 of 100, tot loss = 4.415811728458015, l1: 9.921110897118282e-05, l2: 0.00034237005931980985   Iteration 50 of 100, tot loss = 4.411176171302795, l1: 9.848702196904924e-05, l2: 0.0003426305906032212   Iteration 51 of 100, tot loss = 4.432303442674525, l1: 9.852099052902001e-05, l2: 0.0003447093485566952   Iteration 52 of 100, tot loss = 4.449926756895506, l1: 9.882266196934954e-05, l2: 0.00034617000934444007   Iteration 53 of 100, tot loss = 4.491117265989196, l1: 9.916706609524312e-05, l2: 0.0003499446559218549   Iteration 54 of 100, tot loss = 4.495998325171294, l1: 9.907882654429327e-05, l2: 0.0003505210013288349   Iteration 55 of 100, tot loss = 4.533439189737494, l1: 9.970331851615231e-05, l2: 0.0003536405965199017   Iteration 56 of 100, tot loss = 4.545754964862551, l1: 9.968956114140124e-05, l2: 0.0003548859310415407   Iteration 57 of 100, tot loss = 4.529373177310877, l1: 9.991501154441371e-05, l2: 0.00035302230211081015   Iteration 58 of 100, tot loss = 4.559114974120567, l1: 0.00010076179293311474, l2: 0.0003551497003204061   Iteration 59 of 100, tot loss = 4.543287297426644, l1: 0.00010054895620530676, l2: 0.00035377976931513176   Iteration 60 of 100, tot loss = 4.5248574415842695, l1: 0.00010008699288543236, l2: 0.00035239874729692624   Iteration 61 of 100, tot loss = 4.496154292685087, l1: 9.985197044204784e-05, l2: 0.0003497634549648119   Iteration 62 of 100, tot loss = 4.54030639894547, l1: 0.00010074480903107718, l2: 0.0003532858273472577   Iteration 63 of 100, tot loss = 4.543328860449413, l1: 0.00010089730870002689, l2: 0.00035343557367071746   Iteration 64 of 100, tot loss = 4.5522220730781555, l1: 0.00010082592001481316, l2: 0.0003543962843650661   Iteration 65 of 100, tot loss = 4.554087704878587, l1: 0.00010089012072197734, l2: 0.0003545186468937363   Iteration 66 of 100, tot loss = 4.553710149996208, l1: 0.0001010282783020309, l2: 0.00035434273366153127   Iteration 67 of 100, tot loss = 4.542501072385418, l1: 0.00010064181192116047, l2: 0.00035360829214871266   Iteration 68 of 100, tot loss = 4.516586535117206, l1: 0.0001002883120557218, l2: 0.0003513703381673515   Iteration 69 of 100, tot loss = 4.50252033662105, l1: 0.00010006816291538796, l2: 0.000350183867419179   Iteration 70 of 100, tot loss = 4.532186933926174, l1: 0.00010052472950649514, l2: 0.0003526939605113252   Iteration 71 of 100, tot loss = 4.510322822651393, l1: 9.995442271990125e-05, l2: 0.00035107785607950474   Iteration 72 of 100, tot loss = 4.505630370643404, l1: 9.995065152502825e-05, l2: 0.00035061238223635074   Iteration 73 of 100, tot loss = 4.468179123042381, l1: 9.905457224940868e-05, l2: 0.00034776333682015793   Iteration 74 of 100, tot loss = 4.467817443448144, l1: 9.911681504367939e-05, l2: 0.0003476649260453603   Iteration 75 of 100, tot loss = 4.520568563143413, l1: 0.00010016845582867973, l2: 0.0003518883971264586   Iteration 76 of 100, tot loss = 4.520879590197613, l1: 0.00010028131734878498, l2: 0.00035180663824001767   Iteration 77 of 100, tot loss = 4.5147110966892985, l1: 0.00010025240275850096, l2: 0.0003512187038061463   Iteration 78 of 100, tot loss = 4.51379900253736, l1: 0.00010047559048987937, l2: 0.0003509043064043045   Iteration 79 of 100, tot loss = 4.525380317168899, l1: 0.0001003152331727103, l2: 0.0003522227953344324   Iteration 80 of 100, tot loss = 4.532198835909367, l1: 0.00010055890811599966, l2: 0.00035266097202111266   Iteration 81 of 100, tot loss = 4.53478083934313, l1: 0.00010052092177010096, l2: 0.0003529571589257821   Iteration 82 of 100, tot loss = 4.545325420251706, l1: 0.00010086461988088913, l2: 0.0003536679186054687   Iteration 83 of 100, tot loss = 4.548377256795584, l1: 0.0001011670672552834, l2: 0.0003536706550133762   Iteration 84 of 100, tot loss = 4.54512009876115, l1: 0.00010135063889936733, l2: 0.0003531613673701594   Iteration 85 of 100, tot loss = 4.545599462004269, l1: 0.00010131226783200605, l2: 0.00035324767488739727   Iteration 86 of 100, tot loss = 4.5498083039771675, l1: 0.00010149783977181488, l2: 0.00035348298724572887   Iteration 87 of 100, tot loss = 4.53007823982458, l1: 0.000101183897850101, l2: 0.0003518239227351037   Iteration 88 of 100, tot loss = 4.526137664914131, l1: 0.0001012617092535286, l2: 0.00035135205408468295   Iteration 89 of 100, tot loss = 4.507774973183535, l1: 0.00010098432960986746, l2: 0.00034979316474623937   Iteration 90 of 100, tot loss = 4.4980237258805165, l1: 0.0001006922525751482, l2: 0.0003491101170463177   Iteration 91 of 100, tot loss = 4.502098171265571, l1: 0.00010103648853875891, l2: 0.00034917332567811874   Iteration 92 of 100, tot loss = 4.490303289631139, l1: 0.0001009771284098742, l2: 0.00034805319766114144   Iteration 93 of 100, tot loss = 4.527362065930521, l1: 0.00010170951950825721, l2: 0.0003510266838167163   Iteration 94 of 100, tot loss = 4.520642525338112, l1: 0.00010174836151758853, l2: 0.0003503158877720125   Iteration 95 of 100, tot loss = 4.537673424419604, l1: 0.00010190838407939545, l2: 0.0003518589556302973   Iteration 96 of 100, tot loss = 4.522532389809688, l1: 0.00010163662026722402, l2: 0.000350616615984715   Iteration 97 of 100, tot loss = 4.528801383431425, l1: 0.00010200598007566103, l2: 0.00035087415512252755   Iteration 98 of 100, tot loss = 4.535289687769754, l1: 0.00010193655770289836, l2: 0.0003515924079098967   Iteration 99 of 100, tot loss = 4.567325487281337, l1: 0.0001025855435911599, l2: 0.0003541470024643014   Iteration 100 of 100, tot loss = 4.5455625379085545, l1: 0.00010199281467066613, l2: 0.0003525634364632424
   End of epoch 1148; saving model... 

Epoch 1149 of 2000
   Iteration 1 of 100, tot loss = 3.944956064224243, l1: 9.139112080447376e-05, l2: 0.0003031044907402247   Iteration 2 of 100, tot loss = 4.265873312950134, l1: 9.713508916320279e-05, l2: 0.0003294522321084514   Iteration 3 of 100, tot loss = 3.973783095677694, l1: 8.797328337095678e-05, l2: 0.0003094050140740971   Iteration 4 of 100, tot loss = 4.029085218906403, l1: 8.399183934670873e-05, l2: 0.0003189166818629019   Iteration 5 of 100, tot loss = 4.4092343807220455, l1: 8.900547109078616e-05, l2: 0.0003519179706927389   Iteration 6 of 100, tot loss = 4.186689257621765, l1: 8.812379746814258e-05, l2: 0.0003305451318738051   Iteration 7 of 100, tot loss = 4.3212935243334085, l1: 9.244672712936466e-05, l2: 0.0003396826276522396   Iteration 8 of 100, tot loss = 4.246640354394913, l1: 9.213640623784158e-05, l2: 0.0003325276320538251   Iteration 9 of 100, tot loss = 4.6092283725738525, l1: 0.0001015098451817822, l2: 0.00035941299898291216   Iteration 10 of 100, tot loss = 4.5916279554367065, l1: 9.835039163590409e-05, l2: 0.0003608124112361111   Iteration 11 of 100, tot loss = 4.640792001377452, l1: 0.00010055992209924047, l2: 0.000363519286937927   Iteration 12 of 100, tot loss = 4.580807665983836, l1: 9.905797620983019e-05, l2: 0.0003590227994815602   Iteration 13 of 100, tot loss = 4.595510427768414, l1: 9.979077787228071e-05, l2: 0.0003597602760867001   Iteration 14 of 100, tot loss = 4.583452412060329, l1: 0.00010062137204970765, l2: 0.000357723879070753   Iteration 15 of 100, tot loss = 4.525743166605632, l1: 9.96205106882068e-05, l2: 0.000352953815793929   Iteration 16 of 100, tot loss = 4.393122464418411, l1: 9.748924503583112e-05, l2: 0.00034182301078544697   Iteration 17 of 100, tot loss = 4.357444075977101, l1: 9.708266845657764e-05, l2: 0.0003386617489227587   Iteration 18 of 100, tot loss = 4.320057312647502, l1: 9.70005013287947e-05, l2: 0.00033500523891739757   Iteration 19 of 100, tot loss = 4.231281870289853, l1: 9.59136687972779e-05, l2: 0.00032721452589612454   Iteration 20 of 100, tot loss = 4.270163428783417, l1: 9.655653011577669e-05, l2: 0.000330459820543183   Iteration 21 of 100, tot loss = 4.349337566466558, l1: 9.705657291869145e-05, l2: 0.00033787719231830645   Iteration 22 of 100, tot loss = 4.328614549203352, l1: 9.63922701141035e-05, l2: 0.00033646919407394967   Iteration 23 of 100, tot loss = 4.445113897323608, l1: 9.73838392857705e-05, l2: 0.00034712755903297955   Iteration 24 of 100, tot loss = 4.430407712856929, l1: 9.805829874191356e-05, l2: 0.0003449824804799088   Iteration 25 of 100, tot loss = 4.525002946853638, l1: 9.965173114323989e-05, l2: 0.00035284857091028244   Iteration 26 of 100, tot loss = 4.576483552272503, l1: 0.00010005315114260436, l2: 0.000357595210236748   Iteration 27 of 100, tot loss = 4.61448218204357, l1: 0.00010117594286351016, l2: 0.0003602722820317097   Iteration 28 of 100, tot loss = 4.596083377088819, l1: 0.0001003436141867756, l2: 0.0003592647309622927   Iteration 29 of 100, tot loss = 4.59381706139137, l1: 0.0001004205633793829, l2: 0.0003589611507526695   Iteration 30 of 100, tot loss = 4.676032614707947, l1: 0.00010135364112405417, l2: 0.0003662496276471453   Iteration 31 of 100, tot loss = 4.700401313843265, l1: 0.00010235133302030004, l2: 0.00036768880513151205   Iteration 32 of 100, tot loss = 4.593975432217121, l1: 0.0001000995716253783, l2: 0.0003592979783206829   Iteration 33 of 100, tot loss = 4.612605969111125, l1: 0.0001002092799767111, l2: 0.0003610513222432046   Iteration 34 of 100, tot loss = 4.630918229327483, l1: 0.00010055939893551541, l2: 0.00036253242904101224   Iteration 35 of 100, tot loss = 4.645569276809693, l1: 0.00010113393816157311, l2: 0.0003634229942690581   Iteration 36 of 100, tot loss = 4.649471779664357, l1: 0.00010181881584180196, l2: 0.00036312836649206776   Iteration 37 of 100, tot loss = 4.6379017378832845, l1: 0.00010199039914712901, l2: 0.0003617997784670946   Iteration 38 of 100, tot loss = 4.615209134001481, l1: 0.00010192090308249278, l2: 0.0003596000146659973   Iteration 39 of 100, tot loss = 4.5787225258656035, l1: 0.00010149917491220642, l2: 0.00035637308200164576   Iteration 40 of 100, tot loss = 4.566504275798797, l1: 0.00010162973935621267, l2: 0.0003550206925865496   Iteration 41 of 100, tot loss = 4.5871693797227815, l1: 0.00010208940684828033, l2: 0.0003566275348619944   Iteration 42 of 100, tot loss = 4.571908093634105, l1: 0.00010156155877623297, l2: 0.0003556292542184348   Iteration 43 of 100, tot loss = 4.616781783658404, l1: 0.00010203691970165764, l2: 0.0003596412618292582   Iteration 44 of 100, tot loss = 4.655508902939883, l1: 0.00010278195800724048, l2: 0.0003627689356190703   Iteration 45 of 100, tot loss = 4.659321048524645, l1: 0.0001029523946069983, l2: 0.0003629797135039957   Iteration 46 of 100, tot loss = 4.641899176265882, l1: 0.00010237473602698984, l2: 0.0003618151851064202   Iteration 47 of 100, tot loss = 4.693069879044878, l1: 0.00010359834766162787, l2: 0.000365708644088219   Iteration 48 of 100, tot loss = 4.662609621882439, l1: 0.00010315649975230674, l2: 0.00036310446588079987   Iteration 49 of 100, tot loss = 4.627348520317856, l1: 0.00010280662992991726, l2: 0.0003599282259083524   Iteration 50 of 100, tot loss = 4.639673919677734, l1: 0.00010332603731512791, l2: 0.0003606413590023294   Iteration 51 of 100, tot loss = 4.68366065679812, l1: 0.0001041337261852402, l2: 0.00036423234440221945   Iteration 52 of 100, tot loss = 4.716739269403311, l1: 0.00010422313100487093, l2: 0.0003674508004153792   Iteration 53 of 100, tot loss = 4.735365219835965, l1: 0.00010454922033565005, l2: 0.000368987306741611   Iteration 54 of 100, tot loss = 4.763729837205675, l1: 0.0001048701990132233, l2: 0.0003715027899791797   Iteration 55 of 100, tot loss = 4.742824810201472, l1: 0.00010463536901542366, l2: 0.0003696471172257919   Iteration 56 of 100, tot loss = 4.787775060960224, l1: 0.00010504188851103078, l2: 0.000373735623205513   Iteration 57 of 100, tot loss = 4.7503761994211295, l1: 0.00010461322046084158, l2: 0.0003704244052386728   Iteration 58 of 100, tot loss = 4.7375973093098604, l1: 0.00010456047835634371, l2: 0.0003691992580946856   Iteration 59 of 100, tot loss = 4.742418305348542, l1: 0.00010462756185684851, l2: 0.0003696142733333853   Iteration 60 of 100, tot loss = 4.732991139094035, l1: 0.00010447507638673414, l2: 0.0003688240418947923   Iteration 61 of 100, tot loss = 4.733909841443672, l1: 0.00010464072260756588, l2: 0.000368750265314717   Iteration 62 of 100, tot loss = 4.715701168583285, l1: 0.00010442303271467549, l2: 0.00036714708802455494   Iteration 63 of 100, tot loss = 4.702054731429569, l1: 0.00010423599702508433, l2: 0.0003659694796494607   Iteration 64 of 100, tot loss = 4.707249891012907, l1: 0.00010406648183902689, l2: 0.00036665851030193153   Iteration 65 of 100, tot loss = 4.704497091586774, l1: 0.00010416338703250333, l2: 0.00036628632492815644   Iteration 66 of 100, tot loss = 4.732269753109325, l1: 0.00010467109972744242, l2: 0.00036855587853951323   Iteration 67 of 100, tot loss = 4.720488808048305, l1: 0.00010463876033235695, l2: 0.0003674101234768142   Iteration 68 of 100, tot loss = 4.711464170147391, l1: 0.00010447832367871717, l2: 0.0003666680961888393   Iteration 69 of 100, tot loss = 4.708766788676165, l1: 0.00010442692763572795, l2: 0.00036644975425324577   Iteration 70 of 100, tot loss = 4.702418398857117, l1: 0.000104216447003377, l2: 0.00036602539600737925   Iteration 71 of 100, tot loss = 4.6704199414857674, l1: 0.00010374777865391979, l2: 0.0003632942186324188   Iteration 72 of 100, tot loss = 4.668394671546088, l1: 0.00010372882416555108, l2: 0.0003631106460994084   Iteration 73 of 100, tot loss = 4.692505340053611, l1: 0.00010377393868709563, l2: 0.0003654765982215918   Iteration 74 of 100, tot loss = 4.704985676585017, l1: 0.00010400806105600488, l2: 0.0003664905098786049   Iteration 75 of 100, tot loss = 4.7280006663004555, l1: 0.00010456984113261569, l2: 0.00036823022897200037   Iteration 76 of 100, tot loss = 4.712430163433678, l1: 0.00010444947711699857, l2: 0.0003667935427648971   Iteration 77 of 100, tot loss = 4.713153727642902, l1: 0.00010429873459956566, l2: 0.0003670166417058291   Iteration 78 of 100, tot loss = 4.692679197360308, l1: 0.00010402616650632505, l2: 0.00036524175681197684   Iteration 79 of 100, tot loss = 4.716752867155437, l1: 0.00010434656712588341, l2: 0.000367328722790308   Iteration 80 of 100, tot loss = 4.728441053628922, l1: 0.00010434586517931166, l2: 0.000368498243187787   Iteration 81 of 100, tot loss = 4.722300429403046, l1: 0.0001042011972683134, l2: 0.0003680288482652862   Iteration 82 of 100, tot loss = 4.701233328842536, l1: 0.00010350333754680965, l2: 0.00036661999787079033   Iteration 83 of 100, tot loss = 4.685666871358113, l1: 0.00010350173496075272, l2: 0.00036506495471477107   Iteration 84 of 100, tot loss = 4.699713020097642, l1: 0.00010359950796815351, l2: 0.00036637179662301646   Iteration 85 of 100, tot loss = 4.696805628608255, l1: 0.0001036166489297209, l2: 0.0003660639164188658   Iteration 86 of 100, tot loss = 4.7126592480859095, l1: 0.00010381905390521603, l2: 0.00036744687311638275   Iteration 87 of 100, tot loss = 4.682747632607646, l1: 0.00010329329804868582, l2: 0.0003649814674248062   Iteration 88 of 100, tot loss = 4.694070946086537, l1: 0.00010367948712800253, l2: 0.00036572761002224235   Iteration 89 of 100, tot loss = 4.688714364941201, l1: 0.00010363066854204569, l2: 0.0003652407703044814   Iteration 90 of 100, tot loss = 4.6790419154697, l1: 0.0001035547864780205, l2: 0.0003643494071891635   Iteration 91 of 100, tot loss = 4.693007993174123, l1: 0.00010389343869074755, l2: 0.0003654073626711289   Iteration 92 of 100, tot loss = 4.692414402961731, l1: 0.00010378113709362958, l2: 0.0003654603052659102   Iteration 93 of 100, tot loss = 4.695804283183108, l1: 0.00010362490977586862, l2: 0.00036595552056271745   Iteration 94 of 100, tot loss = 4.701769854160065, l1: 0.00010355878547419164, l2: 0.0003666182017966887   Iteration 95 of 100, tot loss = 4.711305046081543, l1: 0.00010340699773223606, l2: 0.0003677235090627188   Iteration 96 of 100, tot loss = 4.687856790920098, l1: 0.00010289285082383988, l2: 0.0003658928305109536   Iteration 97 of 100, tot loss = 4.676872752376439, l1: 0.00010258482842866365, l2: 0.0003651024491079728   Iteration 98 of 100, tot loss = 4.67405159862674, l1: 0.0001025040582859145, l2: 0.00036490110387046805   Iteration 99 of 100, tot loss = 4.66745863297973, l1: 0.00010245601664461323, l2: 0.00036428984862545296   Iteration 100 of 100, tot loss = 4.673289444446564, l1: 0.0001027079874438641, l2: 0.00036462095915339886
   End of epoch 1149; saving model... 

Epoch 1150 of 2000
   Iteration 1 of 100, tot loss = 4.018923759460449, l1: 0.00011041099787689745, l2: 0.00029148138128221035   Iteration 2 of 100, tot loss = 4.243738412857056, l1: 0.00011045500286854804, l2: 0.00031391884840559214   Iteration 3 of 100, tot loss = 4.195914268493652, l1: 0.0001064501241974843, l2: 0.00031314130562047165   Iteration 4 of 100, tot loss = 4.426694869995117, l1: 0.00011064082900702488, l2: 0.000332028663251549   Iteration 5 of 100, tot loss = 4.581452941894531, l1: 0.00010978033533319831, l2: 0.0003483649692498147   Iteration 6 of 100, tot loss = 4.73511815071106, l1: 0.00011144013842567801, l2: 0.00036207168401839834   Iteration 7 of 100, tot loss = 4.586024863379342, l1: 0.00011129499034723267, l2: 0.0003473075048532337   Iteration 8 of 100, tot loss = 4.672758251428604, l1: 0.00011061609620810486, l2: 0.0003566597333701793   Iteration 9 of 100, tot loss = 4.421891821755303, l1: 0.00010648734557131927, l2: 0.00033570184152469866   Iteration 10 of 100, tot loss = 4.682402634620667, l1: 0.00010888323304243386, l2: 0.00035935703490395097   Iteration 11 of 100, tot loss = 4.593052972446788, l1: 0.00010700205364793709, l2: 0.0003523032487878068   Iteration 12 of 100, tot loss = 4.5141973694165545, l1: 0.00010446587233066869, l2: 0.0003469538690599923   Iteration 13 of 100, tot loss = 4.462359281686636, l1: 0.00010302119055547967, l2: 0.0003432147428751565   Iteration 14 of 100, tot loss = 4.47808643749782, l1: 0.00010237933487847581, l2: 0.00034542931410084875   Iteration 15 of 100, tot loss = 4.414683198928833, l1: 0.00010142380051547661, l2: 0.00034004452366692326   Iteration 16 of 100, tot loss = 4.406752809882164, l1: 0.00010129143356607528, l2: 0.0003393838524061721   Iteration 17 of 100, tot loss = 4.4469294127296, l1: 0.00010299119714866666, l2: 0.0003417017493013512   Iteration 18 of 100, tot loss = 4.4984338945812645, l1: 0.00010429143165108851, l2: 0.000345551963416963   Iteration 19 of 100, tot loss = 4.578700805965223, l1: 0.00010555821752763892, l2: 0.0003523118671421942   Iteration 20 of 100, tot loss = 4.567738950252533, l1: 0.00010629609678289854, l2: 0.00035047780256718396   Iteration 21 of 100, tot loss = 4.538424798420498, l1: 0.00010602244202302591, l2: 0.0003478200427101304   Iteration 22 of 100, tot loss = 4.534553386948326, l1: 0.00010703823003992015, l2: 0.00034641711284745145   Iteration 23 of 100, tot loss = 4.510606278543887, l1: 0.00010643709348464061, l2: 0.0003446235382200583   Iteration 24 of 100, tot loss = 4.472485621770223, l1: 0.00010643298295083999, l2: 0.00034081558260368183   Iteration 25 of 100, tot loss = 4.4696900749206545, l1: 0.00010596962558338418, l2: 0.00034099938464351   Iteration 26 of 100, tot loss = 4.468071222305298, l1: 0.00010601191444528432, l2: 0.0003407952104820512   Iteration 27 of 100, tot loss = 4.436609347661336, l1: 0.00010607131601621707, l2: 0.0003375896216042478   Iteration 28 of 100, tot loss = 4.345543112073626, l1: 0.00010432403593897886, l2: 0.0003302302778008327   Iteration 29 of 100, tot loss = 4.3393292920342805, l1: 0.00010421359434860341, l2: 0.00032971933803216006   Iteration 30 of 100, tot loss = 4.351605876286825, l1: 0.00010485151651664636, l2: 0.00033030907361535354   Iteration 31 of 100, tot loss = 4.355189492625575, l1: 0.00010439165893213584, l2: 0.00033112729368205634   Iteration 32 of 100, tot loss = 4.30336356908083, l1: 0.00010345561281610571, l2: 0.0003268807477070368   Iteration 33 of 100, tot loss = 4.270860202384718, l1: 0.00010181909557573046, l2: 0.00032526692798869175   Iteration 34 of 100, tot loss = 4.3105718458400055, l1: 0.00010227844220079372, l2: 0.00032877874550382225   Iteration 35 of 100, tot loss = 4.348997749601092, l1: 0.00010300500351669532, l2: 0.00033189477482145385   Iteration 36 of 100, tot loss = 4.314656522538927, l1: 0.00010239741681693381, l2: 0.0003290682381905046   Iteration 37 of 100, tot loss = 4.278527833319999, l1: 0.00010189383681643301, l2: 0.00032595894889390046   Iteration 38 of 100, tot loss = 4.228147368682058, l1: 0.0001003271883842229, l2: 0.00032248755068346665   Iteration 39 of 100, tot loss = 4.298415208474184, l1: 0.00010186717433195251, l2: 0.00032797434967226133   Iteration 40 of 100, tot loss = 4.2584822416305546, l1: 0.00010157652113775839, l2: 0.0003242717062676093   Iteration 41 of 100, tot loss = 4.268778056633182, l1: 0.00010180120328341884, l2: 0.0003250766053608414   Iteration 42 of 100, tot loss = 4.290695962451753, l1: 0.0001014510674264914, l2: 0.00032761853106508384   Iteration 43 of 100, tot loss = 4.321205360944881, l1: 0.00010156126509807722, l2: 0.00033055927345886565   Iteration 44 of 100, tot loss = 4.309148143638264, l1: 0.0001014430686568217, l2: 0.0003294717478803994   Iteration 45 of 100, tot loss = 4.276975446277195, l1: 0.00010046189886957614, l2: 0.00032723564816276645   Iteration 46 of 100, tot loss = 4.31269982068435, l1: 0.0001014496635395082, l2: 0.00032982032058973107   Iteration 47 of 100, tot loss = 4.299768199311926, l1: 0.00010122104697771608, l2: 0.00032875577496870007   Iteration 48 of 100, tot loss = 4.309539005160332, l1: 0.00010101174045000032, l2: 0.0003299421620492164   Iteration 49 of 100, tot loss = 4.276753552105962, l1: 0.00010047418009537291, l2: 0.00032720117703821433   Iteration 50 of 100, tot loss = 4.315926017761231, l1: 0.0001012085608090274, l2: 0.00033038404246326535   Iteration 51 of 100, tot loss = 4.344455971437342, l1: 0.00010142837623682092, l2: 0.00033301722278873273   Iteration 52 of 100, tot loss = 4.315944465307089, l1: 0.00010039654716820223, l2: 0.0003311979011274301   Iteration 53 of 100, tot loss = 4.279350231278618, l1: 9.975339790404212e-05, l2: 0.0003281816269626911   Iteration 54 of 100, tot loss = 4.278172682832788, l1: 9.963876079752852e-05, l2: 0.00032817850916655044   Iteration 55 of 100, tot loss = 4.270936502109874, l1: 9.997157235788604e-05, l2: 0.0003271220794307407   Iteration 56 of 100, tot loss = 4.253305988652365, l1: 9.972099782576802e-05, l2: 0.0003256096024415456   Iteration 57 of 100, tot loss = 4.277337969395152, l1: 0.00010021137521171273, l2: 0.00032752242323272584   Iteration 58 of 100, tot loss = 4.272115205896312, l1: 0.00010005885137772942, l2: 0.00032715267072239054   Iteration 59 of 100, tot loss = 4.300903296066543, l1: 0.0001002493236382626, l2: 0.00032984100674847297   Iteration 60 of 100, tot loss = 4.308832597732544, l1: 0.00010021530209390525, l2: 0.00033066795876948165   Iteration 61 of 100, tot loss = 4.310484276443232, l1: 0.00010063826016075986, l2: 0.0003304101687046837   Iteration 62 of 100, tot loss = 4.3176400892196165, l1: 0.00010086071867384993, l2: 0.00033090329135677986   Iteration 63 of 100, tot loss = 4.298180220619081, l1: 0.00010007382020592645, l2: 0.00032974420289050727   Iteration 64 of 100, tot loss = 4.316860754042864, l1: 0.00010019320865239933, l2: 0.0003314928676445561   Iteration 65 of 100, tot loss = 4.305021440065824, l1: 0.00010042323998417347, l2: 0.0003300789047408706   Iteration 66 of 100, tot loss = 4.294890291763075, l1: 9.968600093226556e-05, l2: 0.00032980302873483833   Iteration 67 of 100, tot loss = 4.28916011995344, l1: 9.986180005249317e-05, l2: 0.0003290542120325949   Iteration 68 of 100, tot loss = 4.269935674527112, l1: 9.940290687261629e-05, l2: 0.00032759066059096604   Iteration 69 of 100, tot loss = 4.264130754747253, l1: 9.94911164182839e-05, l2: 0.00032692195900776625   Iteration 70 of 100, tot loss = 4.2630463225500925, l1: 9.975189038544028e-05, l2: 0.00032655274158709547   Iteration 71 of 100, tot loss = 4.249207644395425, l1: 9.919830648561375e-05, l2: 0.00032572245777053363   Iteration 72 of 100, tot loss = 4.2494532863299055, l1: 9.91331642884082e-05, l2: 0.0003258121640909748   Iteration 73 of 100, tot loss = 4.23969712975907, l1: 9.866678613446946e-05, l2: 0.00032530292690801754   Iteration 74 of 100, tot loss = 4.218964032224707, l1: 9.829868562322903e-05, l2: 0.0003235977173726251   Iteration 75 of 100, tot loss = 4.261200615564982, l1: 9.908831615272598e-05, l2: 0.00032703174472165607   Iteration 76 of 100, tot loss = 4.248275276861693, l1: 9.872864398187773e-05, l2: 0.0003260988830190495   Iteration 77 of 100, tot loss = 4.3045554935158075, l1: 9.971103891193588e-05, l2: 0.000330744509648701   Iteration 78 of 100, tot loss = 4.322197049092024, l1: 9.988795299693428e-05, l2: 0.0003323317512881775   Iteration 79 of 100, tot loss = 4.320587330226656, l1: 9.98594974888978e-05, l2: 0.00033219923490740926   Iteration 80 of 100, tot loss = 4.349051883816719, l1: 0.00010043198358289374, l2: 0.0003344732042023679   Iteration 81 of 100, tot loss = 4.355341608141676, l1: 0.00010055419337687106, l2: 0.0003349799666421511   Iteration 82 of 100, tot loss = 4.378796295421879, l1: 0.00010114456696411675, l2: 0.00033673506177587023   Iteration 83 of 100, tot loss = 4.4210873827876815, l1: 0.0001017484995476896, l2: 0.0003403602381859603   Iteration 84 of 100, tot loss = 4.414703868684315, l1: 0.00010157535217288005, l2: 0.0003398950339899221   Iteration 85 of 100, tot loss = 4.38071515419904, l1: 0.00010076610154047718, l2: 0.00033730541318244137   Iteration 86 of 100, tot loss = 4.365813637888709, l1: 0.00010065527368529137, l2: 0.0003359260893397055   Iteration 87 of 100, tot loss = 4.342006872440207, l1: 9.993339900292682e-05, l2: 0.0003342672875726424   Iteration 88 of 100, tot loss = 4.347280651330948, l1: 0.00010010315703435812, l2: 0.00033462490732331804   Iteration 89 of 100, tot loss = 4.344772330830606, l1: 9.974230091797784e-05, l2: 0.00033473493148745406   Iteration 90 of 100, tot loss = 4.348657091458638, l1: 9.991598081897893e-05, l2: 0.00033494972756468793   Iteration 91 of 100, tot loss = 4.375962983120929, l1: 0.00010032971816714393, l2: 0.0003372665795633201   Iteration 92 of 100, tot loss = 4.363891199879024, l1: 0.0001003013533630545, l2: 0.0003360877662603541   Iteration 93 of 100, tot loss = 4.373127022097187, l1: 0.00010034189319647398, l2: 0.0003369708087981268   Iteration 94 of 100, tot loss = 4.3849614402081105, l1: 0.00010072839943453065, l2: 0.0003377677441792602   Iteration 95 of 100, tot loss = 4.39945354210703, l1: 0.00010107650331950648, l2: 0.0003388688504377282   Iteration 96 of 100, tot loss = 4.39927788823843, l1: 0.00010119909874598913, l2: 0.00033872868971229764   Iteration 97 of 100, tot loss = 4.415260292820094, l1: 0.0001011868063363453, l2: 0.00034033922213321725   Iteration 98 of 100, tot loss = 4.426213373943251, l1: 0.00010101609865271211, l2: 0.00034160523768039233   Iteration 99 of 100, tot loss = 4.423304569841635, l1: 0.00010123467746789266, l2: 0.0003410957781880191   Iteration 100 of 100, tot loss = 4.4228268504142765, l1: 0.00010130952290637651, l2: 0.00034097316063707694
   End of epoch 1150; saving model... 

Epoch 1151 of 2000
   Iteration 1 of 100, tot loss = 3.9678025245666504, l1: 6.277083593886346e-05, l2: 0.0003340094117447734   Iteration 2 of 100, tot loss = 4.722705364227295, l1: 8.579700806876644e-05, l2: 0.00038647351902909577   Iteration 3 of 100, tot loss = 4.118017911911011, l1: 8.038805147710566e-05, l2: 0.00033141373326846707   Iteration 4 of 100, tot loss = 3.6920071840286255, l1: 7.712489059485961e-05, l2: 0.00029207582338131033   Iteration 5 of 100, tot loss = 4.097091197967529, l1: 8.79333572811447e-05, l2: 0.0003217757592210546   Iteration 6 of 100, tot loss = 4.308225313822429, l1: 9.465939729125239e-05, l2: 0.00033616313157835975   Iteration 7 of 100, tot loss = 4.228511026927403, l1: 9.370925233400027e-05, l2: 0.0003291418480720105   Iteration 8 of 100, tot loss = 4.255237907171249, l1: 9.732111084304051e-05, l2: 0.000328202679156675   Iteration 9 of 100, tot loss = 4.4122499624888105, l1: 9.915213336676566e-05, l2: 0.0003420728624203346   Iteration 10 of 100, tot loss = 4.18934485912323, l1: 9.634271191316657e-05, l2: 0.000322591773874592   Iteration 11 of 100, tot loss = 4.324513543735851, l1: 9.952665674394335e-05, l2: 0.0003329246974317357   Iteration 12 of 100, tot loss = 4.419282019138336, l1: 0.00010260486972886913, l2: 0.0003393233309907373   Iteration 13 of 100, tot loss = 4.591713483516987, l1: 0.00010302189072647777, l2: 0.00035614945786969306   Iteration 14 of 100, tot loss = 4.539136682237897, l1: 0.0001034564889518411, l2: 0.0003504571785534998   Iteration 15 of 100, tot loss = 4.5671209017435705, l1: 0.0001026208211745446, l2: 0.000354091267217882   Iteration 16 of 100, tot loss = 4.618302673101425, l1: 0.00010311879123037215, l2: 0.00035871147429134   Iteration 17 of 100, tot loss = 4.551520291496725, l1: 0.00010232152699780486, l2: 0.00035283050023923246   Iteration 18 of 100, tot loss = 4.527882443534003, l1: 0.00010227628606824308, l2: 0.00035051195704404055   Iteration 19 of 100, tot loss = 4.5313841418216105, l1: 0.00010334629233080992, l2: 0.00034979212107014303   Iteration 20 of 100, tot loss = 4.4206350445747375, l1: 0.00010189134845859372, l2: 0.0003401721543923486   Iteration 21 of 100, tot loss = 4.408289988835652, l1: 0.00010176150057564623, l2: 0.0003390674974070862   Iteration 22 of 100, tot loss = 4.41378356109966, l1: 0.00010275487511535175, l2: 0.0003386234807707793   Iteration 23 of 100, tot loss = 4.630790181781935, l1: 0.00010539233746543607, l2: 0.00035768667926870125   Iteration 24 of 100, tot loss = 4.557902306318283, l1: 0.00010425088839838281, l2: 0.00035153934125749703   Iteration 25 of 100, tot loss = 4.537758531570435, l1: 0.0001041565669584088, l2: 0.00034961928497068584   Iteration 26 of 100, tot loss = 4.557092235638545, l1: 0.00010451654004730069, l2: 0.00035119268264120014   Iteration 27 of 100, tot loss = 4.498242360574228, l1: 0.00010355060712080794, l2: 0.00034627362846448604   Iteration 28 of 100, tot loss = 4.428290503365653, l1: 0.00010245071488108286, l2: 0.00034037833548999127   Iteration 29 of 100, tot loss = 4.452605444809486, l1: 0.00010303606716755392, l2: 0.0003422244782117998   Iteration 30 of 100, tot loss = 4.5072869777679445, l1: 0.00010384484242725497, l2: 0.0003468838564003818   Iteration 31 of 100, tot loss = 4.509720494670253, l1: 0.00010392770538249263, l2: 0.0003470443456711608   Iteration 32 of 100, tot loss = 4.522481203079224, l1: 0.00010453918298480858, l2: 0.0003477089380794496   Iteration 33 of 100, tot loss = 4.4794315135840215, l1: 0.00010382380159254271, l2: 0.00034411935044000995   Iteration 34 of 100, tot loss = 4.5347176159129425, l1: 0.00010389821845284827, l2: 0.0003495735445005052   Iteration 35 of 100, tot loss = 4.519131871632167, l1: 0.00010354658069055794, l2: 0.00034836660821123844   Iteration 36 of 100, tot loss = 4.51200850142373, l1: 0.00010287396960646017, l2: 0.00034832688187533576   Iteration 37 of 100, tot loss = 4.535396633921443, l1: 0.00010321973771882571, l2: 0.0003503199259284884   Iteration 38 of 100, tot loss = 4.53982937335968, l1: 0.00010389905965942154, l2: 0.0003500838777797885   Iteration 39 of 100, tot loss = 4.475109430459829, l1: 0.00010280482857547796, l2: 0.0003447061149367633   Iteration 40 of 100, tot loss = 4.4810937404632565, l1: 0.00010298900033376413, l2: 0.000345120373822283   Iteration 41 of 100, tot loss = 4.483922970004198, l1: 0.00010191690387550696, l2: 0.0003464753929775481   Iteration 42 of 100, tot loss = 4.451458590371268, l1: 0.00010114581008175654, l2: 0.0003440000488938365   Iteration 43 of 100, tot loss = 4.439151331435802, l1: 0.0001013287277770402, l2: 0.00034258640499861345   Iteration 44 of 100, tot loss = 4.430924588983709, l1: 0.00010120599885836286, l2: 0.0003418864603604148   Iteration 45 of 100, tot loss = 4.412226825290256, l1: 0.0001010261822052093, l2: 0.0003401965009591853   Iteration 46 of 100, tot loss = 4.433333324349445, l1: 0.00010131986994605066, l2: 0.0003420134639979907   Iteration 47 of 100, tot loss = 4.4224831398497235, l1: 0.00010136440563501116, l2: 0.000340883910192791   Iteration 48 of 100, tot loss = 4.466464797655742, l1: 0.00010175116631216952, l2: 0.0003448953154171856   Iteration 49 of 100, tot loss = 4.42683449083445, l1: 0.00010123534400695555, l2: 0.00034144810728766785   Iteration 50 of 100, tot loss = 4.3996038913726805, l1: 0.00010075597107061185, l2: 0.0003392044204520062   Iteration 51 of 100, tot loss = 4.372208525152767, l1: 0.00010020969470953751, l2: 0.0003370111599888689   Iteration 52 of 100, tot loss = 4.355317262502817, l1: 9.972627944639848e-05, l2: 0.00033580544858033623   Iteration 53 of 100, tot loss = 4.353318016484098, l1: 9.97856020474947e-05, l2: 0.00033554620189671315   Iteration 54 of 100, tot loss = 4.382192602864018, l1: 0.00010012682873315903, l2: 0.0003380924331226938   Iteration 55 of 100, tot loss = 4.413063344088468, l1: 0.00010051362809132447, l2: 0.0003407927079189738   Iteration 56 of 100, tot loss = 4.460331218583243, l1: 0.00010146207569050603, l2: 0.00034457104707793666   Iteration 57 of 100, tot loss = 4.465523318240517, l1: 0.00010155875753027208, l2: 0.00034499357494305034   Iteration 58 of 100, tot loss = 4.47441605041767, l1: 0.00010186738855454365, l2: 0.0003455742169472646   Iteration 59 of 100, tot loss = 4.492607351076805, l1: 0.0001013224310107466, l2: 0.0003479383049741924   Iteration 60 of 100, tot loss = 4.48579969406128, l1: 0.0001014127037099873, l2: 0.0003471672663484545   Iteration 61 of 100, tot loss = 4.489909258045134, l1: 0.00010170542408872517, l2: 0.00034728550211885243   Iteration 62 of 100, tot loss = 4.5656786503330355, l1: 0.00010265527018946745, l2: 0.0003539125959392667   Iteration 63 of 100, tot loss = 4.610704686906603, l1: 0.0001033410309381517, l2: 0.00035772943847982714   Iteration 64 of 100, tot loss = 4.593100640922785, l1: 0.00010331352200410038, l2: 0.000355996542793946   Iteration 65 of 100, tot loss = 4.598316504405095, l1: 0.00010372693314611052, l2: 0.0003561047185660125   Iteration 66 of 100, tot loss = 4.562963850570448, l1: 0.00010310362973321003, l2: 0.00035319275665449714   Iteration 67 of 100, tot loss = 4.555748957306592, l1: 0.00010325905103271188, l2: 0.0003523158462050913   Iteration 68 of 100, tot loss = 4.558452630744261, l1: 0.00010330923888592653, l2: 0.00035253602554268366   Iteration 69 of 100, tot loss = 4.586098301237908, l1: 0.00010383105499372967, l2: 0.00035477877616463905   Iteration 70 of 100, tot loss = 4.572000581877572, l1: 0.0001036229678512817, l2: 0.0003535770910925099   Iteration 71 of 100, tot loss = 4.583863429620233, l1: 0.00010364783745796852, l2: 0.00035473850677052225   Iteration 72 of 100, tot loss = 4.589390426874161, l1: 0.0001036134948056618, l2: 0.0003553255495110837   Iteration 73 of 100, tot loss = 4.574852228164673, l1: 0.00010318769497257874, l2: 0.0003542975296923406   Iteration 74 of 100, tot loss = 4.597832328564412, l1: 0.00010372869301873662, l2: 0.0003560545415353231   Iteration 75 of 100, tot loss = 4.596153262456258, l1: 0.00010376169064935918, l2: 0.00035585363705952964   Iteration 76 of 100, tot loss = 4.588679354441793, l1: 0.0001035574167824387, l2: 0.000355310519807972   Iteration 77 of 100, tot loss = 4.598699709037682, l1: 0.00010357004491681862, l2: 0.00035629992701367223   Iteration 78 of 100, tot loss = 4.583034946368291, l1: 0.00010333147554028516, l2: 0.00035497202024532435   Iteration 79 of 100, tot loss = 4.565295566486407, l1: 0.00010309446860205005, l2: 0.00035343508923550053   Iteration 80 of 100, tot loss = 4.608548077940941, l1: 0.0001037217032717308, l2: 0.0003571331058992655   Iteration 81 of 100, tot loss = 4.611414094030121, l1: 0.0001037029831894432, l2: 0.00035743842732856903   Iteration 82 of 100, tot loss = 4.6211549683315, l1: 0.00010355694340854293, l2: 0.0003585585546693992   Iteration 83 of 100, tot loss = 4.60803848576833, l1: 0.00010333508719460119, l2: 0.00035746876274499233   Iteration 84 of 100, tot loss = 4.592942640894935, l1: 0.00010283095997736036, l2: 0.0003564633057283659   Iteration 85 of 100, tot loss = 4.573260960859411, l1: 0.00010263201145468937, l2: 0.00035469408612698317   Iteration 86 of 100, tot loss = 4.6313462617785435, l1: 0.00010367644113843156, l2: 0.00035945818689110323   Iteration 87 of 100, tot loss = 4.654498229081604, l1: 0.00010398597128344057, l2: 0.0003614638538645773   Iteration 88 of 100, tot loss = 4.653675548054955, l1: 0.00010416273587245748, l2: 0.0003612048213571225   Iteration 89 of 100, tot loss = 4.6532779838261975, l1: 0.00010414967933745552, l2: 0.00036117812164724293   Iteration 90 of 100, tot loss = 4.650235268804762, l1: 0.00010434970746347163, l2: 0.0003606738218675471   Iteration 91 of 100, tot loss = 4.650663257955195, l1: 0.00010426625450126377, l2: 0.000360800073914496   Iteration 92 of 100, tot loss = 4.646868822367295, l1: 0.00010399777127768966, l2: 0.0003606891133012655   Iteration 93 of 100, tot loss = 4.6632475724784275, l1: 0.00010427550750068559, l2: 0.0003620492521628377   Iteration 94 of 100, tot loss = 4.655690723277153, l1: 0.0001041086219861717, l2: 0.00036146045269870613   Iteration 95 of 100, tot loss = 4.637772324210719, l1: 0.00010380039480208468, l2: 0.00035997683988075313   Iteration 96 of 100, tot loss = 4.660032277305921, l1: 0.00010401724307484983, l2: 0.0003619859870317062   Iteration 97 of 100, tot loss = 4.656828870478365, l1: 0.00010414373919610222, l2: 0.0003615391502797738   Iteration 98 of 100, tot loss = 4.657914132488017, l1: 0.00010435807550437653, l2: 0.0003614333401522029   Iteration 99 of 100, tot loss = 4.634547048144871, l1: 0.00010399698621400565, l2: 0.00035945772098711304   Iteration 100 of 100, tot loss = 4.624280648231506, l1: 0.00010400328028481454, l2: 0.0003584247869730461
   End of epoch 1151; saving model... 

Epoch 1152 of 2000
   Iteration 1 of 100, tot loss = 4.963721752166748, l1: 0.00010797338472912088, l2: 0.0003883987956214696   Iteration 2 of 100, tot loss = 4.988709449768066, l1: 0.00010870025653275661, l2: 0.0003901706950273365   Iteration 3 of 100, tot loss = 4.464706500371297, l1: 9.674951676667358e-05, l2: 0.00034972114372067153   Iteration 4 of 100, tot loss = 4.034145712852478, l1: 8.838315443426836e-05, l2: 0.00031503141872235574   Iteration 5 of 100, tot loss = 4.1169756889343265, l1: 8.339730993611738e-05, l2: 0.0003283002675743774   Iteration 6 of 100, tot loss = 4.0454005400339765, l1: 8.037972778159504e-05, l2: 0.00032416033354820684   Iteration 7 of 100, tot loss = 3.921161413192749, l1: 7.928418719010162e-05, l2: 0.00031283196169949533   Iteration 8 of 100, tot loss = 3.899037688970566, l1: 8.261395669251215e-05, l2: 0.000307289819829748   Iteration 9 of 100, tot loss = 4.135993348227607, l1: 8.970272837258462e-05, l2: 0.00032389661161384237   Iteration 10 of 100, tot loss = 4.030076408386231, l1: 8.793382876319811e-05, l2: 0.00031507381791016085   Iteration 11 of 100, tot loss = 4.053560170260343, l1: 8.878656213214113e-05, l2: 0.0003165694612438198   Iteration 12 of 100, tot loss = 4.259720961252849, l1: 9.361063833542478e-05, l2: 0.00033236146191484295   Iteration 13 of 100, tot loss = 4.296752489530123, l1: 9.392612320460522e-05, l2: 0.0003357491301498018   Iteration 14 of 100, tot loss = 4.25958662373679, l1: 9.227258851751685e-05, l2: 0.0003336860775432017   Iteration 15 of 100, tot loss = 4.292660220464071, l1: 9.374438571588447e-05, l2: 0.0003355216380441561   Iteration 16 of 100, tot loss = 4.361129000782967, l1: 9.420204241905594e-05, l2: 0.00034191086251667   Iteration 17 of 100, tot loss = 4.6494702311123115, l1: 9.776926548083258e-05, l2: 0.0003671777607558076   Iteration 18 of 100, tot loss = 4.856218351258172, l1: 0.00010083056662956046, l2: 0.0003847912699307522   Iteration 19 of 100, tot loss = 4.932176552320781, l1: 0.0001031046987535726, l2: 0.00039011295685716167   Iteration 20 of 100, tot loss = 4.895113956928253, l1: 0.00010327095042157452, l2: 0.00038624044609605337   Iteration 21 of 100, tot loss = 4.836257911863781, l1: 0.00010194767396786206, l2: 0.000381678117611002   Iteration 22 of 100, tot loss = 4.934690670533613, l1: 0.00010323938410940833, l2: 0.00039022968253861603   Iteration 23 of 100, tot loss = 4.861130558926126, l1: 0.0001024260048364775, l2: 0.00038368705123581964   Iteration 24 of 100, tot loss = 4.747653077046077, l1: 0.00010031179484334037, l2: 0.0003744535133591853   Iteration 25 of 100, tot loss = 4.813128023147583, l1: 0.0001008260044909548, l2: 0.00038048679707571866   Iteration 26 of 100, tot loss = 4.923813700675964, l1: 0.00010230150082162044, l2: 0.00039007986868874956   Iteration 27 of 100, tot loss = 4.861563373495032, l1: 0.0001014571099841743, l2: 0.00038469922771953324   Iteration 28 of 100, tot loss = 4.8265363574028015, l1: 0.00010067911080113845, l2: 0.00038197452522581443   Iteration 29 of 100, tot loss = 4.845087388466144, l1: 0.00010032460842670969, l2: 0.0003841841314389402   Iteration 30 of 100, tot loss = 4.967510930697123, l1: 0.00010227077546005603, l2: 0.0003944803194220488   Iteration 31 of 100, tot loss = 4.960980730672037, l1: 0.00010229446918181625, l2: 0.00039380360601259576   Iteration 32 of 100, tot loss = 5.034946508705616, l1: 0.00010416173483918101, l2: 0.0003993329182776506   Iteration 33 of 100, tot loss = 5.041244008324363, l1: 0.00010421528768898553, l2: 0.0003999091162713188   Iteration 34 of 100, tot loss = 5.084173784536474, l1: 0.00010546249723120127, l2: 0.0004029548829695319   Iteration 35 of 100, tot loss = 5.033435794285365, l1: 0.00010519599007758578, l2: 0.0003981475913730849   Iteration 36 of 100, tot loss = 5.009963631629944, l1: 0.00010536459063183024, l2: 0.0003956317745582459   Iteration 37 of 100, tot loss = 4.972611807488106, l1: 0.00010432587077957578, l2: 0.00039293531232475733   Iteration 38 of 100, tot loss = 5.05957211318769, l1: 0.00010452688241601113, l2: 0.0004014303309591732   Iteration 39 of 100, tot loss = 4.997783116805247, l1: 0.00010309428911662112, l2: 0.0003966840248232564   Iteration 40 of 100, tot loss = 5.005868369340897, l1: 0.0001037755189827294, l2: 0.0003968113196606282   Iteration 41 of 100, tot loss = 5.00681074072675, l1: 0.00010416956431027956, l2: 0.00039651151249616795   Iteration 42 of 100, tot loss = 4.9545961164292835, l1: 0.00010331158808271756, l2: 0.0003921480262458014   Iteration 43 of 100, tot loss = 4.927734602329343, l1: 0.00010283990728609307, l2: 0.00038993355532469185   Iteration 44 of 100, tot loss = 4.955416262149811, l1: 0.00010393700110646684, l2: 0.00039160462851710196   Iteration 45 of 100, tot loss = 4.890757677290175, l1: 0.00010286690044773019, l2: 0.000386208870784483   Iteration 46 of 100, tot loss = 4.899754855943763, l1: 0.0001032363792467589, l2: 0.00038673910919738853   Iteration 47 of 100, tot loss = 4.910683276805472, l1: 0.00010399795962470346, l2: 0.00038707036985718507   Iteration 48 of 100, tot loss = 4.881928006807963, l1: 0.00010373534306988101, l2: 0.00038445745910091017   Iteration 49 of 100, tot loss = 4.892017413158806, l1: 0.0001040198967158937, l2: 0.00038518184589279094   Iteration 50 of 100, tot loss = 4.851339087486267, l1: 0.00010347769122745376, l2: 0.0003816562189604156   Iteration 51 of 100, tot loss = 4.858201162487853, l1: 0.0001037632251145802, l2: 0.0003820568926326529   Iteration 52 of 100, tot loss = 4.824385702610016, l1: 0.0001034471999026629, l2: 0.0003789913719699073   Iteration 53 of 100, tot loss = 4.870992997907242, l1: 0.00010417622437465602, l2: 0.0003829230774053425   Iteration 54 of 100, tot loss = 4.851836142716585, l1: 0.00010342251934397845, l2: 0.00038176109686631845   Iteration 55 of 100, tot loss = 4.873059108040549, l1: 0.0001036777211993467, l2: 0.0003836281920402226   Iteration 56 of 100, tot loss = 4.933414161205292, l1: 0.00010481493602908034, l2: 0.0003885264819213522   Iteration 57 of 100, tot loss = 4.946587964108116, l1: 0.00010503895228203761, l2: 0.00038961984580709484   Iteration 58 of 100, tot loss = 4.988163512328575, l1: 0.00010602318566113887, l2: 0.00039279316792842643   Iteration 59 of 100, tot loss = 4.944890139466625, l1: 0.00010503316310459306, l2: 0.0003894558531563666   Iteration 60 of 100, tot loss = 4.939313042163849, l1: 0.00010475622669522029, l2: 0.0003891750801509867   Iteration 61 of 100, tot loss = 4.933141774818545, l1: 0.00010486681109244676, l2: 0.00038844736943738996   Iteration 62 of 100, tot loss = 4.914657754282797, l1: 0.00010467762307774635, l2: 0.0003867881553765807   Iteration 63 of 100, tot loss = 4.936161245618548, l1: 0.00010482753945296512, l2: 0.00038878858830439786   Iteration 64 of 100, tot loss = 4.93208122253418, l1: 0.00010477655212071113, l2: 0.00038843157335577416   Iteration 65 of 100, tot loss = 4.909256726044875, l1: 0.00010443979531723576, l2: 0.00038648588054526886   Iteration 66 of 100, tot loss = 4.877275004531398, l1: 0.00010400235260021873, l2: 0.0003837251508340352   Iteration 67 of 100, tot loss = 4.837831945561651, l1: 0.00010322368814116483, l2: 0.00038055950926039924   Iteration 68 of 100, tot loss = 4.860859394073486, l1: 0.00010360074670375873, l2: 0.00038248519537292477   Iteration 69 of 100, tot loss = 4.8924070717631905, l1: 0.00010405103819431278, l2: 0.0003851896712465374   Iteration 70 of 100, tot loss = 4.918157924924578, l1: 0.00010440135437030611, l2: 0.0003874144402548804   Iteration 71 of 100, tot loss = 4.923213676667549, l1: 0.0001045137465091973, l2: 0.000387807623245491   Iteration 72 of 100, tot loss = 4.902442187070847, l1: 0.00010419023338828184, l2: 0.0003860539875151719   Iteration 73 of 100, tot loss = 4.874013753786479, l1: 0.00010377337796492375, l2: 0.00038362799945115177   Iteration 74 of 100, tot loss = 4.85614174121135, l1: 0.00010326501944439008, l2: 0.00038234915681076363   Iteration 75 of 100, tot loss = 4.833530400594076, l1: 0.00010285044632231196, l2: 0.0003805025959930693   Iteration 76 of 100, tot loss = 4.835066977300142, l1: 0.00010320654626794796, l2: 0.0003803001537131709   Iteration 77 of 100, tot loss = 4.803402197825444, l1: 0.00010272975562763243, l2: 0.00037761046622982613   Iteration 78 of 100, tot loss = 4.798546500695058, l1: 0.00010267871085745402, l2: 0.00037717594088276086   Iteration 79 of 100, tot loss = 4.80647937859161, l1: 0.00010264526470195903, l2: 0.00037800267473588213   Iteration 80 of 100, tot loss = 4.823233631253243, l1: 0.00010276011335008661, l2: 0.00037956325159029804   Iteration 81 of 100, tot loss = 4.863483361255976, l1: 0.00010358040028087864, l2: 0.00038276793759310456   Iteration 82 of 100, tot loss = 4.8435018295195045, l1: 0.00010337773337818274, l2: 0.00038097245123935863   Iteration 83 of 100, tot loss = 4.844184622707137, l1: 0.00010347486973644784, l2: 0.0003809435943531501   Iteration 84 of 100, tot loss = 4.839537291299729, l1: 0.00010353876955161381, l2: 0.00038041496130366187   Iteration 85 of 100, tot loss = 4.863896128710579, l1: 0.00010390696843387559, l2: 0.000382482646519373   Iteration 86 of 100, tot loss = 4.8639610922613805, l1: 0.00010400606883787226, l2: 0.0003823900423053284   Iteration 87 of 100, tot loss = 4.856793551609434, l1: 0.00010400280943836204, l2: 0.0003816765475812122   Iteration 88 of 100, tot loss = 4.871863674033772, l1: 0.00010450549945579206, l2: 0.0003826808700647565   Iteration 89 of 100, tot loss = 4.863368205809861, l1: 0.00010448711487043419, l2: 0.00038184970797149444   Iteration 90 of 100, tot loss = 4.861106411616007, l1: 0.00010465986957165619, l2: 0.00038145077366304273   Iteration 91 of 100, tot loss = 4.848603623253958, l1: 0.0001046396766349182, l2: 0.0003802206875451912   Iteration 92 of 100, tot loss = 4.858026807722838, l1: 0.00010470416719212865, l2: 0.0003810985152718469   Iteration 93 of 100, tot loss = 4.870978855317639, l1: 0.00010483834577376344, l2: 0.000382259541149852   Iteration 94 of 100, tot loss = 4.864946946184686, l1: 0.00010486243155220166, l2: 0.00038163226417964324   Iteration 95 of 100, tot loss = 4.840249992671766, l1: 0.00010450709508548147, l2: 0.00037951790513225686   Iteration 96 of 100, tot loss = 4.86077972004811, l1: 0.00010481622477224543, l2: 0.0003812617478615721   Iteration 97 of 100, tot loss = 4.889017864600899, l1: 0.00010521743665917383, l2: 0.00038368435013684033   Iteration 98 of 100, tot loss = 4.899905216937163, l1: 0.00010543808375772716, l2: 0.0003845524381399535   Iteration 99 of 100, tot loss = 4.90585631312746, l1: 0.00010553638437866337, l2: 0.0003850492472393495   Iteration 100 of 100, tot loss = 4.889899220466614, l1: 0.00010509690931939986, l2: 0.00038389301305869595
   End of epoch 1152; saving model... 

Epoch 1153 of 2000
   Iteration 1 of 100, tot loss = 2.4083189964294434, l1: 7.063446537358686e-05, l2: 0.00017019742517732084   Iteration 2 of 100, tot loss = 4.416297674179077, l1: 0.00011490298857097514, l2: 0.00032672678935341537   Iteration 3 of 100, tot loss = 4.9776506423950195, l1: 0.00011539436673047021, l2: 0.00038237070354322594   Iteration 4 of 100, tot loss = 4.861953973770142, l1: 0.00011779063606809359, l2: 0.00036840476968791336   Iteration 5 of 100, tot loss = 5.4672853469848635, l1: 0.00012674130994128063, l2: 0.00041998722590506075   Iteration 6 of 100, tot loss = 5.462419907251994, l1: 0.00012115402326647502, l2: 0.00042508797196205705   Iteration 7 of 100, tot loss = 5.387444087437221, l1: 0.00011907676110110645, l2: 0.0004196676517104996   Iteration 8 of 100, tot loss = 5.259436547756195, l1: 0.00011769406319217524, l2: 0.0004082495979673695   Iteration 9 of 100, tot loss = 5.671315458085802, l1: 0.00012333641724479903, l2: 0.00044379512999310263   Iteration 10 of 100, tot loss = 5.614048004150391, l1: 0.0001194700955238659, l2: 0.0004419347067596391   Iteration 11 of 100, tot loss = 5.584041205319491, l1: 0.00012023487347918986, l2: 0.0004381692507939244   Iteration 12 of 100, tot loss = 5.539276758829753, l1: 0.00011934600661334116, l2: 0.00043458167298619327   Iteration 13 of 100, tot loss = 5.527015465956468, l1: 0.00011829449235497472, l2: 0.0004344070576525365   Iteration 14 of 100, tot loss = 5.450596979686192, l1: 0.00011774187286001896, l2: 0.00042731782845554074   Iteration 15 of 100, tot loss = 5.530861949920654, l1: 0.00011819711799034848, l2: 0.0004348890797700733   Iteration 16 of 100, tot loss = 5.445715576410294, l1: 0.00011618725693551823, l2: 0.000428384304541396   Iteration 17 of 100, tot loss = 5.466369292315314, l1: 0.00011700614328112672, l2: 0.00042963078971404363   Iteration 18 of 100, tot loss = 5.3915662500593395, l1: 0.00011666413249460877, l2: 0.0004224924955956845   Iteration 19 of 100, tot loss = 5.4787917137146, l1: 0.00011781647864202234, l2: 0.0004300626935314779   Iteration 20 of 100, tot loss = 5.523947930335998, l1: 0.0001184399410703918, l2: 0.0004339548511779867   Iteration 21 of 100, tot loss = 5.450812544141497, l1: 0.00011805371537948737, l2: 0.0004270275392281335   Iteration 22 of 100, tot loss = 5.382746338844299, l1: 0.00011677496249004352, l2: 0.0004214996720706536   Iteration 23 of 100, tot loss = 5.331249931584233, l1: 0.00011491520189852251, l2: 0.00041820979095064104   Iteration 24 of 100, tot loss = 5.292461842298508, l1: 0.00011400073041537932, l2: 0.0004152454530412797   Iteration 25 of 100, tot loss = 5.357666292190552, l1: 0.00011509336181916296, l2: 0.000420673267217353   Iteration 26 of 100, tot loss = 5.29496391919943, l1: 0.00011350274396066267, l2: 0.00041599364843792643   Iteration 27 of 100, tot loss = 5.257094392070064, l1: 0.00011358467276699634, l2: 0.00041212476746834537   Iteration 28 of 100, tot loss = 5.197058004992349, l1: 0.00011268289563304279, l2: 0.00040702290529484993   Iteration 29 of 100, tot loss = 5.191333614546677, l1: 0.00011322714949371936, l2: 0.0004059062118577803   Iteration 30 of 100, tot loss = 5.250696428616842, l1: 0.00011366130517368826, l2: 0.0004114083363674581   Iteration 31 of 100, tot loss = 5.239523418488041, l1: 0.00011382878062337818, l2: 0.0004101235601645444   Iteration 32 of 100, tot loss = 5.254241399466991, l1: 0.0001146611032254441, l2: 0.00041076303568843286   Iteration 33 of 100, tot loss = 5.247376824870254, l1: 0.00011479143436052695, l2: 0.00040994624688402945   Iteration 34 of 100, tot loss = 5.209113773177652, l1: 0.00011435720615041005, l2: 0.00040655416994602145   Iteration 35 of 100, tot loss = 5.283574451719011, l1: 0.00011590072930890268, l2: 0.0004124567153797086   Iteration 36 of 100, tot loss = 5.221042937702602, l1: 0.00011478149139697457, l2: 0.00040732280144260987   Iteration 37 of 100, tot loss = 5.232150902619233, l1: 0.00011527688144957899, l2: 0.0004079382075311465   Iteration 38 of 100, tot loss = 5.223344012310631, l1: 0.00011523343920615796, l2: 0.0004071009599628221   Iteration 39 of 100, tot loss = 5.175187178147145, l1: 0.00011422742029072907, l2: 0.0004032912950676221   Iteration 40 of 100, tot loss = 5.209208244085312, l1: 0.0001152951021140325, l2: 0.00040562571957707404   Iteration 41 of 100, tot loss = 5.17465920564605, l1: 0.00011481689030117151, l2: 0.0004026490276418172   Iteration 42 of 100, tot loss = 5.214546975635347, l1: 0.00011498140104647194, l2: 0.0004064732936621156   Iteration 43 of 100, tot loss = 5.2292231404504115, l1: 0.00011540900612639826, l2: 0.00040751330413704   Iteration 44 of 100, tot loss = 5.214814597910101, l1: 0.00011510555916836232, l2: 0.0004063758970005438   Iteration 45 of 100, tot loss = 5.186732223298814, l1: 0.00011465527447095762, l2: 0.0004040179444321742   Iteration 46 of 100, tot loss = 5.19061145056849, l1: 0.00011444614969862059, l2: 0.000404614991250286   Iteration 47 of 100, tot loss = 5.184131738987375, l1: 0.00011437021578572928, l2: 0.00040404295407612113   Iteration 48 of 100, tot loss = 5.178809170921643, l1: 0.00011424488063009146, l2: 0.0004036360326911866   Iteration 49 of 100, tot loss = 5.150112877086717, l1: 0.0001132718867406116, l2: 0.00040173939754711277   Iteration 50 of 100, tot loss = 5.183754782676697, l1: 0.00011429586695157923, l2: 0.0004040796082699671   Iteration 51 of 100, tot loss = 5.180787418402877, l1: 0.00011460694233891463, l2: 0.00040347179553161067   Iteration 52 of 100, tot loss = 5.191094797391158, l1: 0.00011450571498873894, l2: 0.00040460376084620994   Iteration 53 of 100, tot loss = 5.149441534618162, l1: 0.00011340352874919158, l2: 0.0004015406206544524   Iteration 54 of 100, tot loss = 5.141802712723061, l1: 0.00011324441011647987, l2: 0.00040093585724847527   Iteration 55 of 100, tot loss = 5.149940911206332, l1: 0.00011330739223260686, l2: 0.0004016866952456026   Iteration 56 of 100, tot loss = 5.13118993810245, l1: 0.0001129720391353268, l2: 0.00040014695098631946   Iteration 57 of 100, tot loss = 5.1795141111340435, l1: 0.0001138021808608819, l2: 0.00040414922675648866   Iteration 58 of 100, tot loss = 5.15310760202079, l1: 0.000113458724971961, l2: 0.00040185203196697644   Iteration 59 of 100, tot loss = 5.162806599827136, l1: 0.0001138024840733802, l2: 0.00040247817291933413   Iteration 60 of 100, tot loss = 5.185952313741049, l1: 0.00011398405725534151, l2: 0.0004046111709612887   Iteration 61 of 100, tot loss = 5.170795643915896, l1: 0.00011400858671013189, l2: 0.0004030709747954073   Iteration 62 of 100, tot loss = 5.156331054625973, l1: 0.00011350722102619212, l2: 0.00040212588130323696   Iteration 63 of 100, tot loss = 5.153782519083174, l1: 0.00011342803323743052, l2: 0.00040195021563069156   Iteration 64 of 100, tot loss = 5.118210025131702, l1: 0.00011246462310054994, l2: 0.00039935637664711976   Iteration 65 of 100, tot loss = 5.134887079092173, l1: 0.00011287674340285147, l2: 0.00040061196173505427   Iteration 66 of 100, tot loss = 5.137179468617295, l1: 0.00011273430732906104, l2: 0.00040098363652677193   Iteration 67 of 100, tot loss = 5.112144950610488, l1: 0.00011230301123447785, l2: 0.0003989114812245148   Iteration 68 of 100, tot loss = 5.103641527540543, l1: 0.0001120410116688136, l2: 0.0003983231386775836   Iteration 69 of 100, tot loss = 5.075279723043027, l1: 0.00011117708967556582, l2: 0.0003963508801382926   Iteration 70 of 100, tot loss = 5.07476144177573, l1: 0.00011092469254175999, l2: 0.0003965514493756928   Iteration 71 of 100, tot loss = 5.045027944403635, l1: 0.0001100436286794999, l2: 0.0003944591635060956   Iteration 72 of 100, tot loss = 5.045118182897568, l1: 0.00010978494300515094, l2: 0.00039472687280471693   Iteration 73 of 100, tot loss = 5.048861395822812, l1: 0.00010997075803796379, l2: 0.0003949153794801781   Iteration 74 of 100, tot loss = 5.037505140175691, l1: 0.00011018248231856567, l2: 0.0003935680297955342   Iteration 75 of 100, tot loss = 5.010686639149983, l1: 0.00010966747897327877, l2: 0.00039140118305416155   Iteration 76 of 100, tot loss = 4.997647956797951, l1: 0.00010944317857233967, l2: 0.0003903216150463681   Iteration 77 of 100, tot loss = 4.982791247305932, l1: 0.00010904505102957188, l2: 0.0003892340715868737   Iteration 78 of 100, tot loss = 4.953782368928958, l1: 0.00010857978229931831, l2: 0.0003867984522889273   Iteration 79 of 100, tot loss = 4.946328235577933, l1: 0.00010849824564824695, l2: 0.0003861345756353952   Iteration 80 of 100, tot loss = 4.938984727859497, l1: 0.0001084639284727018, l2: 0.00038543454229511553   Iteration 81 of 100, tot loss = 4.947857485877143, l1: 0.0001086277791919089, l2: 0.0003861579672148865   Iteration 82 of 100, tot loss = 4.953025864391792, l1: 0.00010887394096133038, l2: 0.00038642864322954246   Iteration 83 of 100, tot loss = 4.991804421666157, l1: 0.0001096619641200039, l2: 0.0003895184759075854   Iteration 84 of 100, tot loss = 5.04008902822222, l1: 0.00011013746028592799, l2: 0.0003938714405757353   Iteration 85 of 100, tot loss = 5.009522791469799, l1: 0.00010969064695767455, l2: 0.0003912616301985348   Iteration 86 of 100, tot loss = 4.9830820421839865, l1: 0.0001093905314963533, l2: 0.0003889176707246427   Iteration 87 of 100, tot loss = 4.969791595963226, l1: 0.00010922219041077923, l2: 0.00038775696734303673   Iteration 88 of 100, tot loss = 4.978871749206022, l1: 0.00010926088184119594, l2: 0.0003886262911692029   Iteration 89 of 100, tot loss = 4.97120214044378, l1: 0.00010916838404268622, l2: 0.0003879518279860717   Iteration 90 of 100, tot loss = 4.958986189630297, l1: 0.00010919364904111717, l2: 0.0003867049680492427   Iteration 91 of 100, tot loss = 4.935197143764286, l1: 0.00010882628587321481, l2: 0.00038469342657513304   Iteration 92 of 100, tot loss = 4.921007700588392, l1: 0.00010863176996043994, l2: 0.0003834689982202532   Iteration 93 of 100, tot loss = 4.92014903406943, l1: 0.0001086237665027335, l2: 0.00038339113500545063   Iteration 94 of 100, tot loss = 4.908155656875448, l1: 0.00010855486854121951, l2: 0.00038226069545031505   Iteration 95 of 100, tot loss = 4.899838455099808, l1: 0.00010861518289618145, l2: 0.0003813686610603901   Iteration 96 of 100, tot loss = 4.923990122973919, l1: 0.0001091897574572916, l2: 0.0003832092529592046   Iteration 97 of 100, tot loss = 4.937088319935749, l1: 0.00010932285482509148, l2: 0.00038438597553020783   Iteration 98 of 100, tot loss = 4.937533419959399, l1: 0.00010895270831812867, l2: 0.0003848006318938652   Iteration 99 of 100, tot loss = 4.943392987203116, l1: 0.00010907500561160938, l2: 0.000385264291179912   Iteration 100 of 100, tot loss = 4.93524717092514, l1: 0.00010881714715651469, l2: 0.0003847075680096168
   End of epoch 1153; saving model... 

Epoch 1154 of 2000
   Iteration 1 of 100, tot loss = 5.888906955718994, l1: 0.0001530743611510843, l2: 0.00043581632780842483   Iteration 2 of 100, tot loss = 7.1163084506988525, l1: 0.00014574329543393105, l2: 0.0005658875306835398   Iteration 3 of 100, tot loss = 6.250666777292888, l1: 0.00013374933769227937, l2: 0.0004913173227881392   Iteration 4 of 100, tot loss = 5.500156819820404, l1: 0.00011868176807183772, l2: 0.000431333901360631   Iteration 5 of 100, tot loss = 5.413286352157593, l1: 0.00011860128870466724, l2: 0.00042272733408026396   Iteration 6 of 100, tot loss = 5.374019742012024, l1: 0.00012064888505847193, l2: 0.00041675307632734376   Iteration 7 of 100, tot loss = 5.633893932615008, l1: 0.00012401173444231972, l2: 0.0004393776512837836   Iteration 8 of 100, tot loss = 5.560313671827316, l1: 0.00012137689736846369, l2: 0.00043465446651680395   Iteration 9 of 100, tot loss = 5.384755902820164, l1: 0.00011986037861788645, l2: 0.00041861520730890334   Iteration 10 of 100, tot loss = 5.5167392015457155, l1: 0.0001225930951477494, l2: 0.0004290808195946738   Iteration 11 of 100, tot loss = 5.432366999712857, l1: 0.0001220208492172374, l2: 0.000421215846224434   Iteration 12 of 100, tot loss = 5.649264355500539, l1: 0.0001258680846755548, l2: 0.00043905834657683346   Iteration 13 of 100, tot loss = 5.612345457077026, l1: 0.0001233146568455805, l2: 0.0004379198832723957   Iteration 14 of 100, tot loss = 5.681128655161176, l1: 0.0001241153180931828, l2: 0.0004439975434382047   Iteration 15 of 100, tot loss = 5.53548854192098, l1: 0.00012209548537308972, l2: 0.0004314533648236344   Iteration 16 of 100, tot loss = 5.338784456253052, l1: 0.0001182314433663123, l2: 0.00041564699859009124   Iteration 17 of 100, tot loss = 5.335530729854808, l1: 0.00011752611467005302, l2: 0.00041602695573066524   Iteration 18 of 100, tot loss = 5.211120327313741, l1: 0.00011484261328102245, l2: 0.00040626941659461916   Iteration 19 of 100, tot loss = 5.0721103015698885, l1: 0.00011284376964077207, l2: 0.00039436725905704265   Iteration 20 of 100, tot loss = 5.062244319915772, l1: 0.00011235808869969333, l2: 0.00039386634161928666   Iteration 21 of 100, tot loss = 5.1045649619329545, l1: 0.0001112740160135131, l2: 0.0003991824771565873   Iteration 22 of 100, tot loss = 5.0801333514126865, l1: 0.00011070227413835131, l2: 0.000397311057895422   Iteration 23 of 100, tot loss = 5.111177879831065, l1: 0.0001113311096243864, l2: 0.00039978667653327727   Iteration 24 of 100, tot loss = 5.18175729115804, l1: 0.00011172663910959575, l2: 0.00040644908949616365   Iteration 25 of 100, tot loss = 5.256462306976318, l1: 0.00011313655952108092, l2: 0.0004125096707139164   Iteration 26 of 100, tot loss = 5.298352956771851, l1: 0.00011330028526292433, l2: 0.0004165350099076302   Iteration 27 of 100, tot loss = 5.298923669037996, l1: 0.0001126594173978514, l2: 0.00041723294890727157   Iteration 28 of 100, tot loss = 5.297815578324454, l1: 0.00011348794773117074, l2: 0.0004162936082658624   Iteration 29 of 100, tot loss = 5.243221792681464, l1: 0.0001122927498162708, l2: 0.0004120294279659745   Iteration 30 of 100, tot loss = 5.2687933603922525, l1: 0.00011276887974721224, l2: 0.00041411045531276615   Iteration 31 of 100, tot loss = 5.27551995554278, l1: 0.00011324613871927854, l2: 0.00041430585570783625   Iteration 32 of 100, tot loss = 5.304069384932518, l1: 0.00011399459583572025, l2: 0.00041641234201961197   Iteration 33 of 100, tot loss = 5.284279577659838, l1: 0.00011397264185661422, l2: 0.00041445531568377083   Iteration 34 of 100, tot loss = 5.289524891797234, l1: 0.00011460754349707927, l2: 0.00041434494607314905   Iteration 35 of 100, tot loss = 5.243854352406093, l1: 0.00011383550039941578, l2: 0.000410549935518897   Iteration 36 of 100, tot loss = 5.247666696707408, l1: 0.00011380295968087416, l2: 0.00041096371003530093   Iteration 37 of 100, tot loss = 5.231374154219756, l1: 0.00011328136443390158, l2: 0.0004098560508479037   Iteration 38 of 100, tot loss = 5.22117440951498, l1: 0.00011325575188062718, l2: 0.0004088616880931352   Iteration 39 of 100, tot loss = 5.149827706508147, l1: 0.0001123564787774693, l2: 0.00040262629082486127   Iteration 40 of 100, tot loss = 5.15817044377327, l1: 0.00011275153146925732, l2: 0.0004030655120004667   Iteration 41 of 100, tot loss = 5.186004504924867, l1: 0.0001137146931929548, l2: 0.0004048857567462752   Iteration 42 of 100, tot loss = 5.170959682691665, l1: 0.00011370536274279446, l2: 0.000403390605199439   Iteration 43 of 100, tot loss = 5.19742706764576, l1: 0.00011396867016311411, l2: 0.0004057740372318645   Iteration 44 of 100, tot loss = 5.19394539161162, l1: 0.00011379384911627064, l2: 0.0004056006903490263   Iteration 45 of 100, tot loss = 5.152424452039931, l1: 0.00011365347074590519, l2: 0.0004015889749603553   Iteration 46 of 100, tot loss = 5.137300284012504, l1: 0.00011321886956031211, l2: 0.00040051115945022065   Iteration 47 of 100, tot loss = 5.124444961547852, l1: 0.00011288368106297058, l2: 0.00039956081618971014   Iteration 48 of 100, tot loss = 5.139235486586888, l1: 0.00011275665284908125, l2: 0.0004011668970633764   Iteration 49 of 100, tot loss = 5.170110284065713, l1: 0.0001133061485568166, l2: 0.0004037048807842847   Iteration 50 of 100, tot loss = 5.160805025100708, l1: 0.00011300008212856482, l2: 0.00040308042080141606   Iteration 51 of 100, tot loss = 5.167083123150994, l1: 0.00011297315979711985, l2: 0.00040373515324009695   Iteration 52 of 100, tot loss = 5.167699437875014, l1: 0.00011300500262642835, l2: 0.00040376494199825596   Iteration 53 of 100, tot loss = 5.2671701053403455, l1: 0.00011454168002874555, l2: 0.00041217533190611677   Iteration 54 of 100, tot loss = 5.258899291356404, l1: 0.0001145575310214621, l2: 0.0004113323995153661   Iteration 55 of 100, tot loss = 5.194888771664012, l1: 0.00011351735665812157, l2: 0.00040597152189796113   Iteration 56 of 100, tot loss = 5.217120006680489, l1: 0.00011363419675295258, l2: 0.0004080778049423576   Iteration 57 of 100, tot loss = 5.191351650053995, l1: 0.00011348532120392887, l2: 0.0004056498446356783   Iteration 58 of 100, tot loss = 5.202136222658487, l1: 0.00011396455995744528, l2: 0.00040624906313465106   Iteration 59 of 100, tot loss = 5.224787253444478, l1: 0.00011427738151061191, l2: 0.0004082013443714181   Iteration 60 of 100, tot loss = 5.2407523413499195, l1: 0.00011468799511931138, l2: 0.000409387239900146   Iteration 61 of 100, tot loss = 5.308288083701838, l1: 0.00011594517252619806, l2: 0.00041488363547346053   Iteration 62 of 100, tot loss = 5.2783732087381425, l1: 0.00011543212771826919, l2: 0.00041240519309330043   Iteration 63 of 100, tot loss = 5.309192103052896, l1: 0.00011573700087157906, l2: 0.0004151822092050376   Iteration 64 of 100, tot loss = 5.303432395681739, l1: 0.00011581631173385176, l2: 0.0004145269273294616   Iteration 65 of 100, tot loss = 5.3126764205785895, l1: 0.00011620897337427148, l2: 0.00041505866784763597   Iteration 66 of 100, tot loss = 5.338248540054668, l1: 0.00011651112188040067, l2: 0.00041731373174556984   Iteration 67 of 100, tot loss = 5.298562727757354, l1: 0.00011559978276039405, l2: 0.0004142564898417609   Iteration 68 of 100, tot loss = 5.331503407043569, l1: 0.00011596028964370763, l2: 0.0004171900512600961   Iteration 69 of 100, tot loss = 5.334416059480197, l1: 0.000115814137709516, l2: 0.0004176274687515747   Iteration 70 of 100, tot loss = 5.345557739053453, l1: 0.00011565745142953737, l2: 0.0004188983230311091   Iteration 71 of 100, tot loss = 5.309242894951726, l1: 0.00011503828034004932, l2: 0.0004158860098523035   Iteration 72 of 100, tot loss = 5.291326617201169, l1: 0.0001147890822418655, l2: 0.00041434357980405266   Iteration 73 of 100, tot loss = 5.285565306062567, l1: 0.00011478359504022368, l2: 0.00041377293542011525   Iteration 74 of 100, tot loss = 5.286239838277972, l1: 0.00011508610249792755, l2: 0.00041353788155656484   Iteration 75 of 100, tot loss = 5.250971738497416, l1: 0.00011426472949096933, l2: 0.0004108324448073593   Iteration 76 of 100, tot loss = 5.2578809214265725, l1: 0.00011441586981450918, l2: 0.00041137222270711053   Iteration 77 of 100, tot loss = 5.290495421979334, l1: 0.00011473186168586835, l2: 0.0004143176815591982   Iteration 78 of 100, tot loss = 5.277194605423854, l1: 0.00011471588844030451, l2: 0.0004130035734142905   Iteration 79 of 100, tot loss = 5.283394066593315, l1: 0.00011474551350066934, l2: 0.0004135938946126748   Iteration 80 of 100, tot loss = 5.284218956530094, l1: 0.0001147454746387666, l2: 0.0004136764221584599   Iteration 81 of 100, tot loss = 5.255901008476446, l1: 0.0001141982020833242, l2: 0.00041139189969176243   Iteration 82 of 100, tot loss = 5.272393053624688, l1: 0.00011393273429925235, l2: 0.00041330657182786637   Iteration 83 of 100, tot loss = 5.236113199268479, l1: 0.0001132439393553664, l2: 0.00041036738118883334   Iteration 84 of 100, tot loss = 5.240062374444235, l1: 0.00011337773984061121, l2: 0.000410628498035034   Iteration 85 of 100, tot loss = 5.250519383654875, l1: 0.00011371295091949458, l2: 0.0004113389876746463   Iteration 86 of 100, tot loss = 5.243038062439409, l1: 0.00011346090557412407, l2: 0.00041084290104092446   Iteration 87 of 100, tot loss = 5.254923076465212, l1: 0.00011386296929471763, l2: 0.0004116293388631612   Iteration 88 of 100, tot loss = 5.250283123417334, l1: 0.00011372676127311107, l2: 0.00041130155188346345   Iteration 89 of 100, tot loss = 5.238662817504968, l1: 0.00011356105517662716, l2: 0.0004103052274688942   Iteration 90 of 100, tot loss = 5.240035169654423, l1: 0.00011336190157938594, l2: 0.0004106416165692887   Iteration 91 of 100, tot loss = 5.2240759029493224, l1: 0.00011316197912322592, l2: 0.0004092456124803553   Iteration 92 of 100, tot loss = 5.214478014603905, l1: 0.00011293220533211159, l2: 0.00040851559756645105   Iteration 93 of 100, tot loss = 5.238117114190133, l1: 0.00011305337208003977, l2: 0.0004107583409159004   Iteration 94 of 100, tot loss = 5.224714597488972, l1: 0.00011275144304895644, l2: 0.0004097200183791079   Iteration 95 of 100, tot loss = 5.216413255741721, l1: 0.00011269108673379953, l2: 0.00040895024055651833   Iteration 96 of 100, tot loss = 5.206573778142531, l1: 0.00011267856905305962, l2: 0.0004079788106992055   Iteration 97 of 100, tot loss = 5.216656678730679, l1: 0.00011293145384040622, l2: 0.000408734216162162   Iteration 98 of 100, tot loss = 5.200009631867311, l1: 0.00011286658126645845, l2: 0.00040713438402792455   Iteration 99 of 100, tot loss = 5.182033120983779, l1: 0.00011273566313761738, l2: 0.00040546765119117223   Iteration 100 of 100, tot loss = 5.168199652433396, l1: 0.00011248527454881695, l2: 0.0004043346930848202
   End of epoch 1154; saving model... 

Epoch 1155 of 2000
   Iteration 1 of 100, tot loss = 4.185876846313477, l1: 9.305740240961313e-05, l2: 0.0003255303017795086   Iteration 2 of 100, tot loss = 5.124967575073242, l1: 0.00011692642146954313, l2: 0.0003955703432438895   Iteration 3 of 100, tot loss = 4.918377081553142, l1: 0.0001090182534729441, l2: 0.0003828194555050383   Iteration 4 of 100, tot loss = 4.311954855918884, l1: 9.926342863764148e-05, l2: 0.0003319320530863479   Iteration 5 of 100, tot loss = 4.292300891876221, l1: 0.00010333834507036954, l2: 0.00032589174224995077   Iteration 6 of 100, tot loss = 4.630799690882365, l1: 0.00011076473553354542, l2: 0.00035231523603821796   Iteration 7 of 100, tot loss = 4.50007997240339, l1: 0.00010995107212303472, l2: 0.00034005692993689863   Iteration 8 of 100, tot loss = 4.902659237384796, l1: 0.00011763292241084855, l2: 0.0003726330105564557   Iteration 9 of 100, tot loss = 4.882853190104167, l1: 0.00011187268359612467, l2: 0.00037641264498233795   Iteration 10 of 100, tot loss = 4.624587726593018, l1: 0.00010868942772503942, l2: 0.0003537693541147746   Iteration 11 of 100, tot loss = 4.514721675352617, l1: 0.0001040967082107355, l2: 0.00034737546891185707   Iteration 12 of 100, tot loss = 4.491977870464325, l1: 0.00010324275899620261, l2: 0.00034595503651265364   Iteration 13 of 100, tot loss = 4.254207510214585, l1: 9.811439807750643e-05, l2: 0.0003273063607943746   Iteration 14 of 100, tot loss = 4.410878224032266, l1: 0.00010184746211702336, l2: 0.0003392403679234641   Iteration 15 of 100, tot loss = 4.289571245511373, l1: 0.00010020666547158422, l2: 0.00032875046502643576   Iteration 16 of 100, tot loss = 4.287847183644772, l1: 0.00010094124149873096, l2: 0.0003278434842286515   Iteration 17 of 100, tot loss = 4.364209111999063, l1: 0.00010266076659557738, l2: 0.0003337601519888267   Iteration 18 of 100, tot loss = 4.311608400609758, l1: 0.00010142030906637147, l2: 0.00032974053707827505   Iteration 19 of 100, tot loss = 4.228644502790351, l1: 0.00010040890150489041, l2: 0.0003224555556172211   Iteration 20 of 100, tot loss = 4.169475895166397, l1: 9.941444823198253e-05, l2: 0.0003175331468810327   Iteration 21 of 100, tot loss = 4.278918816929772, l1: 0.00010118470613850237, l2: 0.00032670718168706765   Iteration 22 of 100, tot loss = 4.2277284481308675, l1: 0.00010033464737220625, l2: 0.0003224382023538717   Iteration 23 of 100, tot loss = 4.13935447257498, l1: 9.875613819610368e-05, l2: 0.0003151793137911464   Iteration 24 of 100, tot loss = 4.143050268292427, l1: 9.879783935199764e-05, l2: 0.0003155071917717578   Iteration 25 of 100, tot loss = 4.187324843406677, l1: 9.902283156407066e-05, l2: 0.00031970965734217315   Iteration 26 of 100, tot loss = 4.180417001247406, l1: 9.834098720882769e-05, l2: 0.00031970071778722253   Iteration 27 of 100, tot loss = 4.210750999274077, l1: 9.885144673818204e-05, l2: 0.00032222365768609087   Iteration 28 of 100, tot loss = 4.222547680139542, l1: 9.961108805457895e-05, l2: 0.00032264368357053693   Iteration 29 of 100, tot loss = 4.158623724148192, l1: 9.85446349659469e-05, l2: 0.0003173177407508523   Iteration 30 of 100, tot loss = 4.114935982227325, l1: 9.767198704745776e-05, l2: 0.00031382161396322774   Iteration 31 of 100, tot loss = 4.106895466004649, l1: 9.789117863082567e-05, l2: 0.00031279837064850596   Iteration 32 of 100, tot loss = 4.155653167515993, l1: 9.885663155273505e-05, l2: 0.0003167086892972293   Iteration 33 of 100, tot loss = 4.1982804392323345, l1: 9.909679879042828e-05, l2: 0.00032073124990953755   Iteration 34 of 100, tot loss = 4.287566917784074, l1: 0.00010044553014535374, l2: 0.0003283111663596869   Iteration 35 of 100, tot loss = 4.3445756128856114, l1: 0.00010069972808456182, l2: 0.00033375783865007436   Iteration 36 of 100, tot loss = 4.353609922859404, l1: 0.00010067143668695482, l2: 0.0003346895613908095   Iteration 37 of 100, tot loss = 4.358195269430006, l1: 0.00010046724010199132, l2: 0.00033535229276576256   Iteration 38 of 100, tot loss = 4.449810796662381, l1: 0.00010174492078811837, l2: 0.00034323616426646416   Iteration 39 of 100, tot loss = 4.556653771644983, l1: 0.00010318562654458966, l2: 0.00035247975514115143   Iteration 40 of 100, tot loss = 4.625267305970192, l1: 0.00010455079473103979, l2: 0.0003579759406420635   Iteration 41 of 100, tot loss = 4.6250983592940536, l1: 0.00010498905018122481, l2: 0.000357520790336222   Iteration 42 of 100, tot loss = 4.587579616478512, l1: 0.00010388398515628761, l2: 0.0003548739807142521   Iteration 43 of 100, tot loss = 4.643516709638196, l1: 0.00010474491568328271, l2: 0.0003596067595766684   Iteration 44 of 100, tot loss = 4.617099732160568, l1: 0.00010467933258372464, l2: 0.00035703064449163236   Iteration 45 of 100, tot loss = 4.596391134791904, l1: 0.00010449277817517415, l2: 0.0003551463392795995   Iteration 46 of 100, tot loss = 4.60006375675616, l1: 0.0001048835850309368, l2: 0.00035512279472364435   Iteration 47 of 100, tot loss = 4.5627872512695635, l1: 0.00010372845149794692, l2: 0.0003525502775409358   Iteration 48 of 100, tot loss = 4.5721373582879705, l1: 0.00010360533353074668, l2: 0.000353608406536902   Iteration 49 of 100, tot loss = 4.630512940640352, l1: 0.00010449817065299223, l2: 0.00035855312812693263   Iteration 50 of 100, tot loss = 4.633745882511139, l1: 0.00010387557013018523, l2: 0.00035949902317952367   Iteration 51 of 100, tot loss = 4.6403654907263965, l1: 0.00010420809385501415, l2: 0.0003598284594915515   Iteration 52 of 100, tot loss = 4.586897985293315, l1: 0.00010337550926264017, l2: 0.00035531429351137305   Iteration 53 of 100, tot loss = 4.577620045194086, l1: 0.00010326908989743558, l2: 0.0003544929185289152   Iteration 54 of 100, tot loss = 4.578179516174175, l1: 0.00010308698889545028, l2: 0.0003547309668782098   Iteration 55 of 100, tot loss = 4.5844199939207595, l1: 0.0001032290350518782, l2: 0.0003552129690598866   Iteration 56 of 100, tot loss = 4.611663186124393, l1: 0.00010358649264422379, l2: 0.0003575798309611855   Iteration 57 of 100, tot loss = 4.623900654023154, l1: 0.00010363609499906663, l2: 0.0003587539747394131   Iteration 58 of 100, tot loss = 4.602218391566441, l1: 0.00010345726896675529, l2: 0.00035676457477605035   Iteration 59 of 100, tot loss = 4.622869323875944, l1: 0.00010338573301234677, l2: 0.0003589012036573584   Iteration 60 of 100, tot loss = 4.638629374901454, l1: 0.00010353761742104931, l2: 0.00036032532467894876   Iteration 61 of 100, tot loss = 4.633025038437765, l1: 0.0001031774198531448, l2: 0.0003601250889497336   Iteration 62 of 100, tot loss = 4.63283892023948, l1: 0.00010333287178993135, l2: 0.0003599510251191415   Iteration 63 of 100, tot loss = 4.637963682886154, l1: 0.00010372078845865049, l2: 0.00036007558487148746   Iteration 64 of 100, tot loss = 4.622826239094138, l1: 0.00010353672576002282, l2: 0.0003587459032132756   Iteration 65 of 100, tot loss = 4.658371013861436, l1: 0.00010420727832332397, l2: 0.00036162982802264966   Iteration 66 of 100, tot loss = 4.648455596331394, l1: 0.000104500469068157, l2: 0.00036034509530784845   Iteration 67 of 100, tot loss = 4.640747509785553, l1: 0.00010433202444787708, l2: 0.0003597427315057825   Iteration 68 of 100, tot loss = 4.629807687857571, l1: 0.0001041947488214218, l2: 0.000358786024769549   Iteration 69 of 100, tot loss = 4.620781682539677, l1: 0.0001039255377321936, l2: 0.00035815263517956805   Iteration 70 of 100, tot loss = 4.601861669336047, l1: 0.00010381688688149942, l2: 0.00035636928473000546   Iteration 71 of 100, tot loss = 4.584990509798829, l1: 0.00010310495802091474, l2: 0.0003553940976058251   Iteration 72 of 100, tot loss = 4.608178279466099, l1: 0.0001032517598105187, l2: 0.0003575660727316669   Iteration 73 of 100, tot loss = 4.624720098221139, l1: 0.00010384932964320984, l2: 0.0003586226850205854   Iteration 74 of 100, tot loss = 4.6137526566917835, l1: 0.00010404877915838419, l2: 0.00035732649140865415   Iteration 75 of 100, tot loss = 4.572097590764364, l1: 0.00010317162392311729, l2: 0.000354038139921613   Iteration 76 of 100, tot loss = 4.605263844916695, l1: 0.0001040202475786301, l2: 0.00035650614173439525   Iteration 77 of 100, tot loss = 4.657317591952039, l1: 0.00010460018335846132, l2: 0.00036113158033936976   Iteration 78 of 100, tot loss = 4.666123326008137, l1: 0.00010485593086811535, l2: 0.00036175640576775785   Iteration 79 of 100, tot loss = 4.648796401446378, l1: 0.00010439467389258863, l2: 0.00036048497050511356   Iteration 80 of 100, tot loss = 4.660652673244476, l1: 0.00010444780841680767, l2: 0.00036161746302241224   Iteration 81 of 100, tot loss = 4.6414860560570235, l1: 0.0001041011322583618, l2: 0.00036004747739809074   Iteration 82 of 100, tot loss = 4.621820824902232, l1: 0.00010372667397527171, l2: 0.0003584554126463467   Iteration 83 of 100, tot loss = 4.618315923644836, l1: 0.00010349861570074313, l2: 0.00035833298082811284   Iteration 84 of 100, tot loss = 4.612030253523872, l1: 0.00010364006980775489, l2: 0.00035756295948799347   Iteration 85 of 100, tot loss = 4.6632844672483555, l1: 0.00010427934034583707, l2: 0.0003620491109733634   Iteration 86 of 100, tot loss = 4.662841311720914, l1: 0.00010404244782220367, l2: 0.00036224168772690084   Iteration 87 of 100, tot loss = 4.680366557219933, l1: 0.00010455661807403278, l2: 0.0003634800421256015   Iteration 88 of 100, tot loss = 4.71393646164374, l1: 0.00010477009664100478, l2: 0.0003666235537490469   Iteration 89 of 100, tot loss = 4.700725871525454, l1: 0.00010476805296042421, l2: 0.0003653045385955634   Iteration 90 of 100, tot loss = 4.72811434533861, l1: 0.00010511421023693402, l2: 0.00036769722840593506   Iteration 91 of 100, tot loss = 4.7556000489455, l1: 0.00010555602740758651, l2: 0.0003700039818355875   Iteration 92 of 100, tot loss = 4.76480658158012, l1: 0.00010577019380118303, l2: 0.00037071046845876083   Iteration 93 of 100, tot loss = 4.749641080056468, l1: 0.00010551696546064059, l2: 0.0003694471466900801   Iteration 94 of 100, tot loss = 4.7628414935254035, l1: 0.00010584443299058933, l2: 0.00037043972004036557   Iteration 95 of 100, tot loss = 4.817525286423533, l1: 0.00010671939844063385, l2: 0.00037503313414115265   Iteration 96 of 100, tot loss = 4.805895308653514, l1: 0.00010656101157261826, l2: 0.00037402852315911633   Iteration 97 of 100, tot loss = 4.795759124854176, l1: 0.00010666218324481586, l2: 0.0003729137329783137   Iteration 98 of 100, tot loss = 4.795625713406777, l1: 0.00010677377409889714, l2: 0.0003727888008365788   Iteration 99 of 100, tot loss = 4.821924173470699, l1: 0.00010708875370163687, l2: 0.00037510366728200076   Iteration 100 of 100, tot loss = 4.8581635594367985, l1: 0.0001077239440564881, l2: 0.00037809241533977913
   End of epoch 1155; saving model... 

Epoch 1156 of 2000
   Iteration 1 of 100, tot loss = 5.927304744720459, l1: 0.00013895121810492128, l2: 0.00045377924107015133   Iteration 2 of 100, tot loss = 5.406585454940796, l1: 0.00012492700989241712, l2: 0.000415731527027674   Iteration 3 of 100, tot loss = 4.679628054300944, l1: 0.0001124152234600236, l2: 0.0003555475753576805   Iteration 4 of 100, tot loss = 5.84734034538269, l1: 0.00012761961988871917, l2: 0.00045711440179729834   Iteration 5 of 100, tot loss = 5.777537441253662, l1: 0.00012911977537442, l2: 0.0004486339574214071   Iteration 6 of 100, tot loss = 5.684500376383464, l1: 0.00012763310102551864, l2: 0.0004408169285549472   Iteration 7 of 100, tot loss = 5.542242595127651, l1: 0.00011987920359907938, l2: 0.0004343450501827257   Iteration 8 of 100, tot loss = 5.735913872718811, l1: 0.00012081601744284853, l2: 0.0004527753626462072   Iteration 9 of 100, tot loss = 5.610258314344618, l1: 0.00011958379683265876, l2: 0.0004414420270930148   Iteration 10 of 100, tot loss = 5.510649919509888, l1: 0.00011730494879884646, l2: 0.0004337600345024839   Iteration 11 of 100, tot loss = 5.390025702389804, l1: 0.00011295280141480775, l2: 0.00042604976227845657   Iteration 12 of 100, tot loss = 5.326789339383443, l1: 0.00011343581960924591, l2: 0.00041924310789909214   Iteration 13 of 100, tot loss = 5.2272933813241815, l1: 0.00011101529749934204, l2: 0.0004117140353908046   Iteration 14 of 100, tot loss = 5.031040940965925, l1: 0.00010847227011774001, l2: 0.0003946318205895035   Iteration 15 of 100, tot loss = 5.021973292032878, l1: 0.00010765977979948123, l2: 0.0003945375472540036   Iteration 16 of 100, tot loss = 5.072638511657715, l1: 0.00010911619847320253, l2: 0.0003981476484113955   Iteration 17 of 100, tot loss = 5.076754037071677, l1: 0.00011089756170643822, l2: 0.00039677783720167903   Iteration 18 of 100, tot loss = 5.194230318069458, l1: 0.00011163572425074462, l2: 0.0004077873042357775   Iteration 19 of 100, tot loss = 5.121610001513832, l1: 0.00011114798958021168, l2: 0.0004010130076877479   Iteration 20 of 100, tot loss = 5.045413410663604, l1: 0.00011004617117578163, l2: 0.00039449516698368824   Iteration 21 of 100, tot loss = 4.997947931289673, l1: 0.00011048072822935258, l2: 0.0003893140632758981   Iteration 22 of 100, tot loss = 4.880731669339267, l1: 0.00010859905689192766, l2: 0.000379474108657715   Iteration 23 of 100, tot loss = 4.987263057542884, l1: 0.00011043441760272759, l2: 0.0003882918845983627   Iteration 24 of 100, tot loss = 4.924700806538264, l1: 0.00010808519179287639, l2: 0.0003843848850616875   Iteration 25 of 100, tot loss = 4.88655499458313, l1: 0.00010750787871074863, l2: 0.00038114761700853706   Iteration 26 of 100, tot loss = 4.838325454638555, l1: 0.00010547961532071125, l2: 0.0003783529268380684   Iteration 27 of 100, tot loss = 4.892056262051618, l1: 0.00010583877870144702, l2: 0.00038336684465994713   Iteration 28 of 100, tot loss = 4.94583318063191, l1: 0.00010660187724300028, l2: 0.0003879814373379174   Iteration 29 of 100, tot loss = 4.962996491070451, l1: 0.00010616829629349201, l2: 0.0003901313506628804   Iteration 30 of 100, tot loss = 4.983072336514791, l1: 0.00010644571605856375, l2: 0.0003918615150420616   Iteration 31 of 100, tot loss = 4.986599022342313, l1: 0.00010619374734233133, l2: 0.0003924661535664552   Iteration 32 of 100, tot loss = 4.974097080528736, l1: 0.00010595636524612928, l2: 0.0003914533417628263   Iteration 33 of 100, tot loss = 5.027000983556111, l1: 0.00010598649764274755, l2: 0.0003967135994709238   Iteration 34 of 100, tot loss = 5.050841647035935, l1: 0.00010664108445138588, l2: 0.0003984430796288721   Iteration 35 of 100, tot loss = 5.060097415106637, l1: 0.00010663492295342232, l2: 0.0003993748179969511   Iteration 36 of 100, tot loss = 5.040448897414738, l1: 0.00010666134251853994, l2: 0.00039738354583581287   Iteration 37 of 100, tot loss = 5.077251427882427, l1: 0.0001067498375738757, l2: 0.0004009753040893859   Iteration 38 of 100, tot loss = 5.058397738557113, l1: 0.00010662652197137696, l2: 0.0003992132513207923   Iteration 39 of 100, tot loss = 5.096539527941973, l1: 0.00010757281555785224, l2: 0.0004020811376484254   Iteration 40 of 100, tot loss = 5.123735815286636, l1: 0.0001078770995263767, l2: 0.0004044964829517994   Iteration 41 of 100, tot loss = 5.160808615568207, l1: 0.00010799798891218038, l2: 0.00040808287321408165   Iteration 42 of 100, tot loss = 5.180451557749794, l1: 0.00010868587690181033, l2: 0.0004093592793824861   Iteration 43 of 100, tot loss = 5.138336231542188, l1: 0.00010776450391520399, l2: 0.00040606911995361535   Iteration 44 of 100, tot loss = 5.154358338225972, l1: 0.00010779203107218068, l2: 0.0004076438027285886   Iteration 45 of 100, tot loss = 5.146062601937188, l1: 0.00010764386590583147, l2: 0.00040696239383477303   Iteration 46 of 100, tot loss = 5.1390153376952465, l1: 0.00010735494122301411, l2: 0.0004065465915988645   Iteration 47 of 100, tot loss = 5.109653691028027, l1: 0.00010750958384225204, l2: 0.00040345578429665653   Iteration 48 of 100, tot loss = 5.068308149774869, l1: 0.00010640333865315672, l2: 0.00040042747544551577   Iteration 49 of 100, tot loss = 5.07435071225069, l1: 0.00010631702374369952, l2: 0.00040111804680366604   Iteration 50 of 100, tot loss = 5.108306317329407, l1: 0.00010634455240506213, l2: 0.0004044860776048154   Iteration 51 of 100, tot loss = 5.079698338228114, l1: 0.0001056257304512923, l2: 0.0004023441014236168   Iteration 52 of 100, tot loss = 5.083846119733957, l1: 0.00010573868453670562, l2: 0.0004026459260225798   Iteration 53 of 100, tot loss = 5.075350770410502, l1: 0.00010531746326860059, l2: 0.0004022176122679463   Iteration 54 of 100, tot loss = 5.0482551786634655, l1: 0.00010474057146186371, l2: 0.0004000849445152131   Iteration 55 of 100, tot loss = 5.048640632629395, l1: 0.00010474131512720222, l2: 0.0004001227461478927   Iteration 56 of 100, tot loss = 5.072711757251194, l1: 0.00010525191471190607, l2: 0.00040201925938682895   Iteration 57 of 100, tot loss = 5.0547202177215045, l1: 0.0001044395257690268, l2: 0.00040103249474461274   Iteration 58 of 100, tot loss = 5.074491221329262, l1: 0.00010432826569865875, l2: 0.0004031208553552178   Iteration 59 of 100, tot loss = 5.030845278400486, l1: 0.00010400236339083383, l2: 0.00039908216311201704   Iteration 60 of 100, tot loss = 5.017810066541036, l1: 0.00010371351333257431, l2: 0.00039806749215737605   Iteration 61 of 100, tot loss = 5.047214328265581, l1: 0.0001039961883731446, l2: 0.0004007252430367726   Iteration 62 of 100, tot loss = 5.024374627297925, l1: 0.00010386913779638558, l2: 0.0003985683236556548   Iteration 63 of 100, tot loss = 5.007255584474594, l1: 0.00010386229867378013, l2: 0.0003968632585066001   Iteration 64 of 100, tot loss = 5.007992222905159, l1: 0.00010394997377716209, l2: 0.0003968492478634289   Iteration 65 of 100, tot loss = 4.965651691876925, l1: 0.00010319712002036305, l2: 0.0003933680485575818   Iteration 66 of 100, tot loss = 4.9600845719828754, l1: 0.00010302012351255117, l2: 0.00039298833342565393   Iteration 67 of 100, tot loss = 4.971135477521527, l1: 0.00010326602742720548, l2: 0.0003938475196291484   Iteration 68 of 100, tot loss = 4.9727005713126236, l1: 0.000103519029193301, l2: 0.0003937510277062435   Iteration 69 of 100, tot loss = 4.971206502638001, l1: 0.000103415292065567, l2: 0.0003937053581888693   Iteration 70 of 100, tot loss = 4.953114295005799, l1: 0.00010322750193673918, l2: 0.0003920839276231293   Iteration 71 of 100, tot loss = 4.93673781609871, l1: 0.00010310966228618628, l2: 0.0003905641193777113   Iteration 72 of 100, tot loss = 4.931165254778332, l1: 0.00010306170431956869, l2: 0.00039005482095591206   Iteration 73 of 100, tot loss = 4.970002432392068, l1: 0.00010339350066640516, l2: 0.000393606742687702   Iteration 74 of 100, tot loss = 4.941855952546403, l1: 0.00010315115586972506, l2: 0.00039103443929422145   Iteration 75 of 100, tot loss = 4.933030789693197, l1: 0.00010310230994946323, l2: 0.00039020076859742404   Iteration 76 of 100, tot loss = 4.9081029954709505, l1: 0.00010272451268783218, l2: 0.00038808578641405083   Iteration 77 of 100, tot loss = 4.909832564267245, l1: 0.00010291965079815852, l2: 0.00038806360548319805   Iteration 78 of 100, tot loss = 4.926865241466424, l1: 0.00010336663702522249, l2: 0.00038931988689373847   Iteration 79 of 100, tot loss = 4.922707823258412, l1: 0.00010325650477677266, l2: 0.00038901427698091756   Iteration 80 of 100, tot loss = 4.936730879545212, l1: 0.00010323126775801938, l2: 0.00039044181958161064   Iteration 81 of 100, tot loss = 4.915022070025221, l1: 0.00010300686793656226, l2: 0.0003884953384166928   Iteration 82 of 100, tot loss = 4.921105899461886, l1: 0.00010323512412092319, l2: 0.00038887546561011   Iteration 83 of 100, tot loss = 4.90335787922503, l1: 0.00010284542773902158, l2: 0.00038749035987834706   Iteration 84 of 100, tot loss = 4.916487307775588, l1: 0.00010266150392063926, l2: 0.0003889872262204465   Iteration 85 of 100, tot loss = 4.919957581688376, l1: 0.00010277738761300605, l2: 0.0003892183697590714   Iteration 86 of 100, tot loss = 4.944995170415834, l1: 0.00010354154957979539, l2: 0.0003909579668394398   Iteration 87 of 100, tot loss = 4.950420198769405, l1: 0.0001039156272834108, l2: 0.00039112639222722286   Iteration 88 of 100, tot loss = 4.976299535144459, l1: 0.00010457079162568499, l2: 0.00039305916156577456   Iteration 89 of 100, tot loss = 4.970855996849831, l1: 0.00010456745202942186, l2: 0.0003925181475331944   Iteration 90 of 100, tot loss = 4.961829932530721, l1: 0.0001043000262951763, l2: 0.00039188296715211537   Iteration 91 of 100, tot loss = 4.974473628369006, l1: 0.000104776181235518, l2: 0.0003926711816437911   Iteration 92 of 100, tot loss = 4.973694215650144, l1: 0.00010506656101073452, l2: 0.00039230286062735576   Iteration 93 of 100, tot loss = 4.969420756063154, l1: 0.00010523973496772238, l2: 0.0003917023407480609   Iteration 94 of 100, tot loss = 4.957063545571997, l1: 0.00010517790093565945, l2: 0.0003905284538632259   Iteration 95 of 100, tot loss = 4.951787348797446, l1: 0.00010515272630360222, l2: 0.0003900260088573161   Iteration 96 of 100, tot loss = 4.940029561519623, l1: 0.0001050815316148146, l2: 0.0003889214249284123   Iteration 97 of 100, tot loss = 4.924751957667243, l1: 0.00010514454885365528, l2: 0.00038733064738875157   Iteration 98 of 100, tot loss = 4.954048421918129, l1: 0.00010584512655237185, l2: 0.00038955971638420216   Iteration 99 of 100, tot loss = 4.93154603062254, l1: 0.00010554505919424977, l2: 0.00038760954486511914   Iteration 100 of 100, tot loss = 4.9315657353401186, l1: 0.00010569746631517773, l2: 0.00038745910846046174
   End of epoch 1156; saving model... 

Epoch 1157 of 2000
   Iteration 1 of 100, tot loss = 5.72107458114624, l1: 0.00011664284102153033, l2: 0.0004554646147880703   Iteration 2 of 100, tot loss = 5.6221301555633545, l1: 0.00011524981528054923, l2: 0.00044696321128867567   Iteration 3 of 100, tot loss = 5.411864598592122, l1: 0.00011322437785565853, l2: 0.00042796210618689656   Iteration 4 of 100, tot loss = 5.5653979778289795, l1: 0.00011993936277576722, l2: 0.00043660045048454776   Iteration 5 of 100, tot loss = 5.01366491317749, l1: 0.00011057753581553698, l2: 0.00039078896807041017   Iteration 6 of 100, tot loss = 4.70076851050059, l1: 0.00010790538120393951, l2: 0.00036217147620239604   Iteration 7 of 100, tot loss = 4.992328132901873, l1: 0.00011428085105892803, l2: 0.0003849519624574376   Iteration 8 of 100, tot loss = 4.914981633424759, l1: 0.00011323514991090633, l2: 0.000378263011953095   Iteration 9 of 100, tot loss = 4.893451982074314, l1: 0.00011503004740613203, l2: 0.00037431514809011586   Iteration 10 of 100, tot loss = 4.735503220558167, l1: 0.00011171127334819175, l2: 0.00036183904740028083   Iteration 11 of 100, tot loss = 4.808561736887151, l1: 0.00010950418006607585, l2: 0.0003713519942142408   Iteration 12 of 100, tot loss = 4.628235777219136, l1: 0.00010532095469291865, l2: 0.0003575026227432924   Iteration 13 of 100, tot loss = 4.458530444365281, l1: 0.00010231926297330155, l2: 0.0003435337813033794   Iteration 14 of 100, tot loss = 4.54868083340781, l1: 0.0001034346363927138, l2: 0.00035143344887598815   Iteration 15 of 100, tot loss = 4.602880462010702, l1: 0.00010484470719044718, l2: 0.0003554433409590274   Iteration 16 of 100, tot loss = 4.501148000359535, l1: 0.00010205453099842998, l2: 0.0003480602717900183   Iteration 17 of 100, tot loss = 4.509426130967982, l1: 0.00010097323769621332, l2: 0.00034996937713859715   Iteration 18 of 100, tot loss = 4.621344606081645, l1: 0.00010358543497406774, l2: 0.00035854902752261196   Iteration 19 of 100, tot loss = 4.546985877187629, l1: 0.00010145298859386362, l2: 0.0003532456007393959   Iteration 20 of 100, tot loss = 4.4601389288902284, l1: 0.00010044326882052701, l2: 0.00034557062535895967   Iteration 21 of 100, tot loss = 4.570275181815738, l1: 0.00010256615149999215, l2: 0.0003544613684339094   Iteration 22 of 100, tot loss = 4.562416932799599, l1: 0.00010244253512196751, l2: 0.00035379915855380454   Iteration 23 of 100, tot loss = 4.507010211115298, l1: 0.00010201080800945182, l2: 0.0003486902126536259   Iteration 24 of 100, tot loss = 4.528687139352162, l1: 0.00010217423065720747, l2: 0.0003506944825251897   Iteration 25 of 100, tot loss = 4.5041161632537845, l1: 0.00010228062310488895, l2: 0.0003481309930793941   Iteration 26 of 100, tot loss = 4.503766545882592, l1: 0.0001025615518021648, l2: 0.0003478151025214734   Iteration 27 of 100, tot loss = 4.555378516515096, l1: 0.000103820606589714, l2: 0.00035171724502342167   Iteration 28 of 100, tot loss = 4.480397428785052, l1: 0.00010207172103296866, l2: 0.00034596802184491286   Iteration 29 of 100, tot loss = 4.510339967135725, l1: 0.00010212634051327016, l2: 0.0003489076555305129   Iteration 30 of 100, tot loss = 4.5149936040242515, l1: 0.00010222039503181198, l2: 0.00034927896534403167   Iteration 31 of 100, tot loss = 4.468155268699892, l1: 0.00010089104738830017, l2: 0.00034592447956393083   Iteration 32 of 100, tot loss = 4.501067735254765, l1: 0.00010223283629784419, l2: 0.00034787393633450847   Iteration 33 of 100, tot loss = 4.56904032013633, l1: 0.00010284287991728915, l2: 0.0003540611518970267   Iteration 34 of 100, tot loss = 4.6031637542388015, l1: 0.00010353638871867374, l2: 0.0003567799850476577   Iteration 35 of 100, tot loss = 4.618123347418649, l1: 0.00010274629000507827, l2: 0.0003590660435812814   Iteration 36 of 100, tot loss = 4.614181419213613, l1: 0.00010252799782190575, l2: 0.00035889014300967875   Iteration 37 of 100, tot loss = 4.596769403766942, l1: 0.00010272391463749463, l2: 0.00035695302476351326   Iteration 38 of 100, tot loss = 4.725667219412954, l1: 0.00010473584916326217, l2: 0.00036783087048924675   Iteration 39 of 100, tot loss = 4.744732227080908, l1: 0.00010515274203871973, l2: 0.0003693204780574888   Iteration 40 of 100, tot loss = 4.727076584100724, l1: 0.00010420774360682117, l2: 0.0003684999122924637   Iteration 41 of 100, tot loss = 4.7190622643726625, l1: 0.00010429026073458173, l2: 0.00036761596324717306   Iteration 42 of 100, tot loss = 4.7056367340542025, l1: 0.0001042233977516714, l2: 0.000366340273536653   Iteration 43 of 100, tot loss = 4.7005601539168245, l1: 0.00010451225907038247, l2: 0.0003655437545222772   Iteration 44 of 100, tot loss = 4.67290968244726, l1: 0.00010402497570215597, l2: 0.00036326599100985646   Iteration 45 of 100, tot loss = 4.660126166873508, l1: 0.00010382936258489887, l2: 0.0003621832532290783   Iteration 46 of 100, tot loss = 4.703438064326411, l1: 0.0001045111294724452, l2: 0.0003658326762809378   Iteration 47 of 100, tot loss = 4.738696280946123, l1: 0.00010512276407717629, l2: 0.00036874686323225183   Iteration 48 of 100, tot loss = 4.761164277791977, l1: 0.00010541809069763985, l2: 0.00037069833585216355   Iteration 49 of 100, tot loss = 4.769561738384013, l1: 0.00010523589937329977, l2: 0.00037172027419283226   Iteration 50 of 100, tot loss = 4.729324812889099, l1: 0.00010489390377188101, l2: 0.0003680385774350725   Iteration 51 of 100, tot loss = 4.68682414410161, l1: 0.00010386883656667801, l2: 0.00036481357814159755   Iteration 52 of 100, tot loss = 4.698644729760977, l1: 0.00010325027995769723, l2: 0.00036661419345853996   Iteration 53 of 100, tot loss = 4.6851404882826895, l1: 0.00010352969328314944, l2: 0.0003649843560162721   Iteration 54 of 100, tot loss = 4.6289695964919195, l1: 0.00010263862235728152, l2: 0.0003602583376589421   Iteration 55 of 100, tot loss = 4.648554322936318, l1: 0.0001029046331082513, l2: 0.0003619507998915982   Iteration 56 of 100, tot loss = 4.699053489736149, l1: 0.00010352301478633308, l2: 0.00036638233476488883   Iteration 57 of 100, tot loss = 4.774418192997313, l1: 0.0001047295075917837, l2: 0.00037271231211203135   Iteration 58 of 100, tot loss = 4.756432925832683, l1: 0.00010450463422126507, l2: 0.00037113865870071157   Iteration 59 of 100, tot loss = 4.746268941184222, l1: 0.00010451475932544997, l2: 0.0003701121349260606   Iteration 60 of 100, tot loss = 4.771866470575333, l1: 0.00010454862137218393, l2: 0.00037263802526770937   Iteration 61 of 100, tot loss = 4.757392799268003, l1: 0.00010456920981447243, l2: 0.00037117006954236995   Iteration 62 of 100, tot loss = 4.77393719650084, l1: 0.00010494509344073688, l2: 0.0003724486249989094   Iteration 63 of 100, tot loss = 4.808063365164257, l1: 0.0001055284164543818, l2: 0.0003752779186239451   Iteration 64 of 100, tot loss = 4.83858216740191, l1: 0.00010575926040701233, l2: 0.00037809895513873926   Iteration 65 of 100, tot loss = 4.824556594628554, l1: 0.00010575885563202274, l2: 0.0003766968028055719   Iteration 66 of 100, tot loss = 4.857606109344598, l1: 0.00010628543603877804, l2: 0.0003794751737035712   Iteration 67 of 100, tot loss = 4.863625869822147, l1: 0.00010616646065513964, l2: 0.00038019612524050304   Iteration 68 of 100, tot loss = 4.876665692119038, l1: 0.00010637118478317622, l2: 0.00038129538274113336   Iteration 69 of 100, tot loss = 4.86320446021315, l1: 0.0001058286691039794, l2: 0.0003804917751172004   Iteration 70 of 100, tot loss = 4.862777895586831, l1: 0.00010572141935491735, l2: 0.0003805563682232917   Iteration 71 of 100, tot loss = 4.8508097235585605, l1: 0.00010584187086197732, l2: 0.00037923909965197517   Iteration 72 of 100, tot loss = 4.845429912209511, l1: 0.00010565314576322433, l2: 0.00037888984348278935   Iteration 73 of 100, tot loss = 4.852826414042956, l1: 0.00010565553130207214, l2: 0.0003796271081377548   Iteration 74 of 100, tot loss = 4.837099044709592, l1: 0.00010560805502471929, l2: 0.00037810184752546576   Iteration 75 of 100, tot loss = 4.8430306323369345, l1: 0.00010596003611378061, l2: 0.00037834302531943346   Iteration 76 of 100, tot loss = 4.8465021550655365, l1: 0.00010609034387399381, l2: 0.0003785598705450722   Iteration 77 of 100, tot loss = 4.849637305581725, l1: 0.00010620210439385544, l2: 0.00037876162506026134   Iteration 78 of 100, tot loss = 4.863957932362189, l1: 0.00010639439759147652, l2: 0.0003800013947534936   Iteration 79 of 100, tot loss = 4.848913806903211, l1: 0.00010644593534886519, l2: 0.0003784454444790247   Iteration 80 of 100, tot loss = 4.8323142871260645, l1: 0.00010623176644912747, l2: 0.00037699966169384423   Iteration 81 of 100, tot loss = 4.854460090766718, l1: 0.00010666508181056548, l2: 0.00037878092687740854   Iteration 82 of 100, tot loss = 4.883891110013171, l1: 0.00010716828018769042, l2: 0.00038122083104809   Iteration 83 of 100, tot loss = 4.852977041738579, l1: 0.00010659380973539741, l2: 0.00037870389461609907   Iteration 84 of 100, tot loss = 4.8164576930659155, l1: 0.00010578778151934134, l2: 0.00037585798807932794   Iteration 85 of 100, tot loss = 4.808350648599513, l1: 0.00010574025847531362, l2: 0.0003750948068473543   Iteration 86 of 100, tot loss = 4.79676841719206, l1: 0.00010546708523010196, l2: 0.00037420975684992385   Iteration 87 of 100, tot loss = 4.798333657198939, l1: 0.0001054571321533174, l2: 0.000374376234326153   Iteration 88 of 100, tot loss = 4.77696458724412, l1: 0.0001048101724915217, l2: 0.0003728862870544121   Iteration 89 of 100, tot loss = 4.800946470057027, l1: 0.00010511890785268958, l2: 0.000374975740230051   Iteration 90 of 100, tot loss = 4.821185412671831, l1: 0.00010527952003435025, l2: 0.00037683902232351507   Iteration 91 of 100, tot loss = 4.816109540698292, l1: 0.00010532565447847246, l2: 0.0003762853004525743   Iteration 92 of 100, tot loss = 4.810170390035795, l1: 0.0001050675373730685, l2: 0.00037594950252083515   Iteration 93 of 100, tot loss = 4.813061953872762, l1: 0.00010519415316652627, l2: 0.00037611204286178536   Iteration 94 of 100, tot loss = 4.8015692880813114, l1: 0.00010483287459987394, l2: 0.0003753240549834098   Iteration 95 of 100, tot loss = 4.783334876361646, l1: 0.00010455888933440866, l2: 0.00037377459896225973   Iteration 96 of 100, tot loss = 4.767554850627978, l1: 0.00010430150920607655, l2: 0.0003724539764486205   Iteration 97 of 100, tot loss = 4.773933446284422, l1: 0.00010434369691323547, l2: 0.00037304964839161576   Iteration 98 of 100, tot loss = 4.767459875466872, l1: 0.0001040506005145809, l2: 0.0003726953875638155   Iteration 99 of 100, tot loss = 4.7476516239570845, l1: 0.0001038085622830561, l2: 0.0003709566008183174   Iteration 100 of 100, tot loss = 4.763584393262863, l1: 0.00010416163131594658, l2: 0.00037219680882117246
   End of epoch 1157; saving model... 

Epoch 1158 of 2000
   Iteration 1 of 100, tot loss = 3.7216029167175293, l1: 9.303908882429823e-05, l2: 0.00027912118821404874   Iteration 2 of 100, tot loss = 5.2170045375823975, l1: 0.00010081557411467656, l2: 0.00042088488407898694   Iteration 3 of 100, tot loss = 5.417754491170247, l1: 0.00010419086902402341, l2: 0.00043758457953420776   Iteration 4 of 100, tot loss = 4.6123270988464355, l1: 9.546277215122245e-05, l2: 0.00036576993443304673   Iteration 5 of 100, tot loss = 4.224282169342041, l1: 8.981605351436883e-05, l2: 0.000332612163037993   Iteration 6 of 100, tot loss = 4.427357912063599, l1: 9.606042294763029e-05, l2: 0.0003466753623797558   Iteration 7 of 100, tot loss = 4.291846479688372, l1: 9.439474940466295e-05, l2: 0.0003347898954026667   Iteration 8 of 100, tot loss = 3.98560631275177, l1: 8.841242697599228e-05, l2: 0.0003101482016063528   Iteration 9 of 100, tot loss = 4.021128495534261, l1: 8.849751005376068e-05, l2: 0.0003136153374927946   Iteration 10 of 100, tot loss = 4.045968675613404, l1: 9.094292581721675e-05, l2: 0.0003136539409752004   Iteration 11 of 100, tot loss = 3.9822015545584937, l1: 9.176599037759429e-05, l2: 0.00030645416435700923   Iteration 12 of 100, tot loss = 3.9732183615366616, l1: 9.128157914043793e-05, l2: 0.0003060402547513756   Iteration 13 of 100, tot loss = 3.8321422980381894, l1: 8.857839678127605e-05, l2: 0.00029463583016947197   Iteration 14 of 100, tot loss = 3.947632363864354, l1: 9.229738809933354e-05, l2: 0.0003024658456394848   Iteration 15 of 100, tot loss = 3.973688014348348, l1: 9.383128247767067e-05, l2: 0.0003035375179024413   Iteration 16 of 100, tot loss = 3.9977002292871475, l1: 9.564634865455446e-05, l2: 0.00030412367323151557   Iteration 17 of 100, tot loss = 3.8816752013038185, l1: 9.233029627734247e-05, l2: 0.00029583722233142266   Iteration 18 of 100, tot loss = 3.8172412051094904, l1: 9.007716875607407e-05, l2: 0.0002916469505483595   Iteration 19 of 100, tot loss = 3.800997759166517, l1: 8.981883021126697e-05, l2: 0.0002902809440121545   Iteration 20 of 100, tot loss = 3.8338759899139405, l1: 8.99838587429258e-05, l2: 0.00029340373803279365   Iteration 21 of 100, tot loss = 3.966670513153076, l1: 9.290891033569573e-05, l2: 0.0003037581385190909   Iteration 22 of 100, tot loss = 4.027635249224576, l1: 9.440601157125043e-05, l2: 0.000308357513197486   Iteration 23 of 100, tot loss = 4.1394660783850625, l1: 9.652084665266675e-05, l2: 0.0003174257612756818   Iteration 24 of 100, tot loss = 4.134718477725983, l1: 9.624640445811868e-05, l2: 0.00031722544311681605   Iteration 25 of 100, tot loss = 4.184446868896484, l1: 9.813168973778375e-05, l2: 0.0003203129972098395   Iteration 26 of 100, tot loss = 4.310605801068819, l1: 9.955700988939498e-05, l2: 0.0003315035691440034   Iteration 27 of 100, tot loss = 4.315829859839545, l1: 9.961217748005529e-05, l2: 0.00033197080753140016   Iteration 28 of 100, tot loss = 4.272641062736511, l1: 9.819370071844398e-05, l2: 0.000329070403885063   Iteration 29 of 100, tot loss = 4.311154233998265, l1: 9.866233700324364e-05, l2: 0.00033245308491288973   Iteration 30 of 100, tot loss = 4.329314470291138, l1: 9.942547937195438e-05, l2: 0.0003335059664095752   Iteration 31 of 100, tot loss = 4.430200715218821, l1: 0.00010101241036130475, l2: 0.00034200765930431624   Iteration 32 of 100, tot loss = 4.358419358730316, l1: 9.96871010556788e-05, l2: 0.0003361548328939534   Iteration 33 of 100, tot loss = 4.386273644187233, l1: 9.97486056417763e-05, l2: 0.00033887875730270576   Iteration 34 of 100, tot loss = 4.351428403573878, l1: 9.898549786371673e-05, l2: 0.0003361573407720939   Iteration 35 of 100, tot loss = 4.345106431416103, l1: 9.940484388997512e-05, l2: 0.00033510579752536225   Iteration 36 of 100, tot loss = 4.353937592771318, l1: 9.97768155583698e-05, l2: 0.00033561694201327756   Iteration 37 of 100, tot loss = 4.348816543012052, l1: 0.00010023548856151064, l2: 0.0003346461638323711   Iteration 38 of 100, tot loss = 4.373591805759229, l1: 0.00010038494503150057, l2: 0.00033697423312237094   Iteration 39 of 100, tot loss = 4.401154096309956, l1: 0.00010100116457742376, l2: 0.00033911424338256416   Iteration 40 of 100, tot loss = 4.428870397806167, l1: 0.00010144216612388846, l2: 0.0003414448718103813   Iteration 41 of 100, tot loss = 4.3899991570449455, l1: 0.00010085595550197291, l2: 0.00033814395868741884   Iteration 42 of 100, tot loss = 4.360538227217538, l1: 0.00010013916073062102, l2: 0.0003359146607441029   Iteration 43 of 100, tot loss = 4.369397335274275, l1: 0.00010054700041384718, l2: 0.00033639273186604127   Iteration 44 of 100, tot loss = 4.370088051665913, l1: 0.00010100934080335058, l2: 0.00033599946288053286   Iteration 45 of 100, tot loss = 4.3633262263404, l1: 0.00010059194319183007, l2: 0.00033574067767606013   Iteration 46 of 100, tot loss = 4.339704912641774, l1: 0.00010029761514211397, l2: 0.0003336728743464767   Iteration 47 of 100, tot loss = 4.317938373443928, l1: 9.965036975274338e-05, l2: 0.0003321434657877073   Iteration 48 of 100, tot loss = 4.318572506308556, l1: 9.975806612298281e-05, l2: 0.00033209918213591055   Iteration 49 of 100, tot loss = 4.360828492106224, l1: 0.00010056028942035853, l2: 0.0003355225570124517   Iteration 50 of 100, tot loss = 4.366434788703918, l1: 0.00010062450615805574, l2: 0.0003360189698287286   Iteration 51 of 100, tot loss = 4.375969554863724, l1: 0.00010050235141534358, l2: 0.0003370946012414517   Iteration 52 of 100, tot loss = 4.420264642972213, l1: 0.0001014559400993256, l2: 0.000340570520795434   Iteration 53 of 100, tot loss = 4.395376115475061, l1: 0.00010053955249429048, l2: 0.0003389980556676164   Iteration 54 of 100, tot loss = 4.366852358535484, l1: 9.991522495477477e-05, l2: 0.00033677000788480253   Iteration 55 of 100, tot loss = 4.3867170897397125, l1: 0.0001007348351281094, l2: 0.0003379368707431819   Iteration 56 of 100, tot loss = 4.452780506440571, l1: 0.00010175093244210334, l2: 0.00034352711450732646   Iteration 57 of 100, tot loss = 4.4256063176874525, l1: 0.00010093188655190987, l2: 0.00034162874171191727   Iteration 58 of 100, tot loss = 4.440294898789505, l1: 0.00010088487736005657, l2: 0.00034314460961091945   Iteration 59 of 100, tot loss = 4.452055324942378, l1: 0.00010060156686665585, l2: 0.0003446039634705442   Iteration 60 of 100, tot loss = 4.438842463493347, l1: 0.00010060850217996631, l2: 0.0003432757422463813   Iteration 61 of 100, tot loss = 4.427813870007874, l1: 0.00010025596315781663, l2: 0.0003425254217887175   Iteration 62 of 100, tot loss = 4.480013082104344, l1: 0.00010099233012653948, l2: 0.0003470089758329484   Iteration 63 of 100, tot loss = 4.473989997591291, l1: 0.00010114637749095906, l2: 0.0003462526201185519   Iteration 64 of 100, tot loss = 4.47512635961175, l1: 0.00010128325357072754, l2: 0.00034622938051143137   Iteration 65 of 100, tot loss = 4.440396158511822, l1: 0.00010068528589237338, l2: 0.00034335432820416126   Iteration 66 of 100, tot loss = 4.441292946988886, l1: 0.00010079835699412577, l2: 0.0003433309362717961   Iteration 67 of 100, tot loss = 4.423875089901597, l1: 0.00010071342032135172, l2: 0.0003416740870687054   Iteration 68 of 100, tot loss = 4.416663236477795, l1: 0.0001001643710932352, l2: 0.0003415019510772682   Iteration 69 of 100, tot loss = 4.444424625756084, l1: 0.00010075026406464504, l2: 0.0003436921966646838   Iteration 70 of 100, tot loss = 4.414434157099042, l1: 0.00010015471629490744, l2: 0.000341288697589854   Iteration 71 of 100, tot loss = 4.40597158082774, l1: 0.00010020663000726876, l2: 0.00034039052618129676   Iteration 72 of 100, tot loss = 4.429266128275129, l1: 0.00010087034100555077, l2: 0.0003420562704074352   Iteration 73 of 100, tot loss = 4.404533111885803, l1: 0.00010048832381758137, l2: 0.0003399649859814305   Iteration 74 of 100, tot loss = 4.397327481089412, l1: 0.0001004247377430737, l2: 0.0003393080091022106   Iteration 75 of 100, tot loss = 4.397668463389079, l1: 0.00010034300207432049, l2: 0.0003394238425729175   Iteration 76 of 100, tot loss = 4.42049347099505, l1: 0.00010063682197812198, l2: 0.0003414125239942223   Iteration 77 of 100, tot loss = 4.46193130914267, l1: 0.00010158365788716859, l2: 0.00034460947204792463   Iteration 78 of 100, tot loss = 4.4768634881728735, l1: 0.00010176816277425342, l2: 0.00034591818537312344   Iteration 79 of 100, tot loss = 4.477768354778048, l1: 0.00010205666943345679, l2: 0.00034572016499358926   Iteration 80 of 100, tot loss = 4.49785418510437, l1: 0.00010250858645122207, l2: 0.0003472768312349217   Iteration 81 of 100, tot loss = 4.500828784189107, l1: 0.00010288403620821263, l2: 0.00034719884121089164   Iteration 82 of 100, tot loss = 4.504134515436684, l1: 0.00010322644629923161, l2: 0.0003471870043474   Iteration 83 of 100, tot loss = 4.494479595896709, l1: 0.00010291823675661034, l2: 0.0003465297218154084   Iteration 84 of 100, tot loss = 4.508531806014833, l1: 0.00010310055703095193, l2: 0.00034775262264745486   Iteration 85 of 100, tot loss = 4.530825146506815, l1: 0.00010347024683998076, l2: 0.0003496122666333309   Iteration 86 of 100, tot loss = 4.515077771142471, l1: 0.00010311362470823698, l2: 0.0003483941515252471   Iteration 87 of 100, tot loss = 4.51757402803706, l1: 0.00010334980340732059, l2: 0.00034840759853350706   Iteration 88 of 100, tot loss = 4.50137040831826, l1: 0.00010296741026071355, l2: 0.00034716962959051176   Iteration 89 of 100, tot loss = 4.491463414738687, l1: 0.0001028288571341251, l2: 0.00034631748328618495   Iteration 90 of 100, tot loss = 4.512769524256388, l1: 0.00010319472005196277, l2: 0.0003480822315193816   Iteration 91 of 100, tot loss = 4.520438817831186, l1: 0.00010327294232690922, l2: 0.0003487709383569801   Iteration 92 of 100, tot loss = 4.52910492212876, l1: 0.00010327514724928693, l2: 0.00034963534388858216   Iteration 93 of 100, tot loss = 4.529237947156353, l1: 0.0001032899236898013, l2: 0.0003496338702520476   Iteration 94 of 100, tot loss = 4.544737014364689, l1: 0.00010347651059032131, l2: 0.00035099719048532204   Iteration 95 of 100, tot loss = 4.5725929109673755, l1: 0.00010373486611091433, l2: 0.0003535244244726767   Iteration 96 of 100, tot loss = 4.613214835524559, l1: 0.00010448897095708769, l2: 0.00035683251235241187   Iteration 97 of 100, tot loss = 4.644566250830581, l1: 0.00010497409754496347, l2: 0.00035948252720043023   Iteration 98 of 100, tot loss = 4.646285563099141, l1: 0.0001052884313789296, l2: 0.00035934012465782425   Iteration 99 of 100, tot loss = 4.661461570046165, l1: 0.00010542965839079563, l2: 0.00036071649869560556   Iteration 100 of 100, tot loss = 4.68699282169342, l1: 0.00010601736063108547, l2: 0.00036268192212446593
   End of epoch 1158; saving model... 

Epoch 1159 of 2000
   Iteration 1 of 100, tot loss = 4.339306831359863, l1: 8.968374459072948e-05, l2: 0.0003442469460424036   Iteration 2 of 100, tot loss = 3.9440231323242188, l1: 9.014426177600399e-05, l2: 0.0003042580501642078   Iteration 3 of 100, tot loss = 4.311853090922038, l1: 0.00010124420320304732, l2: 0.0003299411037005484   Iteration 4 of 100, tot loss = 4.020946681499481, l1: 9.619529009796679e-05, l2: 0.00030589937523473054   Iteration 5 of 100, tot loss = 3.9265795230865477, l1: 9.079577575903386e-05, l2: 0.00030186217045411465   Iteration 6 of 100, tot loss = 4.379759669303894, l1: 9.94934647072417e-05, l2: 0.0003384824958629906   Iteration 7 of 100, tot loss = 4.391856840678623, l1: 0.00010033297024035295, l2: 0.0003388527090594705   Iteration 8 of 100, tot loss = 4.3686725199222565, l1: 0.00010006942920881556, l2: 0.0003367978206370026   Iteration 9 of 100, tot loss = 4.3808719052208795, l1: 9.923928478706835e-05, l2: 0.00033884790415565175   Iteration 10 of 100, tot loss = 4.248695683479309, l1: 9.654188761487603e-05, l2: 0.00032832768047228453   Iteration 11 of 100, tot loss = 4.561438365416094, l1: 0.00010307773746634749, l2: 0.0003530660972253165   Iteration 12 of 100, tot loss = 4.471690793832143, l1: 9.880780665601681e-05, l2: 0.0003483612696678999   Iteration 13 of 100, tot loss = 4.423429415776179, l1: 9.994316524870765e-05, l2: 0.0003423997729371947   Iteration 14 of 100, tot loss = 4.536369153431484, l1: 0.000101012685263413, l2: 0.00035262422586259035   Iteration 15 of 100, tot loss = 4.792629464467367, l1: 0.00010403824877964022, l2: 0.0003752246925917765   Iteration 16 of 100, tot loss = 4.716308459639549, l1: 0.00010402134171272337, l2: 0.0003676095002447255   Iteration 17 of 100, tot loss = 4.666055412853465, l1: 0.00010447817454833592, l2: 0.00036212736364070545   Iteration 18 of 100, tot loss = 4.739517225159539, l1: 0.00010489336374222249, l2: 0.0003690583576422392   Iteration 19 of 100, tot loss = 4.6768981657530135, l1: 0.00010439613891465247, l2: 0.0003632936767596555   Iteration 20 of 100, tot loss = 4.6427342891693115, l1: 0.00010527306421863614, l2: 0.00035900036455132066   Iteration 21 of 100, tot loss = 4.686726797194708, l1: 0.00010595814144055891, l2: 0.0003627145377409068   Iteration 22 of 100, tot loss = 4.6908675757321445, l1: 0.0001048115067202492, l2: 0.00036427525099663234   Iteration 23 of 100, tot loss = 4.743262850719949, l1: 0.00010571261325663806, l2: 0.00036861367002332014   Iteration 24 of 100, tot loss = 4.792687813440959, l1: 0.00010612895994199789, l2: 0.00037313981859673123   Iteration 25 of 100, tot loss = 4.812127132415771, l1: 0.00010647124974639155, l2: 0.0003747414599638432   Iteration 26 of 100, tot loss = 4.757328088466938, l1: 0.00010546810503561013, l2: 0.00037026470034526516   Iteration 27 of 100, tot loss = 4.790018293592665, l1: 0.00010528402304367369, l2: 0.00037371780274918784   Iteration 28 of 100, tot loss = 4.801585180418832, l1: 0.00010492124225233315, l2: 0.000375237271198005   Iteration 29 of 100, tot loss = 4.815558071794181, l1: 0.00010616703479297876, l2: 0.000375388769216129   Iteration 30 of 100, tot loss = 4.773062721888224, l1: 0.00010571843946915275, l2: 0.0003715878294315189   Iteration 31 of 100, tot loss = 4.737325522207445, l1: 0.00010514080126848703, l2: 0.0003685917481855159   Iteration 32 of 100, tot loss = 4.781925700604916, l1: 0.00010621151579925936, l2: 0.0003719810520124156   Iteration 33 of 100, tot loss = 4.767585963913889, l1: 0.00010635755759060636, l2: 0.00037040103625534385   Iteration 34 of 100, tot loss = 4.797879688880023, l1: 0.00010694423615538707, l2: 0.0003728437304209151   Iteration 35 of 100, tot loss = 4.798241131646293, l1: 0.00010726395173281032, l2: 0.0003725601593032479   Iteration 36 of 100, tot loss = 4.764449642764197, l1: 0.00010708009045225076, l2: 0.0003693648719086519   Iteration 37 of 100, tot loss = 4.745842128186612, l1: 0.00010665016796682155, l2: 0.0003679340423676311   Iteration 38 of 100, tot loss = 4.749194578120583, l1: 0.00010730637481365672, l2: 0.00036761307963292653   Iteration 39 of 100, tot loss = 4.732929248076219, l1: 0.00010638098982058298, l2: 0.0003669119316141288   Iteration 40 of 100, tot loss = 4.7072159349918365, l1: 0.00010614641023494187, l2: 0.00036457517999224367   Iteration 41 of 100, tot loss = 4.690252275001712, l1: 0.00010555285242423074, l2: 0.0003634723714019013   Iteration 42 of 100, tot loss = 4.680946106002445, l1: 0.00010544833059525228, l2: 0.0003626462760924672   Iteration 43 of 100, tot loss = 4.699078654133996, l1: 0.0001055734445439749, l2: 0.0003643344165919738   Iteration 44 of 100, tot loss = 4.697203262285753, l1: 0.00010565620959823718, l2: 0.00036406411196697843   Iteration 45 of 100, tot loss = 4.671142212549845, l1: 0.00010517966640893267, l2: 0.0003619345506497969   Iteration 46 of 100, tot loss = 4.65955274519713, l1: 0.00010421593082276598, l2: 0.0003617393402079039   Iteration 47 of 100, tot loss = 4.717650266403847, l1: 0.00010524787610344728, l2: 0.00036651714695023097   Iteration 48 of 100, tot loss = 4.7467222561438875, l1: 0.00010559420411482279, l2: 0.00036907801768393256   Iteration 49 of 100, tot loss = 4.7607726223614755, l1: 0.00010563326963610776, l2: 0.00037044398985537034   Iteration 50 of 100, tot loss = 4.704154431819916, l1: 0.00010451167392602656, l2: 0.0003659037666511722   Iteration 51 of 100, tot loss = 4.676792871718313, l1: 0.00010400374045346718, l2: 0.00036367554446983645   Iteration 52 of 100, tot loss = 4.690631350645652, l1: 0.00010403772963540038, l2: 0.0003650254033205923   Iteration 53 of 100, tot loss = 4.738831364883567, l1: 0.00010498208349366355, l2: 0.00036890105095050595   Iteration 54 of 100, tot loss = 4.732579432151936, l1: 0.00010537429942040169, l2: 0.00036788364108091356   Iteration 55 of 100, tot loss = 4.799363992430947, l1: 0.00010664345192543062, l2: 0.00037329294480679725   Iteration 56 of 100, tot loss = 4.767064769353185, l1: 0.00010586087832312583, l2: 0.0003708455962103991   Iteration 57 of 100, tot loss = 4.752186187526636, l1: 0.0001055746653404193, l2: 0.00036964395079402284   Iteration 58 of 100, tot loss = 4.783254226733899, l1: 0.00010587548710865467, l2: 0.0003724499326600725   Iteration 59 of 100, tot loss = 4.76846574322652, l1: 0.0001058523263609613, l2: 0.0003709942452664787   Iteration 60 of 100, tot loss = 4.762108713388443, l1: 0.00010544855692084336, l2: 0.0003707623114072097   Iteration 61 of 100, tot loss = 4.781028659617315, l1: 0.00010625474074078708, l2: 0.00037184812235416937   Iteration 62 of 100, tot loss = 4.766102231317951, l1: 0.00010577695402682894, l2: 0.00037083326653021597   Iteration 63 of 100, tot loss = 4.748247034966, l1: 0.00010541231145792168, l2: 0.0003694123893399679   Iteration 64 of 100, tot loss = 4.758815618231893, l1: 0.0001053690538697083, l2: 0.0003705125045598834   Iteration 65 of 100, tot loss = 4.761168085611784, l1: 0.00010565636548562907, l2: 0.00037046043972413127   Iteration 66 of 100, tot loss = 4.7474453178319065, l1: 0.00010521006515773479, l2: 0.00036953446308898765   Iteration 67 of 100, tot loss = 4.762895144633393, l1: 0.00010537592221334674, l2: 0.00037091358828900465   Iteration 68 of 100, tot loss = 4.762685833608403, l1: 0.00010540522383534528, l2: 0.00037086335599751156   Iteration 69 of 100, tot loss = 4.802704771359761, l1: 0.00010600114483563432, l2: 0.0003742693292408966   Iteration 70 of 100, tot loss = 4.77711091211864, l1: 0.00010554794566165323, l2: 0.00037216314272622446   Iteration 71 of 100, tot loss = 4.735414394190614, l1: 0.00010474290243558205, l2: 0.0003687985342661653   Iteration 72 of 100, tot loss = 4.70272257592943, l1: 0.00010430873696047153, l2: 0.00036596351775288995   Iteration 73 of 100, tot loss = 4.70011926677129, l1: 0.00010448832048720374, l2: 0.0003655236034992164   Iteration 74 of 100, tot loss = 4.6746504628980485, l1: 0.00010398783662232156, l2: 0.0003634772068392989   Iteration 75 of 100, tot loss = 4.669164473215739, l1: 0.0001038205110914229, l2: 0.00036309593357145784   Iteration 76 of 100, tot loss = 4.724870662940176, l1: 0.00010488480041347334, l2: 0.0003676022637304605   Iteration 77 of 100, tot loss = 4.703993218285697, l1: 0.00010446537195896887, l2: 0.0003659339478411772   Iteration 78 of 100, tot loss = 4.699455276513711, l1: 0.00010432028177139074, l2: 0.000365625243620553   Iteration 79 of 100, tot loss = 4.702563687215878, l1: 0.00010441767494452091, l2: 0.00036583869158779566   Iteration 80 of 100, tot loss = 4.715580400824547, l1: 0.00010489914780009713, l2: 0.00036665888965217166   Iteration 81 of 100, tot loss = 4.711803186086961, l1: 0.00010494082259053054, l2: 0.00036623949320831645   Iteration 82 of 100, tot loss = 4.686447707618155, l1: 0.00010429836752669739, l2: 0.000364346400424327   Iteration 83 of 100, tot loss = 4.715193398027535, l1: 0.00010482484034889557, l2: 0.0003666944971170658   Iteration 84 of 100, tot loss = 4.729828323636736, l1: 0.0001049865509014039, l2: 0.0003679962788910849   Iteration 85 of 100, tot loss = 4.711588211620556, l1: 0.0001045831418784527, l2: 0.00036657567680402494   Iteration 86 of 100, tot loss = 4.700810238372448, l1: 0.00010429903810544665, l2: 0.0003657819830804392   Iteration 87 of 100, tot loss = 4.718303927059831, l1: 0.0001047846209107304, l2: 0.0003670457686664237   Iteration 88 of 100, tot loss = 4.737265299667012, l1: 0.00010507636575725469, l2: 0.0003686501614455747   Iteration 89 of 100, tot loss = 4.742933659071333, l1: 0.00010537482964710773, l2: 0.0003689185337207981   Iteration 90 of 100, tot loss = 4.777996026145088, l1: 0.00010565770834445074, l2: 0.0003721418915018957   Iteration 91 of 100, tot loss = 4.768726388176719, l1: 0.00010530397680218352, l2: 0.00037156865925011274   Iteration 92 of 100, tot loss = 4.758776843547821, l1: 0.00010495119753930182, l2: 0.00037092648420619537   Iteration 93 of 100, tot loss = 4.767531443667668, l1: 0.00010495219616405917, l2: 0.00037180094542207134   Iteration 94 of 100, tot loss = 4.775537848472595, l1: 0.00010487643324248642, l2: 0.00037267734900369565   Iteration 95 of 100, tot loss = 4.79087378100345, l1: 0.0001051212567146745, l2: 0.00037396611863330597   Iteration 96 of 100, tot loss = 4.795632667839527, l1: 0.00010539592703177429, l2: 0.0003741673367585463   Iteration 97 of 100, tot loss = 4.792526011614456, l1: 0.00010509545609238558, l2: 0.0003741571420777263   Iteration 98 of 100, tot loss = 4.780062668177546, l1: 0.00010487237400690103, l2: 0.000373133889854439   Iteration 99 of 100, tot loss = 4.769038361732406, l1: 0.00010487732325058497, l2: 0.00037202651002750067   Iteration 100 of 100, tot loss = 4.763642528057098, l1: 0.00010481465727934846, l2: 0.0003715495923825074
   End of epoch 1159; saving model... 

Epoch 1160 of 2000
   Iteration 1 of 100, tot loss = 6.127991199493408, l1: 0.00012390123447403312, l2: 0.0004888979019597173   Iteration 2 of 100, tot loss = 5.922785043716431, l1: 0.00011271784751443192, l2: 0.0004795606655534357   Iteration 3 of 100, tot loss = 6.40303103129069, l1: 0.00012159689504187554, l2: 0.000518706200333933   Iteration 4 of 100, tot loss = 6.126963257789612, l1: 0.0001181540974357631, l2: 0.0004945422260789201   Iteration 5 of 100, tot loss = 5.489305305480957, l1: 0.00010897950560320168, l2: 0.0004399510216899216   Iteration 6 of 100, tot loss = 5.901111841201782, l1: 0.00011949842397977288, l2: 0.00047061276078845066   Iteration 7 of 100, tot loss = 5.489065817424229, l1: 0.0001136890537704208, l2: 0.0004352175268910027   Iteration 8 of 100, tot loss = 5.750356703996658, l1: 0.00011688796075759456, l2: 0.0004581477060128236   Iteration 9 of 100, tot loss = 5.7452189392513695, l1: 0.00011895559792820778, l2: 0.0004555662890197709   Iteration 10 of 100, tot loss = 5.426633501052857, l1: 0.00011469179444247856, l2: 0.00042797154892468823   Iteration 11 of 100, tot loss = 5.547663645310835, l1: 0.00011736744967162271, l2: 0.00043739891341167754   Iteration 12 of 100, tot loss = 5.579855402310689, l1: 0.0001180523686343804, l2: 0.0004399331701279152   Iteration 13 of 100, tot loss = 5.431435951819787, l1: 0.00011535199882928282, l2: 0.0004277915969186534   Iteration 14 of 100, tot loss = 5.494813544409616, l1: 0.00011627650383161381, l2: 0.0004332048525351898   Iteration 15 of 100, tot loss = 5.484321467081705, l1: 0.00011402395272549862, l2: 0.00043440819426905365   Iteration 16 of 100, tot loss = 5.317884147167206, l1: 0.000111585156446381, l2: 0.0004202032587272697   Iteration 17 of 100, tot loss = 5.388257643755744, l1: 0.00011334300934857525, l2: 0.0004254827551502625   Iteration 18 of 100, tot loss = 5.237715429729885, l1: 0.00011149897101050656, l2: 0.00041227257073236007   Iteration 19 of 100, tot loss = 5.322063395851536, l1: 0.00011154314683823798, l2: 0.0004206631902458244   Iteration 20 of 100, tot loss = 5.274589943885803, l1: 0.0001110819066525437, l2: 0.0004163770849118009   Iteration 21 of 100, tot loss = 5.306953112284343, l1: 0.00011081292212197913, l2: 0.0004198823867010928   Iteration 22 of 100, tot loss = 5.280974713238803, l1: 0.0001113923416679344, l2: 0.00041670512820763344   Iteration 23 of 100, tot loss = 5.276600796243419, l1: 0.00011233152137087335, l2: 0.00041532855728924596   Iteration 24 of 100, tot loss = 5.164764801661174, l1: 0.00011045493041213679, l2: 0.000406021548769786   Iteration 25 of 100, tot loss = 5.142633724212646, l1: 0.00010973381955409423, l2: 0.000404529552324675   Iteration 26 of 100, tot loss = 5.066691206051753, l1: 0.00010872528078974798, l2: 0.00039794384009348083   Iteration 27 of 100, tot loss = 5.079895363913642, l1: 0.00010977331435747651, l2: 0.0003982162236718944   Iteration 28 of 100, tot loss = 5.058582672051021, l1: 0.0001094892930788254, l2: 0.00039636897521891763   Iteration 29 of 100, tot loss = 5.023838363844773, l1: 0.00010884258091240993, l2: 0.0003935412557972274   Iteration 30 of 100, tot loss = 4.9868389447530115, l1: 0.00010836748867101656, l2: 0.00039031640626490115   Iteration 31 of 100, tot loss = 4.93603777885437, l1: 0.0001075685199353874, l2: 0.00038603525852874644   Iteration 32 of 100, tot loss = 4.927252568304539, l1: 0.00010774616453090857, l2: 0.0003849790919048246   Iteration 33 of 100, tot loss = 4.875732812014493, l1: 0.00010658873626115647, l2: 0.0003809845445423641   Iteration 34 of 100, tot loss = 4.813924200394574, l1: 0.00010561624645454097, l2: 0.00037577617331407964   Iteration 35 of 100, tot loss = 4.828714656829834, l1: 0.00010628518378195752, l2: 0.0003765862809294569   Iteration 36 of 100, tot loss = 4.815660609139337, l1: 0.0001061001063741666, l2: 0.00037546595396836184   Iteration 37 of 100, tot loss = 4.808508447698645, l1: 0.00010588785969318721, l2: 0.0003749629839738781   Iteration 38 of 100, tot loss = 4.839795664737099, l1: 0.00010636495062499307, l2: 0.0003776146147988345   Iteration 39 of 100, tot loss = 4.899406555371407, l1: 0.00010703189335076902, l2: 0.00038290876023566874   Iteration 40 of 100, tot loss = 4.885993146896363, l1: 0.00010727953831519698, l2: 0.00038131977489683775   Iteration 41 of 100, tot loss = 4.862957384528183, l1: 0.00010665163089714309, l2: 0.00037964410574470715   Iteration 42 of 100, tot loss = 4.828640801565988, l1: 0.00010577310119212295, l2: 0.0003770909769671215   Iteration 43 of 100, tot loss = 4.830057676448378, l1: 0.00010546420373937635, l2: 0.0003775415615839234   Iteration 44 of 100, tot loss = 4.783542546358976, l1: 0.00010502313886139414, l2: 0.00037333111388513566   Iteration 45 of 100, tot loss = 4.768680593702528, l1: 0.00010546736415967138, l2: 0.0003714006940653134   Iteration 46 of 100, tot loss = 4.77603468687638, l1: 0.00010524084918079492, l2: 0.00037236261888882956   Iteration 47 of 100, tot loss = 4.76313808116507, l1: 0.00010526195840979431, l2: 0.00037105184976021424   Iteration 48 of 100, tot loss = 4.79710465669632, l1: 0.00010576827238158633, l2: 0.0003739421933156943   Iteration 49 of 100, tot loss = 4.738477689879281, l1: 0.00010440672211151817, l2: 0.0003694410469116909   Iteration 50 of 100, tot loss = 4.751486532688141, l1: 0.00010464555940416176, l2: 0.0003705030941637233   Iteration 51 of 100, tot loss = 4.749645291590223, l1: 0.00010502773433222947, l2: 0.00036993679503782417   Iteration 52 of 100, tot loss = 4.751385370126138, l1: 0.00010492061210243264, l2: 0.0003702179254525198   Iteration 53 of 100, tot loss = 4.763893651512434, l1: 0.00010533474490347424, l2: 0.0003710546199109335   Iteration 54 of 100, tot loss = 4.754240179503405, l1: 0.00010500699978298076, l2: 0.00037041701768800894   Iteration 55 of 100, tot loss = 4.734812933748419, l1: 0.00010505538025013679, l2: 0.00036842591256241905   Iteration 56 of 100, tot loss = 4.713236844965389, l1: 0.00010453787522887328, l2: 0.0003667858088322516   Iteration 57 of 100, tot loss = 4.728346358265793, l1: 0.00010524890935732656, l2: 0.0003675857263723421   Iteration 58 of 100, tot loss = 4.765280232347291, l1: 0.00010554123751905442, l2: 0.0003709867852539274   Iteration 59 of 100, tot loss = 4.734114459005453, l1: 0.00010508389151270374, l2: 0.0003683275537727148   Iteration 60 of 100, tot loss = 4.732411144177119, l1: 0.00010495702426851494, l2: 0.00036828408944226493   Iteration 61 of 100, tot loss = 4.78518015243968, l1: 0.00010592124376447917, l2: 0.00037259677092002735   Iteration 62 of 100, tot loss = 4.787532496836878, l1: 0.00010623081993065669, l2: 0.00037252242867496886   Iteration 63 of 100, tot loss = 4.7608698644335306, l1: 0.00010576417353438464, l2: 0.00037032281213055646   Iteration 64 of 100, tot loss = 4.739743007346988, l1: 0.00010581303860135449, l2: 0.000368161261349087   Iteration 65 of 100, tot loss = 4.710445240827707, l1: 0.00010527671071870897, l2: 0.0003657678126518686   Iteration 66 of 100, tot loss = 4.750163842331279, l1: 0.00010607499649323114, l2: 0.00036894138765725955   Iteration 67 of 100, tot loss = 4.780991182398441, l1: 0.00010648824824254834, l2: 0.0003716108693703036   Iteration 68 of 100, tot loss = 4.773444103844025, l1: 0.00010668935598968186, l2: 0.00037065505367696886   Iteration 69 of 100, tot loss = 4.790980385697407, l1: 0.00010695311773789412, l2: 0.00037214492065528327   Iteration 70 of 100, tot loss = 4.763034985746656, l1: 0.00010628362059443524, l2: 0.00037001987782007615   Iteration 71 of 100, tot loss = 4.738374607663759, l1: 0.00010598376151469445, l2: 0.0003678536991109993   Iteration 72 of 100, tot loss = 4.710199086202516, l1: 0.00010563635522784252, l2: 0.0003653835531925627   Iteration 73 of 100, tot loss = 4.70082845589886, l1: 0.00010576558818447416, l2: 0.00036431725730175435   Iteration 74 of 100, tot loss = 4.689615505772668, l1: 0.00010588837667947282, l2: 0.00036307317409286827   Iteration 75 of 100, tot loss = 4.687232804298401, l1: 0.000105621997693864, l2: 0.0003631012828554958   Iteration 76 of 100, tot loss = 4.671390738926436, l1: 0.00010507295423610088, l2: 0.00036206611969790076   Iteration 77 of 100, tot loss = 4.686464766403297, l1: 0.00010517922830284126, l2: 0.0003634672483441408   Iteration 78 of 100, tot loss = 4.757336089244256, l1: 0.00010629151690339383, l2: 0.00036944209251743863   Iteration 79 of 100, tot loss = 4.7733590799041945, l1: 0.00010646549814242186, l2: 0.0003708704102849234   Iteration 80 of 100, tot loss = 4.833078108727932, l1: 0.00010723840432547149, l2: 0.00037606940713885706   Iteration 81 of 100, tot loss = 4.825807881943973, l1: 0.00010743827164176001, l2: 0.0003751425171719381   Iteration 82 of 100, tot loss = 4.8385356562893564, l1: 0.0001076230588163215, l2: 0.00037623050717761877   Iteration 83 of 100, tot loss = 4.824742887393538, l1: 0.00010704743693945985, l2: 0.00037542685230980703   Iteration 84 of 100, tot loss = 4.808024390822365, l1: 0.00010675831639500324, l2: 0.00037404412328864316   Iteration 85 of 100, tot loss = 4.799733769192415, l1: 0.00010638782965971212, l2: 0.0003735855478546856   Iteration 86 of 100, tot loss = 4.818520386551702, l1: 0.00010695516112020921, l2: 0.0003748968785742384   Iteration 87 of 100, tot loss = 4.794843390070159, l1: 0.00010636493264417828, l2: 0.00037311940757027857   Iteration 88 of 100, tot loss = 4.775446492162618, l1: 0.00010583828332693719, l2: 0.00037170636724939953   Iteration 89 of 100, tot loss = 4.784244779790385, l1: 0.0001061577950780845, l2: 0.00037226668448485644   Iteration 90 of 100, tot loss = 4.805393686559465, l1: 0.00010628077159506373, l2: 0.0003742585985390987   Iteration 91 of 100, tot loss = 4.792956031285799, l1: 0.00010609344047095095, l2: 0.00037320216386553393   Iteration 92 of 100, tot loss = 4.760895933793939, l1: 0.00010562336541150454, l2: 0.0003704662291135679   Iteration 93 of 100, tot loss = 4.746523329006728, l1: 0.00010548074539157472, l2: 0.00036917158880681603   Iteration 94 of 100, tot loss = 4.741375243410151, l1: 0.00010560874208285314, l2: 0.00036852878389602645   Iteration 95 of 100, tot loss = 4.717080645812185, l1: 0.00010523141812116495, l2: 0.00036647664811952335   Iteration 96 of 100, tot loss = 4.757894304891427, l1: 0.00010583019623785124, l2: 0.00036995923619542737   Iteration 97 of 100, tot loss = 4.755169866011315, l1: 0.00010601466536022646, l2: 0.0003695023229907353   Iteration 98 of 100, tot loss = 4.770604223621135, l1: 0.0001061430669565476, l2: 0.000370917357421214   Iteration 99 of 100, tot loss = 4.769909167530561, l1: 0.00010638608290893825, l2: 0.00037060483597510616   Iteration 100 of 100, tot loss = 4.768053205013275, l1: 0.00010639968866598792, l2: 0.0003704056341666728
   End of epoch 1160; saving model... 

Epoch 1161 of 2000
   Iteration 1 of 100, tot loss = 5.591485977172852, l1: 0.00011810203432105482, l2: 0.0004410466062836349   Iteration 2 of 100, tot loss = 5.855584144592285, l1: 0.00011355331662343815, l2: 0.0004720051074400544   Iteration 3 of 100, tot loss = 4.965718587239583, l1: 0.00010717075929278508, l2: 0.00038940110728920746   Iteration 4 of 100, tot loss = 4.747733473777771, l1: 0.00010521792682993691, l2: 0.0003695554223668296   Iteration 5 of 100, tot loss = 4.7038318634033205, l1: 0.00010618718806654215, l2: 0.0003641960007371381   Iteration 6 of 100, tot loss = 4.618143955866496, l1: 0.0001033028796276388, l2: 0.0003585115167273519   Iteration 7 of 100, tot loss = 4.173281601497105, l1: 9.340234828414395e-05, l2: 0.00032392581384296397   Iteration 8 of 100, tot loss = 4.104482084512711, l1: 9.277134904550621e-05, l2: 0.00031767686414241325   Iteration 9 of 100, tot loss = 4.162302361594306, l1: 9.643464889248005e-05, l2: 0.000319795590864184   Iteration 10 of 100, tot loss = 4.338429427146911, l1: 9.862176812021062e-05, l2: 0.0003352211802848615   Iteration 11 of 100, tot loss = 4.267551703886553, l1: 9.841587730492888e-05, l2: 0.00032833929898598314   Iteration 12 of 100, tot loss = 4.15682989358902, l1: 9.475232673139544e-05, l2: 0.00032093066814316745   Iteration 13 of 100, tot loss = 4.095611608945406, l1: 9.452037724818532e-05, l2: 0.0003150407876371621   Iteration 14 of 100, tot loss = 4.079248360225132, l1: 9.421579904613151e-05, l2: 0.0003137090420100971   Iteration 15 of 100, tot loss = 4.3482720692952475, l1: 9.770417018444278e-05, l2: 0.00033712304139044136   Iteration 16 of 100, tot loss = 4.245833843946457, l1: 9.569320968694228e-05, l2: 0.0003288901798441657   Iteration 17 of 100, tot loss = 4.180849902770099, l1: 9.372839485877194e-05, l2: 0.0003243565999368644   Iteration 18 of 100, tot loss = 4.142454160584344, l1: 9.404622788780317e-05, l2: 0.0003201991922752414   Iteration 19 of 100, tot loss = 4.023603288750899, l1: 9.117243768598296e-05, l2: 0.00031118789507002617   Iteration 20 of 100, tot loss = 4.1426303625106815, l1: 9.277869394281879e-05, l2: 0.00032148434620467016   Iteration 21 of 100, tot loss = 4.208384718213763, l1: 9.390529504300849e-05, l2: 0.000326933180552996   Iteration 22 of 100, tot loss = 4.351681687615135, l1: 9.690847450242886e-05, l2: 0.00033825969744198534   Iteration 23 of 100, tot loss = 4.379153147987697, l1: 9.756731584354344e-05, l2: 0.0003403480014126019   Iteration 24 of 100, tot loss = 4.373342990875244, l1: 9.70891775674924e-05, l2: 0.0003402451238798676   Iteration 25 of 100, tot loss = 4.281489162445069, l1: 9.512919568805955e-05, l2: 0.0003330197226023301   Iteration 26 of 100, tot loss = 4.261644308383648, l1: 9.45265697877263e-05, l2: 0.0003316378631727555   Iteration 27 of 100, tot loss = 4.27021544067948, l1: 9.496826034457492e-05, l2: 0.00033205328563107523   Iteration 28 of 100, tot loss = 4.28427973815373, l1: 9.61281975833117e-05, l2: 0.0003322997784250349   Iteration 29 of 100, tot loss = 4.248289149383019, l1: 9.615376507883474e-05, l2: 0.00032867515130482355   Iteration 30 of 100, tot loss = 4.203989720344543, l1: 9.560396902088541e-05, l2: 0.00032479500417442373   Iteration 31 of 100, tot loss = 4.305493377870129, l1: 9.703096046934896e-05, l2: 0.0003335183784891401   Iteration 32 of 100, tot loss = 4.32624114304781, l1: 9.652974347318377e-05, l2: 0.000336094372869411   Iteration 33 of 100, tot loss = 4.4021171873266045, l1: 9.78768533086898e-05, l2: 0.0003423348667142405   Iteration 34 of 100, tot loss = 4.417708992958069, l1: 9.805149120973725e-05, l2: 0.0003437194091555498   Iteration 35 of 100, tot loss = 4.450750780105591, l1: 9.872219015960582e-05, l2: 0.00034635288840425865   Iteration 36 of 100, tot loss = 4.481572581662072, l1: 9.942937595042167e-05, l2: 0.00034872788334420574   Iteration 37 of 100, tot loss = 4.503807873339267, l1: 0.00010011731353950233, l2: 0.0003502634755096625   Iteration 38 of 100, tot loss = 4.498403505275124, l1: 0.00010035754693442294, l2: 0.00034948280556004887   Iteration 39 of 100, tot loss = 4.502990325291951, l1: 0.00010044580052729147, l2: 0.00034985323406708165   Iteration 40 of 100, tot loss = 4.4999660670757295, l1: 0.00010122722424057428, l2: 0.00034876938443630936   Iteration 41 of 100, tot loss = 4.505061271714001, l1: 0.0001018086794185986, l2: 0.000348697449137434   Iteration 42 of 100, tot loss = 4.493119324956622, l1: 0.00010173569076176223, l2: 0.00034757624338713607   Iteration 43 of 100, tot loss = 4.46577021133068, l1: 0.00010174662739425616, l2: 0.00034483039526940257   Iteration 44 of 100, tot loss = 4.428618620742451, l1: 0.00010059356571384177, l2: 0.00034226829798585226   Iteration 45 of 100, tot loss = 4.4622422483232285, l1: 0.00010145460797098673, l2: 0.000344769618823193   Iteration 46 of 100, tot loss = 4.492256325224171, l1: 0.00010162160493643529, l2: 0.00034760402891349616   Iteration 47 of 100, tot loss = 4.5028361411804845, l1: 0.00010151385186931138, l2: 0.00034876976334341264   Iteration 48 of 100, tot loss = 4.46947439511617, l1: 0.00010088846018637317, l2: 0.000346058980539965   Iteration 49 of 100, tot loss = 4.492609043510592, l1: 0.00010095811895942981, l2: 0.00034830278725296793   Iteration 50 of 100, tot loss = 4.513949728012085, l1: 0.0001008303005801281, l2: 0.00035056467313552273   Iteration 51 of 100, tot loss = 4.482718883776197, l1: 0.00010037303660532879, l2: 0.00034789885274435887   Iteration 52 of 100, tot loss = 4.560597754441774, l1: 0.00010184922946511785, l2: 0.0003542105469722838   Iteration 53 of 100, tot loss = 4.536591129482917, l1: 0.0001016127861699063, l2: 0.00035204632753585376   Iteration 54 of 100, tot loss = 4.492209257902922, l1: 0.00010075728816984877, l2: 0.00034846363834933273   Iteration 55 of 100, tot loss = 4.468488277088512, l1: 0.00010036791632459922, l2: 0.00034648091241251677   Iteration 56 of 100, tot loss = 4.492647818156651, l1: 0.00010110151717006894, l2: 0.00034816326530874776   Iteration 57 of 100, tot loss = 4.491258378614459, l1: 0.00010097845485066308, l2: 0.0003481473839584444   Iteration 58 of 100, tot loss = 4.455996797002595, l1: 0.00010013361692284073, l2: 0.0003454660641025463   Iteration 59 of 100, tot loss = 4.481407791881238, l1: 0.00010050928897926806, l2: 0.00034763149190023224   Iteration 60 of 100, tot loss = 4.5281315843264265, l1: 0.00010153458206332288, l2: 0.0003512785786976262   Iteration 61 of 100, tot loss = 4.551656867637009, l1: 0.00010186495601970581, l2: 0.0003533007333257434   Iteration 62 of 100, tot loss = 4.513821201939737, l1: 0.00010135652454693862, l2: 0.0003500255981869545   Iteration 63 of 100, tot loss = 4.516561750381712, l1: 0.00010161119130244182, l2: 0.00035004498643083647   Iteration 64 of 100, tot loss = 4.515360578894615, l1: 0.00010155564837077691, l2: 0.0003499804117836902   Iteration 65 of 100, tot loss = 4.519234334505521, l1: 0.00010175260775633014, l2: 0.0003501708281244366   Iteration 66 of 100, tot loss = 4.51055105527242, l1: 0.0001015696005638209, l2: 0.0003494855074236649   Iteration 67 of 100, tot loss = 4.483563465858573, l1: 0.00010085866369312254, l2: 0.00034749768557089535   Iteration 68 of 100, tot loss = 4.460377356585334, l1: 0.00010051609018880888, l2: 0.00034552164823467406   Iteration 69 of 100, tot loss = 4.462329470592996, l1: 0.00010060880842924024, l2: 0.0003456241411976921   Iteration 70 of 100, tot loss = 4.45817916733878, l1: 0.00010065180919939718, l2: 0.0003451661100760768   Iteration 71 of 100, tot loss = 4.482596807076898, l1: 0.00010111981648894382, l2: 0.00034713986699475946   Iteration 72 of 100, tot loss = 4.480108042558034, l1: 0.00010108742334422359, l2: 0.00034692338380813855   Iteration 73 of 100, tot loss = 4.462631597910842, l1: 0.00010078596739594391, l2: 0.000345477195255688   Iteration 74 of 100, tot loss = 4.456338131749952, l1: 0.00010044673276629387, l2: 0.0003451870831446974   Iteration 75 of 100, tot loss = 4.506653451919556, l1: 0.00010102362541753488, l2: 0.0003496417232478658   Iteration 76 of 100, tot loss = 4.498365957486002, l1: 0.00010096581828340851, l2: 0.0003488707811093742   Iteration 77 of 100, tot loss = 4.587264237465797, l1: 0.00010194174773188295, l2: 0.0003567846792798441   Iteration 78 of 100, tot loss = 4.633966106634873, l1: 0.0001025732212721591, l2: 0.00036082339311878267   Iteration 79 of 100, tot loss = 4.646696775774412, l1: 0.00010272404015958722, l2: 0.0003619456411003359   Iteration 80 of 100, tot loss = 4.6214301735162735, l1: 0.0001020083869661903, l2: 0.00036013463395647707   Iteration 81 of 100, tot loss = 4.62112659583857, l1: 0.00010180756730677123, l2: 0.0003603050962412431   Iteration 82 of 100, tot loss = 4.622124983043205, l1: 0.00010182749185066602, l2: 0.0003603850100927691   Iteration 83 of 100, tot loss = 4.640502857874675, l1: 0.00010203830413913063, l2: 0.0003620119855166918   Iteration 84 of 100, tot loss = 4.639453589916229, l1: 0.00010211400010684017, l2: 0.00036183136260314354   Iteration 85 of 100, tot loss = 4.634488063700059, l1: 0.00010224928666823817, l2: 0.00036119952325380463   Iteration 86 of 100, tot loss = 4.632579933765323, l1: 0.00010224448953183952, l2: 0.00036101350777370983   Iteration 87 of 100, tot loss = 4.644291891448799, l1: 0.00010247632013781038, l2: 0.0003619528731502507   Iteration 88 of 100, tot loss = 4.671576730229638, l1: 0.00010257458498844326, l2: 0.00036458309213313356   Iteration 89 of 100, tot loss = 4.661410079913193, l1: 0.00010256856600453114, l2: 0.0003635724460355561   Iteration 90 of 100, tot loss = 4.636912642584907, l1: 0.00010212349217100482, l2: 0.0003615677759322959   Iteration 91 of 100, tot loss = 4.614335565776615, l1: 0.00010168817395454396, l2: 0.00035974538643910946   Iteration 92 of 100, tot loss = 4.600792330244313, l1: 0.00010124444143793485, l2: 0.00035883479548552396   Iteration 93 of 100, tot loss = 4.583848455900787, l1: 0.00010106666616016438, l2: 0.000357318183398425   Iteration 94 of 100, tot loss = 4.583255970731694, l1: 0.00010123723366129351, l2: 0.00035708836707697703   Iteration 95 of 100, tot loss = 4.613932499132658, l1: 0.0001019065586948081, l2: 0.0003594866952258407   Iteration 96 of 100, tot loss = 4.621883600950241, l1: 0.00010213158748229034, l2: 0.0003600567765715823   Iteration 97 of 100, tot loss = 4.6213274297026015, l1: 0.0001022112979607647, l2: 0.0003599214489684565   Iteration 98 of 100, tot loss = 4.618711169885129, l1: 0.00010224030662282865, l2: 0.0003596308143103818   Iteration 99 of 100, tot loss = 4.605419392537589, l1: 0.0001018025390239376, l2: 0.00035873940405600486   Iteration 100 of 100, tot loss = 4.6086488175392155, l1: 0.00010178820914006792, l2: 0.0003590766763954889
   End of epoch 1161; saving model... 

Epoch 1162 of 2000
   Iteration 1 of 100, tot loss = 4.180675983428955, l1: 9.346529986942187e-05, l2: 0.0003246022970415652   Iteration 2 of 100, tot loss = 4.058438777923584, l1: 8.808877828414552e-05, l2: 0.00031775509705767035   Iteration 3 of 100, tot loss = 4.932466824849446, l1: 9.780072772021715e-05, l2: 0.00039544594862187904   Iteration 4 of 100, tot loss = 4.566354691982269, l1: 9.155897532764357e-05, l2: 0.00036507649201666936   Iteration 5 of 100, tot loss = 4.885195016860962, l1: 0.00010034733422799036, l2: 0.00038817216409370303   Iteration 6 of 100, tot loss = 5.011532823244731, l1: 0.00010646178634488024, l2: 0.00039469149002494913   Iteration 7 of 100, tot loss = 5.285726990018572, l1: 0.00010917196777882054, l2: 0.0004194007321660008   Iteration 8 of 100, tot loss = 5.317757397890091, l1: 0.00011017281121894484, l2: 0.00042160292650805786   Iteration 9 of 100, tot loss = 5.209689060846965, l1: 0.00010698299027151531, l2: 0.000413985913231348   Iteration 10 of 100, tot loss = 5.0925767660140995, l1: 0.00010672435164451599, l2: 0.00040253332117572426   Iteration 11 of 100, tot loss = 5.394574317065152, l1: 0.00011328757682349533, l2: 0.0004261698499745266   Iteration 12 of 100, tot loss = 5.4323998888333636, l1: 0.00011468595645662087, l2: 0.00042855402959200245   Iteration 13 of 100, tot loss = 5.526539234014658, l1: 0.00011547965494593463, l2: 0.00043717426552365604   Iteration 14 of 100, tot loss = 5.527929902076721, l1: 0.00011534054543257557, l2: 0.0004374524412144508   Iteration 15 of 100, tot loss = 5.512281878789266, l1: 0.0001141114213775533, l2: 0.00043711676262319087   Iteration 16 of 100, tot loss = 5.579656288027763, l1: 0.00011719228814399685, l2: 0.0004407733395055402   Iteration 17 of 100, tot loss = 5.507593196981094, l1: 0.00011704058852046728, l2: 0.00043371873085989673   Iteration 18 of 100, tot loss = 5.458923856417338, l1: 0.00011700972390826792, l2: 0.00042888266196112253   Iteration 19 of 100, tot loss = 5.391085812920018, l1: 0.0001160106265543666, l2: 0.0004230979538988322   Iteration 20 of 100, tot loss = 5.364744126796722, l1: 0.0001152249300503172, l2: 0.0004212494823150337   Iteration 21 of 100, tot loss = 5.43828102520534, l1: 0.00011511740497856711, l2: 0.00042871069551135105   Iteration 22 of 100, tot loss = 5.415615396066145, l1: 0.0001158847176570403, l2: 0.00042567682050337845   Iteration 23 of 100, tot loss = 5.446949637454489, l1: 0.00011701134961027572, l2: 0.0004276836116332561   Iteration 24 of 100, tot loss = 5.460059970617294, l1: 0.00011627493343742874, l2: 0.0004297310612552489   Iteration 25 of 100, tot loss = 5.541619920730591, l1: 0.00011744259332772344, l2: 0.00043671939754858614   Iteration 26 of 100, tot loss = 5.461948550664461, l1: 0.00011548773069919732, l2: 0.00043070712350667094   Iteration 27 of 100, tot loss = 5.479837567717941, l1: 0.00011573210994592282, l2: 0.0004322516464593786   Iteration 28 of 100, tot loss = 5.381581536361149, l1: 0.00011472867065874328, l2: 0.00042342948186809996   Iteration 29 of 100, tot loss = 5.344747091161794, l1: 0.00011488057071832812, l2: 0.0004195941377286639   Iteration 30 of 100, tot loss = 5.403827818234761, l1: 0.00011525604713824578, l2: 0.0004251267343837147   Iteration 31 of 100, tot loss = 5.387022656779135, l1: 0.00011528567730208798, l2: 0.00042341658826767196   Iteration 32 of 100, tot loss = 5.350948683917522, l1: 0.00011458659059826459, l2: 0.0004205082777843927   Iteration 33 of 100, tot loss = 5.357084368214463, l1: 0.00011451727297037107, l2: 0.0004211911635304039   Iteration 34 of 100, tot loss = 5.314717867795159, l1: 0.00011353681903154425, l2: 0.00041793496690361817   Iteration 35 of 100, tot loss = 5.296722684587751, l1: 0.0001141483533761597, l2: 0.0004155239140215729   Iteration 36 of 100, tot loss = 5.238967842525906, l1: 0.00011345610336219479, l2: 0.00041044067943908484   Iteration 37 of 100, tot loss = 5.300218930115571, l1: 0.00011422599208025211, l2: 0.0004157958988012192   Iteration 38 of 100, tot loss = 5.232924197849474, l1: 0.00011277310142376289, l2: 0.00041051931666994567   Iteration 39 of 100, tot loss = 5.261799164307424, l1: 0.0001137462016068196, l2: 0.0004124337134178346   Iteration 40 of 100, tot loss = 5.279246604442596, l1: 0.00011375428121027654, l2: 0.0004141703771892935   Iteration 41 of 100, tot loss = 5.360524119400397, l1: 0.00011458019727752998, l2: 0.00042147221321966956   Iteration 42 of 100, tot loss = 5.287962992986043, l1: 0.00011351306039571119, l2: 0.0004152832372624072   Iteration 43 of 100, tot loss = 5.290364487226619, l1: 0.00011332306750646663, l2: 0.000415713380112551   Iteration 44 of 100, tot loss = 5.329880671067671, l1: 0.00011417107146950333, l2: 0.00041881699474867094   Iteration 45 of 100, tot loss = 5.254493769009908, l1: 0.00011249100554980233, l2: 0.0004129583704828595   Iteration 46 of 100, tot loss = 5.234656409077022, l1: 0.00011243958159985081, l2: 0.0004110260592626772   Iteration 47 of 100, tot loss = 5.2155666376682035, l1: 0.00011224476033834086, l2: 0.00040931190380162776   Iteration 48 of 100, tot loss = 5.192747664948304, l1: 0.00011145656700743227, l2: 0.00040781820007396163   Iteration 49 of 100, tot loss = 5.143802504150235, l1: 0.00011112282017889737, l2: 0.00040325743107039636   Iteration 50 of 100, tot loss = 5.091773912906647, l1: 0.00011039160795917269, l2: 0.00039878578420029956   Iteration 51 of 100, tot loss = 5.099850320348553, l1: 0.0001105022130621935, l2: 0.000399482819107909   Iteration 52 of 100, tot loss = 5.0952175328364735, l1: 0.00011006168565822569, l2: 0.00039946006766583125   Iteration 53 of 100, tot loss = 5.055595049318278, l1: 0.0001094780311417845, l2: 0.00039608147404956155   Iteration 54 of 100, tot loss = 5.061686288427423, l1: 0.00010945890534020253, l2: 0.00039670972344336205   Iteration 55 of 100, tot loss = 5.013460863720287, l1: 0.00010897725197454829, l2: 0.00039236883431757714   Iteration 56 of 100, tot loss = 4.98773002411638, l1: 0.00010902857288134069, l2: 0.00038974442941253074   Iteration 57 of 100, tot loss = 5.023486294244465, l1: 0.00010956503983674848, l2: 0.00039278358995113846   Iteration 58 of 100, tot loss = 4.990187733337797, l1: 0.0001088697792573808, l2: 0.00039014899410740956   Iteration 59 of 100, tot loss = 4.968515676967169, l1: 0.00010805818435027643, l2: 0.00038879338338166096   Iteration 60 of 100, tot loss = 4.933868843317032, l1: 0.00010749054448145519, l2: 0.0003858963398670312   Iteration 61 of 100, tot loss = 4.933525208567009, l1: 0.000107769709622934, l2: 0.00038558281084988266   Iteration 62 of 100, tot loss = 4.980225830308853, l1: 0.00010877611584287332, l2: 0.00038924646651139483   Iteration 63 of 100, tot loss = 4.949662846232218, l1: 0.00010782360596561967, l2: 0.0003871426777636987   Iteration 64 of 100, tot loss = 4.936706090345979, l1: 0.00010743942829094522, l2: 0.00038623117984570854   Iteration 65 of 100, tot loss = 4.953852970783527, l1: 0.00010742952587091936, l2: 0.0003879557706218643   Iteration 66 of 100, tot loss = 4.918441987398899, l1: 0.00010682481887201764, l2: 0.0003850193792128597   Iteration 67 of 100, tot loss = 4.959404203429151, l1: 0.00010721685601365335, l2: 0.0003887235639623797   Iteration 68 of 100, tot loss = 4.929242030662649, l1: 0.00010641135202818345, l2: 0.00038651285050197174   Iteration 69 of 100, tot loss = 4.927145752353945, l1: 0.00010617971064926098, l2: 0.00038653486420947087   Iteration 70 of 100, tot loss = 4.911724330697741, l1: 0.00010608448589794404, l2: 0.0003850879469869791   Iteration 71 of 100, tot loss = 4.932027093121703, l1: 0.00010623948821350342, l2: 0.0003869632211051435   Iteration 72 of 100, tot loss = 4.957950070500374, l1: 0.00010685825039521053, l2: 0.0003889367562806001   Iteration 73 of 100, tot loss = 4.950873376571969, l1: 0.00010677104932854067, l2: 0.0003883162880561965   Iteration 74 of 100, tot loss = 4.956826553151414, l1: 0.00010698935762480392, l2: 0.0003886932970852138   Iteration 75 of 100, tot loss = 4.930710428555806, l1: 0.00010632425682463993, l2: 0.0003867467851766075   Iteration 76 of 100, tot loss = 4.91867871504081, l1: 0.000106155207265003, l2: 0.00038571266352511503   Iteration 77 of 100, tot loss = 4.897830853214511, l1: 0.00010588028174885489, l2: 0.0003839028028199788   Iteration 78 of 100, tot loss = 4.902475841534444, l1: 0.00010602814225608316, l2: 0.00038421944172557397   Iteration 79 of 100, tot loss = 4.932535456705697, l1: 0.00010625318754125457, l2: 0.00038700035785743376   Iteration 80 of 100, tot loss = 4.917932216823101, l1: 0.00010592905473458814, l2: 0.00038586416685575385   Iteration 81 of 100, tot loss = 4.960079809765757, l1: 0.00010668096958262915, l2: 0.00038932701181131703   Iteration 82 of 100, tot loss = 4.966690294626282, l1: 0.00010679426948006106, l2: 0.00038987476035821954   Iteration 83 of 100, tot loss = 4.984449212809643, l1: 0.00010690511206406202, l2: 0.00039153980986770604   Iteration 84 of 100, tot loss = 4.99344046768688, l1: 0.00010667180971982557, l2: 0.0003926722376837417   Iteration 85 of 100, tot loss = 4.985704959140104, l1: 0.00010648923746296478, l2: 0.0003920812589486184   Iteration 86 of 100, tot loss = 4.997437880482784, l1: 0.00010659876631486208, l2: 0.00039314502172664806   Iteration 87 of 100, tot loss = 5.009173265818892, l1: 0.00010711463815907295, l2: 0.0003938026886484211   Iteration 88 of 100, tot loss = 5.03229857981205, l1: 0.00010736660607852338, l2: 0.0003958632519656517   Iteration 89 of 100, tot loss = 5.032011443309569, l1: 0.00010733700241616226, l2: 0.0003958641418342196   Iteration 90 of 100, tot loss = 5.036647477414873, l1: 0.00010776303162048053, l2: 0.000395901716062023   Iteration 91 of 100, tot loss = 5.025439014801612, l1: 0.00010769949241856031, l2: 0.0003948444089593857   Iteration 92 of 100, tot loss = 5.011656544778658, l1: 0.00010770347019032393, l2: 0.0003934621843496722   Iteration 93 of 100, tot loss = 5.002229804633766, l1: 0.00010755081972112298, l2: 0.0003926721606491214   Iteration 94 of 100, tot loss = 4.998406685413198, l1: 0.00010756042072589093, l2: 0.00039228024794051345   Iteration 95 of 100, tot loss = 4.991986170567964, l1: 0.00010707944548860389, l2: 0.0003921191715083918   Iteration 96 of 100, tot loss = 5.010838693628709, l1: 0.00010715939727864073, l2: 0.0003939244717609351   Iteration 97 of 100, tot loss = 5.010049241105306, l1: 0.00010703754633516266, l2: 0.000393967377471968   Iteration 98 of 100, tot loss = 5.018218834789431, l1: 0.00010722133298927196, l2: 0.00039460055030554494   Iteration 99 of 100, tot loss = 5.014008455806309, l1: 0.00010718344415319294, l2: 0.00039421740112087754   Iteration 100 of 100, tot loss = 5.007460128068924, l1: 0.0001070233899372397, l2: 0.0003937226223933976
   End of epoch 1162; saving model... 

Epoch 1163 of 2000
   Iteration 1 of 100, tot loss = 3.143317937850952, l1: 0.00010459472832735628, l2: 0.00020973707432858646   Iteration 2 of 100, tot loss = 4.533378958702087, l1: 0.00011121403440483846, l2: 0.0003421238507144153   Iteration 3 of 100, tot loss = 4.512487967809041, l1: 0.00011321472751054291, l2: 0.00033803406404331326   Iteration 4 of 100, tot loss = 4.089453101158142, l1: 0.00010189490967604797, l2: 0.0003070503953495063   Iteration 5 of 100, tot loss = 4.134675693511963, l1: 9.720561356516555e-05, l2: 0.000316261948319152   Iteration 6 of 100, tot loss = 4.787505229314168, l1: 0.000104028655186994, l2: 0.0003747218628025924   Iteration 7 of 100, tot loss = 4.723028319222586, l1: 0.00010315241005238411, l2: 0.00036915041606075   Iteration 8 of 100, tot loss = 4.873220264911652, l1: 0.00010402650423202431, l2: 0.0003832955153484363   Iteration 9 of 100, tot loss = 4.797021971808539, l1: 0.00010274462692905217, l2: 0.0003769575657012562   Iteration 10 of 100, tot loss = 4.656582570075988, l1: 0.00010152728791581466, l2: 0.00036413096531759945   Iteration 11 of 100, tot loss = 4.782780235463923, l1: 0.00010217755334451795, l2: 0.00037610046406784517   Iteration 12 of 100, tot loss = 4.797491014003754, l1: 0.00010164136801904533, l2: 0.0003781077272530335   Iteration 13 of 100, tot loss = 4.781428465476403, l1: 0.00010366344213252887, l2: 0.00037447939841793134   Iteration 14 of 100, tot loss = 4.598095263753619, l1: 0.0001002203269828377, l2: 0.0003595891934570058   Iteration 15 of 100, tot loss = 4.774013725916545, l1: 0.00010290947102475911, l2: 0.00037449189791611084   Iteration 16 of 100, tot loss = 4.7806078642606735, l1: 0.00010261153283863678, l2: 0.00037544925089605385   Iteration 17 of 100, tot loss = 4.749098960091086, l1: 0.0001028435499927796, l2: 0.00037206634314155537   Iteration 18 of 100, tot loss = 4.897354483604431, l1: 0.00010634910803572793, l2: 0.00038338633910623484   Iteration 19 of 100, tot loss = 4.940757487949572, l1: 0.00010714748312143217, l2: 0.00038692826561409195   Iteration 20 of 100, tot loss = 4.962378346920014, l1: 0.00010754452014225536, l2: 0.0003886933140165638   Iteration 21 of 100, tot loss = 4.994708912713187, l1: 0.00010870884233597844, l2: 0.0003907620481359551   Iteration 22 of 100, tot loss = 5.0929590897126635, l1: 0.00010987393431026827, l2: 0.0003994219726089134   Iteration 23 of 100, tot loss = 5.114026473916096, l1: 0.00010813151311594993, l2: 0.00040327113200201774   Iteration 24 of 100, tot loss = 5.145421336094539, l1: 0.00010951094676177793, l2: 0.00040503118543711025   Iteration 25 of 100, tot loss = 5.229564180374146, l1: 0.00011106084333732724, l2: 0.00041189557348843666   Iteration 26 of 100, tot loss = 5.268363118171692, l1: 0.00011238005166747965, l2: 0.0004144562604219223   Iteration 27 of 100, tot loss = 5.17763567853857, l1: 0.00011141688284826362, l2: 0.00040634668483916257   Iteration 28 of 100, tot loss = 5.117070913314819, l1: 0.00011038723459932953, l2: 0.00040131985588232055   Iteration 29 of 100, tot loss = 5.065197221164046, l1: 0.00010947369722496911, l2: 0.0003970460243651579   Iteration 30 of 100, tot loss = 5.024777205785115, l1: 0.00010890813573496416, l2: 0.00039356958489709845   Iteration 31 of 100, tot loss = 5.019318042262908, l1: 0.00010909243499026484, l2: 0.0003928393688834002   Iteration 32 of 100, tot loss = 4.948315195739269, l1: 0.00010785383733491472, l2: 0.000386977682410361   Iteration 33 of 100, tot loss = 4.894175753448948, l1: 0.0001064768505536697, l2: 0.00038294072455204457   Iteration 34 of 100, tot loss = 4.950621878399568, l1: 0.00010829396213760928, l2: 0.00038676822484429814   Iteration 35 of 100, tot loss = 5.0937150342123845, l1: 0.00011084970444374319, l2: 0.00039852179927817944   Iteration 36 of 100, tot loss = 5.130184868971507, l1: 0.00011130843510424408, l2: 0.00040171005214991357   Iteration 37 of 100, tot loss = 5.077565953538224, l1: 0.00011053441955261185, l2: 0.00039722217597709214   Iteration 38 of 100, tot loss = 5.021907467591135, l1: 0.00010977356827137747, l2: 0.0003924171791081072   Iteration 39 of 100, tot loss = 5.01928543433165, l1: 0.00010916297544146147, l2: 0.00039276556867079285   Iteration 40 of 100, tot loss = 4.9944269299507145, l1: 0.00010840628838195698, l2: 0.0003910364051989745   Iteration 41 of 100, tot loss = 5.015142638508866, l1: 0.00010881893938015465, l2: 0.00039269532525639347   Iteration 42 of 100, tot loss = 5.0362063476017545, l1: 0.00010970117017721004, l2: 0.0003939194651874935   Iteration 43 of 100, tot loss = 5.062775744948277, l1: 0.00010971184687954266, l2: 0.0003965657277487565   Iteration 44 of 100, tot loss = 5.077737873250788, l1: 0.00010994936647793193, l2: 0.0003978244203608483   Iteration 45 of 100, tot loss = 5.06372660530938, l1: 0.0001098765006948573, l2: 0.00039649615969715847   Iteration 46 of 100, tot loss = 5.059016476506772, l1: 0.0001096396296381262, l2: 0.00039626201728622067   Iteration 47 of 100, tot loss = 5.011032535674724, l1: 0.00010838902914869361, l2: 0.0003927142236708723   Iteration 48 of 100, tot loss = 5.018538102507591, l1: 0.00010826570640650364, l2: 0.0003935881031793542   Iteration 49 of 100, tot loss = 5.030247089814167, l1: 0.00010843353833924808, l2: 0.00039459117011129096   Iteration 50 of 100, tot loss = 5.007239460945129, l1: 0.00010828403894265648, l2: 0.00039243990671820937   Iteration 51 of 100, tot loss = 5.054395175447651, l1: 0.0001095009198858558, l2: 0.0003959385977199703   Iteration 52 of 100, tot loss = 5.0695113356296835, l1: 0.00010973927272481468, l2: 0.0003972118614071335   Iteration 53 of 100, tot loss = 5.052085808987887, l1: 0.00010949078987666651, l2: 0.00039571779121135204   Iteration 54 of 100, tot loss = 5.072378578009428, l1: 0.00011013110708046481, l2: 0.00039710675170041485   Iteration 55 of 100, tot loss = 5.051488568566063, l1: 0.0001096647954413625, l2: 0.00039548406229269773   Iteration 56 of 100, tot loss = 5.040228541408267, l1: 0.00010947902325954471, l2: 0.0003945438313946527   Iteration 57 of 100, tot loss = 5.038890441258748, l1: 0.00010945951943173433, l2: 0.00039442952542582103   Iteration 58 of 100, tot loss = 5.00900780743566, l1: 0.0001089352040410098, l2: 0.0003919655774270409   Iteration 59 of 100, tot loss = 5.03553200576265, l1: 0.00010923533526523496, l2: 0.00039431786606624973   Iteration 60 of 100, tot loss = 5.019360677401225, l1: 0.00010917279584343002, l2: 0.0003927632729755715   Iteration 61 of 100, tot loss = 5.017626793658147, l1: 0.00010902743344529166, l2: 0.000392735246797932   Iteration 62 of 100, tot loss = 4.98973060423328, l1: 0.00010815399582497776, l2: 0.00039081906546808537   Iteration 63 of 100, tot loss = 4.998811547718351, l1: 0.00010816026920182927, l2: 0.00039172088653440516   Iteration 64 of 100, tot loss = 4.983009569346905, l1: 0.00010803123279856663, l2: 0.00039026972490319167   Iteration 65 of 100, tot loss = 4.962974368608915, l1: 0.0001075352247705898, l2: 0.0003887622129136267   Iteration 66 of 100, tot loss = 4.962185400905031, l1: 0.00010784763755068663, l2: 0.0003883709026867467   Iteration 67 of 100, tot loss = 4.941503645768806, l1: 0.00010759552502865666, l2: 0.0003865548394752472   Iteration 68 of 100, tot loss = 4.91726150933434, l1: 0.00010681745184496428, l2: 0.0003849086993688937   Iteration 69 of 100, tot loss = 4.925248090771661, l1: 0.00010682468353585084, l2: 0.00038570012564878857   Iteration 70 of 100, tot loss = 4.966526140485491, l1: 0.00010696832219504618, l2: 0.00038968429144006225   Iteration 71 of 100, tot loss = 4.950072812362456, l1: 0.00010667242762357206, l2: 0.0003883348532762169   Iteration 72 of 100, tot loss = 4.945871889591217, l1: 0.00010664078824144478, l2: 0.00038794640042599186   Iteration 73 of 100, tot loss = 4.9208333035037946, l1: 0.00010597140703482668, l2: 0.0003861119230724361   Iteration 74 of 100, tot loss = 4.916234444927525, l1: 0.00010584522420391, l2: 0.00038577822026028926   Iteration 75 of 100, tot loss = 4.908430763880411, l1: 0.00010580618317665842, l2: 0.00038503689342178404   Iteration 76 of 100, tot loss = 4.920676240795537, l1: 0.00010588656344847742, l2: 0.00038618106076395824   Iteration 77 of 100, tot loss = 4.905710173891737, l1: 0.00010563366500813478, l2: 0.0003849373527125201   Iteration 78 of 100, tot loss = 4.896737615267436, l1: 0.00010551350589174454, l2: 0.0003841602555416429   Iteration 79 of 100, tot loss = 4.8992053919200655, l1: 0.00010579984836446864, l2: 0.0003841206909575842   Iteration 80 of 100, tot loss = 4.928384014964104, l1: 0.00010596647939564718, l2: 0.00038687192245561163   Iteration 81 of 100, tot loss = 4.9148919170285446, l1: 0.0001060436090348823, l2: 0.0003854455828758669   Iteration 82 of 100, tot loss = 4.942679559312215, l1: 0.00010657661493119454, l2: 0.0003876913411304264   Iteration 83 of 100, tot loss = 4.9596966002360885, l1: 0.00010675410556863623, l2: 0.00038921555464930207   Iteration 84 of 100, tot loss = 5.005662319206056, l1: 0.00010761875745672677, l2: 0.0003929474748604532   Iteration 85 of 100, tot loss = 5.023347941566916, l1: 0.00010794814798453659, l2: 0.00039438664680346847   Iteration 86 of 100, tot loss = 4.996705462766248, l1: 0.00010744279816842822, l2: 0.00039222774868776894   Iteration 87 of 100, tot loss = 4.989744485109702, l1: 0.0001070839472080918, l2: 0.00039189050192195364   Iteration 88 of 100, tot loss = 5.00038665803996, l1: 0.00010735238906404861, l2: 0.000392686277667632   Iteration 89 of 100, tot loss = 4.987916306163488, l1: 0.00010712824861114855, l2: 0.0003916633827481   Iteration 90 of 100, tot loss = 5.000583831469218, l1: 0.00010741390914416923, l2: 0.0003926444746967819   Iteration 91 of 100, tot loss = 4.991419354637901, l1: 0.00010725861907953263, l2: 0.00039188331696665387   Iteration 92 of 100, tot loss = 4.998442864936331, l1: 0.00010767544996348948, l2: 0.00039216883717175654   Iteration 93 of 100, tot loss = 5.016760136491509, l1: 0.00010793773844363719, l2: 0.00039373827573683075   Iteration 94 of 100, tot loss = 5.035181819124425, l1: 0.0001080415783709465, l2: 0.00039547660414060144   Iteration 95 of 100, tot loss = 5.020705699920654, l1: 0.00010804760397724366, l2: 0.00039402296656350556   Iteration 96 of 100, tot loss = 5.011145368218422, l1: 0.00010805994084724564, l2: 0.0003930545966189432   Iteration 97 of 100, tot loss = 5.010821652166622, l1: 0.0001081800025353117, l2: 0.0003929021633254153   Iteration 98 of 100, tot loss = 5.023206107470454, l1: 0.00010827133263198764, l2: 0.0003940492790615262   Iteration 99 of 100, tot loss = 5.031133661366472, l1: 0.00010837673894135573, l2: 0.00039473662771416284   Iteration 100 of 100, tot loss = 5.019566552639008, l1: 0.00010818266604474047, l2: 0.0003937739896355197
   End of epoch 1163; saving model... 

Epoch 1164 of 2000
   Iteration 1 of 100, tot loss = 4.344550132751465, l1: 0.00012167770182713866, l2: 0.00031277729431167245   Iteration 2 of 100, tot loss = 4.405694246292114, l1: 0.00011046210420317948, l2: 0.0003301073011243716   Iteration 3 of 100, tot loss = 4.835707346598308, l1: 0.00011883394230001916, l2: 0.00036473678968225914   Iteration 4 of 100, tot loss = 4.4473190903663635, l1: 0.00011155774154758547, l2: 0.0003331741645524744   Iteration 5 of 100, tot loss = 4.163554334640503, l1: 9.906336781568825e-05, l2: 0.0003172920638462529   Iteration 6 of 100, tot loss = 4.383681734402974, l1: 0.00010335071904895206, l2: 0.00033501745929243043   Iteration 7 of 100, tot loss = 4.26346400805882, l1: 0.00010483326020351212, l2: 0.00032151314579615634   Iteration 8 of 100, tot loss = 4.063505083322525, l1: 9.974187832995085e-05, l2: 0.00030660863376397174   Iteration 9 of 100, tot loss = 4.201319244172838, l1: 9.701622992805723e-05, l2: 0.0003231157008687862   Iteration 10 of 100, tot loss = 4.1399986267089846, l1: 9.793931167223491e-05, l2: 0.00031606055708834904   Iteration 11 of 100, tot loss = 4.208148652857, l1: 0.0001004763888142241, l2: 0.0003203384840162471   Iteration 12 of 100, tot loss = 4.227295319239299, l1: 0.00010242980170005467, l2: 0.00032029973590397276   Iteration 13 of 100, tot loss = 4.12072020310622, l1: 9.972826172508157e-05, l2: 0.00031234376365318894   Iteration 14 of 100, tot loss = 4.130045141492571, l1: 9.957570936031905e-05, l2: 0.0003134288085025868   Iteration 15 of 100, tot loss = 4.040630435943603, l1: 9.83435859476837e-05, l2: 0.0003057194602054854   Iteration 16 of 100, tot loss = 3.943802982568741, l1: 9.605591776562505e-05, l2: 0.0002983243830385618   Iteration 17 of 100, tot loss = 4.133706373326919, l1: 9.783212504664655e-05, l2: 0.00031553851509028496   Iteration 18 of 100, tot loss = 4.191562122768826, l1: 9.853219348264651e-05, l2: 0.0003206240216968581   Iteration 19 of 100, tot loss = 4.278868348974931, l1: 0.00010050022530393969, l2: 0.00032738661301616386   Iteration 20 of 100, tot loss = 4.291234397888184, l1: 0.00010025145129475277, l2: 0.00032887199195101856   Iteration 21 of 100, tot loss = 4.177665000870114, l1: 9.74083980379094e-05, l2: 0.0003203581048486133   Iteration 22 of 100, tot loss = 4.199476605111903, l1: 9.888587837022814e-05, l2: 0.0003210617856397717   Iteration 23 of 100, tot loss = 4.130730623784273, l1: 9.741536506107482e-05, l2: 0.0003156577004119754   Iteration 24 of 100, tot loss = 4.236223792036374, l1: 0.00010009313640087687, l2: 0.000323529246088583   Iteration 25 of 100, tot loss = 4.279962401390076, l1: 0.00010086815484100953, l2: 0.0003271280892658979   Iteration 26 of 100, tot loss = 4.348957112202277, l1: 0.0001014866701063091, l2: 0.0003334090456169528   Iteration 27 of 100, tot loss = 4.3608041471905175, l1: 0.00010111849756126671, l2: 0.00033496192190796137   Iteration 28 of 100, tot loss = 4.434628618615014, l1: 0.0001026033972136377, l2: 0.0003408594701405881   Iteration 29 of 100, tot loss = 4.468810225355214, l1: 0.00010341658093364395, l2: 0.00034346444741019914   Iteration 30 of 100, tot loss = 4.480258715152741, l1: 0.0001039534438556681, l2: 0.00034407243365421893   Iteration 31 of 100, tot loss = 4.4944356603007165, l1: 0.00010492430104228157, l2: 0.00034451927058398724   Iteration 32 of 100, tot loss = 4.487192954868078, l1: 0.0001037135104979825, l2: 0.00034500579113228014   Iteration 33 of 100, tot loss = 4.531364263910236, l1: 0.0001045484733009344, l2: 0.00034858795962381094   Iteration 34 of 100, tot loss = 4.492463395876043, l1: 0.00010443926550899906, l2: 0.0003448070805033614   Iteration 35 of 100, tot loss = 4.48456586088453, l1: 0.00010401859513616987, l2: 0.0003444379973058988   Iteration 36 of 100, tot loss = 4.52432178457578, l1: 0.00010461087893216043, l2: 0.00034782130508877646   Iteration 37 of 100, tot loss = 4.543541769723634, l1: 0.00010453733262398305, l2: 0.00034981684930735846   Iteration 38 of 100, tot loss = 4.506646667656145, l1: 0.00010353457477513554, l2: 0.0003471300974763979   Iteration 39 of 100, tot loss = 4.5002436607311935, l1: 0.00010312980008072768, l2: 0.00034689457112481486   Iteration 40 of 100, tot loss = 4.463973888754845, l1: 0.00010275738350173924, l2: 0.0003436400100326864   Iteration 41 of 100, tot loss = 4.506505137536584, l1: 0.0001034706163009992, l2: 0.00034717990265784374   Iteration 42 of 100, tot loss = 4.492939219588325, l1: 0.00010313510332101335, l2: 0.000346158824132068   Iteration 43 of 100, tot loss = 4.4812712807988015, l1: 0.00010282004256813982, l2: 0.0003453070906310364   Iteration 44 of 100, tot loss = 4.515409526499835, l1: 0.00010320720899802505, l2: 0.0003483337485787078   Iteration 45 of 100, tot loss = 4.540519229571024, l1: 0.0001039280266720905, l2: 0.00035012390160570956   Iteration 46 of 100, tot loss = 4.540850942549498, l1: 0.00010391953480354024, l2: 0.0003501655645896276   Iteration 47 of 100, tot loss = 4.4947865998491325, l1: 0.00010306239183843533, l2: 0.0003464162734243028   Iteration 48 of 100, tot loss = 4.470092914998531, l1: 0.0001027895289856436, l2: 0.0003442197676122305   Iteration 49 of 100, tot loss = 4.459822355484476, l1: 0.00010274712035517988, l2: 0.00034323512019389976   Iteration 50 of 100, tot loss = 4.474626772403717, l1: 0.00010315064660971984, l2: 0.0003443120353040285   Iteration 51 of 100, tot loss = 4.427705056527081, l1: 0.00010187166102981541, l2: 0.00034089884909061603   Iteration 52 of 100, tot loss = 4.402533285892927, l1: 0.00010108930186666279, l2: 0.00033916403117473237   Iteration 53 of 100, tot loss = 4.405184136246735, l1: 0.00010088808678874728, l2: 0.00033963033089300023   Iteration 54 of 100, tot loss = 4.466095705827077, l1: 0.00010191430171996924, l2: 0.0003446952730343953   Iteration 55 of 100, tot loss = 4.465562874620611, l1: 0.00010211725027395667, l2: 0.00034443904111288825   Iteration 56 of 100, tot loss = 4.453223434942109, l1: 0.00010164808509216527, l2: 0.0003436742624346932   Iteration 57 of 100, tot loss = 4.462225506180211, l1: 0.00010189221562509294, l2: 0.0003443303394944227   Iteration 58 of 100, tot loss = 4.460177029001302, l1: 0.00010197935130392956, l2: 0.00034403835612967416   Iteration 59 of 100, tot loss = 4.431796964952501, l1: 0.00010166121063452072, l2: 0.0003415184900543447   Iteration 60 of 100, tot loss = 4.435065883398056, l1: 0.00010160181269990669, l2: 0.00034190477939167373   Iteration 61 of 100, tot loss = 4.42884741650253, l1: 0.00010150133938190802, l2: 0.0003413834063275183   Iteration 62 of 100, tot loss = 4.444680246614641, l1: 0.00010192281074376942, l2: 0.000342545218318851   Iteration 63 of 100, tot loss = 4.4288274730954855, l1: 0.0001015216643974695, l2: 0.00034136108736256286   Iteration 64 of 100, tot loss = 4.415369963273406, l1: 0.00010138503006373867, l2: 0.0003401519707040279   Iteration 65 of 100, tot loss = 4.41366083255181, l1: 0.00010122884710793957, l2: 0.0003401372402619857   Iteration 66 of 100, tot loss = 4.414147541378483, l1: 0.00010127047839473975, l2: 0.00034014427960342306   Iteration 67 of 100, tot loss = 4.435956746784608, l1: 0.000101530233820737, l2: 0.000342065444539315   Iteration 68 of 100, tot loss = 4.463703469318502, l1: 0.00010204971097075887, l2: 0.00034432063993710256   Iteration 69 of 100, tot loss = 4.467328588167827, l1: 0.00010226725308988851, l2: 0.0003444656094475447   Iteration 70 of 100, tot loss = 4.507448208332062, l1: 0.00010272576291754376, l2: 0.00034801906149368734   Iteration 71 of 100, tot loss = 4.478226660003124, l1: 0.00010187368809030733, l2: 0.0003459489814983025   Iteration 72 of 100, tot loss = 4.488334614369604, l1: 0.00010189082235633073, l2: 0.00034694264306583337   Iteration 73 of 100, tot loss = 4.552356091264176, l1: 0.00010301078898371325, l2: 0.00035222482385930336   Iteration 74 of 100, tot loss = 4.538944445751809, l1: 0.00010251959055666875, l2: 0.00035137485758388867   Iteration 75 of 100, tot loss = 4.526153931617737, l1: 0.00010220681409312722, l2: 0.0003504085827929278   Iteration 76 of 100, tot loss = 4.516768739411705, l1: 0.00010215981240578817, l2: 0.0003495170653638381   Iteration 77 of 100, tot loss = 4.4944961489021, l1: 0.00010178866819818746, l2: 0.0003476609502746663   Iteration 78 of 100, tot loss = 4.484747683390593, l1: 0.00010171356893223651, l2: 0.0003467612030083099   Iteration 79 of 100, tot loss = 4.508896089807341, l1: 0.00010224427392189037, l2: 0.00034864533849537986   Iteration 80 of 100, tot loss = 4.54698074311018, l1: 0.00010246545921290817, l2: 0.00035223261820647165   Iteration 81 of 100, tot loss = 4.582727913503294, l1: 0.00010281116605548039, l2: 0.0003554616283329296   Iteration 82 of 100, tot loss = 4.568002263220345, l1: 0.00010252996129033486, l2: 0.0003542702678071403   Iteration 83 of 100, tot loss = 4.5517460825931595, l1: 0.00010237518908541526, l2: 0.00035279942185517   Iteration 84 of 100, tot loss = 4.546694143897011, l1: 0.00010235294857487869, l2: 0.00035231646844684813   Iteration 85 of 100, tot loss = 4.549908151346094, l1: 0.00010266933048328878, l2: 0.00035232148742686737   Iteration 86 of 100, tot loss = 4.576714910740076, l1: 0.00010315596942788498, l2: 0.000354515524989882   Iteration 87 of 100, tot loss = 4.561865372219305, l1: 0.00010263171518989571, l2: 0.0003535548253814508   Iteration 88 of 100, tot loss = 4.57409492270513, l1: 0.00010305751369411486, l2: 0.0003543519815139007   Iteration 89 of 100, tot loss = 4.559065702256192, l1: 0.00010280973138026057, l2: 0.00035309684175077115   Iteration 90 of 100, tot loss = 4.588254119290246, l1: 0.00010343151710306605, l2: 0.00035539389743159216   Iteration 91 of 100, tot loss = 4.63748748092861, l1: 0.0001041037081689625, l2: 0.0003596450429880521   Iteration 92 of 100, tot loss = 4.6226053846919015, l1: 0.00010387925763295598, l2: 0.00035838128394022095   Iteration 93 of 100, tot loss = 4.611599769643558, l1: 0.00010363589410522392, l2: 0.00035752408595765713   Iteration 94 of 100, tot loss = 4.597628115339482, l1: 0.00010346473240172194, l2: 0.00035629808228398535   Iteration 95 of 100, tot loss = 4.58191504854905, l1: 0.00010308478499370578, l2: 0.00035510672299240374   Iteration 96 of 100, tot loss = 4.593154039233923, l1: 0.00010330288599410171, l2: 0.00035601252056949306   Iteration 97 of 100, tot loss = 4.608895873286061, l1: 0.00010336864845870427, l2: 0.0003575209411792457   Iteration 98 of 100, tot loss = 4.625205814838409, l1: 0.00010379716354227454, l2: 0.00035872342028151437   Iteration 99 of 100, tot loss = 4.61451927700428, l1: 0.00010356227214673929, l2: 0.00035788965760730207   Iteration 100 of 100, tot loss = 4.6036950051784515, l1: 0.00010327531505026855, l2: 0.00035709418763872234
   End of epoch 1164; saving model... 

Epoch 1165 of 2000
   Iteration 1 of 100, tot loss = 2.658935785293579, l1: 7.328475476242602e-05, l2: 0.0001926088152686134   Iteration 2 of 100, tot loss = 4.12981641292572, l1: 9.865192987490445e-05, l2: 0.00031432969990419224   Iteration 3 of 100, tot loss = 4.60922646522522, l1: 0.00010927395972733696, l2: 0.0003516486758599058   Iteration 4 of 100, tot loss = 4.395126223564148, l1: 0.00010676256897568237, l2: 0.00033275004170718603   Iteration 5 of 100, tot loss = 4.506552791595459, l1: 0.00010775805421872064, l2: 0.0003428972064284608   Iteration 6 of 100, tot loss = 4.770770867665608, l1: 0.00011481151765716883, l2: 0.00036226555190902826   Iteration 7 of 100, tot loss = 4.537276029586792, l1: 0.0001129988075782811, l2: 0.00034072878146876713   Iteration 8 of 100, tot loss = 4.77923771739006, l1: 0.00011272332812950481, l2: 0.00036520043249765877   Iteration 9 of 100, tot loss = 4.66436489423116, l1: 0.00011013508952196894, l2: 0.00035630139205346093   Iteration 10 of 100, tot loss = 4.890979075431824, l1: 0.00011382288721506484, l2: 0.00037527501553995534   Iteration 11 of 100, tot loss = 4.984088876030662, l1: 0.00011625697516137734, l2: 0.00038215191108809614   Iteration 12 of 100, tot loss = 5.004226227601369, l1: 0.00011539423030626494, l2: 0.0003850283913682991   Iteration 13 of 100, tot loss = 4.964037289986243, l1: 0.00011446800971708189, l2: 0.00038193571582758945   Iteration 14 of 100, tot loss = 4.859909483364651, l1: 0.00011250855199510365, l2: 0.00037348239129642025   Iteration 15 of 100, tot loss = 4.8798357804616295, l1: 0.00011252286785747855, l2: 0.0003754607062243546   Iteration 16 of 100, tot loss = 4.809195727109909, l1: 0.00011228877610847121, l2: 0.00036863079276372446   Iteration 17 of 100, tot loss = 4.766320060281193, l1: 0.00011112569250947083, l2: 0.00036550631065277714   Iteration 18 of 100, tot loss = 4.916749874750773, l1: 0.00011296582953137759, l2: 0.0003787091579094219   Iteration 19 of 100, tot loss = 4.913317655262194, l1: 0.00011234270258635086, l2: 0.0003789890623394106   Iteration 20 of 100, tot loss = 4.9470614194869995, l1: 0.0001121408164181048, l2: 0.00038256532498053274   Iteration 21 of 100, tot loss = 4.834493126188006, l1: 0.00011076143779237533, l2: 0.00037268787508808253   Iteration 22 of 100, tot loss = 4.805608890273354, l1: 0.0001108837036140771, l2: 0.0003696771871711297   Iteration 23 of 100, tot loss = 4.7621660232543945, l1: 0.000109880514134167, l2: 0.00036633608966519165   Iteration 24 of 100, tot loss = 4.744442284107208, l1: 0.00010935686198839296, l2: 0.00036508736775431316   Iteration 25 of 100, tot loss = 4.680503177642822, l1: 0.0001087808475131169, l2: 0.00035926947195548566   Iteration 26 of 100, tot loss = 4.634376599238469, l1: 0.00010749077157995019, l2: 0.0003559468901725128   Iteration 27 of 100, tot loss = 4.653461403316921, l1: 0.00010762709765306984, l2: 0.00035771904286876734   Iteration 28 of 100, tot loss = 4.631082074982779, l1: 0.00010765352427759873, l2: 0.00035545468303358315   Iteration 29 of 100, tot loss = 4.5633547717127305, l1: 0.00010650643751683549, l2: 0.00034982903964881756   Iteration 30 of 100, tot loss = 4.524951545397441, l1: 0.00010648566870562112, l2: 0.00034600948565639557   Iteration 31 of 100, tot loss = 4.515486278841572, l1: 0.000106105410293, l2: 0.000345443217684665   Iteration 32 of 100, tot loss = 4.522263683378696, l1: 0.00010650423678271181, l2: 0.00034572213189676404   Iteration 33 of 100, tot loss = 4.524912711345788, l1: 0.00010549528087722138, l2: 0.0003469959906810387   Iteration 34 of 100, tot loss = 4.509096867897931, l1: 0.00010425850183203104, l2: 0.0003466511852285989   Iteration 35 of 100, tot loss = 4.527435323170253, l1: 0.00010480108259279015, l2: 0.0003479424498177   Iteration 36 of 100, tot loss = 4.5134319927957325, l1: 0.0001048915204996269, l2: 0.0003464516784232627   Iteration 37 of 100, tot loss = 4.546123949257103, l1: 0.00010513064114855149, l2: 0.00034948175370290473   Iteration 38 of 100, tot loss = 4.569033654112565, l1: 0.00010515827402981047, l2: 0.000351745091488977   Iteration 39 of 100, tot loss = 4.5256853470435505, l1: 0.00010376424212993767, l2: 0.00034880429321231367   Iteration 40 of 100, tot loss = 4.494838863611221, l1: 0.00010287261429766658, l2: 0.00034661127283470706   Iteration 41 of 100, tot loss = 4.531361992766217, l1: 0.00010315731463090675, l2: 0.0003499788851136478   Iteration 42 of 100, tot loss = 4.547956949188595, l1: 0.00010307230453084533, l2: 0.00035172339093627496   Iteration 43 of 100, tot loss = 4.526958399040755, l1: 0.00010304438876838229, l2: 0.0003496514515528932   Iteration 44 of 100, tot loss = 4.578033924102783, l1: 0.00010358285842151169, l2: 0.0003542205343943682   Iteration 45 of 100, tot loss = 4.533819203906589, l1: 0.00010286438304723965, l2: 0.00035051753802690656   Iteration 46 of 100, tot loss = 4.502392240192579, l1: 0.00010230098458756085, l2: 0.00034793824021977815   Iteration 47 of 100, tot loss = 4.488456315182625, l1: 0.00010183608785464845, l2: 0.0003470095444812459   Iteration 48 of 100, tot loss = 4.467683533827464, l1: 0.00010103429303853773, l2: 0.0003457340611324374   Iteration 49 of 100, tot loss = 4.533508115885209, l1: 0.00010188496808226847, l2: 0.00035146584440906516   Iteration 50 of 100, tot loss = 4.498690323829651, l1: 0.00010150359594263136, l2: 0.0003483654369483702   Iteration 51 of 100, tot loss = 4.528656094681983, l1: 0.00010227425395985883, l2: 0.00035059135595528297   Iteration 52 of 100, tot loss = 4.564396541852218, l1: 0.0001032835697025383, l2: 0.00035315608441193873   Iteration 53 of 100, tot loss = 4.557128937739246, l1: 0.00010322576459005195, l2: 0.0003524871286606627   Iteration 54 of 100, tot loss = 4.5934332167660745, l1: 0.00010401802918985831, l2: 0.00035532529242724803   Iteration 55 of 100, tot loss = 4.586685219678012, l1: 0.00010381271435604008, l2: 0.00035485580791084267   Iteration 56 of 100, tot loss = 4.59524981038911, l1: 0.0001038899963532458, l2: 0.00035563498516109703   Iteration 57 of 100, tot loss = 4.559924464476736, l1: 0.000103387848515954, l2: 0.00035260459831956643   Iteration 58 of 100, tot loss = 4.544641128901778, l1: 0.00010290869851877656, l2: 0.000351555414893263   Iteration 59 of 100, tot loss = 4.5654570167347535, l1: 0.0001035583927197445, l2: 0.00035298730900818956   Iteration 60 of 100, tot loss = 4.555959995587667, l1: 0.00010341657277119035, l2: 0.0003521794266513704   Iteration 61 of 100, tot loss = 4.55999487736186, l1: 0.00010379673373984692, l2: 0.0003522027536299171   Iteration 62 of 100, tot loss = 4.557189326132497, l1: 0.0001040289479856109, l2: 0.00035168998475719785   Iteration 63 of 100, tot loss = 4.5854015880160865, l1: 0.00010468353450830494, l2: 0.0003538566236365734   Iteration 64 of 100, tot loss = 4.546516999602318, l1: 0.00010380358730799344, l2: 0.0003508481120206852   Iteration 65 of 100, tot loss = 4.514537022663997, l1: 0.00010323646431341051, l2: 0.00034821723753478953   Iteration 66 of 100, tot loss = 4.514373602289142, l1: 0.00010311725925544813, l2: 0.0003483201005358503   Iteration 67 of 100, tot loss = 4.492487370078243, l1: 0.00010273053980634006, l2: 0.000346518197025645   Iteration 68 of 100, tot loss = 4.466940108467551, l1: 0.00010221327530042104, l2: 0.0003444807354004725   Iteration 69 of 100, tot loss = 4.458633747653685, l1: 0.00010212277055066754, l2: 0.000343740603927037   Iteration 70 of 100, tot loss = 4.46143445287432, l1: 0.00010215403485095262, l2: 0.00034398941040438203   Iteration 71 of 100, tot loss = 4.45002255305438, l1: 0.0001018301685894905, l2: 0.0003431720866552803   Iteration 72 of 100, tot loss = 4.459998809629017, l1: 0.00010194149803687146, l2: 0.00034405838272909424   Iteration 73 of 100, tot loss = 4.455875870299666, l1: 0.0001019844110562364, l2: 0.000343603176090305   Iteration 74 of 100, tot loss = 4.440025342477335, l1: 0.0001013683377095731, l2: 0.0003426341966071443   Iteration 75 of 100, tot loss = 4.456612529754639, l1: 0.0001018603422077528, l2: 0.0003438009104381005   Iteration 76 of 100, tot loss = 4.466075533314755, l1: 0.0001022674369347729, l2: 0.00034434011621143376   Iteration 77 of 100, tot loss = 4.466128943802475, l1: 0.00010218548153976112, l2: 0.00034442741246954475   Iteration 78 of 100, tot loss = 4.484863158984062, l1: 0.00010282022809312762, l2: 0.0003456660876205812   Iteration 79 of 100, tot loss = 4.487348810026917, l1: 0.00010288149943461795, l2: 0.0003458533816988568   Iteration 80 of 100, tot loss = 4.510206001996994, l1: 0.0001033650781664619, l2: 0.00034765552227327133   Iteration 81 of 100, tot loss = 4.529464692245295, l1: 0.00010370925915201862, l2: 0.00034923721025812863   Iteration 82 of 100, tot loss = 4.529849442040048, l1: 0.00010403995514845918, l2: 0.0003489449896378352   Iteration 83 of 100, tot loss = 4.566427311265325, l1: 0.00010437545494046006, l2: 0.0003522672765489937   Iteration 84 of 100, tot loss = 4.538962384064992, l1: 0.0001039340192749348, l2: 0.000349962219410199   Iteration 85 of 100, tot loss = 4.551265321058385, l1: 0.00010403838057433913, l2: 0.00035108815132202034   Iteration 86 of 100, tot loss = 4.560639755670414, l1: 0.00010397620640423007, l2: 0.00035208776907258947   Iteration 87 of 100, tot loss = 4.551011244455974, l1: 0.00010387684994185846, l2: 0.00035122427460469903   Iteration 88 of 100, tot loss = 4.554663305932825, l1: 0.00010409088748598482, l2: 0.0003513754431465217   Iteration 89 of 100, tot loss = 4.542261281709992, l1: 0.0001039573882449434, l2: 0.0003502687398119296   Iteration 90 of 100, tot loss = 4.548456655608283, l1: 0.00010415786758433872, l2: 0.0003506877980220856   Iteration 91 of 100, tot loss = 4.54987000633072, l1: 0.00010411215023870662, l2: 0.0003508748505827408   Iteration 92 of 100, tot loss = 4.5650781729946965, l1: 0.00010432978022277973, l2: 0.00035217803720708775   Iteration 93 of 100, tot loss = 4.559752825767763, l1: 0.00010427609524941221, l2: 0.0003516991874353299   Iteration 94 of 100, tot loss = 4.545585769288083, l1: 0.00010392568920190357, l2: 0.0003506328880866157   Iteration 95 of 100, tot loss = 4.581318719763505, l1: 0.00010436152366118653, l2: 0.0003537703483169408   Iteration 96 of 100, tot loss = 4.567409781118234, l1: 0.00010435356364268955, l2: 0.00035238741444724536   Iteration 97 of 100, tot loss = 4.588471218482735, l1: 0.00010471204463026447, l2: 0.0003541350774539001   Iteration 98 of 100, tot loss = 4.581044090037444, l1: 0.00010471260904932005, l2: 0.00035339180018980894   Iteration 99 of 100, tot loss = 4.592305896258114, l1: 0.00010487191082005427, l2: 0.0003543586794738517   Iteration 100 of 100, tot loss = 4.591666779518127, l1: 0.00010488883996004006, l2: 0.0003542778387782164
   End of epoch 1165; saving model... 

Epoch 1166 of 2000
   Iteration 1 of 100, tot loss = 2.7811195850372314, l1: 7.342729077208787e-05, l2: 0.00020468467846512794   Iteration 2 of 100, tot loss = 2.9887888431549072, l1: 8.06950920377858e-05, l2: 0.0002181837917305529   Iteration 3 of 100, tot loss = 3.7697043418884277, l1: 8.818585289797436e-05, l2: 0.00028878457184570533   Iteration 4 of 100, tot loss = 3.9577836990356445, l1: 9.604187835066114e-05, l2: 0.00029973647906444967   Iteration 5 of 100, tot loss = 4.164132595062256, l1: 9.879313438432291e-05, l2: 0.00031762010767124595   Iteration 6 of 100, tot loss = 4.112479408582051, l1: 9.651158325141296e-05, l2: 0.00031473634589929134   Iteration 7 of 100, tot loss = 4.210079635892596, l1: 9.884929957999182e-05, l2: 0.00032215865212492645   Iteration 8 of 100, tot loss = 4.348258942365646, l1: 0.00010254373955831397, l2: 0.00033228214306291193   Iteration 9 of 100, tot loss = 4.603594753477308, l1: 0.00010592410560800797, l2: 0.0003544353595417407   Iteration 10 of 100, tot loss = 4.432283878326416, l1: 0.00010303575618308969, l2: 0.0003401926209335215   Iteration 11 of 100, tot loss = 4.249301953749224, l1: 0.00010178818774875253, l2: 0.00032314199647358197   Iteration 12 of 100, tot loss = 4.303430716196696, l1: 0.00010318451071119246, l2: 0.0003271585495288794   Iteration 13 of 100, tot loss = 4.280348814450777, l1: 0.00010373307192751851, l2: 0.0003243017999920994   Iteration 14 of 100, tot loss = 4.537948165621076, l1: 0.00010775351516453416, l2: 0.0003460412948957777   Iteration 15 of 100, tot loss = 4.416522789001465, l1: 0.00010653096445215246, l2: 0.00033512130903545767   Iteration 16 of 100, tot loss = 4.615104079246521, l1: 0.0001095773086490226, l2: 0.0003519330921335495   Iteration 17 of 100, tot loss = 4.642956088570988, l1: 0.00011085333758006421, l2: 0.000353442262755433   Iteration 18 of 100, tot loss = 4.671671734915839, l1: 0.00011144056487763819, l2: 0.00035572660191165697   Iteration 19 of 100, tot loss = 4.59279233530948, l1: 0.00010818753511749061, l2: 0.00035109169134796644   Iteration 20 of 100, tot loss = 4.424115908145905, l1: 0.00010431694317958318, l2: 0.0003380946411198238   Iteration 21 of 100, tot loss = 4.386909927640643, l1: 0.00010202369940955014, l2: 0.0003366672875320849   Iteration 22 of 100, tot loss = 4.441833723675121, l1: 0.0001034161462236873, l2: 0.00034076722228994845   Iteration 23 of 100, tot loss = 4.4409757178762685, l1: 0.00010356539259310406, l2: 0.0003405321764623058   Iteration 24 of 100, tot loss = 4.457802961270015, l1: 0.00010350002624666861, l2: 0.0003422802683417103   Iteration 25 of 100, tot loss = 4.397859268188476, l1: 0.00010156752177863382, l2: 0.00033821840275777504   Iteration 26 of 100, tot loss = 4.417030206093421, l1: 0.00010164217509065146, l2: 0.0003400608433226947   Iteration 27 of 100, tot loss = 4.541977476190637, l1: 0.00010384498414674049, l2: 0.0003503527623252012   Iteration 28 of 100, tot loss = 4.486991550241198, l1: 0.00010315727052199821, l2: 0.00034554188329950975   Iteration 29 of 100, tot loss = 4.460755290656254, l1: 0.0001026722538567967, l2: 0.0003434032739184251   Iteration 30 of 100, tot loss = 4.438471666971842, l1: 0.00010206173719780054, l2: 0.00034178542822094947   Iteration 31 of 100, tot loss = 4.4515570209872335, l1: 0.00010288650915871614, l2: 0.0003422691921925082   Iteration 32 of 100, tot loss = 4.446730434894562, l1: 0.00010172637360028602, l2: 0.0003429466685247462   Iteration 33 of 100, tot loss = 4.458285707415956, l1: 0.00010258742754972032, l2: 0.0003432411412535602   Iteration 34 of 100, tot loss = 4.515272280749152, l1: 0.00010323870564581525, l2: 0.0003482885211483603   Iteration 35 of 100, tot loss = 4.510635730198452, l1: 0.00010394364165092287, l2: 0.00034711992962651754   Iteration 36 of 100, tot loss = 4.509522438049316, l1: 0.00010341206598241115, l2: 0.0003475401768405896   Iteration 37 of 100, tot loss = 4.5870648976918815, l1: 0.00010473982911207713, l2: 0.00035396666007236   Iteration 38 of 100, tot loss = 4.603438854217529, l1: 0.00010501109107664026, l2: 0.0003553327947692627   Iteration 39 of 100, tot loss = 4.6147972620450535, l1: 0.00010496671921618354, l2: 0.00035651300818128034   Iteration 40 of 100, tot loss = 4.615180134773254, l1: 0.00010515086769373738, l2: 0.0003563671472875285   Iteration 41 of 100, tot loss = 4.566641266753034, l1: 0.00010366749022689807, l2: 0.0003529966384010594   Iteration 42 of 100, tot loss = 4.6572132507960005, l1: 0.00010534879021135912, l2: 0.0003603725370859528   Iteration 43 of 100, tot loss = 4.66483518134716, l1: 0.00010592091949060993, l2: 0.00036056260060556884   Iteration 44 of 100, tot loss = 4.661399120634252, l1: 0.00010593274865558752, l2: 0.00036020716503447727   Iteration 45 of 100, tot loss = 4.653284396065606, l1: 0.00010561436097810252, l2: 0.00035971408085768217   Iteration 46 of 100, tot loss = 4.632834926895473, l1: 0.000105167035223491, l2: 0.0003581164598037266   Iteration 47 of 100, tot loss = 4.678042528477121, l1: 0.00010575164738606583, l2: 0.00036205260805721257   Iteration 48 of 100, tot loss = 4.63789668182532, l1: 0.00010494653285301563, l2: 0.0003588431380497544   Iteration 49 of 100, tot loss = 4.66977164696674, l1: 0.0001051924330994905, l2: 0.00036178473445553597   Iteration 50 of 100, tot loss = 4.661723084449768, l1: 0.00010429466856294311, l2: 0.00036187764271744527   Iteration 51 of 100, tot loss = 4.780693096273086, l1: 0.00010582054785439087, l2: 0.0003722487646668656   Iteration 52 of 100, tot loss = 4.772718929327452, l1: 0.00010560190387947772, l2: 0.00037166999181163893   Iteration 53 of 100, tot loss = 4.886893222916801, l1: 0.00010751320086507444, l2: 0.00038117612561074804   Iteration 54 of 100, tot loss = 4.893056079193398, l1: 0.00010759765212208515, l2: 0.00038170796041312214   Iteration 55 of 100, tot loss = 4.87475814819336, l1: 0.00010713373379654843, l2: 0.00038034208555473017   Iteration 56 of 100, tot loss = 4.940792049680438, l1: 0.00010811513857333921, l2: 0.0003859640711587937   Iteration 57 of 100, tot loss = 4.942300261112681, l1: 0.00010789093523686588, l2: 0.00038633909571217373   Iteration 58 of 100, tot loss = 4.937040945579266, l1: 0.00010798163415968483, l2: 0.0003857224652039867   Iteration 59 of 100, tot loss = 4.953824802980584, l1: 0.00010820775405282818, l2: 0.00038717473056502796   Iteration 60 of 100, tot loss = 4.92206787665685, l1: 0.0001076905199928054, l2: 0.00038451627151516734   Iteration 61 of 100, tot loss = 4.913675773339193, l1: 0.00010759478421084063, l2: 0.00038377279675752687   Iteration 62 of 100, tot loss = 4.935692244960416, l1: 0.0001077220733878353, l2: 0.000385847154018026   Iteration 63 of 100, tot loss = 4.920080885054573, l1: 0.00010749997189065205, l2: 0.00038450811945718697   Iteration 64 of 100, tot loss = 4.930913116782904, l1: 0.00010747994451776322, l2: 0.00038561137000669987   Iteration 65 of 100, tot loss = 4.925002894034752, l1: 0.00010747631246116585, l2: 0.00038502398002982283   Iteration 66 of 100, tot loss = 4.903476530855352, l1: 0.00010706983051601459, l2: 0.00038327782551112415   Iteration 67 of 100, tot loss = 4.878460976614881, l1: 0.00010633181710857829, l2: 0.0003815142833323691   Iteration 68 of 100, tot loss = 4.86554887715508, l1: 0.00010626222100494059, l2: 0.00038029266932640253   Iteration 69 of 100, tot loss = 4.830759729164234, l1: 0.00010541438862581845, l2: 0.00037766158683276126   Iteration 70 of 100, tot loss = 4.809920372281756, l1: 0.00010504705907286345, l2: 0.0003759449804160145   Iteration 71 of 100, tot loss = 4.7883663614031295, l1: 0.00010484172828359411, l2: 0.0003739949097507633   Iteration 72 of 100, tot loss = 4.7744483053684235, l1: 0.00010459683688976738, l2: 0.00037284799560034624   Iteration 73 of 100, tot loss = 4.809733139325495, l1: 0.00010525311792089145, l2: 0.0003757201977774233   Iteration 74 of 100, tot loss = 4.862876051181072, l1: 0.00010604135344250165, l2: 0.000380246253346716   Iteration 75 of 100, tot loss = 4.869519993464152, l1: 0.00010633319786090094, l2: 0.00038061880341653403   Iteration 76 of 100, tot loss = 4.868171174275248, l1: 0.0001066595554412312, l2: 0.000380157563803981   Iteration 77 of 100, tot loss = 4.8920633854804105, l1: 0.00010693745358579477, l2: 0.00038226888628068757   Iteration 78 of 100, tot loss = 4.901379625002543, l1: 0.00010718807061493862, l2: 0.0003829498931829734   Iteration 79 of 100, tot loss = 4.925853840912445, l1: 0.00010758935733767569, l2: 0.0003849960279855818   Iteration 80 of 100, tot loss = 4.9294189542531965, l1: 0.00010778534774544824, l2: 0.00038515654914590415   Iteration 81 of 100, tot loss = 4.931998391210297, l1: 0.00010784708582731482, l2: 0.00038535275486122566   Iteration 82 of 100, tot loss = 4.952409909992683, l1: 0.00010816900093476118, l2: 0.000387071992008714   Iteration 83 of 100, tot loss = 4.925356385219528, l1: 0.00010792564309492472, l2: 0.00038460999734612767   Iteration 84 of 100, tot loss = 4.938356101512909, l1: 0.00010800391945459913, l2: 0.0003858316930356003   Iteration 85 of 100, tot loss = 4.948252574135275, l1: 0.0001083404703582774, l2: 0.00038648478914504215   Iteration 86 of 100, tot loss = 4.933711947396744, l1: 0.00010817169700309277, l2: 0.0003851994999507063   Iteration 87 of 100, tot loss = 4.9416683158655275, l1: 0.00010775592028071044, l2: 0.00038641091309362036   Iteration 88 of 100, tot loss = 4.93626762249253, l1: 0.00010751091556151302, l2: 0.00038611584824882976   Iteration 89 of 100, tot loss = 4.942463995365614, l1: 0.00010776745522194647, l2: 0.0003864789458306956   Iteration 90 of 100, tot loss = 4.927910399436951, l1: 0.00010719302687599945, l2: 0.00038559801444838136   Iteration 91 of 100, tot loss = 4.897228466285454, l1: 0.00010649378888486824, l2: 0.0003832290592701891   Iteration 92 of 100, tot loss = 4.884660632713981, l1: 0.00010638666963527415, l2: 0.00038207939526372405   Iteration 93 of 100, tot loss = 4.863386028556413, l1: 0.00010605710274596217, l2: 0.00038028150189465154   Iteration 94 of 100, tot loss = 4.889290898404223, l1: 0.00010658740339227308, l2: 0.00038234168808211257   Iteration 95 of 100, tot loss = 4.876125687047055, l1: 0.00010639248798006999, l2: 0.0003812200824062242   Iteration 96 of 100, tot loss = 4.875782489776611, l1: 0.00010653185264194083, l2: 0.00038104639793345996   Iteration 97 of 100, tot loss = 4.863509713988943, l1: 0.0001065342424310787, l2: 0.0003798167306309345   Iteration 98 of 100, tot loss = 4.8538275640838, l1: 0.00010629307021468887, l2: 0.00037908968799308   Iteration 99 of 100, tot loss = 4.857282315841829, l1: 0.00010644211487478878, l2: 0.00037928611871191405   Iteration 100 of 100, tot loss = 4.8411286616325375, l1: 0.00010615718671033391, l2: 0.00037795568139699755
   End of epoch 1166; saving model... 

Epoch 1167 of 2000
   Iteration 1 of 100, tot loss = 4.783303260803223, l1: 0.00011185277253389359, l2: 0.00036647755769081414   Iteration 2 of 100, tot loss = 4.450529098510742, l1: 0.0001005030280794017, l2: 0.0003445498732617125   Iteration 3 of 100, tot loss = 4.02534818649292, l1: 9.346050743867333e-05, l2: 0.00030907430724861723   Iteration 4 of 100, tot loss = 4.4799216985702515, l1: 9.695053631730843e-05, l2: 0.00035104163544019684   Iteration 5 of 100, tot loss = 4.841527652740479, l1: 0.00010711949580581859, l2: 0.0003770332725252956   Iteration 6 of 100, tot loss = 4.442700982093811, l1: 0.00010063833057453546, l2: 0.00034363176867676276   Iteration 7 of 100, tot loss = 4.201393570218768, l1: 9.945082586325173e-05, l2: 0.0003206885324159105   Iteration 8 of 100, tot loss = 4.384489923715591, l1: 9.623696496419143e-05, l2: 0.0003422120225877734   Iteration 9 of 100, tot loss = 4.2749663723839655, l1: 9.611349539934761e-05, l2: 0.00033138313544138026   Iteration 10 of 100, tot loss = 4.136327862739563, l1: 9.487161878496408e-05, l2: 0.0003187611626344733   Iteration 11 of 100, tot loss = 4.333268490704623, l1: 9.55120320378972e-05, l2: 0.000337814815628173   Iteration 12 of 100, tot loss = 4.290248155593872, l1: 9.619351961494733e-05, l2: 0.00033283129475118284   Iteration 13 of 100, tot loss = 4.234790802001953, l1: 9.640843256000572e-05, l2: 0.0003270706455133712   Iteration 14 of 100, tot loss = 4.358827148165021, l1: 9.665566904004663e-05, l2: 0.0003392270456450725   Iteration 15 of 100, tot loss = 4.65545244216919, l1: 0.000101736814637358, l2: 0.00036380843084771186   Iteration 16 of 100, tot loss = 4.557774096727371, l1: 9.893435117191984e-05, l2: 0.000356843059307721   Iteration 17 of 100, tot loss = 4.521201905082254, l1: 9.811068294391803e-05, l2: 0.0003540095083949649   Iteration 18 of 100, tot loss = 4.632086316744487, l1: 9.863646482699551e-05, l2: 0.00036457216790747933   Iteration 19 of 100, tot loss = 4.678532638047871, l1: 0.00010042163998341973, l2: 0.00036743162490893155   Iteration 20 of 100, tot loss = 4.708130705356598, l1: 0.00010158650438825134, l2: 0.0003692265680001583   Iteration 21 of 100, tot loss = 4.701613641920543, l1: 0.00010184596543521842, l2: 0.00036831540152585757   Iteration 22 of 100, tot loss = 4.65365102074363, l1: 0.00010152412505703978, l2: 0.00036384098017482427   Iteration 23 of 100, tot loss = 4.896590025528617, l1: 0.00010485076136714981, l2: 0.000384808244036875   Iteration 24 of 100, tot loss = 4.902797559897105, l1: 0.00010490969937867096, l2: 0.00038537006003025454   Iteration 25 of 100, tot loss = 4.924072113037109, l1: 0.00010537948255660013, l2: 0.00038702773104887456   Iteration 26 of 100, tot loss = 4.910625787881704, l1: 0.00010556993332172099, l2: 0.00038549264774281677   Iteration 27 of 100, tot loss = 4.832564618852404, l1: 0.00010484723747207749, l2: 0.0003784092259593308   Iteration 28 of 100, tot loss = 4.808414101600647, l1: 0.00010535580388802503, l2: 0.0003754856069073347   Iteration 29 of 100, tot loss = 4.802520488870555, l1: 0.00010607511021903362, l2: 0.0003741769399092501   Iteration 30 of 100, tot loss = 4.759866078694661, l1: 0.00010614023218901518, l2: 0.00036984637748294823   Iteration 31 of 100, tot loss = 4.748302152079921, l1: 0.00010620559131043152, l2: 0.00036862462612559957   Iteration 32 of 100, tot loss = 4.731836453080177, l1: 0.00010635447961249156, l2: 0.00036682916743302485   Iteration 33 of 100, tot loss = 4.7043046734549785, l1: 0.00010669692247492176, l2: 0.0003637335471329138   Iteration 34 of 100, tot loss = 4.703598716679742, l1: 0.00010690519283183247, l2: 0.00036345468149246536   Iteration 35 of 100, tot loss = 4.692529508045742, l1: 0.00010747617592902056, l2: 0.00036177677767617363   Iteration 36 of 100, tot loss = 4.6830918457773, l1: 0.00010720210613928632, l2: 0.00036110708040521585   Iteration 37 of 100, tot loss = 4.677736031042563, l1: 0.00010676551244467044, l2: 0.00036100809292465046   Iteration 38 of 100, tot loss = 4.623924016952515, l1: 0.00010597692801782519, l2: 0.00035641547583509237   Iteration 39 of 100, tot loss = 4.597570193119538, l1: 0.00010545306371895071, l2: 0.00035430395748848334   Iteration 40 of 100, tot loss = 4.621267586946487, l1: 0.00010514235127629945, l2: 0.0003569844091543928   Iteration 41 of 100, tot loss = 4.599588871002197, l1: 0.00010405574890106304, l2: 0.00035590314016700157   Iteration 42 of 100, tot loss = 4.576639981496902, l1: 0.00010381381582105643, l2: 0.00035385018452957625   Iteration 43 of 100, tot loss = 4.566430491070415, l1: 0.00010401487503089769, l2: 0.00035262817644167606   Iteration 44 of 100, tot loss = 4.559733791784807, l1: 0.00010360346517369511, l2: 0.0003523699164030735   Iteration 45 of 100, tot loss = 4.592056655883789, l1: 0.0001044358906963478, l2: 0.00035476977742897965   Iteration 46 of 100, tot loss = 4.531796286935392, l1: 0.00010323306299181675, l2: 0.00034994656807216614   Iteration 47 of 100, tot loss = 4.523860502750315, l1: 0.00010348357527425136, l2: 0.0003489024771199106   Iteration 48 of 100, tot loss = 4.5146109734972315, l1: 0.00010364072454649431, l2: 0.00034782037437253166   Iteration 49 of 100, tot loss = 4.508169930808398, l1: 0.00010391194303579419, l2: 0.00034690505114612076   Iteration 50 of 100, tot loss = 4.605895707607269, l1: 0.00010526666424993891, l2: 0.00035532290756236764   Iteration 51 of 100, tot loss = 4.540955592604244, l1: 0.00010383011006827776, l2: 0.0003502654504058335   Iteration 52 of 100, tot loss = 4.5465021798243885, l1: 0.00010388373196362339, l2: 0.0003507664872673698   Iteration 53 of 100, tot loss = 4.515155092725214, l1: 0.00010299191785205314, l2: 0.00034852359303325976   Iteration 54 of 100, tot loss = 4.524254253617039, l1: 0.00010310743629007548, l2: 0.00034931798991574733   Iteration 55 of 100, tot loss = 4.513481640815735, l1: 0.00010282227601485581, l2: 0.00034852588856169447   Iteration 56 of 100, tot loss = 4.552457943558693, l1: 0.00010369945584898232, l2: 0.000351546338996351   Iteration 57 of 100, tot loss = 4.536083775654173, l1: 0.0001036911924433968, l2: 0.00034991718559084753   Iteration 58 of 100, tot loss = 4.516423716627318, l1: 0.00010323804781364743, l2: 0.0003484043243525807   Iteration 59 of 100, tot loss = 4.525623553890293, l1: 0.00010360292583492869, l2: 0.0003489594303773937   Iteration 60 of 100, tot loss = 4.504774155219396, l1: 0.00010275932454533177, l2: 0.00034771809199204046   Iteration 61 of 100, tot loss = 4.469822596331111, l1: 0.00010213202606747308, l2: 0.00034485023457282146   Iteration 62 of 100, tot loss = 4.489012112540584, l1: 0.0001023439279144382, l2: 0.0003465572850883848   Iteration 63 of 100, tot loss = 4.454464891600231, l1: 0.00010173974032301293, l2: 0.0003437067505382445   Iteration 64 of 100, tot loss = 4.462769301608205, l1: 0.00010173229026122499, l2: 0.00034454464207556157   Iteration 65 of 100, tot loss = 4.457895449491647, l1: 0.00010171717277933987, l2: 0.0003440723738794287   Iteration 66 of 100, tot loss = 4.443606941988974, l1: 0.00010154409144227181, l2: 0.0003428166045606221   Iteration 67 of 100, tot loss = 4.452499348725846, l1: 0.00010142751652078439, l2: 0.0003438224195443622   Iteration 68 of 100, tot loss = 4.475307508426554, l1: 0.00010189317847963299, l2: 0.0003456375739273533   Iteration 69 of 100, tot loss = 4.503173435943714, l1: 0.00010233147564787836, l2: 0.0003479858689546666   Iteration 70 of 100, tot loss = 4.502521646022797, l1: 0.00010263502496984853, l2: 0.0003476171402975784   Iteration 71 of 100, tot loss = 4.474164655510808, l1: 0.00010207415752901538, l2: 0.00034534230846321396   Iteration 72 of 100, tot loss = 4.527559321787622, l1: 0.00010266052519606698, l2: 0.00035009540725796897   Iteration 73 of 100, tot loss = 4.511193384862926, l1: 0.00010239508997510213, l2: 0.0003487242486213066   Iteration 74 of 100, tot loss = 4.497125646552524, l1: 0.0001024741286036678, l2: 0.00034723843595742975   Iteration 75 of 100, tot loss = 4.480794943173726, l1: 0.00010219974467569652, l2: 0.0003458797497053941   Iteration 76 of 100, tot loss = 4.472633997076436, l1: 0.00010231055866346364, l2: 0.00034495284145171016   Iteration 77 of 100, tot loss = 4.476841463671102, l1: 0.00010237007683661597, l2: 0.00034531407011370477   Iteration 78 of 100, tot loss = 4.497827892120068, l1: 0.00010290402240524343, l2: 0.0003468787673717508   Iteration 79 of 100, tot loss = 4.498309547388101, l1: 0.0001026396205390161, l2: 0.0003471913345193467   Iteration 80 of 100, tot loss = 4.493506629765034, l1: 0.00010274107767145324, l2: 0.00034660958554013634   Iteration 81 of 100, tot loss = 4.4944156496613115, l1: 0.00010272005929886133, l2: 0.00034672150569450524   Iteration 82 of 100, tot loss = 4.479791442068612, l1: 0.00010229478024617803, l2: 0.00034568436399795024   Iteration 83 of 100, tot loss = 4.46923986544092, l1: 0.00010207379928471644, l2: 0.00034485018744795435   Iteration 84 of 100, tot loss = 4.4714835697696325, l1: 0.00010232912613684588, l2: 0.0003448192312914346   Iteration 85 of 100, tot loss = 4.48474805355072, l1: 0.00010264671192785232, l2: 0.0003458280938074869   Iteration 86 of 100, tot loss = 4.501850298670835, l1: 0.00010283839113727417, l2: 0.000347346639352699   Iteration 87 of 100, tot loss = 4.488531355200143, l1: 0.00010244157589301628, l2: 0.0003464115601589625   Iteration 88 of 100, tot loss = 4.460884510116144, l1: 0.00010171637483257324, l2: 0.00034437207663855094   Iteration 89 of 100, tot loss = 4.457263913047448, l1: 0.00010164998555287792, l2: 0.0003440764060981697   Iteration 90 of 100, tot loss = 4.457963618967268, l1: 0.00010181839376552185, l2: 0.00034397796844132245   Iteration 91 of 100, tot loss = 4.491658812040811, l1: 0.00010230352555671554, l2: 0.00034686235600959634   Iteration 92 of 100, tot loss = 4.510401211355044, l1: 0.0001023334176036018, l2: 0.00034870670376973146   Iteration 93 of 100, tot loss = 4.517022875047499, l1: 0.00010218828800290832, l2: 0.00034951399986742326   Iteration 94 of 100, tot loss = 4.527289308132009, l1: 0.00010219289226292762, l2: 0.0003505360385888514   Iteration 95 of 100, tot loss = 4.516952771889536, l1: 0.00010193312210551659, l2: 0.0003497621550004145   Iteration 96 of 100, tot loss = 4.508446130901575, l1: 0.00010169329247370722, l2: 0.00034915132043048896   Iteration 97 of 100, tot loss = 4.511026892465415, l1: 0.0001018608560753847, l2: 0.0003492418327368796   Iteration 98 of 100, tot loss = 4.5189777770820925, l1: 0.00010204367301449399, l2: 0.00034985410427313524   Iteration 99 of 100, tot loss = 4.510403330879982, l1: 0.0001018731473786921, l2: 0.0003491671852334732   Iteration 100 of 100, tot loss = 4.546376701593399, l1: 0.00010230869633232941, l2: 0.00035232897324021905
   End of epoch 1167; saving model... 

Epoch 1168 of 2000
   Iteration 1 of 100, tot loss = 3.378826379776001, l1: 6.925983325345442e-05, l2: 0.00026862279628403485   Iteration 2 of 100, tot loss = 4.204589009284973, l1: 8.017126674531028e-05, l2: 0.00034028763184323907   Iteration 3 of 100, tot loss = 4.104142904281616, l1: 8.725381000355507e-05, l2: 0.00032316047387818497   Iteration 4 of 100, tot loss = 4.626659691333771, l1: 9.200092790706549e-05, l2: 0.0003706650313688442   Iteration 5 of 100, tot loss = 4.719840955734253, l1: 9.727899014251306e-05, l2: 0.0003747051057871431   Iteration 6 of 100, tot loss = 4.6071815093358355, l1: 9.60656094927496e-05, l2: 0.00036465254379436374   Iteration 7 of 100, tot loss = 4.480799777167184, l1: 9.614119855021792e-05, l2: 0.0003519387807630535   Iteration 8 of 100, tot loss = 4.497923165559769, l1: 9.895725816022605e-05, l2: 0.00035083506008959375   Iteration 9 of 100, tot loss = 4.637287908130222, l1: 9.983421882174702e-05, l2: 0.0003638945733352254   Iteration 10 of 100, tot loss = 4.661814332008362, l1: 9.955684799933806e-05, l2: 0.0003666245844215155   Iteration 11 of 100, tot loss = 4.656792098825628, l1: 9.982272273522209e-05, l2: 0.00036585648988627577   Iteration 12 of 100, tot loss = 4.548907458782196, l1: 0.00010030286163479711, l2: 0.00035458788746230613   Iteration 13 of 100, tot loss = 4.3928016149080715, l1: 9.63095573332304e-05, l2: 0.0003429706053933702   Iteration 14 of 100, tot loss = 4.599103723253522, l1: 0.00010030959778565116, l2: 0.00035960077470268255   Iteration 15 of 100, tot loss = 4.57785930633545, l1: 9.979418343088279e-05, l2: 0.00035799174705365054   Iteration 16 of 100, tot loss = 4.726884692907333, l1: 0.00010202319845120655, l2: 0.0003706652705659508   Iteration 17 of 100, tot loss = 4.632704286014333, l1: 0.00010013195680906338, l2: 0.000363138471798533   Iteration 18 of 100, tot loss = 4.556559483210246, l1: 9.879616684176856e-05, l2: 0.00035685978137836274   Iteration 19 of 100, tot loss = 4.570499846809788, l1: 9.983693177529953e-05, l2: 0.0003572130532886245   Iteration 20 of 100, tot loss = 4.548779129981995, l1: 9.978022571885959e-05, l2: 0.00035509768640622496   Iteration 21 of 100, tot loss = 4.556617101033528, l1: 0.00010001803491918726, l2: 0.000355643674819952   Iteration 22 of 100, tot loss = 4.560110677372325, l1: 0.00010019672349285842, l2: 0.00035581434415441686   Iteration 23 of 100, tot loss = 4.552287474922512, l1: 0.0001005940699226061, l2: 0.0003546346776167174   Iteration 24 of 100, tot loss = 4.635723014672597, l1: 0.00010081978295299147, l2: 0.0003627525184128899   Iteration 25 of 100, tot loss = 4.827910785675049, l1: 0.00010444287298014388, l2: 0.0003783482045400888   Iteration 26 of 100, tot loss = 4.750469088554382, l1: 0.00010254058641014405, l2: 0.00037250632158247754   Iteration 27 of 100, tot loss = 4.744609099847299, l1: 0.00010218365463919731, l2: 0.00037227725314787003   Iteration 28 of 100, tot loss = 4.734911501407623, l1: 0.00010178828649389158, l2: 0.0003717028620095724   Iteration 29 of 100, tot loss = 4.708681188780686, l1: 0.00010167575513558655, l2: 0.0003691923624940281   Iteration 30 of 100, tot loss = 4.741045586268107, l1: 0.00010296130252148335, l2: 0.00037114325435444093   Iteration 31 of 100, tot loss = 4.717510038806546, l1: 0.0001026713106247987, l2: 0.00036907969221037124   Iteration 32 of 100, tot loss = 4.751893445849419, l1: 0.00010329983024348621, l2: 0.0003718895118254295   Iteration 33 of 100, tot loss = 4.691554380185677, l1: 0.00010224013054870407, l2: 0.0003669153057589111   Iteration 34 of 100, tot loss = 4.797882998690886, l1: 0.00010395134963533458, l2: 0.0003758369481040384   Iteration 35 of 100, tot loss = 4.814437655040196, l1: 0.00010416027819571484, l2: 0.0003772834848080363   Iteration 36 of 100, tot loss = 4.789917005432977, l1: 0.00010463638864166569, l2: 0.0003743553097592667   Iteration 37 of 100, tot loss = 4.832118588524896, l1: 0.00010546777933061978, l2: 0.00037774407757899245   Iteration 38 of 100, tot loss = 4.8399808783280225, l1: 0.00010551375185298782, l2: 0.0003784843337550563   Iteration 39 of 100, tot loss = 4.828898197565323, l1: 0.00010518679501285824, l2: 0.0003777030222595502   Iteration 40 of 100, tot loss = 4.819299554824829, l1: 0.00010555277876846957, l2: 0.0003763771746889688   Iteration 41 of 100, tot loss = 4.853021691485149, l1: 0.00010655654726334189, l2: 0.0003787456193893421   Iteration 42 of 100, tot loss = 4.899153675351824, l1: 0.00010740373510911705, l2: 0.0003825116298338842   Iteration 43 of 100, tot loss = 4.934465020201927, l1: 0.00010827231270986675, l2: 0.0003851741866403541   Iteration 44 of 100, tot loss = 4.935269344936717, l1: 0.00010855779344803358, l2: 0.0003849691374026324   Iteration 45 of 100, tot loss = 4.87005844116211, l1: 0.00010753450624179095, l2: 0.00037947133403374916   Iteration 46 of 100, tot loss = 4.838049722754437, l1: 0.00010710274648812154, l2: 0.0003767022217684628   Iteration 47 of 100, tot loss = 4.829690152026237, l1: 0.00010669832455336136, l2: 0.0003762706870242874   Iteration 48 of 100, tot loss = 4.82278369863828, l1: 0.00010690631036898897, l2: 0.0003753720557142515   Iteration 49 of 100, tot loss = 4.8343197764182575, l1: 0.00010764839160921318, l2: 0.00037578358291173166   Iteration 50 of 100, tot loss = 4.8507723903656, l1: 0.00010789684471092187, l2: 0.0003771803918061778   Iteration 51 of 100, tot loss = 4.879082717147528, l1: 0.00010821876169168664, l2: 0.0003796895074557659   Iteration 52 of 100, tot loss = 4.909260373849135, l1: 0.0001090977349611841, l2: 0.0003818282997459531   Iteration 53 of 100, tot loss = 4.86473818994918, l1: 0.00010784992600575897, l2: 0.0003786238900469265   Iteration 54 of 100, tot loss = 4.836519784397549, l1: 0.0001074108559232964, l2: 0.0003762411196711818   Iteration 55 of 100, tot loss = 4.800406252254139, l1: 0.00010681613732065836, l2: 0.000373224484924735   Iteration 56 of 100, tot loss = 4.793480400528226, l1: 0.00010626231388804237, l2: 0.0003730857227180552   Iteration 57 of 100, tot loss = 4.777421838358829, l1: 0.00010620554888175735, l2: 0.00037153663160687867   Iteration 58 of 100, tot loss = 4.755648719853368, l1: 0.00010531908020343038, l2: 0.00037024578818216407   Iteration 59 of 100, tot loss = 4.7766363499528275, l1: 0.00010561417600572094, l2: 0.0003720494549547843   Iteration 60 of 100, tot loss = 4.7918819745381676, l1: 0.0001061222018203504, l2: 0.00037306599212267124   Iteration 61 of 100, tot loss = 4.784442502944196, l1: 0.00010602308808659875, l2: 0.0003724211591227194   Iteration 62 of 100, tot loss = 4.792583750140283, l1: 0.00010618799951184736, l2: 0.0003730703718892689   Iteration 63 of 100, tot loss = 4.809259687151227, l1: 0.00010667403538328861, l2: 0.00037425193041267375   Iteration 64 of 100, tot loss = 4.778681751340628, l1: 0.00010640919498428048, l2: 0.00037145897704249364   Iteration 65 of 100, tot loss = 4.8098525597498965, l1: 0.00010673717697500252, l2: 0.00037424807535269515   Iteration 66 of 100, tot loss = 4.810025688373681, l1: 0.0001068628170094488, l2: 0.0003741397482702847   Iteration 67 of 100, tot loss = 4.817392281631925, l1: 0.00010697151115728862, l2: 0.000374767713425959   Iteration 68 of 100, tot loss = 4.798732690951404, l1: 0.0001066824176151011, l2: 0.000373190847954795   Iteration 69 of 100, tot loss = 4.765267710754837, l1: 0.00010620266597918774, l2: 0.00037032410150657046   Iteration 70 of 100, tot loss = 4.74726528780801, l1: 0.00010579433848241543, l2: 0.00036893218660095174   Iteration 71 of 100, tot loss = 4.821864588159911, l1: 0.00010682069990598649, l2: 0.0003753657558109795   Iteration 72 of 100, tot loss = 4.7805084768268795, l1: 0.00010594867874008034, l2: 0.0003721021659859818   Iteration 73 of 100, tot loss = 4.768412836610454, l1: 0.00010603800701449485, l2: 0.0003708032739654898   Iteration 74 of 100, tot loss = 4.750616071997462, l1: 0.0001055794997916343, l2: 0.00036948210500004525   Iteration 75 of 100, tot loss = 4.747897578875224, l1: 0.00010541545615221063, l2: 0.0003693742991890758   Iteration 76 of 100, tot loss = 4.736483760570225, l1: 0.00010517513855072473, l2: 0.0003684732353002274   Iteration 77 of 100, tot loss = 4.73186279117287, l1: 0.00010514089610131288, l2: 0.0003680453810375184   Iteration 78 of 100, tot loss = 4.744513601828844, l1: 0.00010560503403375593, l2: 0.0003688463244217042   Iteration 79 of 100, tot loss = 4.725588579721089, l1: 0.00010539811801554376, l2: 0.0003671607384281351   Iteration 80 of 100, tot loss = 4.704240266978741, l1: 0.00010506096241442719, l2: 0.00036536306270136266   Iteration 81 of 100, tot loss = 4.719056339911472, l1: 0.00010532431859942729, l2: 0.0003665813141039653   Iteration 82 of 100, tot loss = 4.710497302253072, l1: 0.00010490894339632289, l2: 0.00036614078537861977   Iteration 83 of 100, tot loss = 4.715208908161485, l1: 0.00010518373340138246, l2: 0.00036633715572828676   Iteration 84 of 100, tot loss = 4.719637114377249, l1: 0.00010504676087986722, l2: 0.0003669169487866817   Iteration 85 of 100, tot loss = 4.737750837382149, l1: 0.00010557009595418896, l2: 0.0003682049862725441   Iteration 86 of 100, tot loss = 4.739512852458066, l1: 0.00010585037772787388, l2: 0.000368100905900572   Iteration 87 of 100, tot loss = 4.719843525996153, l1: 0.00010522116594827445, l2: 0.0003667631851918674   Iteration 88 of 100, tot loss = 4.724502364342863, l1: 0.00010514175102633254, l2: 0.000367308484121829   Iteration 89 of 100, tot loss = 4.761152611689621, l1: 0.00010566377905694495, l2: 0.00037045148086833527   Iteration 90 of 100, tot loss = 4.738194013966455, l1: 0.00010525960711270778, l2: 0.00036855979301940857   Iteration 91 of 100, tot loss = 4.72190167746701, l1: 0.00010499989162621365, l2: 0.00036719027499842284   Iteration 92 of 100, tot loss = 4.7371975792490915, l1: 0.00010536155470775987, l2: 0.00036835820197745267   Iteration 93 of 100, tot loss = 4.765088300551137, l1: 0.00010553101357448125, l2: 0.00037097781539834556   Iteration 94 of 100, tot loss = 4.764806908495883, l1: 0.00010571133269819411, l2: 0.0003707693568356533   Iteration 95 of 100, tot loss = 4.756229053045574, l1: 0.00010563918959202343, l2: 0.0003699837145273034   Iteration 96 of 100, tot loss = 4.723290480673313, l1: 0.00010491212147674862, l2: 0.0003674169253523966   Iteration 97 of 100, tot loss = 4.728986713075146, l1: 0.00010501982791369694, l2: 0.0003678788417186972   Iteration 98 of 100, tot loss = 4.73450811298526, l1: 0.00010519316451884189, l2: 0.00036825764507330877   Iteration 99 of 100, tot loss = 4.7466205804034916, l1: 0.00010528550755951055, l2: 0.00036937654879169935   Iteration 100 of 100, tot loss = 4.731749851703643, l1: 0.00010506459428142989, l2: 0.0003681103892449755
   End of epoch 1168; saving model... 

Epoch 1169 of 2000
   Iteration 1 of 100, tot loss = 4.5568718910217285, l1: 0.00010422864579595625, l2: 0.00035145855508744717   Iteration 2 of 100, tot loss = 4.064857482910156, l1: 9.987168596126139e-05, l2: 0.0003066140634473413   Iteration 3 of 100, tot loss = 4.064002354939778, l1: 0.00010224951741596063, l2: 0.0003041507249387602   Iteration 4 of 100, tot loss = 3.9605149626731873, l1: 0.000105634926512721, l2: 0.00029041657398920506   Iteration 5 of 100, tot loss = 4.584058618545532, l1: 0.00010729769710451364, l2: 0.0003511081682518125   Iteration 6 of 100, tot loss = 4.635381976763408, l1: 0.00010830518537356208, l2: 0.0003552330211581041   Iteration 7 of 100, tot loss = 4.749726942607334, l1: 0.00010927125653584621, l2: 0.0003657014500017145   Iteration 8 of 100, tot loss = 5.225644916296005, l1: 0.00011427198296587449, l2: 0.00040829252611729316   Iteration 9 of 100, tot loss = 5.13578216234843, l1: 0.0001122499169367883, l2: 0.000401328311353508   Iteration 10 of 100, tot loss = 5.101659607887268, l1: 0.00011310556656098924, l2: 0.0003970604040659964   Iteration 11 of 100, tot loss = 5.129505482586947, l1: 0.00011463037927486849, l2: 0.0003983201776546511   Iteration 12 of 100, tot loss = 4.8063665727774305, l1: 0.00010771962267123551, l2: 0.0003729170427201704   Iteration 13 of 100, tot loss = 4.791646122932434, l1: 0.00010951612477163927, l2: 0.00036964849669647473   Iteration 14 of 100, tot loss = 4.677332273551396, l1: 0.00010759497438682177, l2: 0.0003601382616450012   Iteration 15 of 100, tot loss = 4.610874851544698, l1: 0.00010596875387515562, l2: 0.00035511874060224123   Iteration 16 of 100, tot loss = 4.674565441906452, l1: 0.00010516064662624558, l2: 0.00036229590705261216   Iteration 17 of 100, tot loss = 4.690950512886047, l1: 0.00010579892306636525, l2: 0.0003632961379480548   Iteration 18 of 100, tot loss = 4.872723956902822, l1: 0.00010875230327656027, l2: 0.000378520102256314   Iteration 19 of 100, tot loss = 4.856344103813171, l1: 0.00010837045307485631, l2: 0.00037726396610531464   Iteration 20 of 100, tot loss = 4.9752611219882965, l1: 0.0001092795768272481, l2: 0.00038824654220661616   Iteration 21 of 100, tot loss = 5.005948526518686, l1: 0.00011082417231158442, l2: 0.00038977068678442653   Iteration 22 of 100, tot loss = 4.881181754849174, l1: 0.00010867681091688361, l2: 0.00037944137114788066   Iteration 23 of 100, tot loss = 4.853895037070565, l1: 0.00010834006056741245, l2: 0.00037704944849753264   Iteration 24 of 100, tot loss = 4.828279907504718, l1: 0.00010807674622507572, l2: 0.0003747512494859014   Iteration 25 of 100, tot loss = 4.807699275016785, l1: 0.00010744691084255465, l2: 0.0003733230222132988   Iteration 26 of 100, tot loss = 4.856838661890763, l1: 0.00010770642774542257, l2: 0.0003779774440492754   Iteration 27 of 100, tot loss = 4.895624474242881, l1: 0.00010790202011574163, l2: 0.0003816604323758468   Iteration 28 of 100, tot loss = 4.960753521748951, l1: 0.00010894499467082954, l2: 0.0003871303636385294   Iteration 29 of 100, tot loss = 4.976955442593016, l1: 0.00010880591096416726, l2: 0.00038888964100524076   Iteration 30 of 100, tot loss = 4.912619864940643, l1: 0.00010784321142030725, l2: 0.00038341878268208045   Iteration 31 of 100, tot loss = 4.903019339807572, l1: 0.00010836909132194705, l2: 0.00038193284994885026   Iteration 32 of 100, tot loss = 4.963563237339258, l1: 0.00010951916908652493, l2: 0.00038683716115883726   Iteration 33 of 100, tot loss = 4.936941627300147, l1: 0.00010886523909214179, l2: 0.00038482893014037416   Iteration 34 of 100, tot loss = 4.876172272598042, l1: 0.00010782952524416888, l2: 0.0003797877081310349   Iteration 35 of 100, tot loss = 4.8868577786854335, l1: 0.00010860900209601304, l2: 0.00038007678210435965   Iteration 36 of 100, tot loss = 4.833436975876491, l1: 0.00010698900081883444, l2: 0.0003763547030959873   Iteration 37 of 100, tot loss = 4.813169669460606, l1: 0.00010650294987843497, l2: 0.0003748140232936429   Iteration 38 of 100, tot loss = 4.7821034036184615, l1: 0.00010605785887731288, l2: 0.0003721524873598307   Iteration 39 of 100, tot loss = 4.745702997232095, l1: 0.00010573710348510828, l2: 0.0003688332017657395   Iteration 40 of 100, tot loss = 4.73933992087841, l1: 0.00010573988311080029, l2: 0.0003681941141621792   Iteration 41 of 100, tot loss = 4.73442983336565, l1: 0.00010563162273994261, l2: 0.00036781136583247245   Iteration 42 of 100, tot loss = 4.782127184527261, l1: 0.00010599053157853806, l2: 0.0003722221911067165   Iteration 43 of 100, tot loss = 4.730669972508452, l1: 0.00010538191671975954, l2: 0.00036768508526133806   Iteration 44 of 100, tot loss = 4.755204512314363, l1: 0.00010597059620274443, l2: 0.0003695498594011455   Iteration 45 of 100, tot loss = 4.726236255963643, l1: 0.0001053833139141918, l2: 0.00036724031621512647   Iteration 46 of 100, tot loss = 4.773164093494415, l1: 0.00010552769621743052, l2: 0.00037178871776711236   Iteration 47 of 100, tot loss = 4.812641851445462, l1: 0.0001065040464225681, l2: 0.00037476014264765136   Iteration 48 of 100, tot loss = 4.772886288662751, l1: 0.00010576713930277037, l2: 0.0003715214932829743   Iteration 49 of 100, tot loss = 4.779000051167547, l1: 0.00010619327655817591, l2: 0.00037170673244780084   Iteration 50 of 100, tot loss = 4.774777576923371, l1: 0.00010642902270774357, l2: 0.0003710487390344497   Iteration 51 of 100, tot loss = 4.798918847944222, l1: 0.0001070586596124842, l2: 0.00037283322921675613   Iteration 52 of 100, tot loss = 4.777777201854265, l1: 0.0001063879759525522, l2: 0.00037138974794988125   Iteration 53 of 100, tot loss = 4.724935153745255, l1: 0.00010552281172801126, l2: 0.00036697070740870604   Iteration 54 of 100, tot loss = 4.744717235918398, l1: 0.00010540944559680711, l2: 0.0003690622819434524   Iteration 55 of 100, tot loss = 4.760595139590177, l1: 0.0001060348117872226, l2: 0.0003700247062211433   Iteration 56 of 100, tot loss = 4.7272162096841, l1: 0.00010579102269860283, l2: 0.00036693060207783546   Iteration 57 of 100, tot loss = 4.697615150819745, l1: 0.00010529635453662067, l2: 0.00036446516427084835   Iteration 58 of 100, tot loss = 4.665958042802481, l1: 0.00010490095519344322, l2: 0.0003616948525977305   Iteration 59 of 100, tot loss = 4.678719981242034, l1: 0.00010513232170477889, l2: 0.0003627396803556436   Iteration 60 of 100, tot loss = 4.7009870688120525, l1: 0.00010534595821809489, l2: 0.0003647527532545306   Iteration 61 of 100, tot loss = 4.714055108242348, l1: 0.00010560289109755521, l2: 0.00036580262375999334   Iteration 62 of 100, tot loss = 4.7103301555879655, l1: 0.00010537507665914393, l2: 0.00036565794277609507   Iteration 63 of 100, tot loss = 4.690084941803463, l1: 0.0001050111362322544, l2: 0.00036399736217433763   Iteration 64 of 100, tot loss = 4.700815163552761, l1: 0.000105289941529918, l2: 0.00036479157881785795   Iteration 65 of 100, tot loss = 4.673130402198205, l1: 0.00010474503427618542, l2: 0.0003625680095865391   Iteration 66 of 100, tot loss = 4.69358694192135, l1: 0.0001052952530065368, l2: 0.0003640634446734718   Iteration 67 of 100, tot loss = 4.695042232968914, l1: 0.00010525040917529432, l2: 0.0003642538177334259   Iteration 68 of 100, tot loss = 4.69833913971396, l1: 0.00010526652444883332, l2: 0.0003645673931554056   Iteration 69 of 100, tot loss = 4.671175276023754, l1: 0.00010492123144196239, l2: 0.0003621962999771484   Iteration 70 of 100, tot loss = 4.679331987244742, l1: 0.00010532890768705069, l2: 0.00036260429489110333   Iteration 71 of 100, tot loss = 4.644237565322661, l1: 0.0001048713120710577, l2: 0.00035955244849491733   Iteration 72 of 100, tot loss = 4.674057185649872, l1: 0.00010551759724977374, l2: 0.00036188812545232294   Iteration 73 of 100, tot loss = 4.675911263243793, l1: 0.00010575935099439772, l2: 0.0003618317795159895   Iteration 74 of 100, tot loss = 4.686669736295133, l1: 0.00010603183299944912, l2: 0.00036263514463113296   Iteration 75 of 100, tot loss = 4.673795013427735, l1: 0.00010580303283253064, l2: 0.00036157647273891297   Iteration 76 of 100, tot loss = 4.697631773195769, l1: 0.00010602934114438922, l2: 0.0003637338405066035   Iteration 77 of 100, tot loss = 4.700732701784604, l1: 0.00010598207712052511, l2: 0.0003640911967664437   Iteration 78 of 100, tot loss = 4.6632719070483475, l1: 0.00010527850286169455, l2: 0.00036104869156052026   Iteration 79 of 100, tot loss = 4.623319286334364, l1: 0.00010436523375631887, l2: 0.0003579666984858451   Iteration 80 of 100, tot loss = 4.646008913218975, l1: 0.0001045555479322502, l2: 0.00036004534713356405   Iteration 81 of 100, tot loss = 4.6478396506957065, l1: 0.00010463935994962806, l2: 0.00036014460846796103   Iteration 82 of 100, tot loss = 4.618219228779397, l1: 0.00010413669747876843, l2: 0.00035768522866419517   Iteration 83 of 100, tot loss = 4.624783548964075, l1: 0.00010437881218126402, l2: 0.00035809954592932845   Iteration 84 of 100, tot loss = 4.623793901432128, l1: 0.0001042272553951036, l2: 0.0003581521376916152   Iteration 85 of 100, tot loss = 4.622178429715774, l1: 0.00010417685416006648, l2: 0.0003580409916193115   Iteration 86 of 100, tot loss = 4.641210992668951, l1: 0.0001045702014126555, l2: 0.00035955090078100197   Iteration 87 of 100, tot loss = 4.6794760460141065, l1: 0.00010520255898296747, l2: 0.0003627450485551869   Iteration 88 of 100, tot loss = 4.6592648395083165, l1: 0.00010495532255431912, l2: 0.0003609711643135101   Iteration 89 of 100, tot loss = 4.712952671426065, l1: 0.00010556173832953667, l2: 0.0003657335317543226   Iteration 90 of 100, tot loss = 4.706567540433672, l1: 0.00010562932519759569, l2: 0.0003650274319322004   Iteration 91 of 100, tot loss = 4.716293066412538, l1: 0.00010580185447067309, l2: 0.0003658274549557213   Iteration 92 of 100, tot loss = 4.721751590137896, l1: 0.00010577091463504905, l2: 0.000366404247435402   Iteration 93 of 100, tot loss = 4.7237239383882095, l1: 0.00010587624774905302, l2: 0.00036649614917076825   Iteration 94 of 100, tot loss = 4.7369614192780025, l1: 0.00010608037019723115, l2: 0.0003676157748293221   Iteration 95 of 100, tot loss = 4.727351378139697, l1: 0.00010605004384782851, l2: 0.0003666850971823901   Iteration 96 of 100, tot loss = 4.728849255790313, l1: 0.00010610650633680052, l2: 0.0003667784225550956   Iteration 97 of 100, tot loss = 4.747167477902678, l1: 0.0001062032001989548, l2: 0.0003685135507299661   Iteration 98 of 100, tot loss = 4.773249527629541, l1: 0.00010652133263350579, l2: 0.0003708036232897679   Iteration 99 of 100, tot loss = 4.762157660542113, l1: 0.00010634137957234575, l2: 0.000369874389581688   Iteration 100 of 100, tot loss = 4.7451352274417875, l1: 0.00010602091933833435, l2: 0.0003684926065761829
   End of epoch 1169; saving model... 

Epoch 1170 of 2000
   Iteration 1 of 100, tot loss = 5.616031646728516, l1: 0.00010386415669927374, l2: 0.00045773902093060315   Iteration 2 of 100, tot loss = 5.638211488723755, l1: 0.00011890935638803057, l2: 0.0004449117841431871   Iteration 3 of 100, tot loss = 6.4223283131917315, l1: 0.0001263832212619794, l2: 0.0005158496108682206   Iteration 4 of 100, tot loss = 6.192396879196167, l1: 0.00012107295151508879, l2: 0.0004981667298125103   Iteration 5 of 100, tot loss = 6.159732818603516, l1: 0.00011843555839732289, l2: 0.0004975377232767642   Iteration 6 of 100, tot loss = 5.667776942253113, l1: 0.00011068032593660367, l2: 0.000456097370867307   Iteration 7 of 100, tot loss = 5.747375999178205, l1: 0.00011596645656806816, l2: 0.00045877114670085054   Iteration 8 of 100, tot loss = 5.852529674768448, l1: 0.00011757539414247731, l2: 0.0004676775788539089   Iteration 9 of 100, tot loss = 5.6210007137722435, l1: 0.00011569116112595011, l2: 0.0004464089159025914   Iteration 10 of 100, tot loss = 5.423734045028686, l1: 0.00011368219202267937, l2: 0.0004286912182578817   Iteration 11 of 100, tot loss = 5.328162973577326, l1: 0.00011085542064392939, l2: 0.0004219608831177042   Iteration 12 of 100, tot loss = 5.106587648391724, l1: 0.00010714041187990612, l2: 0.0004035183592350222   Iteration 13 of 100, tot loss = 5.25690639936007, l1: 0.00010766402556328103, l2: 0.0004180266203967711   Iteration 14 of 100, tot loss = 5.167815242494855, l1: 0.00010761274565343879, l2: 0.0004091687837249732   Iteration 15 of 100, tot loss = 5.088387711842855, l1: 0.00010730000164282198, l2: 0.00040153877343982456   Iteration 16 of 100, tot loss = 5.112814128398895, l1: 0.00010828302265508682, l2: 0.0004029983956570504   Iteration 17 of 100, tot loss = 5.038142625023337, l1: 0.00010884451031041167, l2: 0.00039496975745457937   Iteration 18 of 100, tot loss = 5.108189503351848, l1: 0.00010924816079851653, l2: 0.00040157079476759664   Iteration 19 of 100, tot loss = 5.059201140152781, l1: 0.00010858321835066338, l2: 0.000397336900565087   Iteration 20 of 100, tot loss = 4.944627821445465, l1: 0.00010670354604371823, l2: 0.00038775923967477867   Iteration 21 of 100, tot loss = 5.058397576922462, l1: 0.00010890094494070148, l2: 0.00039693881505324197   Iteration 22 of 100, tot loss = 5.0433468060059985, l1: 0.00010885623024395582, l2: 0.0003954784512064758   Iteration 23 of 100, tot loss = 5.017763687216717, l1: 0.00010823364952913202, l2: 0.0003935427211336387   Iteration 24 of 100, tot loss = 4.995996842781703, l1: 0.00010784280342098403, l2: 0.00039175688349738874   Iteration 25 of 100, tot loss = 4.9325253295898435, l1: 0.00010715190204791724, l2: 0.0003861006331862882   Iteration 26 of 100, tot loss = 4.9813480743995076, l1: 0.00010730129137160614, l2: 0.0003908335196543843   Iteration 27 of 100, tot loss = 4.974737502910473, l1: 0.00010786678440463557, l2: 0.00038960696946122443   Iteration 28 of 100, tot loss = 4.909905791282654, l1: 0.00010697579065371039, l2: 0.00038401479209174534   Iteration 29 of 100, tot loss = 4.875584791446554, l1: 0.00010630759864178454, l2: 0.00038125088382191184   Iteration 30 of 100, tot loss = 4.87222600777944, l1: 0.00010552923389089604, l2: 0.0003816933700970064   Iteration 31 of 100, tot loss = 4.89214514916943, l1: 0.00010606258893535743, l2: 0.0003831519279628992   Iteration 32 of 100, tot loss = 4.910299472510815, l1: 0.00010693230024116929, l2: 0.00038409764965763316   Iteration 33 of 100, tot loss = 4.9312967170368545, l1: 0.00010697288653869747, l2: 0.00038615678733384067   Iteration 34 of 100, tot loss = 4.955273102311527, l1: 0.00010768829045392682, l2: 0.00038783902196240993   Iteration 35 of 100, tot loss = 4.878869077137538, l1: 0.00010618595946912787, l2: 0.00038170095025894364   Iteration 36 of 100, tot loss = 4.901850349373287, l1: 0.00010585111542443176, l2: 0.00038433392061657895   Iteration 37 of 100, tot loss = 4.8751139511933195, l1: 0.00010450568879605271, l2: 0.00038300570756561955   Iteration 38 of 100, tot loss = 4.834301176824067, l1: 0.00010323627937300204, l2: 0.00038019383952671076   Iteration 39 of 100, tot loss = 4.908742886323195, l1: 0.000104864844242678, l2: 0.00038600944688555616   Iteration 40 of 100, tot loss = 4.832272282242775, l1: 0.00010371949010732351, l2: 0.00037950774058117533   Iteration 41 of 100, tot loss = 4.810642070886566, l1: 0.00010340638414718119, l2: 0.0003776578253941474   Iteration 42 of 100, tot loss = 4.794018067064739, l1: 0.00010318978560722567, l2: 0.0003762120232706712   Iteration 43 of 100, tot loss = 4.862656241239503, l1: 0.00010402487126601384, l2: 0.0003822407540377923   Iteration 44 of 100, tot loss = 4.861132602800023, l1: 0.00010366089736635331, l2: 0.00038245236448181623   Iteration 45 of 100, tot loss = 4.8427973773744375, l1: 0.00010337475897459727, l2: 0.00038090498031427463   Iteration 46 of 100, tot loss = 4.82773313574169, l1: 0.00010319803945806242, l2: 0.00037957527535542357   Iteration 47 of 100, tot loss = 4.813044712898579, l1: 0.00010306161160808691, l2: 0.0003782428610336749   Iteration 48 of 100, tot loss = 4.806913477679093, l1: 0.00010315884386121373, l2: 0.00037753250520230114   Iteration 49 of 100, tot loss = 4.782311850664567, l1: 0.00010309326408991629, l2: 0.0003751379221544734   Iteration 50 of 100, tot loss = 4.830329744815827, l1: 0.00010379463870776817, l2: 0.0003792383370455354   Iteration 51 of 100, tot loss = 4.791124444381864, l1: 0.00010294644630002771, l2: 0.00037616599943516226   Iteration 52 of 100, tot loss = 4.7481235930552845, l1: 0.00010219053099437867, l2: 0.0003726218299282034   Iteration 53 of 100, tot loss = 4.724878691277414, l1: 0.00010190299027906906, l2: 0.00037058488051802414   Iteration 54 of 100, tot loss = 4.700195932829821, l1: 0.00010175931850055888, l2: 0.0003682602764456533   Iteration 55 of 100, tot loss = 4.684503483772278, l1: 0.00010187584240074185, l2: 0.00036657450757709075   Iteration 56 of 100, tot loss = 4.663573867508343, l1: 0.00010132838672559177, l2: 0.000365029001581466   Iteration 57 of 100, tot loss = 4.682930500883805, l1: 0.00010189662683395702, l2: 0.0003663964241257003   Iteration 58 of 100, tot loss = 4.704930936468059, l1: 0.0001023084583595909, l2: 0.0003681846365938766   Iteration 59 of 100, tot loss = 4.723305449647419, l1: 0.00010287997355266332, l2: 0.000369450572426273   Iteration 60 of 100, tot loss = 4.700326929489772, l1: 0.0001024831287698665, l2: 0.0003675495650289425   Iteration 61 of 100, tot loss = 4.664508743364303, l1: 0.00010161907045928319, l2: 0.00036483180454405424   Iteration 62 of 100, tot loss = 4.664606319319818, l1: 0.00010161466243810156, l2: 0.0003648459704333706   Iteration 63 of 100, tot loss = 4.666142836449638, l1: 0.00010140961823044419, l2: 0.0003652046663254233   Iteration 64 of 100, tot loss = 4.7046931851655245, l1: 0.00010206670975776433, l2: 0.00036840260941062297   Iteration 65 of 100, tot loss = 4.721723462985112, l1: 0.00010221472693956457, l2: 0.0003699576202332257   Iteration 66 of 100, tot loss = 4.724682022224773, l1: 0.00010251215153954091, l2: 0.00036995605213715777   Iteration 67 of 100, tot loss = 4.69425838207131, l1: 0.000102103995530668, l2: 0.0003673218441744849   Iteration 68 of 100, tot loss = 4.694684426574146, l1: 0.0001020684405696503, l2: 0.00036740000316914283   Iteration 69 of 100, tot loss = 4.692481778670048, l1: 0.00010246844196471089, l2: 0.0003667797368573214   Iteration 70 of 100, tot loss = 4.70551986524037, l1: 0.0001024191076014956, l2: 0.0003681328800407105   Iteration 71 of 100, tot loss = 4.671395729964887, l1: 0.0001019663881968779, l2: 0.0003651731859066222   Iteration 72 of 100, tot loss = 4.660077785452207, l1: 0.00010141286692386429, l2: 0.0003645949129552011   Iteration 73 of 100, tot loss = 4.655563029524398, l1: 0.00010126624568341593, l2: 0.00036429005837680336   Iteration 74 of 100, tot loss = 4.650007578166756, l1: 0.00010115949255516564, l2: 0.00036384126626118715   Iteration 75 of 100, tot loss = 4.6615551519393925, l1: 0.00010117333959594059, l2: 0.0003649821764944742   Iteration 76 of 100, tot loss = 4.695538271414606, l1: 0.00010181016205024207, l2: 0.0003677436658395699   Iteration 77 of 100, tot loss = 4.679973340653754, l1: 0.00010175332367494279, l2: 0.0003662440111685254   Iteration 78 of 100, tot loss = 4.6755247223071565, l1: 0.00010161540637618133, l2: 0.00036593706635101576   Iteration 79 of 100, tot loss = 4.682321951359133, l1: 0.00010198668070775574, l2: 0.0003662455150610001   Iteration 80 of 100, tot loss = 4.663404528796673, l1: 0.00010147064913326175, l2: 0.0003648698042525211   Iteration 81 of 100, tot loss = 4.650474420300236, l1: 0.00010100279079195412, l2: 0.0003640446517283074   Iteration 82 of 100, tot loss = 4.6473213070776405, l1: 0.00010092061631617182, l2: 0.000363811514788966   Iteration 83 of 100, tot loss = 4.67360137025994, l1: 0.00010156688756935007, l2: 0.00036579324984474174   Iteration 84 of 100, tot loss = 4.674098067340397, l1: 0.00010169761290377383, l2: 0.0003657121945122656   Iteration 85 of 100, tot loss = 4.670776772499084, l1: 0.0001020259437021142, l2: 0.00036505173432969435   Iteration 86 of 100, tot loss = 4.706208712832872, l1: 0.00010244186392641453, l2: 0.0003681790085677274   Iteration 87 of 100, tot loss = 4.712798931132788, l1: 0.00010266479227863166, l2: 0.0003686151018997804   Iteration 88 of 100, tot loss = 4.698831581256607, l1: 0.00010245225762032946, l2: 0.0003674309012818743   Iteration 89 of 100, tot loss = 4.669050290343467, l1: 0.0001017484219834402, l2: 0.00036515660791296844   Iteration 90 of 100, tot loss = 4.668243253231049, l1: 0.00010145361646007384, l2: 0.00036537070975302616   Iteration 91 of 100, tot loss = 4.680750473515018, l1: 0.00010153442509453181, l2: 0.0003665406229729242   Iteration 92 of 100, tot loss = 4.677986858979516, l1: 0.00010147513077517151, l2: 0.0003663235558791633   Iteration 93 of 100, tot loss = 4.669369124597119, l1: 0.00010129232853820037, l2: 0.0003656445846264501   Iteration 94 of 100, tot loss = 4.644651312777337, l1: 0.00010080578435240858, l2: 0.00036365934771465813   Iteration 95 of 100, tot loss = 4.631506371498108, l1: 0.0001009255904615489, l2: 0.000362225047367821   Iteration 96 of 100, tot loss = 4.662344012409449, l1: 0.00010137340431507862, l2: 0.00036486099770627334   Iteration 97 of 100, tot loss = 4.66490563043614, l1: 0.00010166524263001623, l2: 0.00036482532138422554   Iteration 98 of 100, tot loss = 4.670882145969236, l1: 0.00010187157330415639, l2: 0.0003652166421061895   Iteration 99 of 100, tot loss = 4.670822835931874, l1: 0.00010199159519150712, l2: 0.00036509068917384315   Iteration 100 of 100, tot loss = 4.663376928567886, l1: 0.00010198966836469481, l2: 0.0003643480253231246
   End of epoch 1170; saving model... 

Epoch 1171 of 2000
   Iteration 1 of 100, tot loss = 5.1172075271606445, l1: 0.00011345794337103143, l2: 0.00039826284046284854   Iteration 2 of 100, tot loss = 5.570881128311157, l1: 0.00011146450196974911, l2: 0.0004456236056284979   Iteration 3 of 100, tot loss = 5.345390319824219, l1: 0.00011216492809277649, l2: 0.00042237410283026594   Iteration 4 of 100, tot loss = 5.511031746864319, l1: 0.00011712283776432741, l2: 0.00043398033449193463   Iteration 5 of 100, tot loss = 5.231926155090332, l1: 0.00010793114197440445, l2: 0.00041526147397235035   Iteration 6 of 100, tot loss = 5.301518281300862, l1: 0.00010804543368673573, l2: 0.0004221063960964481   Iteration 7 of 100, tot loss = 4.866872072219849, l1: 0.00010078798628195987, l2: 0.00038589922145807317   Iteration 8 of 100, tot loss = 4.831881791353226, l1: 0.00010096895994138322, l2: 0.00038221922295633703   Iteration 9 of 100, tot loss = 4.78860788875156, l1: 9.669503237495924e-05, l2: 0.00038216575760290853   Iteration 10 of 100, tot loss = 4.70490529537201, l1: 9.55847110162722e-05, l2: 0.000374905820353888   Iteration 11 of 100, tot loss = 4.966602087020874, l1: 0.00010016939201953143, l2: 0.0003964908193500543   Iteration 12 of 100, tot loss = 4.901179214318593, l1: 0.00010081045153735128, l2: 0.00038930747299067053   Iteration 13 of 100, tot loss = 4.8436859387617845, l1: 9.970001897622402e-05, l2: 0.00038466857782063575   Iteration 14 of 100, tot loss = 4.9110599756240845, l1: 0.00010134780079949581, l2: 0.0003897582002017381   Iteration 15 of 100, tot loss = 4.668172375361125, l1: 9.661702667168962e-05, l2: 0.00037020021360755585   Iteration 16 of 100, tot loss = 4.85881344974041, l1: 0.0001014805145587161, l2: 0.0003844008338091953   Iteration 17 of 100, tot loss = 4.793256984037511, l1: 0.00010062011942329049, l2: 0.0003787055821329191   Iteration 18 of 100, tot loss = 4.821433438195123, l1: 0.00010204624838176339, l2: 0.00038009709977713   Iteration 19 of 100, tot loss = 4.662207434051915, l1: 9.992060164988384e-05, l2: 0.00036630014648452694   Iteration 20 of 100, tot loss = 4.60914027094841, l1: 9.880095149128465e-05, l2: 0.0003621130799729144   Iteration 21 of 100, tot loss = 4.7113373790468485, l1: 9.965619964142596e-05, l2: 0.00037147754136683596   Iteration 22 of 100, tot loss = 4.801983481103724, l1: 0.0001007698326495963, l2: 0.00037942851849948056   Iteration 23 of 100, tot loss = 4.7001172096832935, l1: 9.907703797975758e-05, l2: 0.00037093468563398346   Iteration 24 of 100, tot loss = 4.717524503668149, l1: 0.00010014096536300106, l2: 0.0003716114882384621   Iteration 25 of 100, tot loss = 4.85723195552826, l1: 0.00010343422691221349, l2: 0.0003822889694129117   Iteration 26 of 100, tot loss = 4.836660453906426, l1: 0.00010348690431363558, l2: 0.0003801791421070587   Iteration 27 of 100, tot loss = 4.881806121932136, l1: 0.0001038134148445722, l2: 0.00038436719855612693   Iteration 28 of 100, tot loss = 4.890886115176337, l1: 0.0001044951972549565, l2: 0.00038459341495971397   Iteration 29 of 100, tot loss = 4.856571316719055, l1: 0.00010499572955926976, l2: 0.00038066140304777194   Iteration 30 of 100, tot loss = 4.833869191010793, l1: 0.00010515906530296585, l2: 0.0003782278540408394   Iteration 31 of 100, tot loss = 4.779977886907516, l1: 0.00010434332744485038, l2: 0.00037365446206099625   Iteration 32 of 100, tot loss = 4.759793799370527, l1: 0.00010434364742195612, l2: 0.00037163573347243073   Iteration 33 of 100, tot loss = 4.7279547236182475, l1: 0.00010395213019254504, l2: 0.00036884334302599063   Iteration 34 of 100, tot loss = 4.710122111965628, l1: 0.00010364668314115774, l2: 0.000367365528611973   Iteration 35 of 100, tot loss = 4.767697623797825, l1: 0.00010420204624616807, l2: 0.00037256771569705704   Iteration 36 of 100, tot loss = 4.7531509300072985, l1: 0.00010365298769304193, l2: 0.0003716621045492098   Iteration 37 of 100, tot loss = 4.76393930654268, l1: 0.00010387653320149921, l2: 0.00037251739716314634   Iteration 38 of 100, tot loss = 4.738479316234589, l1: 0.00010346026896226441, l2: 0.00037038766209949054   Iteration 39 of 100, tot loss = 4.6984741779474115, l1: 0.00010307620733585328, l2: 0.0003667712095566691   Iteration 40 of 100, tot loss = 4.692070856690407, l1: 0.00010292171309629339, l2: 0.00036628537218348357   Iteration 41 of 100, tot loss = 4.649240996779465, l1: 0.00010228100246622046, l2: 0.00036264309661729804   Iteration 42 of 100, tot loss = 4.683232628163838, l1: 0.00010284659696307721, l2: 0.0003654766657009965   Iteration 43 of 100, tot loss = 4.79244094948436, l1: 0.0001041718803821294, l2: 0.0003750722130779502   Iteration 44 of 100, tot loss = 4.777684777975082, l1: 0.00010395099417854163, l2: 0.00037381748271317605   Iteration 45 of 100, tot loss = 4.783329637845357, l1: 0.00010383939976842764, l2: 0.00037449356346365274   Iteration 46 of 100, tot loss = 4.74897117977557, l1: 0.0001034544863574885, l2: 0.00037144263051627405   Iteration 47 of 100, tot loss = 4.736015997034438, l1: 0.00010377080407318105, l2: 0.0003698307943735965   Iteration 48 of 100, tot loss = 4.714031897485256, l1: 0.00010304284849856533, l2: 0.0003683603401138195   Iteration 49 of 100, tot loss = 4.692240464444063, l1: 0.00010284402994831017, l2: 0.0003663800151636159   Iteration 50 of 100, tot loss = 4.670255200862885, l1: 0.00010279569549311418, l2: 0.00036422982331714594   Iteration 51 of 100, tot loss = 4.65259419001785, l1: 0.00010267374305198352, l2: 0.00036258567464149866   Iteration 52 of 100, tot loss = 4.634169136102383, l1: 0.00010217163848070553, l2: 0.00036124527370982553   Iteration 53 of 100, tot loss = 4.689935061166871, l1: 0.00010318993248939426, l2: 0.00036580357214607265   Iteration 54 of 100, tot loss = 4.685144793104242, l1: 0.00010355314663212523, l2: 0.00036496133098909113   Iteration 55 of 100, tot loss = 4.744254777648232, l1: 0.00010445415384972215, l2: 0.0003699713215940971   Iteration 56 of 100, tot loss = 4.739152139851025, l1: 0.00010417750536362291, l2: 0.0003697377068030099   Iteration 57 of 100, tot loss = 4.726183989591766, l1: 0.00010395223988892037, l2: 0.00036866615744401586   Iteration 58 of 100, tot loss = 4.729972064495087, l1: 0.00010376041051319495, l2: 0.0003692367938818844   Iteration 59 of 100, tot loss = 4.68105212712692, l1: 0.00010280789366195183, l2: 0.0003652973170836172   Iteration 60 of 100, tot loss = 4.675091020266215, l1: 0.00010248425502747219, l2: 0.0003650248454505345   Iteration 61 of 100, tot loss = 4.649108097201488, l1: 0.00010228346033474109, l2: 0.00036262734766885423   Iteration 62 of 100, tot loss = 4.641754842573596, l1: 0.00010194256144806727, l2: 0.00036223292092120484   Iteration 63 of 100, tot loss = 4.649634346129402, l1: 0.00010200496098915455, l2: 0.00036295847184235406   Iteration 64 of 100, tot loss = 4.615054666996002, l1: 0.00010129954580406775, l2: 0.0003602059192644447   Iteration 65 of 100, tot loss = 4.650047544332651, l1: 0.00010182957294791077, l2: 0.000363175179294418   Iteration 66 of 100, tot loss = 4.71872539953752, l1: 0.00010296907346469887, l2: 0.0003689034648149655   Iteration 67 of 100, tot loss = 4.718735068591673, l1: 0.00010297355407574422, l2: 0.00036889995127280397   Iteration 68 of 100, tot loss = 4.696675030624165, l1: 0.00010258456570548279, l2: 0.00036708293587520634   Iteration 69 of 100, tot loss = 4.702450140662815, l1: 0.00010297135357151105, l2: 0.00036727365891304055   Iteration 70 of 100, tot loss = 4.6894264936447145, l1: 0.0001027629009123692, l2: 0.0003661797470288418   Iteration 71 of 100, tot loss = 4.699815162470643, l1: 0.00010307155650774752, l2: 0.00036690995873125845   Iteration 72 of 100, tot loss = 4.6861475639873085, l1: 0.00010301937072654255, l2: 0.0003655953847151573   Iteration 73 of 100, tot loss = 4.678703445277802, l1: 0.00010316935597124752, l2: 0.0003647009874191709   Iteration 74 of 100, tot loss = 4.653743337940526, l1: 0.00010283832464640884, l2: 0.00036253600793881215   Iteration 75 of 100, tot loss = 4.666786880493164, l1: 0.00010295585971713687, l2: 0.00036372282746015115   Iteration 76 of 100, tot loss = 4.677993887349179, l1: 0.00010334524545454003, l2: 0.00036445414269493134   Iteration 77 of 100, tot loss = 4.674136644834048, l1: 0.00010354172178716459, l2: 0.00036387194211476835   Iteration 78 of 100, tot loss = 4.650276006796421, l1: 0.00010312077015092094, l2: 0.00036190682994157006   Iteration 79 of 100, tot loss = 4.653743768040138, l1: 0.00010337350019504305, l2: 0.0003620008758418085   Iteration 80 of 100, tot loss = 4.638928616046906, l1: 0.00010309077215424623, l2: 0.0003608020885621954   Iteration 81 of 100, tot loss = 4.653276090268736, l1: 0.00010324427987345391, l2: 0.00036208332779624693   Iteration 82 of 100, tot loss = 4.64781814086728, l1: 0.0001031770870457233, l2: 0.00036160472545150964   Iteration 83 of 100, tot loss = 4.657289039657776, l1: 0.00010352104673293947, l2: 0.0003622078554352734   Iteration 84 of 100, tot loss = 4.62615396295275, l1: 0.00010285473122383424, l2: 0.0003597606632649244   Iteration 85 of 100, tot loss = 4.625700541103588, l1: 0.00010281459827263675, l2: 0.000359755454273374   Iteration 86 of 100, tot loss = 4.640370474305263, l1: 0.0001026979212034602, l2: 0.00036133912455921366   Iteration 87 of 100, tot loss = 4.675477614347962, l1: 0.00010326818945073662, l2: 0.0003642795709311031   Iteration 88 of 100, tot loss = 4.667633235454559, l1: 0.00010316819465184712, l2: 0.00036359512796479976   Iteration 89 of 100, tot loss = 4.68217561486062, l1: 0.00010276706581882965, l2: 0.0003654504947140656   Iteration 90 of 100, tot loss = 4.737852382659912, l1: 0.00010363308053153256, l2: 0.00037015215663510994   Iteration 91 of 100, tot loss = 4.714434010641916, l1: 0.00010320090438983822, l2: 0.0003682424956043558   Iteration 92 of 100, tot loss = 4.76381152090819, l1: 0.00010407273776249697, l2: 0.0003723084134000364   Iteration 93 of 100, tot loss = 4.766363543848837, l1: 0.00010407471970494796, l2: 0.000372561633725205   Iteration 94 of 100, tot loss = 4.766918826610484, l1: 0.00010369640756138363, l2: 0.0003729954742277883   Iteration 95 of 100, tot loss = 4.768662091305381, l1: 0.0001036362856546858, l2: 0.00037322992246022056   Iteration 96 of 100, tot loss = 4.772404248515765, l1: 0.00010343513660397245, l2: 0.0003738052871540276   Iteration 97 of 100, tot loss = 4.770923643997035, l1: 0.0001033296611024422, l2: 0.00037376270228912863   Iteration 98 of 100, tot loss = 4.785530562303504, l1: 0.0001035981902309779, l2: 0.00037495486507470224   Iteration 99 of 100, tot loss = 4.795493251145488, l1: 0.0001036749677638514, l2: 0.00037587435618854323   Iteration 100 of 100, tot loss = 4.790884027481079, l1: 0.00010371270160248969, l2: 0.00037537570002314167
   End of epoch 1171; saving model... 

Epoch 1172 of 2000
   Iteration 1 of 100, tot loss = 3.6171817779541016, l1: 8.585497562307864e-05, l2: 0.0002758632181212306   Iteration 2 of 100, tot loss = 6.168481349945068, l1: 0.0001278770068893209, l2: 0.0004889711563009769   Iteration 3 of 100, tot loss = 5.516490936279297, l1: 0.00010822426944893475, l2: 0.00044342483549068373   Iteration 4 of 100, tot loss = 4.957421541213989, l1: 9.663298806117382e-05, l2: 0.00039910917257657275   Iteration 5 of 100, tot loss = 4.7165838241577145, l1: 9.575122967362404e-05, l2: 0.0003759071580134332   Iteration 6 of 100, tot loss = 4.849879662195842, l1: 9.745750018434289e-05, l2: 0.00038753046828787774   Iteration 7 of 100, tot loss = 4.650146382195609, l1: 9.060076886921056e-05, l2: 0.00037441386874499064   Iteration 8 of 100, tot loss = 4.487759470939636, l1: 8.943561442720238e-05, l2: 0.00035934033439843915   Iteration 9 of 100, tot loss = 4.315993441475762, l1: 8.750986631235314e-05, l2: 0.0003440894797677174   Iteration 10 of 100, tot loss = 4.259138035774231, l1: 8.715157309779897e-05, l2: 0.0003387622316950001   Iteration 11 of 100, tot loss = 4.166245677254417, l1: 8.689416559222578e-05, l2: 0.0003297304038741541   Iteration 12 of 100, tot loss = 4.399364233016968, l1: 9.17884541801565e-05, l2: 0.0003481479713324613   Iteration 13 of 100, tot loss = 4.455922493567834, l1: 9.410874022600743e-05, l2: 0.00035148351386082   Iteration 14 of 100, tot loss = 4.6495405946459085, l1: 9.670161463353517e-05, l2: 0.0003682524516729505   Iteration 15 of 100, tot loss = 4.756174405415853, l1: 9.912710035375009e-05, l2: 0.0003764903454187637   Iteration 16 of 100, tot loss = 4.5978323966264725, l1: 9.728587110657827e-05, l2: 0.00036249737331672804   Iteration 17 of 100, tot loss = 4.55953561558443, l1: 9.848267697688083e-05, l2: 0.00035747088858282524   Iteration 18 of 100, tot loss = 4.591081725226508, l1: 9.894225351874613e-05, l2: 0.0003601659223527855   Iteration 19 of 100, tot loss = 4.6080285875420826, l1: 9.942067385724697e-05, l2: 0.0003613821884899057   Iteration 20 of 100, tot loss = 4.48289475440979, l1: 9.72602394540445e-05, l2: 0.00035102923939120954   Iteration 21 of 100, tot loss = 4.552602995009649, l1: 9.82566962193232e-05, l2: 0.0003570036046550653   Iteration 22 of 100, tot loss = 4.519921812144193, l1: 9.757172582877419e-05, l2: 0.0003544204565431838   Iteration 23 of 100, tot loss = 4.508435218230538, l1: 9.765230393183984e-05, l2: 0.0003531912193663985   Iteration 24 of 100, tot loss = 4.471682906150818, l1: 9.739074766912381e-05, l2: 0.0003497775439124477   Iteration 25 of 100, tot loss = 4.4704388236999515, l1: 9.762404268258251e-05, l2: 0.0003494198416592553   Iteration 26 of 100, tot loss = 4.499762094937838, l1: 9.786658273906841e-05, l2: 0.000352109627363881   Iteration 27 of 100, tot loss = 4.515661787103723, l1: 9.869048180580312e-05, l2: 0.0003528756987415599   Iteration 28 of 100, tot loss = 4.543931416102818, l1: 0.00010009246203221014, l2: 0.00035430068289445316   Iteration 29 of 100, tot loss = 4.698118966201256, l1: 0.00010237967555816607, l2: 0.0003674322240464068   Iteration 30 of 100, tot loss = 4.8505396525065105, l1: 0.00010508009351421303, l2: 0.00037997387456319604   Iteration 31 of 100, tot loss = 4.828830488266483, l1: 0.00010464269155730313, l2: 0.000378240360371438   Iteration 32 of 100, tot loss = 4.786874361336231, l1: 0.00010388021212293097, l2: 0.00037480722630789387   Iteration 33 of 100, tot loss = 4.7954323291778564, l1: 0.000104611543396128, l2: 0.00037493169171890867   Iteration 34 of 100, tot loss = 4.750726138844209, l1: 0.00010400535125775016, l2: 0.0003710672651376466   Iteration 35 of 100, tot loss = 4.763913672310966, l1: 0.00010398267300583289, l2: 0.000372408697148785   Iteration 36 of 100, tot loss = 4.817816721068488, l1: 0.00010499188354313244, l2: 0.0003767897910115102   Iteration 37 of 100, tot loss = 4.840822116748707, l1: 0.00010576020763806269, l2: 0.000378322005712402   Iteration 38 of 100, tot loss = 4.811075787795217, l1: 0.00010562995971428648, l2: 0.00037547762086002256   Iteration 39 of 100, tot loss = 4.833790143330892, l1: 0.00010607983644242781, l2: 0.00037729917974688875   Iteration 40 of 100, tot loss = 4.81799863576889, l1: 0.00010558262483755243, l2: 0.00037621724113705567   Iteration 41 of 100, tot loss = 4.8486132389161645, l1: 0.00010582478885558752, l2: 0.00037903653772933997   Iteration 42 of 100, tot loss = 4.861876896449497, l1: 0.00010635596109274221, l2: 0.0003798317311087712   Iteration 43 of 100, tot loss = 4.8438200174376025, l1: 0.00010607929991322656, l2: 0.00037830270390935936   Iteration 44 of 100, tot loss = 4.862365354191173, l1: 0.00010621228193485877, l2: 0.00038002425505758515   Iteration 45 of 100, tot loss = 4.8439893828497995, l1: 0.00010616992149152793, l2: 0.00037822901827490167   Iteration 46 of 100, tot loss = 4.79734398489413, l1: 0.00010532948276111553, l2: 0.0003744049172382802   Iteration 47 of 100, tot loss = 4.79291515654706, l1: 0.00010529978464275817, l2: 0.00037399173201497723   Iteration 48 of 100, tot loss = 4.796567812561989, l1: 0.00010535004207667953, l2: 0.000374306739710543   Iteration 49 of 100, tot loss = 4.799109765461513, l1: 0.00010527358088958343, l2: 0.0003746373968541014   Iteration 50 of 100, tot loss = 4.8322727537155155, l1: 0.00010546288562181872, l2: 0.00037776439101435246   Iteration 51 of 100, tot loss = 4.867366103564992, l1: 0.00010594340717801185, l2: 0.00038079320402888985   Iteration 52 of 100, tot loss = 4.875640782026144, l1: 0.00010589742382412973, l2: 0.0003816666553925293   Iteration 53 of 100, tot loss = 4.877803303160757, l1: 0.00010641578238683683, l2: 0.0003813645497122604   Iteration 54 of 100, tot loss = 4.944418593689248, l1: 0.00010764640555188871, l2: 0.0003867954559003313   Iteration 55 of 100, tot loss = 4.910556303371083, l1: 0.00010723760503672317, l2: 0.00038381802733056247   Iteration 56 of 100, tot loss = 4.920742762940271, l1: 0.00010718633432458904, l2: 0.00038488794442465793   Iteration 57 of 100, tot loss = 4.893455781434712, l1: 0.00010710589202445637, l2: 0.00038223968831038003   Iteration 58 of 100, tot loss = 4.908565636338858, l1: 0.0001073853751410658, l2: 0.00038347119096152744   Iteration 59 of 100, tot loss = 4.885402364245916, l1: 0.00010677420659241329, l2: 0.00038176603266038   Iteration 60 of 100, tot loss = 4.865366375446319, l1: 0.00010635498250242866, l2: 0.00038018165796529504   Iteration 61 of 100, tot loss = 4.914549800216174, l1: 0.00010688069340206117, l2: 0.00038457428941075675   Iteration 62 of 100, tot loss = 4.896891740060622, l1: 0.00010687165740819182, l2: 0.0003828175193179519   Iteration 63 of 100, tot loss = 4.890165639302087, l1: 0.0001070473333548141, l2: 0.00038196923375676667   Iteration 64 of 100, tot loss = 4.863032598048449, l1: 0.00010626260365143025, l2: 0.0003800406589107297   Iteration 65 of 100, tot loss = 4.834659708463229, l1: 0.0001054646649908346, l2: 0.00037800130846265417   Iteration 66 of 100, tot loss = 4.832423311291319, l1: 0.00010561883700023772, l2: 0.0003776234966194765   Iteration 67 of 100, tot loss = 4.8622752801695865, l1: 0.00010635566188773113, l2: 0.00037987186893153544   Iteration 68 of 100, tot loss = 4.854376547476825, l1: 0.00010598365706220433, l2: 0.0003794540004010367   Iteration 69 of 100, tot loss = 4.8296601495881015, l1: 0.00010542415628256951, l2: 0.0003775418613720145   Iteration 70 of 100, tot loss = 4.857927884374346, l1: 0.00010535894523075382, l2: 0.00038043384598235465   Iteration 71 of 100, tot loss = 4.852804644007079, l1: 0.00010510616727553727, l2: 0.000380174300311999   Iteration 72 of 100, tot loss = 4.860712889167997, l1: 0.0001053943869919749, l2: 0.00038067690507482947   Iteration 73 of 100, tot loss = 4.8399959753637445, l1: 0.00010518108180417896, l2: 0.000378818519107283   Iteration 74 of 100, tot loss = 4.8592257016413924, l1: 0.00010545096557581092, l2: 0.0003804716083372163   Iteration 75 of 100, tot loss = 4.853288202285767, l1: 0.00010557838288756708, l2: 0.00037975044106133284   Iteration 76 of 100, tot loss = 4.842144404586993, l1: 0.0001054010019862816, l2: 0.0003788134421675319   Iteration 77 of 100, tot loss = 4.8569534691897305, l1: 0.00010589562731042094, l2: 0.00037979972382475225   Iteration 78 of 100, tot loss = 4.859383408839886, l1: 0.00010604477313627752, l2: 0.0003798935720577645   Iteration 79 of 100, tot loss = 4.872444010988066, l1: 0.0001064077608399329, l2: 0.00038083664493788555   Iteration 80 of 100, tot loss = 4.861916801333427, l1: 0.0001058717552041344, l2: 0.0003803199295361992   Iteration 81 of 100, tot loss = 4.844807654251287, l1: 0.00010577698790088848, l2: 0.0003787037822343179   Iteration 82 of 100, tot loss = 4.856245744519118, l1: 0.00010565383792443132, l2: 0.00037997074087332117   Iteration 83 of 100, tot loss = 4.841177891535931, l1: 0.00010518450798405088, l2: 0.00037893328530971994   Iteration 84 of 100, tot loss = 4.8260916868845625, l1: 0.00010502814993309411, l2: 0.00037758102276018777   Iteration 85 of 100, tot loss = 4.829885589375215, l1: 0.00010502145896145307, l2: 0.0003779671038500965   Iteration 86 of 100, tot loss = 4.8255930556807405, l1: 0.00010513176312088035, l2: 0.0003774275463128592   Iteration 87 of 100, tot loss = 4.817864960637586, l1: 0.00010526014751843016, l2: 0.00037652635230982527   Iteration 88 of 100, tot loss = 4.801568489183079, l1: 0.00010477483796090713, l2: 0.00037538201484659857   Iteration 89 of 100, tot loss = 4.807491246234165, l1: 0.00010501809234749212, l2: 0.00037573103556210656   Iteration 90 of 100, tot loss = 4.774849413500892, l1: 0.00010435522526677232, l2: 0.00037312971932503084   Iteration 91 of 100, tot loss = 4.767503532734546, l1: 0.00010447186838603926, l2: 0.0003722784877626819   Iteration 92 of 100, tot loss = 4.750912869754045, l1: 0.00010410501323823827, l2: 0.00037098627660231176   Iteration 93 of 100, tot loss = 4.727491546702641, l1: 0.00010370992709130239, l2: 0.0003690392303753704   Iteration 94 of 100, tot loss = 4.7399820964387125, l1: 0.00010387442597811002, l2: 0.0003701237863501871   Iteration 95 of 100, tot loss = 4.78121890896245, l1: 0.00010477694262303167, l2: 0.0003733449509845262   Iteration 96 of 100, tot loss = 4.792001991222302, l1: 0.0001050948091005921, l2: 0.00037410539289339795   Iteration 97 of 100, tot loss = 4.803831084487364, l1: 0.00010540869956945319, l2: 0.0003749744118414531   Iteration 98 of 100, tot loss = 4.789557574963083, l1: 0.00010515012158697461, l2: 0.00037380563863196735   Iteration 99 of 100, tot loss = 4.778869032859802, l1: 0.00010499051429426551, l2: 0.0003728963917702192   Iteration 100 of 100, tot loss = 4.7695681250095365, l1: 0.00010486557494004956, l2: 0.000372091240278678
   End of epoch 1172; saving model... 

Epoch 1173 of 2000
   Iteration 1 of 100, tot loss = 3.8007681369781494, l1: 8.514947694493458e-05, l2: 0.00029492733301594853   Iteration 2 of 100, tot loss = 3.5237605571746826, l1: 8.90468254510779e-05, l2: 0.0002633292315294966   Iteration 3 of 100, tot loss = 3.819279352823893, l1: 9.185792926776533e-05, l2: 0.00029007001042676467   Iteration 4 of 100, tot loss = 4.323093771934509, l1: 0.00010108728929481003, l2: 0.0003312220942461863   Iteration 5 of 100, tot loss = 4.231617116928101, l1: 9.950255189323798e-05, l2: 0.00032365916413255034   Iteration 6 of 100, tot loss = 3.894634167353312, l1: 9.54501726179539e-05, l2: 0.00029401324718492106   Iteration 7 of 100, tot loss = 4.144108499799456, l1: 9.94056141020597e-05, l2: 0.0003150052361888811   Iteration 8 of 100, tot loss = 4.4646613001823425, l1: 0.00010477724845259218, l2: 0.0003416888812353136   Iteration 9 of 100, tot loss = 4.414138582017687, l1: 0.00010349260345618759, l2: 0.0003379212527458246   Iteration 10 of 100, tot loss = 4.325168013572693, l1: 0.00010163853512494825, l2: 0.000330878263048362   Iteration 11 of 100, tot loss = 4.190539880232378, l1: 9.906942167700353e-05, l2: 0.0003199845629172738   Iteration 12 of 100, tot loss = 4.308105707168579, l1: 0.0001009866743212721, l2: 0.0003298238916613627   Iteration 13 of 100, tot loss = 4.67388365818904, l1: 0.00010733535880437837, l2: 0.0003600530046521901   Iteration 14 of 100, tot loss = 4.657075950077602, l1: 0.00010696247565127643, l2: 0.00035874511792956455   Iteration 15 of 100, tot loss = 4.652715110778809, l1: 0.0001057340069867981, l2: 0.0003595375019358471   Iteration 16 of 100, tot loss = 4.832606196403503, l1: 0.000108321999960026, l2: 0.00037493861873372225   Iteration 17 of 100, tot loss = 4.882818642784567, l1: 0.00010879445814198869, l2: 0.00037948740731842597   Iteration 18 of 100, tot loss = 4.8167532152599755, l1: 0.00010763188563739984, l2: 0.00037404343715429097   Iteration 19 of 100, tot loss = 4.747900523637471, l1: 0.00010595824010363828, l2: 0.00036883181325576614   Iteration 20 of 100, tot loss = 4.6813963651657104, l1: 0.00010550739534664899, l2: 0.0003626322424679529   Iteration 21 of 100, tot loss = 4.683724880218506, l1: 0.00010612466818808268, l2: 0.0003622478210932708   Iteration 22 of 100, tot loss = 4.852372321215543, l1: 0.00010817239473743194, l2: 0.0003770648373491977   Iteration 23 of 100, tot loss = 4.823154179946236, l1: 0.0001073402770840482, l2: 0.0003749751416288073   Iteration 24 of 100, tot loss = 4.722169349590938, l1: 0.00010589892129549601, l2: 0.00036631801473655895   Iteration 25 of 100, tot loss = 4.762461519241333, l1: 0.00010673257755115628, l2: 0.00036951357673387975   Iteration 26 of 100, tot loss = 4.739621373323294, l1: 0.00010493382097383101, l2: 0.00036902831981513795   Iteration 27 of 100, tot loss = 4.773436731762356, l1: 0.00010585945414344746, l2: 0.00037148422335222777   Iteration 28 of 100, tot loss = 4.760312139987946, l1: 0.00010535593381583957, l2: 0.0003706752837128339   Iteration 29 of 100, tot loss = 4.764771354609523, l1: 0.0001054633652654894, l2: 0.000371013773536984   Iteration 30 of 100, tot loss = 4.732555683453878, l1: 0.0001038413593050791, l2: 0.00036941421227917697   Iteration 31 of 100, tot loss = 4.785114526748657, l1: 0.00010458197768105404, l2: 0.00037392947847218885   Iteration 32 of 100, tot loss = 4.810719288885593, l1: 0.00010436943136937771, l2: 0.00037670250048904563   Iteration 33 of 100, tot loss = 4.8079579454479795, l1: 0.00010489196519660199, l2: 0.0003759038310928383   Iteration 34 of 100, tot loss = 4.738249091541066, l1: 0.00010309856578019476, l2: 0.0003707263450076639   Iteration 35 of 100, tot loss = 4.7288481167384555, l1: 0.0001026302641548682, l2: 0.0003702545492810064   Iteration 36 of 100, tot loss = 4.742441561486986, l1: 0.00010357534029026283, l2: 0.0003706688168879029   Iteration 37 of 100, tot loss = 4.746959286767083, l1: 0.00010382637884577962, l2: 0.00037086955027818377   Iteration 38 of 100, tot loss = 4.7513132471787305, l1: 0.00010364137731238243, l2: 0.00037148994842458417   Iteration 39 of 100, tot loss = 4.7524105707804365, l1: 0.00010344402378392167, l2: 0.0003717970345557357   Iteration 40 of 100, tot loss = 4.770654928684235, l1: 0.00010398768217783072, l2: 0.0003730778127646772   Iteration 41 of 100, tot loss = 4.76952103870671, l1: 0.00010363666421276653, l2: 0.00037331544196448946   Iteration 42 of 100, tot loss = 4.761287598382859, l1: 0.00010290435039635104, l2: 0.0003732244121714584   Iteration 43 of 100, tot loss = 4.780885286109392, l1: 0.00010321308785256237, l2: 0.0003748754432646904   Iteration 44 of 100, tot loss = 4.797184098850597, l1: 0.00010388857175381749, l2: 0.0003758298404467165   Iteration 45 of 100, tot loss = 4.7844694031609425, l1: 0.00010398437904465633, l2: 0.0003744625631952658   Iteration 46 of 100, tot loss = 4.792716358018958, l1: 0.00010436752982850126, l2: 0.000374904107087312   Iteration 47 of 100, tot loss = 4.8578497399675085, l1: 0.00010494001083103246, l2: 0.00038084496485138745   Iteration 48 of 100, tot loss = 4.840972900390625, l1: 0.00010481270654357407, l2: 0.0003792845852027919   Iteration 49 of 100, tot loss = 4.879126120586784, l1: 0.00010561291922812769, l2: 0.0003822996938596385   Iteration 50 of 100, tot loss = 4.864903545379638, l1: 0.00010578962792351376, l2: 0.0003807007274008356   Iteration 51 of 100, tot loss = 4.859918276468913, l1: 0.00010536328637481704, l2: 0.00038062854226012075   Iteration 52 of 100, tot loss = 4.870434385079604, l1: 0.00010546029688367316, l2: 0.00038158314223087823   Iteration 53 of 100, tot loss = 4.851716828796099, l1: 0.00010525754177646343, l2: 0.0003799141413765028   Iteration 54 of 100, tot loss = 4.810758016727589, l1: 0.00010412677120594135, l2: 0.00037694903073349486   Iteration 55 of 100, tot loss = 4.811356362429533, l1: 0.0001041434479041279, l2: 0.00037699218902906235   Iteration 56 of 100, tot loss = 4.813100738184793, l1: 0.00010443174012055221, l2: 0.00037687833433405364   Iteration 57 of 100, tot loss = 4.815696264568128, l1: 0.00010491066904362749, l2: 0.0003766589578478025   Iteration 58 of 100, tot loss = 4.824879046144156, l1: 0.00010502444061346287, l2: 0.00037746346437227753   Iteration 59 of 100, tot loss = 4.810497098049875, l1: 0.00010511497594514799, l2: 0.0003759347339421195   Iteration 60 of 100, tot loss = 4.784812271595001, l1: 0.000104971976664577, l2: 0.0003735092507364849   Iteration 61 of 100, tot loss = 4.795508787280223, l1: 0.00010514939962466415, l2: 0.0003744014796465025   Iteration 62 of 100, tot loss = 4.809336819956379, l1: 0.00010506650916443417, l2: 0.00037586717348125193   Iteration 63 of 100, tot loss = 4.816294469530621, l1: 0.00010532459776778336, l2: 0.0003763048494759474   Iteration 64 of 100, tot loss = 4.806083548814058, l1: 0.00010509953006021533, l2: 0.0003755088246180094   Iteration 65 of 100, tot loss = 4.810298065038828, l1: 0.00010466985314941176, l2: 0.00037635995272117164   Iteration 66 of 100, tot loss = 4.8153858798922915, l1: 0.00010475059752934612, l2: 0.0003767879900282644   Iteration 67 of 100, tot loss = 4.810202808522466, l1: 0.00010463917400312624, l2: 0.0003763811059866045   Iteration 68 of 100, tot loss = 4.775177969652064, l1: 0.00010397883633071186, l2: 0.000373538959726883   Iteration 69 of 100, tot loss = 4.744890223378721, l1: 0.00010341348762807868, l2: 0.00037107553372692746   Iteration 70 of 100, tot loss = 4.76685665334974, l1: 0.00010383013042363538, l2: 0.0003728555338706688   Iteration 71 of 100, tot loss = 4.7557916406174785, l1: 0.0001034914761552961, l2: 0.0003720876868587958   Iteration 72 of 100, tot loss = 4.7649360199769335, l1: 0.00010366379021636223, l2: 0.00037282981086011406   Iteration 73 of 100, tot loss = 4.795040551930258, l1: 0.0001041445706604999, l2: 0.0003753594836070241   Iteration 74 of 100, tot loss = 4.824674409789008, l1: 0.00010498985161452642, l2: 0.0003774775889453579   Iteration 75 of 100, tot loss = 4.824101902643839, l1: 0.0001051124707737472, l2: 0.00037729771904802573   Iteration 76 of 100, tot loss = 4.8099052843294645, l1: 0.00010508486531379228, l2: 0.00037590566257437644   Iteration 77 of 100, tot loss = 4.812585675871217, l1: 0.0001053415798161076, l2: 0.0003759169872841108   Iteration 78 of 100, tot loss = 4.868109391285823, l1: 0.00010591995254426132, l2: 0.0003808909858866499   Iteration 79 of 100, tot loss = 4.866180118126206, l1: 0.0001056724400837947, l2: 0.0003809455711810107   Iteration 80 of 100, tot loss = 4.873807746171951, l1: 0.00010589327671368664, l2: 0.0003814874975432758   Iteration 81 of 100, tot loss = 4.842499847765322, l1: 0.00010524776380294991, l2: 0.00037900222063691203   Iteration 82 of 100, tot loss = 4.862046285373409, l1: 0.00010580631027462404, l2: 0.0003803983174914653   Iteration 83 of 100, tot loss = 4.853636307888721, l1: 0.00010595759000812927, l2: 0.0003794060403927151   Iteration 84 of 100, tot loss = 4.828154992489588, l1: 0.00010549229985675386, l2: 0.000377323198855655   Iteration 85 of 100, tot loss = 4.83490304666407, l1: 0.00010514444015023079, l2: 0.00037834586418124245   Iteration 86 of 100, tot loss = 4.835719699083373, l1: 0.00010528463500905187, l2: 0.0003782873347179618   Iteration 87 of 100, tot loss = 4.832990386020178, l1: 0.00010534061597166422, l2: 0.0003779584226792614   Iteration 88 of 100, tot loss = 4.830984768542376, l1: 0.00010490434161428394, l2: 0.0003781941354232417   Iteration 89 of 100, tot loss = 4.840852011455579, l1: 0.00010522050020942893, l2: 0.0003788647009059787   Iteration 90 of 100, tot loss = 4.87793423599667, l1: 0.00010577878141096639, l2: 0.00038201464275415575   Iteration 91 of 100, tot loss = 4.865084163435212, l1: 0.00010565828395576298, l2: 0.00038085013287715043   Iteration 92 of 100, tot loss = 4.861107673334039, l1: 0.00010591279458268495, l2: 0.00038019797363591823   Iteration 93 of 100, tot loss = 4.847522956068798, l1: 0.00010573437811540169, l2: 0.00037901791849083476   Iteration 94 of 100, tot loss = 4.841301456410834, l1: 0.00010533936798078395, l2: 0.00037879077872469464   Iteration 95 of 100, tot loss = 4.848277744493987, l1: 0.00010552149144228008, l2: 0.0003793062841348154   Iteration 96 of 100, tot loss = 4.845684498548508, l1: 0.00010547361409862788, l2: 0.0003790948370199961   Iteration 97 of 100, tot loss = 4.820072068381555, l1: 0.00010504123978178766, l2: 0.00037696596820713935   Iteration 98 of 100, tot loss = 4.812564302463921, l1: 0.00010498693329133616, l2: 0.0003762694977922365   Iteration 99 of 100, tot loss = 4.797588170176804, l1: 0.00010478362317445143, l2: 0.00037497519474297836   Iteration 100 of 100, tot loss = 4.769079151153565, l1: 0.0001041668135439977, l2: 0.0003727411024738103
   End of epoch 1173; saving model... 

Epoch 1174 of 2000
   Iteration 1 of 100, tot loss = 3.418426990509033, l1: 8.860578236635774e-05, l2: 0.00025323693989776075   Iteration 2 of 100, tot loss = 3.858581066131592, l1: 9.035561379278079e-05, l2: 0.00029550251201726496   Iteration 3 of 100, tot loss = 4.8724565505981445, l1: 0.00010995160846505314, l2: 0.0003772940447864433   Iteration 4 of 100, tot loss = 4.6640366315841675, l1: 0.00010865709919016808, l2: 0.00035774656862486154   Iteration 5 of 100, tot loss = 4.712185096740723, l1: 0.00010859074391191825, l2: 0.0003626277670264244   Iteration 6 of 100, tot loss = 4.193245947360992, l1: 9.686217466272258e-05, l2: 0.0003224624197173398   Iteration 7 of 100, tot loss = 4.692678979464939, l1: 0.00010590764941298403, l2: 0.000363360251607706   Iteration 8 of 100, tot loss = 4.68080048263073, l1: 0.00010410059667265159, l2: 0.00036397945314092794   Iteration 9 of 100, tot loss = 4.65076498190562, l1: 0.0001050837547356625, l2: 0.0003599927432434116   Iteration 10 of 100, tot loss = 4.5823361992836, l1: 0.00010094328499690163, l2: 0.0003572903347958345   Iteration 11 of 100, tot loss = 4.65274820544503, l1: 0.00010363792171119712, l2: 0.0003616368981469846   Iteration 12 of 100, tot loss = 4.507334301869075, l1: 0.00010217577164439717, l2: 0.00034855765928417287   Iteration 13 of 100, tot loss = 4.7262601760717535, l1: 0.00010507768484575745, l2: 0.0003675483340675083   Iteration 14 of 100, tot loss = 4.671712730612073, l1: 0.00010363812361902092, l2: 0.00036353314966878055   Iteration 15 of 100, tot loss = 4.817205007870992, l1: 0.00010544504790838497, l2: 0.00037627545486126717   Iteration 16 of 100, tot loss = 4.7498774304986, l1: 0.00010514940072425816, l2: 0.00036983834570492036   Iteration 17 of 100, tot loss = 4.720338014995351, l1: 0.00010339098084574182, l2: 0.00036864282404113673   Iteration 18 of 100, tot loss = 4.639813111888038, l1: 0.00010273429890932878, l2: 0.0003612470168769101   Iteration 19 of 100, tot loss = 4.603635016240571, l1: 0.00010201438903765705, l2: 0.0003583491178639968   Iteration 20 of 100, tot loss = 4.5934347927570345, l1: 0.00010211407043243525, l2: 0.00035722941502172036   Iteration 21 of 100, tot loss = 4.549002766609192, l1: 0.0001010974736432434, l2: 0.0003538028086907053   Iteration 22 of 100, tot loss = 4.512612987648357, l1: 0.00010046376320877408, l2: 0.0003507975406219802   Iteration 23 of 100, tot loss = 4.476234327191892, l1: 9.879337464909717e-05, l2: 0.0003488300625695442   Iteration 24 of 100, tot loss = 4.444435551762581, l1: 9.798956655989362e-05, l2: 0.0003464539934914986   Iteration 25 of 100, tot loss = 4.466048312187195, l1: 9.80692140001338e-05, l2: 0.0003485356221790425   Iteration 26 of 100, tot loss = 4.463262003201705, l1: 9.876356363327851e-05, l2: 0.00034756264218371   Iteration 27 of 100, tot loss = 4.565526092493975, l1: 0.00010098229662010756, l2: 0.0003555703181148231   Iteration 28 of 100, tot loss = 4.676336573702948, l1: 0.00010294844975890425, l2: 0.0003646852125841958   Iteration 29 of 100, tot loss = 4.693740363778739, l1: 0.00010371063913696381, l2: 0.0003656634008652402   Iteration 30 of 100, tot loss = 4.793780227502187, l1: 0.00010597465237272748, l2: 0.00037340337366913444   Iteration 31 of 100, tot loss = 4.889677090029562, l1: 0.00010747727391126025, l2: 0.00038149043792791123   Iteration 32 of 100, tot loss = 4.961275111883879, l1: 0.00010913238827470195, l2: 0.00038699512583662   Iteration 33 of 100, tot loss = 5.028806133703752, l1: 0.00011045155174956149, l2: 0.00039242906385686984   Iteration 34 of 100, tot loss = 4.993902602616479, l1: 0.00010948157914219123, l2: 0.0003899086832996759   Iteration 35 of 100, tot loss = 5.0061234780720305, l1: 0.00010956479038993296, l2: 0.0003910475591380548   Iteration 36 of 100, tot loss = 4.973911268843545, l1: 0.0001090077363793777, l2: 0.00038838339221709047   Iteration 37 of 100, tot loss = 4.963063004854563, l1: 0.00010899328156704762, l2: 0.00038731302039515277   Iteration 38 of 100, tot loss = 4.959733006201293, l1: 0.00010888816397805625, l2: 0.000387085137366452   Iteration 39 of 100, tot loss = 4.983534320806846, l1: 0.00010905185393848791, l2: 0.00038930157834562496   Iteration 40 of 100, tot loss = 4.933334633708, l1: 0.00010864194146051887, l2: 0.000384691521867353   Iteration 41 of 100, tot loss = 4.925617075547939, l1: 0.00010851428571807379, l2: 0.0003840474214152115   Iteration 42 of 100, tot loss = 4.951252809592655, l1: 0.00010963141791026352, l2: 0.00038549386244128635   Iteration 43 of 100, tot loss = 4.916167117828547, l1: 0.0001090784305212532, l2: 0.0003825382809736766   Iteration 44 of 100, tot loss = 4.937437591227618, l1: 0.00010945441574056696, l2: 0.0003842893431944751   Iteration 45 of 100, tot loss = 4.901177292399936, l1: 0.00010831177245967815, l2: 0.0003818059571333126   Iteration 46 of 100, tot loss = 4.877981831198153, l1: 0.00010834741964493372, l2: 0.000379450763949021   Iteration 47 of 100, tot loss = 4.887377467561276, l1: 0.00010884332941520582, l2: 0.0003798944171842564   Iteration 48 of 100, tot loss = 4.854158965249856, l1: 0.00010830464884747926, l2: 0.0003771112477200707   Iteration 49 of 100, tot loss = 4.872750523139019, l1: 0.00010824088078189395, l2: 0.00037903417176828356   Iteration 50 of 100, tot loss = 4.882027852535248, l1: 0.0001081727185373893, l2: 0.00038003006702638233   Iteration 51 of 100, tot loss = 4.8443842752307065, l1: 0.00010717035076916491, l2: 0.0003772680766083866   Iteration 52 of 100, tot loss = 4.790194724614803, l1: 0.00010632775739885311, l2: 0.00037269171508132084   Iteration 53 of 100, tot loss = 4.790964065857653, l1: 0.00010660644789514097, l2: 0.00037248995864021435   Iteration 54 of 100, tot loss = 4.766196054440957, l1: 0.00010617854753002541, l2: 0.00037044105785618604   Iteration 55 of 100, tot loss = 4.770567336949435, l1: 0.00010647739697808654, l2: 0.0003705793365952559   Iteration 56 of 100, tot loss = 4.749073924762862, l1: 0.00010626562087574192, l2: 0.00036864177140419737   Iteration 57 of 100, tot loss = 4.755612927570677, l1: 0.00010646829679516438, l2: 0.00036909299524268135   Iteration 58 of 100, tot loss = 4.749070868409913, l1: 0.00010635171082836238, l2: 0.0003685553748600169   Iteration 59 of 100, tot loss = 4.747271467063387, l1: 0.00010646878667935429, l2: 0.0003682583587991973   Iteration 60 of 100, tot loss = 4.744150139888128, l1: 0.00010623024663800607, l2: 0.0003681847659512035   Iteration 61 of 100, tot loss = 4.791427754964984, l1: 0.00010710721328562568, l2: 0.00037203556139949617   Iteration 62 of 100, tot loss = 4.779216529861573, l1: 0.0001071247993569423, l2: 0.0003707968523573776   Iteration 63 of 100, tot loss = 4.753673188270084, l1: 0.00010673588760601844, l2: 0.00036863142997303625   Iteration 64 of 100, tot loss = 4.775586841627955, l1: 0.00010717962589978924, l2: 0.00037037905656234216   Iteration 65 of 100, tot loss = 4.790316447844872, l1: 0.00010736159685900649, l2: 0.0003716700458496164   Iteration 66 of 100, tot loss = 4.795713726318244, l1: 0.00010740785061595186, l2: 0.00037216352002084904   Iteration 67 of 100, tot loss = 4.824922389058925, l1: 0.00010733672839271446, l2: 0.00037515550882663275   Iteration 68 of 100, tot loss = 4.814856730839786, l1: 0.00010706595902079512, l2: 0.0003744197125770905   Iteration 69 of 100, tot loss = 4.8000213806179985, l1: 0.00010696124888414963, l2: 0.0003730408876646728   Iteration 70 of 100, tot loss = 4.811383364881788, l1: 0.00010721558121856236, l2: 0.0003739227539751612   Iteration 71 of 100, tot loss = 4.8179096184985735, l1: 0.0001070217202679077, l2: 0.0003747692405251117   Iteration 72 of 100, tot loss = 4.829024084740215, l1: 0.0001074757477807806, l2: 0.00037542665965399163   Iteration 73 of 100, tot loss = 4.862997768676444, l1: 0.00010805851248633562, l2: 0.0003782412632685856   Iteration 74 of 100, tot loss = 4.844193740470989, l1: 0.00010771470944156456, l2: 0.0003767046636730319   Iteration 75 of 100, tot loss = 4.846858118375143, l1: 0.0001080394454281001, l2: 0.0003766463653300889   Iteration 76 of 100, tot loss = 4.870981196039601, l1: 0.00010808751697990566, l2: 0.00037901060175882825   Iteration 77 of 100, tot loss = 4.8767019293525005, l1: 0.00010793519034556887, l2: 0.0003797350017249572   Iteration 78 of 100, tot loss = 4.860648361536173, l1: 0.00010764624848655867, l2: 0.00037841858685192745   Iteration 79 of 100, tot loss = 4.839353255078763, l1: 0.00010737621948775788, l2: 0.0003765591051277728   Iteration 80 of 100, tot loss = 4.855521918833256, l1: 0.0001073675598945556, l2: 0.0003781846310630499   Iteration 81 of 100, tot loss = 4.834276624667791, l1: 0.00010699217625782986, l2: 0.0003764354854708618   Iteration 82 of 100, tot loss = 4.847967852906483, l1: 0.00010723943488279119, l2: 0.0003775573498320982   Iteration 83 of 100, tot loss = 4.851548443357628, l1: 0.00010752453796070416, l2: 0.0003776303058558884   Iteration 84 of 100, tot loss = 4.840681432258515, l1: 0.0001074545540783826, l2: 0.0003766135886551291   Iteration 85 of 100, tot loss = 4.853904010267819, l1: 0.00010759405785767526, l2: 0.00037779634320245616   Iteration 86 of 100, tot loss = 4.866801138534102, l1: 0.00010769907744678527, l2: 0.00037898103644608435   Iteration 87 of 100, tot loss = 4.863477321876877, l1: 0.00010758245609875971, l2: 0.0003787652759487612   Iteration 88 of 100, tot loss = 4.854804487390951, l1: 0.00010744664835080833, l2: 0.00037803380018885946   Iteration 89 of 100, tot loss = 4.843760869476233, l1: 0.00010704493987749414, l2: 0.00037733114658386626   Iteration 90 of 100, tot loss = 4.851168286800385, l1: 0.00010719681041438081, l2: 0.00037792001781377217   Iteration 91 of 100, tot loss = 4.8634275905378574, l1: 0.00010734021974373148, l2: 0.0003790025386459112   Iteration 92 of 100, tot loss = 4.861869613761487, l1: 0.00010712017228797777, l2: 0.00037906678817703124   Iteration 93 of 100, tot loss = 4.841244280979198, l1: 0.00010659930538617685, l2: 0.00037752512176784   Iteration 94 of 100, tot loss = 4.843614921924916, l1: 0.00010632643607869853, l2: 0.00037803505511738937   Iteration 95 of 100, tot loss = 4.846834974539908, l1: 0.00010657346049954771, l2: 0.0003781100362079757   Iteration 96 of 100, tot loss = 4.841533167908589, l1: 0.00010661960902780265, l2: 0.00037753370672059344   Iteration 97 of 100, tot loss = 4.837949756494503, l1: 0.0001064594182082015, l2: 0.0003773355563249218   Iteration 98 of 100, tot loss = 4.836814845094875, l1: 0.0001063542963585897, l2: 0.00037732718719409454   Iteration 99 of 100, tot loss = 4.834503085926325, l1: 0.0001059512596655604, l2: 0.00037749904783200376   Iteration 100 of 100, tot loss = 4.850247062444687, l1: 0.00010613320879201637, l2: 0.0003788914965844015
   End of epoch 1174; saving model... 

Epoch 1175 of 2000
   Iteration 1 of 100, tot loss = 3.4780168533325195, l1: 8.124927990138531e-05, l2: 0.0002665524079930037   Iteration 2 of 100, tot loss = 4.306136131286621, l1: 0.00011307213571853936, l2: 0.00031754147494211793   Iteration 3 of 100, tot loss = 4.321650346120198, l1: 0.00010847978652842964, l2: 0.00032368524504515034   Iteration 4 of 100, tot loss = 4.918884754180908, l1: 0.00011495338731037918, l2: 0.00037693508056690916   Iteration 5 of 100, tot loss = 4.886284351348877, l1: 0.00011540635459823534, l2: 0.00037322207354009154   Iteration 6 of 100, tot loss = 4.732525149981181, l1: 0.00010727787472812149, l2: 0.0003659746386498834   Iteration 7 of 100, tot loss = 4.655712229864938, l1: 0.00010622128736161227, l2: 0.0003593499381427786   Iteration 8 of 100, tot loss = 4.747863441705704, l1: 0.00010341095185140148, l2: 0.00037137539038667455   Iteration 9 of 100, tot loss = 4.5990621248881025, l1: 0.00010142914672744357, l2: 0.0003584770634511693   Iteration 10 of 100, tot loss = 4.470788598060608, l1: 0.00010010067489929497, l2: 0.0003469781804597005   Iteration 11 of 100, tot loss = 4.356568683277477, l1: 9.920076527950269e-05, l2: 0.0003364560993345962   Iteration 12 of 100, tot loss = 4.578806598981221, l1: 0.00010368354984772547, l2: 0.00035419710669278476   Iteration 13 of 100, tot loss = 4.610680726858286, l1: 0.00010420116786218177, l2: 0.00035686690199117246   Iteration 14 of 100, tot loss = 4.595579896654401, l1: 0.000102157023190687, l2: 0.00035740096271703284   Iteration 15 of 100, tot loss = 4.707145182291667, l1: 0.00010392138841173922, l2: 0.0003667931266439458   Iteration 16 of 100, tot loss = 4.7085433304309845, l1: 0.00010376118962085457, l2: 0.00036709314190375153   Iteration 17 of 100, tot loss = 4.640053426518159, l1: 0.00010115772174150847, l2: 0.0003628476201008786   Iteration 18 of 100, tot loss = 4.716342356469896, l1: 0.00010232365053121207, l2: 0.0003693105827551335   Iteration 19 of 100, tot loss = 4.763329443178679, l1: 0.00010358292983207655, l2: 0.00037275001063550775   Iteration 20 of 100, tot loss = 4.952374494075775, l1: 0.00010612417609081603, l2: 0.000389113268465735   Iteration 21 of 100, tot loss = 4.916928847630818, l1: 0.00010593274817524833, l2: 0.0003857601328664238   Iteration 22 of 100, tot loss = 4.865212364630266, l1: 0.00010571943924084984, l2: 0.00038080179365351796   Iteration 23 of 100, tot loss = 4.8446744732234786, l1: 0.00010634067122895351, l2: 0.0003781267732847482   Iteration 24 of 100, tot loss = 4.944278250137965, l1: 0.00010730867294720763, l2: 0.00038711914991533075   Iteration 25 of 100, tot loss = 4.851986560821533, l1: 0.0001060468054492958, l2: 0.00037915184861049057   Iteration 26 of 100, tot loss = 4.847537902685312, l1: 0.00010693686048258454, l2: 0.00037781692830881535   Iteration 27 of 100, tot loss = 4.804642191639653, l1: 0.00010626111556862102, l2: 0.000374203102654536   Iteration 28 of 100, tot loss = 4.800179200513022, l1: 0.00010625696605919594, l2: 0.0003737609534125243   Iteration 29 of 100, tot loss = 4.7424673540838835, l1: 0.00010523477048364243, l2: 0.0003690119649098929   Iteration 30 of 100, tot loss = 4.82490242322286, l1: 0.00010694855178977984, l2: 0.0003755416900579197   Iteration 31 of 100, tot loss = 4.793189548677014, l1: 0.00010658251572280161, l2: 0.0003727364392327746   Iteration 32 of 100, tot loss = 4.782500006258488, l1: 0.00010639564584380423, l2: 0.0003718543553077325   Iteration 33 of 100, tot loss = 4.762529987277406, l1: 0.00010574283827956992, l2: 0.0003705101601857071   Iteration 34 of 100, tot loss = 4.74944794178009, l1: 0.00010595881394692697, l2: 0.00036898598045809194   Iteration 35 of 100, tot loss = 4.76615400995527, l1: 0.00010677128011593595, l2: 0.0003698441212431395   Iteration 36 of 100, tot loss = 4.70210435655382, l1: 0.0001057426921357142, l2: 0.0003644677441722403   Iteration 37 of 100, tot loss = 4.71505492442363, l1: 0.00010635981586371624, l2: 0.0003651456767378526   Iteration 38 of 100, tot loss = 4.681301242426822, l1: 0.00010509851228179239, l2: 0.00036303161208419815   Iteration 39 of 100, tot loss = 4.6680695582658815, l1: 0.0001047881437093915, l2: 0.00036201881205973524   Iteration 40 of 100, tot loss = 4.693172061443329, l1: 0.00010543978305577184, l2: 0.0003638774229330011   Iteration 41 of 100, tot loss = 4.681121279553669, l1: 0.0001051821274769481, l2: 0.00036293000022585435   Iteration 42 of 100, tot loss = 4.702228512082781, l1: 0.00010529757084787845, l2: 0.0003649252806402122   Iteration 43 of 100, tot loss = 4.716754580652991, l1: 0.00010598334982966852, l2: 0.00036569210825228066   Iteration 44 of 100, tot loss = 4.707826560193842, l1: 0.0001058646892653831, l2: 0.000364917966759425   Iteration 45 of 100, tot loss = 4.696076742808024, l1: 0.00010552625131418204, l2: 0.00036408142316051654   Iteration 46 of 100, tot loss = 4.720238270966903, l1: 0.0001058176587083954, l2: 0.00036620616890064883   Iteration 47 of 100, tot loss = 4.689306279446217, l1: 0.00010493010407291412, l2: 0.00036400052448833716   Iteration 48 of 100, tot loss = 4.6679467757542925, l1: 0.0001047101758710293, l2: 0.0003620845018303953   Iteration 49 of 100, tot loss = 4.743556879004654, l1: 0.00010609796663629822, l2: 0.000368257721990575   Iteration 50 of 100, tot loss = 4.7187278985977175, l1: 0.00010539805902226362, l2: 0.0003664747317088768   Iteration 51 of 100, tot loss = 4.716624367470835, l1: 0.00010517684695157953, l2: 0.00036648559091868356   Iteration 52 of 100, tot loss = 4.762885254163009, l1: 0.00010563310661382275, l2: 0.00037065542038852494   Iteration 53 of 100, tot loss = 4.754171492918482, l1: 0.00010536775358937207, l2: 0.0003700493972194996   Iteration 54 of 100, tot loss = 4.755087750929373, l1: 0.00010569180159547143, l2: 0.00036981697485316545   Iteration 55 of 100, tot loss = 4.748288982564753, l1: 0.000105458763044391, l2: 0.0003693701360713352   Iteration 56 of 100, tot loss = 4.709094601018088, l1: 0.00010476246273226155, l2: 0.0003661469984633316   Iteration 57 of 100, tot loss = 4.693036037578917, l1: 0.00010484933125298356, l2: 0.0003644542736940805   Iteration 58 of 100, tot loss = 4.702452955574825, l1: 0.00010498351755437586, l2: 0.0003652617797970065   Iteration 59 of 100, tot loss = 4.714211003255036, l1: 0.00010539064230939958, l2: 0.00036603045971448517   Iteration 60 of 100, tot loss = 4.742981537183126, l1: 0.0001060456456495255, l2: 0.00036825250960343205   Iteration 61 of 100, tot loss = 4.817034416511411, l1: 0.00010727123889574002, l2: 0.00037443220422633724   Iteration 62 of 100, tot loss = 4.800487645210758, l1: 0.00010645566474141237, l2: 0.0003735931010939361   Iteration 63 of 100, tot loss = 4.788883810951596, l1: 0.00010636974825285419, l2: 0.000372518633773166   Iteration 64 of 100, tot loss = 4.793904472142458, l1: 0.00010658050183565138, l2: 0.00037280994592947536   Iteration 65 of 100, tot loss = 4.814392643708449, l1: 0.00010718735088280833, l2: 0.000374251914371808   Iteration 66 of 100, tot loss = 4.793785062703219, l1: 0.00010723528242978676, l2: 0.0003721432248395021   Iteration 67 of 100, tot loss = 4.789562961948452, l1: 0.00010697745767799067, l2: 0.0003719788399584063   Iteration 68 of 100, tot loss = 4.794487514916589, l1: 0.00010679380097024157, l2: 0.00037265495163463875   Iteration 69 of 100, tot loss = 4.7934102424676865, l1: 0.00010677155718229292, l2: 0.0003725694679314325   Iteration 70 of 100, tot loss = 4.772508760860988, l1: 0.00010670188925619836, l2: 0.0003705489877445091   Iteration 71 of 100, tot loss = 4.757623447498805, l1: 0.00010613475777217003, l2: 0.0003696275878251648   Iteration 72 of 100, tot loss = 4.756003575192557, l1: 0.00010557564711335645, l2: 0.00037002471152744774   Iteration 73 of 100, tot loss = 4.746396708161863, l1: 0.00010542993076753241, l2: 0.0003692097411720618   Iteration 74 of 100, tot loss = 4.74479049283105, l1: 0.00010574809233288318, l2: 0.0003687309579470673   Iteration 75 of 100, tot loss = 4.735262257258097, l1: 0.00010555800879956223, l2: 0.000367968218245854   Iteration 76 of 100, tot loss = 4.733387761994412, l1: 0.00010541675706726413, l2: 0.000367922020239731   Iteration 77 of 100, tot loss = 4.746458477788157, l1: 0.00010571330140364454, l2: 0.00036893254780972546   Iteration 78 of 100, tot loss = 4.737205270009163, l1: 0.00010569441110419575, l2: 0.0003680261176598903   Iteration 79 of 100, tot loss = 4.713629659218125, l1: 0.00010532468062408469, l2: 0.0003660382871306796   Iteration 80 of 100, tot loss = 4.718318751454353, l1: 0.00010544232613938221, l2: 0.00036638955116359285   Iteration 81 of 100, tot loss = 4.708346770133502, l1: 0.00010514806342177542, l2: 0.00036568661564587396   Iteration 82 of 100, tot loss = 4.69698123815583, l1: 0.00010490002072091134, l2: 0.00036479810500523167   Iteration 83 of 100, tot loss = 4.6817694514630785, l1: 0.00010431829839243542, l2: 0.00036385864863054353   Iteration 84 of 100, tot loss = 4.667375876790001, l1: 0.00010426564681221775, l2: 0.00036247194293537177   Iteration 85 of 100, tot loss = 4.673185982423671, l1: 0.00010451645973850699, l2: 0.0003628021405722179   Iteration 86 of 100, tot loss = 4.684945023337076, l1: 0.0001046751733452385, l2: 0.0003638193310782595   Iteration 87 of 100, tot loss = 4.6572873181310195, l1: 0.00010426600081324256, l2: 0.00036146273286531455   Iteration 88 of 100, tot loss = 4.6437518271532925, l1: 0.00010423279358788436, l2: 0.0003601423909871797   Iteration 89 of 100, tot loss = 4.656321739882566, l1: 0.00010472076818781246, l2: 0.0003609114076047508   Iteration 90 of 100, tot loss = 4.631752257876926, l1: 0.00010427694643213827, l2: 0.0003588982812491142   Iteration 91 of 100, tot loss = 4.681902162321321, l1: 0.00010499811709211157, l2: 0.00036319210078915233   Iteration 92 of 100, tot loss = 4.680299178413723, l1: 0.00010516214664992573, l2: 0.0003628677729031314   Iteration 93 of 100, tot loss = 4.68159475634175, l1: 0.00010540539557452223, l2: 0.00036275408184406176   Iteration 94 of 100, tot loss = 4.698630997475157, l1: 0.00010565753093923126, l2: 0.0003642055705719766   Iteration 95 of 100, tot loss = 4.698143868697317, l1: 0.00010575982864553991, l2: 0.00036405455995056974   Iteration 96 of 100, tot loss = 4.68143659333388, l1: 0.00010550835509093304, l2: 0.0003626353059189569   Iteration 97 of 100, tot loss = 4.669446188149993, l1: 0.00010543883842363301, l2: 0.0003615057819880564   Iteration 98 of 100, tot loss = 4.710716841172199, l1: 0.00010596288365730065, l2: 0.00036510880172314425   Iteration 99 of 100, tot loss = 4.702381158115888, l1: 0.00010587579516030735, l2: 0.00036436232201392867   Iteration 100 of 100, tot loss = 4.694620690345764, l1: 0.00010569411730102728, l2: 0.0003637679530947935
   End of epoch 1175; saving model... 

Epoch 1176 of 2000
   Iteration 1 of 100, tot loss = 4.248092174530029, l1: 7.518754136981443e-05, l2: 0.0003496216959320009   Iteration 2 of 100, tot loss = 4.887401819229126, l1: 8.62953565956559e-05, l2: 0.00040244484262075275   Iteration 3 of 100, tot loss = 4.74888801574707, l1: 9.381102427141741e-05, l2: 0.00038107778527773917   Iteration 4 of 100, tot loss = 4.5703301429748535, l1: 9.12952655198751e-05, l2: 0.0003657377528725192   Iteration 5 of 100, tot loss = 4.51919469833374, l1: 9.020628203870729e-05, l2: 0.00036171318497508765   Iteration 6 of 100, tot loss = 4.143109838167827, l1: 8.557251445987883e-05, l2: 0.0003287384655171384   Iteration 7 of 100, tot loss = 3.9628684520721436, l1: 8.104200813769629e-05, l2: 0.00031524483139427114   Iteration 8 of 100, tot loss = 4.257813662290573, l1: 8.956130795922945e-05, l2: 0.0003362200532137649   Iteration 9 of 100, tot loss = 4.299909936057197, l1: 9.257177468195248e-05, l2: 0.0003374192149042048   Iteration 10 of 100, tot loss = 4.182562613487244, l1: 9.137021661445032e-05, l2: 0.0003268860426032916   Iteration 11 of 100, tot loss = 4.366551290858876, l1: 9.603665801643564e-05, l2: 0.0003406184683130546   Iteration 12 of 100, tot loss = 4.348557651042938, l1: 9.354342273582006e-05, l2: 0.0003413123413338326   Iteration 13 of 100, tot loss = 4.71070489516625, l1: 9.627584026580174e-05, l2: 0.00037479464887068246   Iteration 14 of 100, tot loss = 4.704845275197711, l1: 9.662245507310477e-05, l2: 0.00037386207051375616   Iteration 15 of 100, tot loss = 4.690421978632609, l1: 9.682898889877834e-05, l2: 0.0003722132086598625   Iteration 16 of 100, tot loss = 4.579491764307022, l1: 9.641038855079387e-05, l2: 0.0003615387877289322   Iteration 17 of 100, tot loss = 4.570260636946735, l1: 9.734377796549405e-05, l2: 0.0003596822866444092   Iteration 18 of 100, tot loss = 4.525414480103387, l1: 9.696412376393305e-05, l2: 0.00035557732513148547   Iteration 19 of 100, tot loss = 4.462049735219855, l1: 9.695709185938253e-05, l2: 0.00034924788173827296   Iteration 20 of 100, tot loss = 4.4577552556991575, l1: 9.760286393429851e-05, l2: 0.0003481726627796888   Iteration 21 of 100, tot loss = 4.411175784610567, l1: 9.681620109144465e-05, l2: 0.0003443013785207378   Iteration 22 of 100, tot loss = 4.41088727387515, l1: 9.776705585615922e-05, l2: 0.00034332167177291757   Iteration 23 of 100, tot loss = 4.4581433068151055, l1: 9.801392155603263e-05, l2: 0.0003478004085912329   Iteration 24 of 100, tot loss = 4.435086439053218, l1: 9.79327661904487e-05, l2: 0.00034557587787276134   Iteration 25 of 100, tot loss = 4.3995859241485595, l1: 9.741807924001477e-05, l2: 0.0003425405139569193   Iteration 26 of 100, tot loss = 4.332977634209853, l1: 9.60682791628642e-05, l2: 0.00033722948473251355   Iteration 27 of 100, tot loss = 4.261124151724356, l1: 9.519238274488425e-05, l2: 0.0003309200331353134   Iteration 28 of 100, tot loss = 4.267529436520168, l1: 9.42510488519994e-05, l2: 0.0003325018951727543   Iteration 29 of 100, tot loss = 4.2964400587410765, l1: 9.473719132827306e-05, l2: 0.0003349068148085719   Iteration 30 of 100, tot loss = 4.350855318705241, l1: 9.589834723252958e-05, l2: 0.0003391871844845203   Iteration 31 of 100, tot loss = 4.32889050053012, l1: 9.556711877174225e-05, l2: 0.00033732193133478324   Iteration 32 of 100, tot loss = 4.380013145506382, l1: 9.636244237753999e-05, l2: 0.00034163887221438927   Iteration 33 of 100, tot loss = 4.454739288850264, l1: 9.838369475222531e-05, l2: 0.00034709023461663025   Iteration 34 of 100, tot loss = 4.4428871309056, l1: 9.804355634996147e-05, l2: 0.0003462451564059045   Iteration 35 of 100, tot loss = 4.394117280415126, l1: 9.70757426070382e-05, l2: 0.0003423359852084624   Iteration 36 of 100, tot loss = 4.430013795693715, l1: 9.775765779017497e-05, l2: 0.0003452437223232765   Iteration 37 of 100, tot loss = 4.4616528523934855, l1: 9.840372188812801e-05, l2: 0.00034776156472444937   Iteration 38 of 100, tot loss = 4.39764382337269, l1: 9.731413133283097e-05, l2: 0.0003424502520877133   Iteration 39 of 100, tot loss = 4.401024787853926, l1: 9.727230141064725e-05, l2: 0.0003428301786651644   Iteration 40 of 100, tot loss = 4.3729532122612, l1: 9.754855473147473e-05, l2: 0.00033974676734942476   Iteration 41 of 100, tot loss = 4.432126045227051, l1: 9.856834385129509e-05, l2: 0.0003446442619528303   Iteration 42 of 100, tot loss = 4.436257839202881, l1: 9.832187390115688e-05, l2: 0.00034530391158547164   Iteration 43 of 100, tot loss = 4.426967321440231, l1: 9.859331408353133e-05, l2: 0.00034410341982567293   Iteration 44 of 100, tot loss = 4.453556331721219, l1: 9.961671358641152e-05, l2: 0.00034573892116895877   Iteration 45 of 100, tot loss = 4.43536589940389, l1: 9.915359890631711e-05, l2: 0.0003443829925446254   Iteration 46 of 100, tot loss = 4.462732957757038, l1: 0.00010002760198245676, l2: 0.0003462456949384195   Iteration 47 of 100, tot loss = 4.443262571984149, l1: 0.00010026529607204522, l2: 0.0003440609623484829   Iteration 48 of 100, tot loss = 4.45372757812341, l1: 0.00010045132027395691, l2: 0.0003449214391366695   Iteration 49 of 100, tot loss = 4.409717812830088, l1: 9.98727172168865e-05, l2: 0.00034109906593993383   Iteration 50 of 100, tot loss = 4.529339714050293, l1: 0.00010076351391035132, l2: 0.00035217045835452156   Iteration 51 of 100, tot loss = 4.508123327704037, l1: 9.97477385253125e-05, l2: 0.00035106459508111297   Iteration 52 of 100, tot loss = 4.514551928410163, l1: 9.972445003278644e-05, l2: 0.0003517307433447478   Iteration 53 of 100, tot loss = 4.474508384488663, l1: 9.909560493432987e-05, l2: 0.0003483552342381785   Iteration 54 of 100, tot loss = 4.482028510835436, l1: 9.930740344754197e-05, l2: 0.00034889544837858047   Iteration 55 of 100, tot loss = 4.458129631389271, l1: 9.902523564629849e-05, l2: 0.000346787728168155   Iteration 56 of 100, tot loss = 4.43673289673669, l1: 9.823286141649337e-05, l2: 0.0003454404289056713   Iteration 57 of 100, tot loss = 4.492623421183803, l1: 9.869748368816402e-05, l2: 0.0003505648594255674   Iteration 58 of 100, tot loss = 4.494097594557138, l1: 9.86428068903604e-05, l2: 0.0003507669534883462   Iteration 59 of 100, tot loss = 4.498061988313319, l1: 9.892086030558241e-05, l2: 0.00035088533960811604   Iteration 60 of 100, tot loss = 4.523533304532369, l1: 9.950577896233881e-05, l2: 0.00035284755261576114   Iteration 61 of 100, tot loss = 4.5571867520691915, l1: 0.00010007282390524481, l2: 0.0003556458527111586   Iteration 62 of 100, tot loss = 4.602912595195155, l1: 0.0001010845833427025, l2: 0.0003592066779909205   Iteration 63 of 100, tot loss = 4.603582896883526, l1: 0.00010074271331033078, l2: 0.00035961557827365127   Iteration 64 of 100, tot loss = 4.632830560207367, l1: 0.00010148265033649295, l2: 0.0003618004077452497   Iteration 65 of 100, tot loss = 4.617515919758723, l1: 0.00010159021873662893, l2: 0.00036016137522752755   Iteration 66 of 100, tot loss = 4.595166083538171, l1: 0.00010121100835518023, l2: 0.0003583056020557485   Iteration 67 of 100, tot loss = 4.630637780943913, l1: 0.00010201731292094771, l2: 0.00036104646669492236   Iteration 68 of 100, tot loss = 4.6188571067417366, l1: 0.00010203615307973985, l2: 0.0003598495591979693   Iteration 69 of 100, tot loss = 4.62811791724053, l1: 0.00010244358623901422, l2: 0.0003603682072961406   Iteration 70 of 100, tot loss = 4.631315548079354, l1: 0.0001026460182142078, l2: 0.0003604855380087559   Iteration 71 of 100, tot loss = 4.639009767854717, l1: 0.00010276719741072362, l2: 0.00036113378083543247   Iteration 72 of 100, tot loss = 4.677397929959827, l1: 0.00010328161629836864, l2: 0.0003644581779048571   Iteration 73 of 100, tot loss = 4.7100652962514795, l1: 0.00010383786971095554, l2: 0.0003671686608369832   Iteration 74 of 100, tot loss = 4.720914476626628, l1: 0.00010390536580588732, l2: 0.00036818608232597644   Iteration 75 of 100, tot loss = 4.729947601954143, l1: 0.00010426129801392866, l2: 0.00036873346272235115   Iteration 76 of 100, tot loss = 4.6990069903825455, l1: 0.00010361563507007974, l2: 0.0003662850643817556   Iteration 77 of 100, tot loss = 4.697240228776808, l1: 0.000103899918975877, l2: 0.0003658241043154562   Iteration 78 of 100, tot loss = 4.701732745537391, l1: 0.00010405656669321709, l2: 0.00036611670806544286   Iteration 79 of 100, tot loss = 4.68808723099624, l1: 0.0001039011025384269, l2: 0.0003649076210007755   Iteration 80 of 100, tot loss = 4.662437584996224, l1: 0.00010362088605688768, l2: 0.0003626228728535352   Iteration 81 of 100, tot loss = 4.6424952936761175, l1: 0.0001033899456228095, l2: 0.00036085958403598423   Iteration 82 of 100, tot loss = 4.633994323451344, l1: 0.00010346600431991491, l2: 0.00035993342813836954   Iteration 83 of 100, tot loss = 4.6414783431823, l1: 0.00010376563392150362, l2: 0.0003603822002902694   Iteration 84 of 100, tot loss = 4.6271726460683915, l1: 0.00010354556616221089, l2: 0.0003591716984893927   Iteration 85 of 100, tot loss = 4.621315922456629, l1: 0.00010305245394941749, l2: 0.0003590791385196259   Iteration 86 of 100, tot loss = 4.585593169511751, l1: 0.00010232684275421022, l2: 0.0003562324745025551   Iteration 87 of 100, tot loss = 4.5698772312580855, l1: 0.00010176849219146527, l2: 0.00035521923112763286   Iteration 88 of 100, tot loss = 4.548622195016254, l1: 0.0001013313772091351, l2: 0.00035353084256877827   Iteration 89 of 100, tot loss = 4.550538072425328, l1: 0.00010141306851887186, l2: 0.0003536407392029912   Iteration 90 of 100, tot loss = 4.5693497326638965, l1: 0.00010166907389551246, l2: 0.00035526589991705907   Iteration 91 of 100, tot loss = 4.590746191831736, l1: 0.00010192933417044345, l2: 0.0003571452854065593   Iteration 92 of 100, tot loss = 4.56860160957212, l1: 0.00010134744649764363, l2: 0.00035551271487391836   Iteration 93 of 100, tot loss = 4.587287929750258, l1: 0.00010158973925162397, l2: 0.0003571390538596578   Iteration 94 of 100, tot loss = 4.594922817767935, l1: 0.0001016382159039185, l2: 0.00035785406640919044   Iteration 95 of 100, tot loss = 4.580379663015667, l1: 0.00010149388395611344, l2: 0.00035654408275149764   Iteration 96 of 100, tot loss = 4.608516952643792, l1: 0.00010187729621217538, l2: 0.0003589743995083457   Iteration 97 of 100, tot loss = 4.5951615449079535, l1: 0.00010173969180353187, l2: 0.0003577764632093922   Iteration 98 of 100, tot loss = 4.603818264542793, l1: 0.00010186591483646652, l2: 0.0003585159120492029   Iteration 99 of 100, tot loss = 4.60836818844381, l1: 0.0001021211840388144, l2: 0.00035871563519285335   Iteration 100 of 100, tot loss = 4.603280094861984, l1: 0.0001021587629293208, l2: 0.0003581692470470443
   End of epoch 1176; saving model... 

Epoch 1177 of 2000
   Iteration 1 of 100, tot loss = 4.8395586013793945, l1: 0.00010209357424173504, l2: 0.0003818622790277004   Iteration 2 of 100, tot loss = 4.751430988311768, l1: 0.00011281713523203507, l2: 0.0003623259690357372   Iteration 3 of 100, tot loss = 4.116677204767863, l1: 0.00010112016510295992, l2: 0.0003105475528476139   Iteration 4 of 100, tot loss = 4.230246722698212, l1: 0.0001030238090606872, l2: 0.00032000085775507614   Iteration 5 of 100, tot loss = 4.357933282852173, l1: 0.00010622166591929272, l2: 0.00032957165967673063   Iteration 6 of 100, tot loss = 4.890673836072286, l1: 0.00011302491354096371, l2: 0.0003760424636614819   Iteration 7 of 100, tot loss = 4.891444376536778, l1: 0.00011270529129043487, l2: 0.0003764391377834337   Iteration 8 of 100, tot loss = 5.1556271612644196, l1: 0.00011501590688567376, l2: 0.0004005467999377288   Iteration 9 of 100, tot loss = 5.042876958847046, l1: 0.00011019233918179655, l2: 0.000394095346564427   Iteration 10 of 100, tot loss = 5.0321943998336796, l1: 0.0001067903394869063, l2: 0.0003964290954172611   Iteration 11 of 100, tot loss = 4.837927428158847, l1: 0.00010183809618783098, l2: 0.00038195464267945766   Iteration 12 of 100, tot loss = 4.6942676703135175, l1: 9.92801093768018e-05, l2: 0.0003701466533433025   Iteration 13 of 100, tot loss = 4.655287082378681, l1: 9.894982940750197e-05, l2: 0.0003665788748409026   Iteration 14 of 100, tot loss = 4.672021695545742, l1: 9.855865833482571e-05, l2: 0.00036864350633030493   Iteration 15 of 100, tot loss = 4.4972959359486895, l1: 9.454538182277854e-05, l2: 0.00035518420627340675   Iteration 16 of 100, tot loss = 4.582447215914726, l1: 9.579565585227101e-05, l2: 0.00036244905822968576   Iteration 17 of 100, tot loss = 4.86586002742543, l1: 0.00010042259793194448, l2: 0.00038616339888368896   Iteration 18 of 100, tot loss = 4.837780435880025, l1: 9.846798517780068e-05, l2: 0.00038531005136772164   Iteration 19 of 100, tot loss = 4.784172785909552, l1: 9.723619778493517e-05, l2: 0.0003811810747720301   Iteration 20 of 100, tot loss = 4.766798734664917, l1: 9.714936750242487e-05, l2: 0.0003795305005041882   Iteration 21 of 100, tot loss = 4.6580989474341985, l1: 9.615254904409605e-05, l2: 0.0003696573410360586   Iteration 22 of 100, tot loss = 4.693408727645874, l1: 9.751349518244916e-05, l2: 0.0003718273722122169   Iteration 23 of 100, tot loss = 4.620196435762488, l1: 9.681222360560913e-05, l2: 0.000365207415281633   Iteration 24 of 100, tot loss = 4.528506050507228, l1: 9.549569798158093e-05, l2: 0.0003573549032201602   Iteration 25 of 100, tot loss = 4.4873621368408205, l1: 9.537701582303271e-05, l2: 0.0003533591941231862   Iteration 26 of 100, tot loss = 4.561846549694355, l1: 9.672416014766965e-05, l2: 0.0003594604926407695   Iteration 27 of 100, tot loss = 4.5699232419331866, l1: 9.736384769591193e-05, l2: 0.000359628474042337   Iteration 28 of 100, tot loss = 4.558778149741037, l1: 9.735955167603347e-05, l2: 0.00035851826136682315   Iteration 29 of 100, tot loss = 4.556908788352177, l1: 9.699191613090706e-05, l2: 0.000358698960365567   Iteration 30 of 100, tot loss = 4.556414937973022, l1: 9.742866920229669e-05, l2: 0.00035821282314524674   Iteration 31 of 100, tot loss = 4.681772893474948, l1: 9.960957780300129e-05, l2: 0.00036856771170747495   Iteration 32 of 100, tot loss = 4.684252470731735, l1: 0.00010007007404055912, l2: 0.00036835517357758363   Iteration 33 of 100, tot loss = 4.710519024820039, l1: 0.00010103907808308688, l2: 0.00037001282422958565   Iteration 34 of 100, tot loss = 4.762638919493732, l1: 0.00010223569380982285, l2: 0.0003740281976848457   Iteration 35 of 100, tot loss = 4.819843210492816, l1: 0.00010339222041823502, l2: 0.0003785920998780057   Iteration 36 of 100, tot loss = 4.790430578920576, l1: 0.00010276580845432666, l2: 0.00037627724870819494   Iteration 37 of 100, tot loss = 4.7222086287833545, l1: 0.00010188859655377078, l2: 0.000370332265644007   Iteration 38 of 100, tot loss = 4.730025843570107, l1: 0.00010224785881674554, l2: 0.00037075472477897023   Iteration 39 of 100, tot loss = 4.786189812880296, l1: 0.0001027996214151454, l2: 0.00037581935892096505   Iteration 40 of 100, tot loss = 4.764911335706711, l1: 0.000102385751233669, l2: 0.00037410538134281525   Iteration 41 of 100, tot loss = 4.742329190417034, l1: 0.00010209473786256617, l2: 0.00037213818030999745   Iteration 42 of 100, tot loss = 4.751177435829526, l1: 0.00010152220870839388, l2: 0.00037359553416969166   Iteration 43 of 100, tot loss = 4.729714526686558, l1: 0.00010119340579474865, l2: 0.0003717780461112514   Iteration 44 of 100, tot loss = 4.6894560998136345, l1: 0.00010116751664926679, l2: 0.0003677780927815051   Iteration 45 of 100, tot loss = 4.683178133434719, l1: 0.00010098042435452549, l2: 0.00036733738767604033   Iteration 46 of 100, tot loss = 4.6555228285167525, l1: 0.0001002544492581839, l2: 0.0003652978326578665   Iteration 47 of 100, tot loss = 4.674176211052752, l1: 0.00010099461902195509, l2: 0.0003664230010125469   Iteration 48 of 100, tot loss = 4.633295824130376, l1: 0.00010046817654559466, l2: 0.0003628614046344107   Iteration 49 of 100, tot loss = 4.6473773547581265, l1: 0.00010112163430193858, l2: 0.00036361609966902784   Iteration 50 of 100, tot loss = 4.6404432201385495, l1: 0.00010091840362292714, l2: 0.0003631259166286327   Iteration 51 of 100, tot loss = 4.672664623634488, l1: 0.0001012861247028352, l2: 0.0003659803361526928   Iteration 52 of 100, tot loss = 4.6975203935916605, l1: 0.0001016059138167363, l2: 0.00036814612422648887   Iteration 53 of 100, tot loss = 4.769701822748724, l1: 0.00010303532532284494, l2: 0.0003739348549575914   Iteration 54 of 100, tot loss = 4.735930893156263, l1: 0.00010284628377929848, l2: 0.0003707468034311508   Iteration 55 of 100, tot loss = 4.740723531896418, l1: 0.00010279795149637556, l2: 0.00037127439969811927   Iteration 56 of 100, tot loss = 4.748977065086365, l1: 0.0001027028061538918, l2: 0.0003721948981235203   Iteration 57 of 100, tot loss = 4.731701148183722, l1: 0.00010224636794750073, l2: 0.0003709237447637542   Iteration 58 of 100, tot loss = 4.77295156182914, l1: 0.00010247746254678751, l2: 0.0003748176923495364   Iteration 59 of 100, tot loss = 4.784678224789894, l1: 0.00010257096548260869, l2: 0.0003758968550601374   Iteration 60 of 100, tot loss = 4.772008474667867, l1: 0.00010276825026570199, l2: 0.0003744325954661084   Iteration 61 of 100, tot loss = 4.765809278019139, l1: 0.00010270462729242921, l2: 0.0003738762986403508   Iteration 62 of 100, tot loss = 4.747124787299864, l1: 0.00010238962591580687, l2: 0.0003723228512699866   Iteration 63 of 100, tot loss = 4.740065847124372, l1: 0.00010219777577153836, l2: 0.00037180880705515545   Iteration 64 of 100, tot loss = 4.710662879049778, l1: 0.0001014873550957418, l2: 0.00036957893075850734   Iteration 65 of 100, tot loss = 4.674569658132699, l1: 0.00010074915150583435, l2: 0.000366707812421597   Iteration 66 of 100, tot loss = 4.675925355969054, l1: 0.00010047730508879546, l2: 0.00036711522834869385   Iteration 67 of 100, tot loss = 4.67849905811139, l1: 0.00010046652845522291, l2: 0.00036738337488469684   Iteration 68 of 100, tot loss = 4.695603517925038, l1: 0.0001005434096976601, l2: 0.0003690169396577403   Iteration 69 of 100, tot loss = 4.668814289397088, l1: 0.00010016858156988114, l2: 0.00036671284507737374   Iteration 70 of 100, tot loss = 4.652963076319013, l1: 0.00010011910162575078, l2: 0.00036517720374311987   Iteration 71 of 100, tot loss = 4.624549580291963, l1: 9.970036754425629e-05, l2: 0.0003627545881153963   Iteration 72 of 100, tot loss = 4.616787231630749, l1: 9.970707009819712e-05, l2: 0.00036197165071724966   Iteration 73 of 100, tot loss = 4.628879720217561, l1: 9.962499257510413e-05, l2: 0.0003632629776217222   Iteration 74 of 100, tot loss = 4.613265098752202, l1: 9.940306753796412e-05, l2: 0.00036192344083935276   Iteration 75 of 100, tot loss = 4.621313117345174, l1: 9.979664217098616e-05, l2: 0.00036233466758858415   Iteration 76 of 100, tot loss = 4.586664907242122, l1: 9.901456202274666e-05, l2: 0.0003596519269003214   Iteration 77 of 100, tot loss = 4.580104442385884, l1: 9.859323874078147e-05, l2: 0.00035941720370947106   Iteration 78 of 100, tot loss = 4.539749753780854, l1: 9.77406769099896e-05, l2: 0.000356234296748134   Iteration 79 of 100, tot loss = 4.548167726661585, l1: 9.744688807736162e-05, l2: 0.00035736988244439724   Iteration 80 of 100, tot loss = 4.587883695960045, l1: 9.806944117372041e-05, l2: 0.0003607189265494526   Iteration 81 of 100, tot loss = 4.618051508326589, l1: 9.864402522065643e-05, l2: 0.0003631611238413398   Iteration 82 of 100, tot loss = 4.600479437083733, l1: 9.867638481210167e-05, l2: 0.00036137155709694505   Iteration 83 of 100, tot loss = 4.6325478984648925, l1: 9.883371047137014e-05, l2: 0.0003644210774365936   Iteration 84 of 100, tot loss = 4.638186094306764, l1: 9.903836845686393e-05, l2: 0.00036478023884271914   Iteration 85 of 100, tot loss = 4.661477669547586, l1: 9.965583175311194e-05, l2: 0.00036649193318547023   Iteration 86 of 100, tot loss = 4.643304056899492, l1: 9.909595741361087e-05, l2: 0.00036523444603432816   Iteration 87 of 100, tot loss = 4.630083034778464, l1: 9.870357357665372e-05, l2: 0.00036430472784726507   Iteration 88 of 100, tot loss = 4.6238849488171665, l1: 9.88758970379422e-05, l2: 0.0003635125956200434   Iteration 89 of 100, tot loss = 4.6070320043671, l1: 9.873677976265958e-05, l2: 0.0003619664184339711   Iteration 90 of 100, tot loss = 4.609365362591213, l1: 9.867869549553789e-05, l2: 0.0003622578385046735   Iteration 91 of 100, tot loss = 4.611558285388318, l1: 9.893017156902895e-05, l2: 0.0003622256546085547   Iteration 92 of 100, tot loss = 4.59607869386673, l1: 9.875304207842261e-05, l2: 0.0003608548250549179   Iteration 93 of 100, tot loss = 4.5822116533915205, l1: 9.82191460582638e-05, l2: 0.00036000201698600424   Iteration 94 of 100, tot loss = 4.592010629938004, l1: 9.847490923191877e-05, l2: 0.00036072615121350067   Iteration 95 of 100, tot loss = 4.58728060973318, l1: 9.838687225342974e-05, l2: 0.0003603411862154883   Iteration 96 of 100, tot loss = 4.5667798444628716, l1: 9.793716014883103e-05, l2: 0.0003587408216390031   Iteration 97 of 100, tot loss = 4.5969076132036975, l1: 9.844977155564228e-05, l2: 0.0003612409874625245   Iteration 98 of 100, tot loss = 4.583357124912496, l1: 9.819490570611825e-05, l2: 0.00036014080451794294   Iteration 99 of 100, tot loss = 4.563943048920295, l1: 9.782092022971777e-05, l2: 0.0003585733823575823   Iteration 100 of 100, tot loss = 4.582591032981872, l1: 9.834592969127697e-05, l2: 0.0003599131714872783
   End of epoch 1177; saving model... 

Epoch 1178 of 2000
   Iteration 1 of 100, tot loss = 5.153539657592773, l1: 0.00010570639278739691, l2: 0.0004096476186532527   Iteration 2 of 100, tot loss = 5.670053243637085, l1: 0.00012989186507184058, l2: 0.00043711348553188145   Iteration 3 of 100, tot loss = 5.4631147384643555, l1: 0.0001256833541750287, l2: 0.00042062812523605925   Iteration 4 of 100, tot loss = 5.3065431118011475, l1: 0.00011336309762555175, l2: 0.0004172912158537656   Iteration 5 of 100, tot loss = 4.935786962509155, l1: 0.00010818849405040965, l2: 0.00038539020461030303   Iteration 6 of 100, tot loss = 5.261510968208313, l1: 0.0001068133193863711, l2: 0.0004193377826595679   Iteration 7 of 100, tot loss = 5.147227593830654, l1: 0.00010680150444386527, l2: 0.00040792126258436056   Iteration 8 of 100, tot loss = 5.227965146303177, l1: 0.00011131140945508378, l2: 0.00041148511081701145   Iteration 9 of 100, tot loss = 5.255781465106541, l1: 0.00010982517334115173, l2: 0.0004157529781676001   Iteration 10 of 100, tot loss = 4.984254312515259, l1: 0.00010576059794402682, l2: 0.0003926648394553922   Iteration 11 of 100, tot loss = 4.917890938845548, l1: 0.00010497706865531985, l2: 0.0003868120296498422   Iteration 12 of 100, tot loss = 4.96349294980367, l1: 0.00010446964430836185, l2: 0.0003918796513365426   Iteration 13 of 100, tot loss = 4.825241070527297, l1: 0.00010297626935285874, l2: 0.0003795478394469963   Iteration 14 of 100, tot loss = 4.796301995004926, l1: 0.00010364771255158953, l2: 0.00037598249036818743   Iteration 15 of 100, tot loss = 4.767123079299926, l1: 0.00010369087782843659, l2: 0.0003730214317329228   Iteration 16 of 100, tot loss = 4.713929861783981, l1: 0.00010464279512234498, l2: 0.0003667501914605964   Iteration 17 of 100, tot loss = 4.564218086354873, l1: 0.00010103717346175793, l2: 0.00035538463528021514   Iteration 18 of 100, tot loss = 4.575007186995612, l1: 0.00010158793985384464, l2: 0.0003559127791150887   Iteration 19 of 100, tot loss = 4.502707594319394, l1: 0.00010167415606893453, l2: 0.0003485966038792149   Iteration 20 of 100, tot loss = 4.415191960334778, l1: 9.977598328987369e-05, l2: 0.0003417432133574039   Iteration 21 of 100, tot loss = 4.511396703265962, l1: 0.0001015344025204069, l2: 0.0003496052675126564   Iteration 22 of 100, tot loss = 4.42426492951133, l1: 9.907432732606222e-05, l2: 0.0003433521653112786   Iteration 23 of 100, tot loss = 4.361667570860489, l1: 9.672420096846864e-05, l2: 0.00033944255593941426   Iteration 24 of 100, tot loss = 4.395269930362701, l1: 9.721988226374378e-05, l2: 0.00034230711207783315   Iteration 25 of 100, tot loss = 4.3292399024963375, l1: 9.630562650272622e-05, l2: 0.0003366183646721765   Iteration 26 of 100, tot loss = 4.357001102887667, l1: 9.742266774992459e-05, l2: 0.00033827744314644055   Iteration 27 of 100, tot loss = 4.389187071058485, l1: 9.755835802665118e-05, l2: 0.00034136034998853037   Iteration 28 of 100, tot loss = 4.363838655608041, l1: 9.760440947762358e-05, l2: 0.0003387794562773446   Iteration 29 of 100, tot loss = 4.332359141316907, l1: 9.733451989977139e-05, l2: 0.00033590139506047527   Iteration 30 of 100, tot loss = 4.364487973848979, l1: 9.789026662474499e-05, l2: 0.00033855853301550574   Iteration 31 of 100, tot loss = 4.333345328607867, l1: 9.74575363685407e-05, l2: 0.00033587699844276595   Iteration 32 of 100, tot loss = 4.357013113796711, l1: 9.765432128006069e-05, l2: 0.0003380469920557516   Iteration 33 of 100, tot loss = 4.403052496187614, l1: 9.879536964669542e-05, l2: 0.00034150988203967273   Iteration 34 of 100, tot loss = 4.432304164942573, l1: 9.886388151618816e-05, l2: 0.00034436653753372785   Iteration 35 of 100, tot loss = 4.420201662608555, l1: 9.864581432858749e-05, l2: 0.00034337435541341877   Iteration 36 of 100, tot loss = 4.361055870850881, l1: 9.77684465194923e-05, l2: 0.00033833714365351223   Iteration 37 of 100, tot loss = 4.343916706136755, l1: 9.753533978820653e-05, l2: 0.00033685633388813585   Iteration 38 of 100, tot loss = 4.29456206999327, l1: 9.68741505379225e-05, l2: 0.000332582059184604   Iteration 39 of 100, tot loss = 4.3051494512802515, l1: 9.721183521786514e-05, l2: 0.0003333031121516027   Iteration 40 of 100, tot loss = 4.303249007463455, l1: 9.702238676254638e-05, l2: 0.00033330251644656526   Iteration 41 of 100, tot loss = 4.2818899096512215, l1: 9.693745321384063e-05, l2: 0.0003312515402241133   Iteration 42 of 100, tot loss = 4.3431995539438155, l1: 9.768398030289627e-05, l2: 0.0003366359773541002   Iteration 43 of 100, tot loss = 4.332067373187043, l1: 9.793504686121894e-05, l2: 0.0003352716925884272   Iteration 44 of 100, tot loss = 4.358747747811404, l1: 9.78041844064137e-05, l2: 0.00033807059298851527   Iteration 45 of 100, tot loss = 4.3690812905629475, l1: 9.796261842388453e-05, l2: 0.0003389455135523652   Iteration 46 of 100, tot loss = 4.3626069141470865, l1: 9.839990968390813e-05, l2: 0.00033786078394927165   Iteration 47 of 100, tot loss = 4.420353478573738, l1: 9.939955959859443e-05, l2: 0.0003426357898786862   Iteration 48 of 100, tot loss = 4.432639275987943, l1: 9.945047198319419e-05, l2: 0.00034381345691751147   Iteration 49 of 100, tot loss = 4.483399094367514, l1: 0.00010066425691510797, l2: 0.00034767565375659615   Iteration 50 of 100, tot loss = 4.465656805038452, l1: 0.00010043114656582474, l2: 0.0003461345352116041   Iteration 51 of 100, tot loss = 4.46599256291109, l1: 0.00010040143131524982, l2: 0.0003461978257309609   Iteration 52 of 100, tot loss = 4.476437596174387, l1: 0.00010049539154211883, l2: 0.000347148368186586   Iteration 53 of 100, tot loss = 4.493700594272253, l1: 0.00010124326918058905, l2: 0.0003481267905851193   Iteration 54 of 100, tot loss = 4.480407851713675, l1: 0.0001008542594381315, l2: 0.0003471865258891032   Iteration 55 of 100, tot loss = 4.521036767959595, l1: 0.00010155715400204909, l2: 0.0003505465227962387   Iteration 56 of 100, tot loss = 4.531927887882505, l1: 0.00010115960295869237, l2: 0.00035203318605324185   Iteration 57 of 100, tot loss = 4.534257842783342, l1: 0.00010121389628661666, l2: 0.00035221188808096934   Iteration 58 of 100, tot loss = 4.561893927639928, l1: 0.00010152028410410476, l2: 0.0003546691092698642   Iteration 59 of 100, tot loss = 4.581200619875374, l1: 0.00010169010808675597, l2: 0.0003564299542508167   Iteration 60 of 100, tot loss = 4.558363648255666, l1: 0.00010123249230673537, l2: 0.0003546038727411845   Iteration 61 of 100, tot loss = 4.564164916022879, l1: 0.0001012029552824612, l2: 0.0003552135363026507   Iteration 62 of 100, tot loss = 4.561253290022573, l1: 0.0001012407083180733, l2: 0.0003548846202207759   Iteration 63 of 100, tot loss = 4.581721105272808, l1: 0.00010156501634305136, l2: 0.00035660709360695725   Iteration 64 of 100, tot loss = 4.5480877161026, l1: 0.00010076436694816948, l2: 0.0003540444040481816   Iteration 65 of 100, tot loss = 4.550426791264461, l1: 0.00010078902380952899, l2: 0.000354253655174174   Iteration 66 of 100, tot loss = 4.549278923959443, l1: 0.00010076725668499641, l2: 0.0003541606354040348   Iteration 67 of 100, tot loss = 4.541355737999304, l1: 0.000100348132389681, l2: 0.0003537874405598963   Iteration 68 of 100, tot loss = 4.575018483049729, l1: 0.000101294932392193, l2: 0.0003562069147900569   Iteration 69 of 100, tot loss = 4.589105018671008, l1: 0.00010126182293617234, l2: 0.00035764867746932566   Iteration 70 of 100, tot loss = 4.605334302357265, l1: 0.00010177484104393184, l2: 0.0003587585873901844   Iteration 71 of 100, tot loss = 4.625829025053642, l1: 0.00010227649478802562, l2: 0.0003603064056656415   Iteration 72 of 100, tot loss = 4.657201786835988, l1: 0.0001026668193541506, l2: 0.0003630533570281437   Iteration 73 of 100, tot loss = 4.682716212860526, l1: 0.00010326611085823571, l2: 0.00036500550867122125   Iteration 74 of 100, tot loss = 4.655676670976587, l1: 0.00010251380523628984, l2: 0.0003630538603583527   Iteration 75 of 100, tot loss = 4.652144982020061, l1: 0.00010246885067317635, l2: 0.0003627456462709233   Iteration 76 of 100, tot loss = 4.648720932634253, l1: 0.0001025344804235685, l2: 0.00036233761152528273   Iteration 77 of 100, tot loss = 4.6658032373948535, l1: 0.00010307033035251034, l2: 0.0003635099922328002   Iteration 78 of 100, tot loss = 4.685909029765007, l1: 0.00010339083755985857, l2: 0.0003652000642689064   Iteration 79 of 100, tot loss = 4.700396709804293, l1: 0.00010400261808057092, l2: 0.00036603705187569834   Iteration 80 of 100, tot loss = 4.709616991877556, l1: 0.00010422457489767112, l2: 0.0003667371232950245   Iteration 81 of 100, tot loss = 4.6798697695320035, l1: 0.00010350568940069664, l2: 0.00036448128661571965   Iteration 82 of 100, tot loss = 4.691206809951038, l1: 0.00010385615829230718, l2: 0.00036526452228228173   Iteration 83 of 100, tot loss = 4.670468876160771, l1: 0.00010332618107183287, l2: 0.00036372070611701403   Iteration 84 of 100, tot loss = 4.666477629116604, l1: 0.00010345951739596093, l2: 0.0003631882451979133   Iteration 85 of 100, tot loss = 4.666026850307689, l1: 0.00010337122169884798, l2: 0.00036323146298801635   Iteration 86 of 100, tot loss = 4.666941537413487, l1: 0.00010333436884413755, l2: 0.00036335978447188903   Iteration 87 of 100, tot loss = 4.675043506183844, l1: 0.00010336735297527015, l2: 0.00036413699686901923   Iteration 88 of 100, tot loss = 4.659943946383216, l1: 0.00010316302194207145, l2: 0.000362831371934259   Iteration 89 of 100, tot loss = 4.652400247166666, l1: 0.00010282747400728358, l2: 0.0003624125501517667   Iteration 90 of 100, tot loss = 4.654572592841254, l1: 0.00010294525030379494, l2: 0.00036251200848103814   Iteration 91 of 100, tot loss = 4.637857214435116, l1: 0.00010251832481826768, l2: 0.00036126739603907363   Iteration 92 of 100, tot loss = 4.6650609633196956, l1: 0.00010277062645637552, l2: 0.0003637354694428327   Iteration 93 of 100, tot loss = 4.653360671894525, l1: 0.00010255377136311064, l2: 0.00036278229534766204   Iteration 94 of 100, tot loss = 4.64467553635861, l1: 0.0001022543026223541, l2: 0.0003622132505196087   Iteration 95 of 100, tot loss = 4.6259637481287905, l1: 0.00010197453329486675, l2: 0.00036062184096265   Iteration 96 of 100, tot loss = 4.606681411465009, l1: 0.00010181939539203692, l2: 0.00035884874523617327   Iteration 97 of 100, tot loss = 4.5758235896985555, l1: 0.00010124818920265864, l2: 0.0003563341691840402   Iteration 98 of 100, tot loss = 4.572223753345256, l1: 0.00010124363013476665, l2: 0.0003559787446461922   Iteration 99 of 100, tot loss = 4.570931331075803, l1: 0.00010101411949062362, l2: 0.00035607901306568897   Iteration 100 of 100, tot loss = 4.568681151866913, l1: 0.00010092789583723061, l2: 0.0003559402187238447
   End of epoch 1178; saving model... 

Epoch 1179 of 2000
   Iteration 1 of 100, tot loss = 2.6293907165527344, l1: 8.311073906952515e-05, l2: 0.0001798283337848261   Iteration 2 of 100, tot loss = 3.8209691047668457, l1: 0.00010751897571026348, l2: 0.00027457794203655794   Iteration 3 of 100, tot loss = 3.2400570710500083, l1: 9.296048907951142e-05, l2: 0.0002310452206681172   Iteration 4 of 100, tot loss = 3.3860946893692017, l1: 8.694118514540605e-05, l2: 0.0002516682870918885   Iteration 5 of 100, tot loss = 3.1206335544586183, l1: 7.842260965844617e-05, l2: 0.0002336407487746328   Iteration 6 of 100, tot loss = 3.1763461033503213, l1: 7.905451275291853e-05, l2: 0.00023858009565932056   Iteration 7 of 100, tot loss = 3.301746129989624, l1: 8.086188920840089e-05, l2: 0.00024931272076043697   Iteration 8 of 100, tot loss = 3.417547196149826, l1: 8.601488298154436e-05, l2: 0.0002557398329372518   Iteration 9 of 100, tot loss = 3.6436039871639676, l1: 9.014616726845916e-05, l2: 0.0002742142306589004   Iteration 10 of 100, tot loss = 3.478895664215088, l1: 8.61174277815735e-05, l2: 0.00026177213730989025   Iteration 11 of 100, tot loss = 3.723915013399991, l1: 8.886640088431622e-05, l2: 0.00028352510178758   Iteration 12 of 100, tot loss = 3.6831931471824646, l1: 8.564772724639624e-05, l2: 0.0002826715875319981   Iteration 13 of 100, tot loss = 3.864582556944627, l1: 8.678875411323343e-05, l2: 0.0002996695010761659   Iteration 14 of 100, tot loss = 3.893760016986302, l1: 8.497521360238482e-05, l2: 0.00030440078678241534   Iteration 15 of 100, tot loss = 4.086840359369914, l1: 8.729073403325552e-05, l2: 0.00032139329996425656   Iteration 16 of 100, tot loss = 4.111589476466179, l1: 8.965788902060012e-05, l2: 0.00032150105562323006   Iteration 17 of 100, tot loss = 4.166854591930614, l1: 9.073114436914158e-05, l2: 0.00032595431462943773   Iteration 18 of 100, tot loss = 4.247412191496955, l1: 9.313475634876845e-05, l2: 0.00033160646287595027   Iteration 19 of 100, tot loss = 4.2525294830924585, l1: 9.421219376810386e-05, l2: 0.0003310407544057326   Iteration 20 of 100, tot loss = 4.398067009449005, l1: 9.610563101887237e-05, l2: 0.00034370106950518673   Iteration 21 of 100, tot loss = 4.473775034859067, l1: 9.681145734031729e-05, l2: 0.00035056604662843583   Iteration 22 of 100, tot loss = 4.512232921340249, l1: 9.795193016708998e-05, l2: 0.0003532713632342744   Iteration 23 of 100, tot loss = 4.522031483442887, l1: 9.888912546043487e-05, l2: 0.0003533140236117027   Iteration 24 of 100, tot loss = 4.4848544498284655, l1: 9.804438802044994e-05, l2: 0.0003504410566771791   Iteration 25 of 100, tot loss = 4.582570734024048, l1: 9.976930159609765e-05, l2: 0.00035848777333740144   Iteration 26 of 100, tot loss = 4.709739272411053, l1: 0.0001017368810537916, l2: 0.00036923704912903934   Iteration 27 of 100, tot loss = 4.6238870797333895, l1: 0.00010001247654953558, l2: 0.00036237623458469496   Iteration 28 of 100, tot loss = 4.556808582374027, l1: 9.906259843514167e-05, l2: 0.0003566182624906235   Iteration 29 of 100, tot loss = 4.524011422847879, l1: 9.928420155935375e-05, l2: 0.0003531169440348408   Iteration 30 of 100, tot loss = 4.464384690920512, l1: 9.827477527627101e-05, l2: 0.000348163696374589   Iteration 31 of 100, tot loss = 4.431233575267177, l1: 9.793942186778651e-05, l2: 0.0003451839387003753   Iteration 32 of 100, tot loss = 4.39347568154335, l1: 9.708171432976087e-05, l2: 0.0003422658569434134   Iteration 33 of 100, tot loss = 4.34158976872762, l1: 9.549324871234906e-05, l2: 0.00033866573049277633   Iteration 34 of 100, tot loss = 4.394996586967917, l1: 9.508716121832978e-05, l2: 0.00034441249932997916   Iteration 35 of 100, tot loss = 4.404032380240304, l1: 9.563023496801698e-05, l2: 0.00034477300484598216   Iteration 36 of 100, tot loss = 4.438914312256707, l1: 9.618887219807625e-05, l2: 0.00034770255984363355   Iteration 37 of 100, tot loss = 4.506655821929106, l1: 9.693258048296352e-05, l2: 0.00035373300331254567   Iteration 38 of 100, tot loss = 4.52394017420317, l1: 9.740038426810458e-05, l2: 0.0003549936353872334   Iteration 39 of 100, tot loss = 4.499216177524665, l1: 9.708530547542092e-05, l2: 0.00035283631437684957   Iteration 40 of 100, tot loss = 4.53946818113327, l1: 9.793552608243772e-05, l2: 0.0003560112938430393   Iteration 41 of 100, tot loss = 4.568403348690126, l1: 9.901304177374852e-05, l2: 0.0003578272955520533   Iteration 42 of 100, tot loss = 4.5334132853008455, l1: 9.862177510166519e-05, l2: 0.0003547195561363229   Iteration 43 of 100, tot loss = 4.563985713692599, l1: 9.942332884705583e-05, l2: 0.0003569752450350152   Iteration 44 of 100, tot loss = 4.549064446579326, l1: 9.902436026591617e-05, l2: 0.00035588208662705836   Iteration 45 of 100, tot loss = 4.570063469145033, l1: 9.972918317847264e-05, l2: 0.00035727716555508475   Iteration 46 of 100, tot loss = 4.624560371689174, l1: 0.00010081607860743331, l2: 0.0003616399613826576   Iteration 47 of 100, tot loss = 4.6113828344548, l1: 0.00010075112174703502, l2: 0.0003603871644256597   Iteration 48 of 100, tot loss = 4.567033638556798, l1: 0.00010023288821988292, l2: 0.00035647047783034697   Iteration 49 of 100, tot loss = 4.5935786597582755, l1: 0.00010121663848510753, l2: 0.00035814122992985865   Iteration 50 of 100, tot loss = 4.617333507537841, l1: 0.00010181223427935038, l2: 0.0003599211195250973   Iteration 51 of 100, tot loss = 4.603018653159048, l1: 0.00010091453234553246, l2: 0.0003593873355866355   Iteration 52 of 100, tot loss = 4.6159513134222765, l1: 0.00010064877145710992, l2: 0.00036094636239935283   Iteration 53 of 100, tot loss = 4.649751406795573, l1: 0.00010108489622548662, l2: 0.0003638902463827212   Iteration 54 of 100, tot loss = 4.674866415836193, l1: 0.00010130395478174453, l2: 0.0003661826889987828   Iteration 55 of 100, tot loss = 4.663464845310558, l1: 0.00010092728235494261, l2: 0.00036541920430450275   Iteration 56 of 100, tot loss = 4.699322015047073, l1: 0.00010114596112284094, l2: 0.0003687862429485124   Iteration 57 of 100, tot loss = 4.7209613155900385, l1: 0.00010109228854784497, l2: 0.0003710038461640739   Iteration 58 of 100, tot loss = 4.737690765282204, l1: 0.00010131155337650602, l2: 0.0003724575262508708   Iteration 59 of 100, tot loss = 4.7122574418278065, l1: 0.00010099446580309216, l2: 0.0003702312816917044   Iteration 60 of 100, tot loss = 4.733888467152913, l1: 0.00010128488350649908, l2: 0.0003721039671897112   Iteration 61 of 100, tot loss = 4.766305720219846, l1: 0.00010192873986718458, l2: 0.00037470183666360365   Iteration 62 of 100, tot loss = 4.761047655536283, l1: 0.00010149830297685244, l2: 0.0003746064666624842   Iteration 63 of 100, tot loss = 4.771368518708244, l1: 0.00010160144631213153, l2: 0.00037553540910483294   Iteration 64 of 100, tot loss = 4.771137490868568, l1: 0.00010194829366128033, l2: 0.0003751654587631492   Iteration 65 of 100, tot loss = 4.760080036750207, l1: 0.00010224458802930223, l2: 0.00037376341844299954   Iteration 66 of 100, tot loss = 4.772058905977191, l1: 0.0001027731233824108, l2: 0.00037443276982451084   Iteration 67 of 100, tot loss = 4.764136755644386, l1: 0.00010259668189648247, l2: 0.00037381699662342833   Iteration 68 of 100, tot loss = 4.783917672493878, l1: 0.00010272751157625702, l2: 0.00037566425860003456   Iteration 69 of 100, tot loss = 4.781599507815596, l1: 0.00010237835485368387, l2: 0.00037578159864769196   Iteration 70 of 100, tot loss = 4.773319653102329, l1: 0.00010241453154386753, l2: 0.00037491743673204577   Iteration 71 of 100, tot loss = 4.773299176928023, l1: 0.00010215343164917881, l2: 0.0003751764890924752   Iteration 72 of 100, tot loss = 4.764365838633643, l1: 0.0001021263373230694, l2: 0.0003743102495516521   Iteration 73 of 100, tot loss = 4.799470973341433, l1: 0.00010277344634930272, l2: 0.00037717365413387496   Iteration 74 of 100, tot loss = 4.851153109524701, l1: 0.00010321815593547276, l2: 0.0003818971578053803   Iteration 75 of 100, tot loss = 4.852758700052897, l1: 0.00010328496694758846, l2: 0.0003819909057347104   Iteration 76 of 100, tot loss = 4.864157726890163, l1: 0.00010359318706545501, l2: 0.0003828225884430331   Iteration 77 of 100, tot loss = 4.870711295635669, l1: 0.00010343980702364443, l2: 0.00038363132493528116   Iteration 78 of 100, tot loss = 4.90548133238768, l1: 0.00010422133546410642, l2: 0.00038632680009005783   Iteration 79 of 100, tot loss = 4.898178124729591, l1: 0.00010409021448554719, l2: 0.0003857276001959874   Iteration 80 of 100, tot loss = 4.87044147849083, l1: 0.00010372855781497492, l2: 0.00038331559244397795   Iteration 81 of 100, tot loss = 4.917764693130682, l1: 0.00010452981515288824, l2: 0.00038724665707523396   Iteration 82 of 100, tot loss = 4.939830640467202, l1: 0.00010498167462284938, l2: 0.00038900139238316246   Iteration 83 of 100, tot loss = 4.959318528692406, l1: 0.00010545460016541015, l2: 0.00039047725531637267   Iteration 84 of 100, tot loss = 4.94992192586263, l1: 0.00010540001784691066, l2: 0.00038959217722766057   Iteration 85 of 100, tot loss = 4.940894031524659, l1: 0.00010557426075512707, l2: 0.0003885151446892825   Iteration 86 of 100, tot loss = 4.910416223282038, l1: 0.00010488122529221623, l2: 0.0003861603993776276   Iteration 87 of 100, tot loss = 4.911998603535794, l1: 0.00010501173432433346, l2: 0.00038618812854027097   Iteration 88 of 100, tot loss = 4.889390032399785, l1: 0.00010470103143234155, l2: 0.00038423797410690565   Iteration 89 of 100, tot loss = 4.864237115624245, l1: 0.00010436272088881697, l2: 0.0003820609927878537   Iteration 90 of 100, tot loss = 4.8574093659718836, l1: 0.00010457938264557419, l2: 0.0003811615558030705   Iteration 91 of 100, tot loss = 4.846727934512463, l1: 0.00010426162615268757, l2: 0.0003804111690772186   Iteration 92 of 100, tot loss = 4.817949108455492, l1: 0.00010387104786154258, l2: 0.0003779238645422637   Iteration 93 of 100, tot loss = 4.824417073239562, l1: 0.00010392810794636257, l2: 0.0003785136010577922   Iteration 94 of 100, tot loss = 4.811507204745678, l1: 0.0001040123442316086, l2: 0.00037713837809860706   Iteration 95 of 100, tot loss = 4.7746755361557005, l1: 0.0001032629411048746, l2: 0.0003742046142609692   Iteration 96 of 100, tot loss = 4.7619100622832775, l1: 0.00010323964435580517, l2: 0.0003729513634122365   Iteration 97 of 100, tot loss = 4.771505661846436, l1: 0.00010370151054284525, l2: 0.0003734490568341553   Iteration 98 of 100, tot loss = 4.764920235896597, l1: 0.00010345343097079811, l2: 0.00037303859396476525   Iteration 99 of 100, tot loss = 4.776877963181698, l1: 0.00010365375298230599, l2: 0.0003740340444809672   Iteration 100 of 100, tot loss = 4.764286552667618, l1: 0.00010341868619434536, l2: 0.00037300997006241233
   End of epoch 1179; saving model... 

Epoch 1180 of 2000
   Iteration 1 of 100, tot loss = 3.033024311065674, l1: 5.833736577187665e-05, l2: 0.0002449650492053479   Iteration 2 of 100, tot loss = 3.3916306495666504, l1: 7.47888861951651e-05, l2: 0.00026437416090629995   Iteration 3 of 100, tot loss = 4.382704734802246, l1: 0.00010068191356064442, l2: 0.0003375885523079584   Iteration 4 of 100, tot loss = 5.21951150894165, l1: 0.0001151894221038674, l2: 0.0004067617337568663   Iteration 5 of 100, tot loss = 4.659735774993896, l1: 0.0001031077372317668, l2: 0.0003628658450907096   Iteration 6 of 100, tot loss = 4.3324153025945025, l1: 9.400306468402657e-05, l2: 0.0003392384702844235   Iteration 7 of 100, tot loss = 4.6857673440660745, l1: 0.00010113151282504467, l2: 0.00036744522471313497   Iteration 8 of 100, tot loss = 4.8310366570949554, l1: 0.00010453569029778009, l2: 0.00037856797462154645   Iteration 9 of 100, tot loss = 4.81677524248759, l1: 0.00010383126856241789, l2: 0.00037784625601489097   Iteration 10 of 100, tot loss = 4.849139523506165, l1: 0.00010324232716811821, l2: 0.00038167162419995294   Iteration 11 of 100, tot loss = 4.783472299575806, l1: 0.00010262518150160427, l2: 0.000375722046803937   Iteration 12 of 100, tot loss = 4.667727688948314, l1: 0.00010104263977458079, l2: 0.00036573012645627995   Iteration 13 of 100, tot loss = 4.51115109370305, l1: 9.73169136649141e-05, l2: 0.0003537981929884364   Iteration 14 of 100, tot loss = 4.522216864994594, l1: 9.713713630584866e-05, l2: 0.0003550845483135033   Iteration 15 of 100, tot loss = 4.658588631947835, l1: 0.00010031301111060506, l2: 0.000365545849975509   Iteration 16 of 100, tot loss = 4.704931229352951, l1: 0.0001000299462248222, l2: 0.0003704631753862486   Iteration 17 of 100, tot loss = 4.951705904567943, l1: 0.00010066052217392579, l2: 0.0003945100684444804   Iteration 18 of 100, tot loss = 5.008776505788167, l1: 0.00010329705805310773, l2: 0.00039758059251148044   Iteration 19 of 100, tot loss = 4.928377101295872, l1: 0.00010151775801970966, l2: 0.0003913199524456439   Iteration 20 of 100, tot loss = 4.94417781829834, l1: 0.00010257115136482752, l2: 0.00039184663255582565   Iteration 21 of 100, tot loss = 4.916917097000849, l1: 0.0001027357783251708, l2: 0.0003889559337126446   Iteration 22 of 100, tot loss = 4.840304537252947, l1: 0.00010041136705364228, l2: 0.0003836190887589262   Iteration 23 of 100, tot loss = 4.819933051648348, l1: 0.0001008088183880318, l2: 0.00038118448941057306   Iteration 24 of 100, tot loss = 4.833171357711156, l1: 0.00010176532229403772, l2: 0.00038155181816061184   Iteration 25 of 100, tot loss = 4.8823667240142825, l1: 0.00010321061869035475, l2: 0.0003850260592298582   Iteration 26 of 100, tot loss = 4.979447153898386, l1: 0.00010545613048844434, l2: 0.00039248859180728544   Iteration 27 of 100, tot loss = 4.944246459890295, l1: 0.00010438711547909338, l2: 0.00039003753691859957   Iteration 28 of 100, tot loss = 4.942784709589822, l1: 0.00010482982790043544, l2: 0.00038944864926244397   Iteration 29 of 100, tot loss = 4.876759561999091, l1: 0.00010409332665811486, l2: 0.0003835826355498284   Iteration 30 of 100, tot loss = 4.911330890655518, l1: 0.00010476800780452322, l2: 0.00038636508688796314   Iteration 31 of 100, tot loss = 4.874431240943171, l1: 0.00010461257918523774, l2: 0.0003828305503233306   Iteration 32 of 100, tot loss = 4.873773530125618, l1: 0.00010500432892968092, l2: 0.00038237302942434326   Iteration 33 of 100, tot loss = 4.893857522444292, l1: 0.00010552890010082841, l2: 0.00038385685625006306   Iteration 34 of 100, tot loss = 4.847251106710995, l1: 0.0001055309036447023, l2: 0.00037919421062243227   Iteration 35 of 100, tot loss = 4.816803857258388, l1: 0.00010419914771253908, l2: 0.0003774812415940687   Iteration 36 of 100, tot loss = 4.802258167001936, l1: 0.00010407478465721296, l2: 0.0003761510350967809   Iteration 37 of 100, tot loss = 4.785687517475438, l1: 0.00010366797829880631, l2: 0.0003749007761094568   Iteration 38 of 100, tot loss = 4.84537156632072, l1: 0.00010505914858520937, l2: 0.0003794780101375899   Iteration 39 of 100, tot loss = 4.8175515333811445, l1: 0.00010429231723463043, l2: 0.00037746283842119365   Iteration 40 of 100, tot loss = 4.796188169717789, l1: 0.00010442331795275095, l2: 0.00037519550132856236   Iteration 41 of 100, tot loss = 4.755428546812476, l1: 0.00010439500107256718, l2: 0.0003711478561077767   Iteration 42 of 100, tot loss = 4.744464806147984, l1: 0.00010371974288621762, l2: 0.0003707267401512668   Iteration 43 of 100, tot loss = 4.7317713249561395, l1: 0.00010329527639293891, l2: 0.0003698818581205858   Iteration 44 of 100, tot loss = 4.752237905155528, l1: 0.00010351313945152056, l2: 0.0003717106529537969   Iteration 45 of 100, tot loss = 4.711924844317966, l1: 0.00010254762390913028, l2: 0.000368644862384018   Iteration 46 of 100, tot loss = 4.694910324138144, l1: 0.00010274019444730821, l2: 0.000366750840128035   Iteration 47 of 100, tot loss = 4.669803583875615, l1: 0.00010198782284204115, l2: 0.000364992537218681   Iteration 48 of 100, tot loss = 4.682742024461429, l1: 0.00010221665206699981, l2: 0.0003660575524312056   Iteration 49 of 100, tot loss = 4.691213671041995, l1: 0.00010168159036654789, l2: 0.00036743977784217164   Iteration 50 of 100, tot loss = 4.676404495239257, l1: 0.00010125201326445677, l2: 0.00036638843710534274   Iteration 51 of 100, tot loss = 4.703252689511168, l1: 0.00010156495052654588, l2: 0.00036876031948600475   Iteration 52 of 100, tot loss = 4.716554109866802, l1: 0.00010169713557563507, l2: 0.00036995827618878905   Iteration 53 of 100, tot loss = 4.7041046574430645, l1: 0.00010098592157998421, l2: 0.0003694245451182689   Iteration 54 of 100, tot loss = 4.693930352175677, l1: 0.00010103131771083335, l2: 0.00036836171865515   Iteration 55 of 100, tot loss = 4.666185318339955, l1: 0.000100111302890582, l2: 0.0003665072298397056   Iteration 56 of 100, tot loss = 4.644167010273252, l1: 9.977078603437153e-05, l2: 0.0003646459159166885   Iteration 57 of 100, tot loss = 4.627938078160872, l1: 9.999747392915499e-05, l2: 0.00036279633518700535   Iteration 58 of 100, tot loss = 4.616657104985467, l1: 9.98129790818423e-05, l2: 0.0003618527327238678   Iteration 59 of 100, tot loss = 4.60089656862162, l1: 9.978757718611524e-05, l2: 0.0003603020810198544   Iteration 60 of 100, tot loss = 4.589371192455292, l1: 9.943298870590903e-05, l2: 0.00035950413196890924   Iteration 61 of 100, tot loss = 4.579800390806354, l1: 9.914906665665617e-05, l2: 0.0003588309737693396   Iteration 62 of 100, tot loss = 4.5915836787992905, l1: 9.946481939902785e-05, l2: 0.0003596935491640902   Iteration 63 of 100, tot loss = 4.613494369718763, l1: 9.999315016246992e-05, l2: 0.000361356287381597   Iteration 64 of 100, tot loss = 4.600433636456728, l1: 9.967831357471368e-05, l2: 0.00036036505071024294   Iteration 65 of 100, tot loss = 4.5907066932091345, l1: 9.949014476240756e-05, l2: 0.00035958052523290884   Iteration 66 of 100, tot loss = 4.59258508682251, l1: 9.956579031660857e-05, l2: 0.0003596927188372127   Iteration 67 of 100, tot loss = 4.6141147827034565, l1: 9.993680442445355e-05, l2: 0.00036147467395874545   Iteration 68 of 100, tot loss = 4.622249126434326, l1: 9.982021257361767e-05, l2: 0.00036240469960509525   Iteration 69 of 100, tot loss = 4.600350556166275, l1: 9.989667829569848e-05, l2: 0.00036013837687779164   Iteration 70 of 100, tot loss = 4.574529998643058, l1: 9.918386635295714e-05, l2: 0.0003582691330978248   Iteration 71 of 100, tot loss = 4.575303873545687, l1: 9.943275751577178e-05, l2: 0.0003580976296855714   Iteration 72 of 100, tot loss = 4.555876122580634, l1: 9.914703984274335e-05, l2: 0.00035644057222978317   Iteration 73 of 100, tot loss = 4.573257661845586, l1: 9.914479123337564e-05, l2: 0.0003581809745910131   Iteration 74 of 100, tot loss = 4.530570202582592, l1: 9.824081894908591e-05, l2: 0.00035481620088339205   Iteration 75 of 100, tot loss = 4.550878516832987, l1: 9.846641648133906e-05, l2: 0.00035662143471806   Iteration 76 of 100, tot loss = 4.541965213261153, l1: 9.814108962975871e-05, l2: 0.0003560554313911493   Iteration 77 of 100, tot loss = 4.549486833733398, l1: 9.856993035230386e-05, l2: 0.00035637875247063335   Iteration 78 of 100, tot loss = 4.557463308175405, l1: 9.892125201123492e-05, l2: 0.000356825078107739   Iteration 79 of 100, tot loss = 4.583091740366779, l1: 9.931032975974988e-05, l2: 0.00035899884382091887   Iteration 80 of 100, tot loss = 4.563812224566936, l1: 9.92498918549245e-05, l2: 0.0003571313298380119   Iteration 81 of 100, tot loss = 4.558127354692529, l1: 9.93231966546133e-05, l2: 0.0003564895378974738   Iteration 82 of 100, tot loss = 4.5492082005593835, l1: 9.929252412736632e-05, l2: 0.00035562829512722293   Iteration 83 of 100, tot loss = 4.544553913265826, l1: 9.931051962151956e-05, l2: 0.0003551448709738205   Iteration 84 of 100, tot loss = 4.521495462883086, l1: 9.878297212361802e-05, l2: 0.00035336657362807145   Iteration 85 of 100, tot loss = 4.550150609016418, l1: 9.925796184688807e-05, l2: 0.00035575709822724627   Iteration 86 of 100, tot loss = 4.556545841139417, l1: 9.929302541422658e-05, l2: 0.00035636155811262987   Iteration 87 of 100, tot loss = 4.5585158624868285, l1: 9.895910340998236e-05, l2: 0.00035689248226384966   Iteration 88 of 100, tot loss = 4.582313452254642, l1: 9.916642565341053e-05, l2: 0.0003590649190631335   Iteration 89 of 100, tot loss = 4.5867401736505915, l1: 9.920802477516987e-05, l2: 0.00035946599218936826   Iteration 90 of 100, tot loss = 4.603237662050459, l1: 9.947608179774963e-05, l2: 0.00036084768443187285   Iteration 91 of 100, tot loss = 4.598323875731165, l1: 9.957890438522304e-05, l2: 0.00036025348334997934   Iteration 92 of 100, tot loss = 4.579288362160973, l1: 9.898738627943321e-05, l2: 0.0003589414499520385   Iteration 93 of 100, tot loss = 4.5880780719941665, l1: 9.900094085824877e-05, l2: 0.00035980686633622336   Iteration 94 of 100, tot loss = 4.587099205940328, l1: 9.898992361063682e-05, l2: 0.000359719996730688   Iteration 95 of 100, tot loss = 4.610759563195078, l1: 9.920558946568665e-05, l2: 0.00036187036695130363   Iteration 96 of 100, tot loss = 4.611047932257255, l1: 9.887557647895544e-05, l2: 0.00036222921653461526   Iteration 97 of 100, tot loss = 4.616841855737352, l1: 9.895791198377804e-05, l2: 0.0003627262733211818   Iteration 98 of 100, tot loss = 4.614294016847805, l1: 9.908717235090562e-05, l2: 0.00036234222901913773   Iteration 99 of 100, tot loss = 4.613357263382035, l1: 9.894342277983363e-05, l2: 0.000362392303132866   Iteration 100 of 100, tot loss = 4.602978180646897, l1: 9.87638102742494e-05, l2: 0.0003615340073883999
   End of epoch 1180; saving model... 

Epoch 1181 of 2000
   Iteration 1 of 100, tot loss = 3.491029977798462, l1: 5.42584020877257e-05, l2: 0.0002948446199297905   Iteration 2 of 100, tot loss = 3.7307454347610474, l1: 7.93053513916675e-05, l2: 0.00029376920429058373   Iteration 3 of 100, tot loss = 4.122765302658081, l1: 8.953373859791706e-05, l2: 0.00032274279510602355   Iteration 4 of 100, tot loss = 4.490584909915924, l1: 9.511924326943699e-05, l2: 0.00035393925645621493   Iteration 5 of 100, tot loss = 4.280478239059448, l1: 9.393080399604515e-05, l2: 0.0003341170260682702   Iteration 6 of 100, tot loss = 4.2899826765060425, l1: 9.188031011338656e-05, l2: 0.00033711796277202666   Iteration 7 of 100, tot loss = 4.237707989556449, l1: 9.165401778383446e-05, l2: 0.0003321167854924819   Iteration 8 of 100, tot loss = 4.138860762119293, l1: 9.136791140917921e-05, l2: 0.00032251816810457967   Iteration 9 of 100, tot loss = 4.220736503601074, l1: 9.405210827632497e-05, l2: 0.0003280215476277388   Iteration 10 of 100, tot loss = 4.103265047073364, l1: 9.260471342713572e-05, l2: 0.0003177217964548618   Iteration 11 of 100, tot loss = 4.219749190590599, l1: 9.43352755381387e-05, l2: 0.0003276396442247047   Iteration 12 of 100, tot loss = 4.305560231208801, l1: 9.591241420518297e-05, l2: 0.0003346436084636177   Iteration 13 of 100, tot loss = 4.285509659693791, l1: 9.761011460795999e-05, l2: 0.00033094085161932383   Iteration 14 of 100, tot loss = 4.3267012323652, l1: 9.883595287517113e-05, l2: 0.00033383416926621327   Iteration 15 of 100, tot loss = 4.402447350819906, l1: 9.981869467689345e-05, l2: 0.00034042603995961446   Iteration 16 of 100, tot loss = 4.387665003538132, l1: 9.987413386625121e-05, l2: 0.0003388923651073128   Iteration 17 of 100, tot loss = 4.4325258872088265, l1: 0.00010055491336114595, l2: 0.00034269767381963045   Iteration 18 of 100, tot loss = 4.388777282502916, l1: 9.990443828024177e-05, l2: 0.00033897328830789775   Iteration 19 of 100, tot loss = 4.466140044362922, l1: 0.00010132348229250822, l2: 0.0003452905220910907   Iteration 20 of 100, tot loss = 4.586913990974426, l1: 0.00010292598453816026, l2: 0.00035576541267801076   Iteration 21 of 100, tot loss = 4.532891897928147, l1: 0.00010181393008679152, l2: 0.00035147525791433596   Iteration 22 of 100, tot loss = 4.5623144995082505, l1: 0.00010177864830686964, l2: 0.00035445279949768025   Iteration 23 of 100, tot loss = 4.664883644684501, l1: 0.00010385151962628183, l2: 0.0003626368428661448   Iteration 24 of 100, tot loss = 4.7142460445563, l1: 0.00010363597387671082, l2: 0.0003677886294705483   Iteration 25 of 100, tot loss = 4.774478254318237, l1: 0.00010442933154990896, l2: 0.0003730184934101999   Iteration 26 of 100, tot loss = 4.710501313209534, l1: 0.00010337896590219595, l2: 0.00036767116454519477   Iteration 27 of 100, tot loss = 4.733928812874688, l1: 0.000104001500908958, l2: 0.00036939137876120014   Iteration 28 of 100, tot loss = 4.754508418696267, l1: 0.00010456506710657518, l2: 0.00037088577508776713   Iteration 29 of 100, tot loss = 4.674091059586098, l1: 0.00010340141131470366, l2: 0.00036400769524083567   Iteration 30 of 100, tot loss = 4.628800789515178, l1: 0.00010265593058041607, l2: 0.0003602241486078128   Iteration 31 of 100, tot loss = 4.735702007047592, l1: 0.00010464179250712116, l2: 0.000368928408152574   Iteration 32 of 100, tot loss = 4.696877859532833, l1: 0.00010450832996866666, l2: 0.0003651794559118571   Iteration 33 of 100, tot loss = 4.674545121915413, l1: 0.00010359416465331434, l2: 0.0003638603472306083   Iteration 34 of 100, tot loss = 4.699995342422934, l1: 0.0001040868232581381, l2: 0.00036591270969331483   Iteration 35 of 100, tot loss = 4.730102164404733, l1: 0.00010510951521739896, l2: 0.00036790069924401387   Iteration 36 of 100, tot loss = 4.7823190622859535, l1: 0.00010535321719847464, l2: 0.000372878688115937   Iteration 37 of 100, tot loss = 4.831970027975134, l1: 0.00010655066442858377, l2: 0.00037664633772864535   Iteration 38 of 100, tot loss = 4.752482941276149, l1: 0.00010483191813124761, l2: 0.00037041637513443435   Iteration 39 of 100, tot loss = 4.721810010763315, l1: 0.00010374187518326112, l2: 0.0003684391252672634   Iteration 40 of 100, tot loss = 4.732339763641358, l1: 0.00010451927619214985, l2: 0.00036871469928883015   Iteration 41 of 100, tot loss = 4.80661323593884, l1: 0.00010626101303808732, l2: 0.00037440030960502423   Iteration 42 of 100, tot loss = 4.769124070803325, l1: 0.00010520278227063717, l2: 0.0003717096239727523   Iteration 43 of 100, tot loss = 4.762739852417347, l1: 0.00010537868492551646, l2: 0.000370895299860168   Iteration 44 of 100, tot loss = 4.83705382455479, l1: 0.00010678223927904831, l2: 0.00037692314270481637   Iteration 45 of 100, tot loss = 4.876721747716268, l1: 0.0001070109783742939, l2: 0.00038066119563558864   Iteration 46 of 100, tot loss = 4.864551176195559, l1: 0.00010661659048903105, l2: 0.0003798385264615164   Iteration 47 of 100, tot loss = 4.866938403312196, l1: 0.00010678240327484053, l2: 0.00037991143688757687   Iteration 48 of 100, tot loss = 4.840273345510165, l1: 0.00010662551668853364, l2: 0.000377401818089614   Iteration 49 of 100, tot loss = 4.909005228354006, l1: 0.00010774372157709654, l2: 0.0003831568013696114   Iteration 50 of 100, tot loss = 4.88075382232666, l1: 0.00010716974727984052, l2: 0.0003809056349564344   Iteration 51 of 100, tot loss = 4.862541180030973, l1: 0.00010691240366249272, l2: 0.0003793417143991546   Iteration 52 of 100, tot loss = 4.8641103230989895, l1: 0.00010724147327542036, l2: 0.0003791695584928115   Iteration 53 of 100, tot loss = 4.834273527253349, l1: 0.00010655628222662446, l2: 0.0003768710698493106   Iteration 54 of 100, tot loss = 4.829033030403985, l1: 0.00010677673391775763, l2: 0.00037612656834190366   Iteration 55 of 100, tot loss = 4.80598653013056, l1: 0.00010664862632190555, l2: 0.0003739500259557231   Iteration 56 of 100, tot loss = 4.803107078586306, l1: 0.00010672647067622165, l2: 0.0003735842359934135   Iteration 57 of 100, tot loss = 4.850362991031847, l1: 0.00010766409034483347, l2: 0.0003773722074631798   Iteration 58 of 100, tot loss = 4.8285590615765805, l1: 0.00010740257512852309, l2: 0.0003754533295407249   Iteration 59 of 100, tot loss = 4.811337660934965, l1: 0.000107215137516397, l2: 0.00037391862725460177   Iteration 60 of 100, tot loss = 4.783242233594259, l1: 0.00010660896083815411, l2: 0.0003717152610382376   Iteration 61 of 100, tot loss = 4.758870703275086, l1: 0.00010611012607352945, l2: 0.00036977694301148416   Iteration 62 of 100, tot loss = 4.734997622428402, l1: 0.00010599177913982496, l2: 0.00036750798185934284   Iteration 63 of 100, tot loss = 4.688869003265623, l1: 0.00010505141755048599, l2: 0.0003638354814722247   Iteration 64 of 100, tot loss = 4.714367900043726, l1: 0.00010576068939371908, l2: 0.00036567609959092806   Iteration 65 of 100, tot loss = 4.683370597545917, l1: 0.00010494794875669938, l2: 0.00036338910979863545   Iteration 66 of 100, tot loss = 4.694036324818929, l1: 0.00010489690292601925, l2: 0.00036450672831216997   Iteration 67 of 100, tot loss = 4.700700112243197, l1: 0.00010468915472790571, l2: 0.0003653808549887487   Iteration 68 of 100, tot loss = 4.670858810929691, l1: 0.00010396080452825546, l2: 0.00036312507504857527   Iteration 69 of 100, tot loss = 4.660641259041386, l1: 0.00010374703086436848, l2: 0.00036231709342193454   Iteration 70 of 100, tot loss = 4.698718292372567, l1: 0.00010438568538769946, l2: 0.00036548614277437863   Iteration 71 of 100, tot loss = 4.7309912392790885, l1: 0.00010507017987678615, l2: 0.0003680289427021807   Iteration 72 of 100, tot loss = 4.715564118491279, l1: 0.0001048581738763864, l2: 0.0003666982366768126   Iteration 73 of 100, tot loss = 4.7091380537372745, l1: 0.00010478626262900509, l2: 0.00036612754147053274   Iteration 74 of 100, tot loss = 4.750537595233402, l1: 0.00010539079576611166, l2: 0.0003696629621019285   Iteration 75 of 100, tot loss = 4.722690073649089, l1: 0.00010490956240876888, l2: 0.00036735944333486257   Iteration 76 of 100, tot loss = 4.708323713980223, l1: 0.00010428244322132454, l2: 0.0003665499262799705   Iteration 77 of 100, tot loss = 4.678092711931699, l1: 0.00010387094304163626, l2: 0.00036393832634565987   Iteration 78 of 100, tot loss = 4.713622383582286, l1: 0.00010466379726727982, l2: 0.00036669843897330935   Iteration 79 of 100, tot loss = 4.702560005308706, l1: 0.00010469590719528438, l2: 0.0003655600914867194   Iteration 80 of 100, tot loss = 4.695873990654945, l1: 0.00010478670360498654, l2: 0.0003648006935691228   Iteration 81 of 100, tot loss = 4.707529600755668, l1: 0.00010495918854477494, l2: 0.0003657937693024445   Iteration 82 of 100, tot loss = 4.726190604814669, l1: 0.00010497497799117742, l2: 0.00036764408074171716   Iteration 83 of 100, tot loss = 4.72018820693694, l1: 0.00010519799809160934, l2: 0.000366820820890569   Iteration 84 of 100, tot loss = 4.7301960701034185, l1: 0.00010547381641247609, l2: 0.0003675457888415882   Iteration 85 of 100, tot loss = 4.725490101645975, l1: 0.0001052632877818884, l2: 0.00036728572073009086   Iteration 86 of 100, tot loss = 4.720417125280513, l1: 0.0001051930826496425, l2: 0.0003668486284833878   Iteration 87 of 100, tot loss = 4.695492284051303, l1: 0.00010481049906664486, l2: 0.00036473872793046225   Iteration 88 of 100, tot loss = 4.6920055096799675, l1: 0.000104642113208022, l2: 0.0003645584365585819   Iteration 89 of 100, tot loss = 4.6770212623510465, l1: 0.00010436666012497023, l2: 0.00036333546498947347   Iteration 90 of 100, tot loss = 4.677565940221151, l1: 0.00010446205257418721, l2: 0.00036329454030945067   Iteration 91 of 100, tot loss = 4.679026912856888, l1: 0.00010438483785857078, l2: 0.000363517852287719   Iteration 92 of 100, tot loss = 4.680956342945928, l1: 0.0001044686818916133, l2: 0.000363626951065785   Iteration 93 of 100, tot loss = 4.659377087828934, l1: 0.00010392278147597237, l2: 0.0003620149259160083   Iteration 94 of 100, tot loss = 4.645588220433986, l1: 0.00010372891982966549, l2: 0.0003608299008734088   Iteration 95 of 100, tot loss = 4.644288479654413, l1: 0.00010344470980558789, l2: 0.0003609841367793515   Iteration 96 of 100, tot loss = 4.618446201086044, l1: 0.00010301084182628983, l2: 0.00035883377692395396   Iteration 97 of 100, tot loss = 4.6142729336453465, l1: 0.00010256610212027882, l2: 0.00035886118982404893   Iteration 98 of 100, tot loss = 4.6353298109404895, l1: 0.00010275897693645675, l2: 0.00036077400326743077   Iteration 99 of 100, tot loss = 4.6498622846121735, l1: 0.00010257166054957038, l2: 0.00036241456668123116   Iteration 100 of 100, tot loss = 4.662204251289368, l1: 0.00010257136182190152, l2: 0.0003636490616190713
   End of epoch 1181; saving model... 

Epoch 1182 of 2000
   Iteration 1 of 100, tot loss = 3.581256866455078, l1: 0.00010829735401785001, l2: 0.00024982832837849855   Iteration 2 of 100, tot loss = 3.3558191061019897, l1: 9.318096999777481e-05, l2: 0.00024240092898253351   Iteration 3 of 100, tot loss = 3.9593096574147544, l1: 9.845967239622648e-05, l2: 0.000297471279433618   Iteration 4 of 100, tot loss = 4.0695595145225525, l1: 9.150178630079608e-05, l2: 0.0003154541554977186   Iteration 5 of 100, tot loss = 4.611094617843628, l1: 0.00010148442670470104, l2: 0.00035962502588517966   Iteration 6 of 100, tot loss = 4.07364147901535, l1: 9.020672769111115e-05, l2: 0.0003171574123067937   Iteration 7 of 100, tot loss = 4.697310192244394, l1: 0.00010291698760868582, l2: 0.00036681402811414694   Iteration 8 of 100, tot loss = 4.794008508324623, l1: 0.00010461865031174966, l2: 0.0003747821947399643   Iteration 9 of 100, tot loss = 4.588786827193366, l1: 0.00010371702349705932, l2: 0.00035516165452362556   Iteration 10 of 100, tot loss = 4.77507997751236, l1: 0.0001079499448678689, l2: 0.00036955805189791134   Iteration 11 of 100, tot loss = 4.615543528036638, l1: 0.00010624511560308747, l2: 0.00035530923725507984   Iteration 12 of 100, tot loss = 4.3778607447942095, l1: 0.0001018205375657999, l2: 0.00033596553657844197   Iteration 13 of 100, tot loss = 4.460793788616474, l1: 0.00010516443105118994, l2: 0.00034091494979712967   Iteration 14 of 100, tot loss = 4.444818530763898, l1: 0.00010443340137758892, l2: 0.0003400484508477218   Iteration 15 of 100, tot loss = 4.517800235748291, l1: 0.0001069159875138818, l2: 0.00034486403310438616   Iteration 16 of 100, tot loss = 4.45819878578186, l1: 0.00010634887507876556, l2: 0.0003394710006432433   Iteration 17 of 100, tot loss = 4.408631296718822, l1: 0.00010537766866036691, l2: 0.0003354854574461248   Iteration 18 of 100, tot loss = 4.471593459447225, l1: 0.00010657876818085142, l2: 0.00034058057361916226   Iteration 19 of 100, tot loss = 4.613439183486135, l1: 0.00010866527832388926, l2: 0.0003526786355283356   Iteration 20 of 100, tot loss = 4.5896118640899655, l1: 0.00010778043415484717, l2: 0.00035118074920319485   Iteration 21 of 100, tot loss = 4.550604093642462, l1: 0.00010705044838860409, l2: 0.00034800995802340495   Iteration 22 of 100, tot loss = 4.541985099965876, l1: 0.00010749023551232477, l2: 0.00034670827186677013   Iteration 23 of 100, tot loss = 4.591235534004543, l1: 0.00010901170231670181, l2: 0.0003501118490416759   Iteration 24 of 100, tot loss = 4.513106485207875, l1: 0.00010775931590008743, l2: 0.0003435513296305241   Iteration 25 of 100, tot loss = 4.596182765960694, l1: 0.00010884692441322841, l2: 0.000350771349912975   Iteration 26 of 100, tot loss = 4.588115673798781, l1: 0.00010926127945892334, l2: 0.0003495502854863074   Iteration 27 of 100, tot loss = 4.592355215990985, l1: 0.00010749633562496726, l2: 0.0003517391831135481   Iteration 28 of 100, tot loss = 4.6945710863385886, l1: 0.0001087258202200506, l2: 0.0003607312851922221   Iteration 29 of 100, tot loss = 4.634901211179536, l1: 0.00010660709123275692, l2: 0.0003568830276169043   Iteration 30 of 100, tot loss = 4.641606871287028, l1: 0.00010664252076821867, l2: 0.00035751816394622435   Iteration 31 of 100, tot loss = 4.621328061626803, l1: 0.00010537344790082574, l2: 0.00035675935519506195   Iteration 32 of 100, tot loss = 4.6571381986141205, l1: 0.00010603276916754112, l2: 0.0003596810468025069   Iteration 33 of 100, tot loss = 4.625727978619662, l1: 0.00010512542928953542, l2: 0.00035744736427053186   Iteration 34 of 100, tot loss = 4.615179812206941, l1: 0.00010560315176382359, l2: 0.0003559148251096143   Iteration 35 of 100, tot loss = 4.699943998881749, l1: 0.00010569260994088835, l2: 0.0003643017854691217   Iteration 36 of 100, tot loss = 4.680447320143382, l1: 0.00010569735352166592, l2: 0.0003623473742967083   Iteration 37 of 100, tot loss = 4.692322801899266, l1: 0.00010607001240714453, l2: 0.0003631622634743139   Iteration 38 of 100, tot loss = 4.716738933011105, l1: 0.00010658572854912276, l2: 0.0003650881616953197   Iteration 39 of 100, tot loss = 4.673005220217583, l1: 0.00010572504060035452, l2: 0.00036157547844758927   Iteration 40 of 100, tot loss = 4.725658446550369, l1: 0.00010649641599229654, l2: 0.00036606942649086704   Iteration 41 of 100, tot loss = 4.7324464321136475, l1: 0.00010689462560992802, l2: 0.00036635001530024657   Iteration 42 of 100, tot loss = 4.727411548296611, l1: 0.00010692848540895197, l2: 0.0003658126666518261   Iteration 43 of 100, tot loss = 4.739953567815381, l1: 0.0001066375651896314, l2: 0.00036735778866027116   Iteration 44 of 100, tot loss = 4.808210118250414, l1: 0.00010758125600577014, l2: 0.0003732397524965398   Iteration 45 of 100, tot loss = 4.813395335939195, l1: 0.00010794882069199553, l2: 0.00037339070873309134   Iteration 46 of 100, tot loss = 4.818612207537112, l1: 0.00010838051323093089, l2: 0.0003734807027997348   Iteration 47 of 100, tot loss = 4.798672270267568, l1: 0.00010843342889974666, l2: 0.00037143379372440554   Iteration 48 of 100, tot loss = 4.783534556627274, l1: 0.00010867254324390767, l2: 0.0003696809082308998   Iteration 49 of 100, tot loss = 4.770629221079301, l1: 0.00010854524745289902, l2: 0.00036851767072221264   Iteration 50 of 100, tot loss = 4.803224372863769, l1: 0.00010907183946983424, l2: 0.00037125059447134845   Iteration 51 of 100, tot loss = 4.838683876336789, l1: 0.00010934208287782145, l2: 0.00037452630175382595   Iteration 52 of 100, tot loss = 4.838735085267287, l1: 0.00010900885850927667, l2: 0.00037486464680114295   Iteration 53 of 100, tot loss = 4.839576208366538, l1: 0.00010911066941466657, l2: 0.0003748469482744903   Iteration 54 of 100, tot loss = 4.803290468675119, l1: 0.00010841202775837801, l2: 0.00037191701577049335   Iteration 55 of 100, tot loss = 4.7876504334536465, l1: 0.000108432422315342, l2: 0.00037033261786828834   Iteration 56 of 100, tot loss = 4.839620415653501, l1: 0.00010952259169373844, l2: 0.0003744394469841999   Iteration 57 of 100, tot loss = 4.865084827991954, l1: 0.0001099245711980314, l2: 0.00037658390940857334   Iteration 58 of 100, tot loss = 4.870413776101737, l1: 0.00011003769251491858, l2: 0.00037700368318774997   Iteration 59 of 100, tot loss = 4.868653228727438, l1: 0.00010973091196057671, l2: 0.0003771344092487105   Iteration 60 of 100, tot loss = 4.883959790070851, l1: 0.00010989423565964292, l2: 0.00037850174182191645   Iteration 61 of 100, tot loss = 4.867904600549917, l1: 0.00010971657529360707, l2: 0.000377073883175369   Iteration 62 of 100, tot loss = 4.858916482617778, l1: 0.00010925220486833789, l2: 0.00037663944186210903   Iteration 63 of 100, tot loss = 4.850494748070126, l1: 0.00010896296202739154, l2: 0.0003760865112694557   Iteration 64 of 100, tot loss = 4.845077581703663, l1: 0.00010909625603972017, l2: 0.0003754115006131542   Iteration 65 of 100, tot loss = 4.834597008044903, l1: 0.00010891151105170138, l2: 0.00037454818789578546   Iteration 66 of 100, tot loss = 4.828321435234764, l1: 0.00010913412998806135, l2: 0.0003736980113237885   Iteration 67 of 100, tot loss = 4.798487072560325, l1: 0.00010862917737703451, l2: 0.0003712195274289767   Iteration 68 of 100, tot loss = 4.818913424716277, l1: 0.00010888630667442853, l2: 0.00037300503380024834   Iteration 69 of 100, tot loss = 4.811478186344755, l1: 0.00010875409661557701, l2: 0.00037239371991692707   Iteration 70 of 100, tot loss = 4.819221040180751, l1: 0.0001087235446383212, l2: 0.0003731985573332557   Iteration 71 of 100, tot loss = 4.788887420170743, l1: 0.00010814022371819256, l2: 0.00037074851621890075   Iteration 72 of 100, tot loss = 4.779578586419423, l1: 0.00010787891991033878, l2: 0.000370078936915282   Iteration 73 of 100, tot loss = 4.7792606745680715, l1: 0.00010814587979327109, l2: 0.00036978018591706706   Iteration 74 of 100, tot loss = 4.737430562844148, l1: 0.00010716686690127087, l2: 0.00036657618772084323   Iteration 75 of 100, tot loss = 4.732083832422893, l1: 0.00010666057957375112, l2: 0.0003665478018228896   Iteration 76 of 100, tot loss = 4.725947753379219, l1: 0.0001066715994487535, l2: 0.00036592317442945817   Iteration 77 of 100, tot loss = 4.721349044279619, l1: 0.00010664339188460635, l2: 0.0003654915111270951   Iteration 78 of 100, tot loss = 4.7424539633286305, l1: 0.00010711297788172185, l2: 0.00036713241691662307   Iteration 79 of 100, tot loss = 4.741807095612152, l1: 0.00010701399496380723, l2: 0.00036716671272614214   Iteration 80 of 100, tot loss = 4.750897601246834, l1: 0.00010717381742324506, l2: 0.00036791594056921894   Iteration 81 of 100, tot loss = 4.769684123404232, l1: 0.00010719369046129918, l2: 0.00036977471977423437   Iteration 82 of 100, tot loss = 4.758193420200813, l1: 0.00010731512365299107, l2: 0.00036850421632307856   Iteration 83 of 100, tot loss = 4.744313774338687, l1: 0.00010712096241971825, l2: 0.00036731041315311836   Iteration 84 of 100, tot loss = 4.7374672549111505, l1: 0.00010669679245655148, l2: 0.0003670499315506147   Iteration 85 of 100, tot loss = 4.7282757422503305, l1: 0.00010611946816104191, l2: 0.00036670810479657066   Iteration 86 of 100, tot loss = 4.746209643607916, l1: 0.00010651111039996342, l2: 0.00036810985270570854   Iteration 87 of 100, tot loss = 4.761296442185325, l1: 0.00010690088014201336, l2: 0.0003692287624373379   Iteration 88 of 100, tot loss = 4.752449870109558, l1: 0.00010674422157370083, l2: 0.00036850076362911335   Iteration 89 of 100, tot loss = 4.734288971075851, l1: 0.00010654517245711759, l2: 0.0003668837229821967   Iteration 90 of 100, tot loss = 4.730626302295261, l1: 0.00010655005486720862, l2: 0.0003665125738835842   Iteration 91 of 100, tot loss = 4.705965704970307, l1: 0.00010617436191251317, l2: 0.00036442220720505996   Iteration 92 of 100, tot loss = 4.6870928328970205, l1: 0.00010590824461795147, l2: 0.00036280103726628863   Iteration 93 of 100, tot loss = 4.674078682417511, l1: 0.00010573240895179772, l2: 0.00036167545797696375   Iteration 94 of 100, tot loss = 4.663111379806032, l1: 0.00010563651232316377, l2: 0.00036067462458456073   Iteration 95 of 100, tot loss = 4.666056846317492, l1: 0.00010569746319608959, l2: 0.00036090821997372826   Iteration 96 of 100, tot loss = 4.681548732022445, l1: 0.00010598353575611934, l2: 0.0003621713359128383   Iteration 97 of 100, tot loss = 4.671883295491798, l1: 0.00010586287620772618, l2: 0.0003613254518791812   Iteration 98 of 100, tot loss = 4.692019246062454, l1: 0.000106223457304604, l2: 0.0003629784657036629   Iteration 99 of 100, tot loss = 4.672755910892679, l1: 0.00010580234370674148, l2: 0.0003614732456914469   Iteration 100 of 100, tot loss = 4.683532004356384, l1: 0.00010591137608571443, l2: 0.00036244182258087675
   End of epoch 1182; saving model... 

Epoch 1183 of 2000
   Iteration 1 of 100, tot loss = 2.738743305206299, l1: 6.487184873549268e-05, l2: 0.00020900247909594327   Iteration 2 of 100, tot loss = 2.7723580598831177, l1: 6.867099000373855e-05, l2: 0.0002085648156935349   Iteration 3 of 100, tot loss = 2.9002365271250405, l1: 7.541158750730877e-05, l2: 0.00021461206294285753   Iteration 4 of 100, tot loss = 3.261669099330902, l1: 8.240230636147317e-05, l2: 0.000243764603510499   Iteration 5 of 100, tot loss = 3.5262260913848875, l1: 8.593985548941419e-05, l2: 0.00026668275240808724   Iteration 6 of 100, tot loss = 4.325308680534363, l1: 0.00010062628401404557, l2: 0.0003319045839210351   Iteration 7 of 100, tot loss = 4.523640326091221, l1: 0.00010380702588008717, l2: 0.0003485570029754724   Iteration 8 of 100, tot loss = 4.760649710893631, l1: 0.00010673056476662168, l2: 0.00036933440424036235   Iteration 9 of 100, tot loss = 4.606368753645155, l1: 0.00010517707414692268, l2: 0.000355459798205023   Iteration 10 of 100, tot loss = 4.877876615524292, l1: 0.00011107621612609364, l2: 0.0003767114452784881   Iteration 11 of 100, tot loss = 4.733385909687389, l1: 0.00010884637264428999, l2: 0.0003644922174068845   Iteration 12 of 100, tot loss = 4.5618414878845215, l1: 0.00010499603801387518, l2: 0.00035118810774292797   Iteration 13 of 100, tot loss = 4.501059953983013, l1: 0.00010158665575615417, l2: 0.0003485193368620597   Iteration 14 of 100, tot loss = 4.6106493302753995, l1: 0.00010362545409796959, l2: 0.0003574394774789523   Iteration 15 of 100, tot loss = 4.73026229540507, l1: 0.0001067863542023891, l2: 0.00036623987252824006   Iteration 16 of 100, tot loss = 4.5907131135463715, l1: 0.00010486535279596865, l2: 0.0003542059548635734   Iteration 17 of 100, tot loss = 4.5119869428522446, l1: 0.00010456862112144282, l2: 0.0003466300686876125   Iteration 18 of 100, tot loss = 4.513730512724982, l1: 0.00010387002536541938, l2: 0.00034750302115248307   Iteration 19 of 100, tot loss = 4.461143782264308, l1: 0.00010308447216582288, l2: 0.00034302990197351103   Iteration 20 of 100, tot loss = 4.407435786724091, l1: 0.00010263820604450302, l2: 0.00033810536842793226   Iteration 21 of 100, tot loss = 4.300256672359648, l1: 0.00010070882420959173, l2: 0.00032931683970327   Iteration 22 of 100, tot loss = 4.318682573058388, l1: 0.00010069291056424845, l2: 0.0003311753435727124   Iteration 23 of 100, tot loss = 4.335629929666934, l1: 0.00010086166619895148, l2: 0.0003327013242675963   Iteration 24 of 100, tot loss = 4.402525633573532, l1: 0.0001020092442255797, l2: 0.00033824331512732897   Iteration 25 of 100, tot loss = 4.3011416864395144, l1: 9.97519720112905e-05, l2: 0.00033036219247151166   Iteration 26 of 100, tot loss = 4.233263359620021, l1: 9.788507192682188e-05, l2: 0.00032544126028929336   Iteration 27 of 100, tot loss = 4.194184705063149, l1: 9.741373624247326e-05, l2: 0.00032200473058899797   Iteration 28 of 100, tot loss = 4.17273137824876, l1: 9.706005662987341e-05, l2: 0.00032021307768965405   Iteration 29 of 100, tot loss = 4.1521805689252655, l1: 9.560294698719096e-05, l2: 0.00031961510680487444   Iteration 30 of 100, tot loss = 4.229019113381704, l1: 9.644113488320727e-05, l2: 0.00032646077258201934   Iteration 31 of 100, tot loss = 4.190892223388918, l1: 9.575721466511999e-05, l2: 0.00032333200356187   Iteration 32 of 100, tot loss = 4.257201585918665, l1: 9.692481364709238e-05, l2: 0.00032879534091989626   Iteration 33 of 100, tot loss = 4.318266091924725, l1: 9.793150302632994e-05, l2: 0.00033389510115904903   Iteration 34 of 100, tot loss = 4.292337119579315, l1: 9.780546347696251e-05, l2: 0.000331428243953269   Iteration 35 of 100, tot loss = 4.288902538163321, l1: 9.85043182611532e-05, l2: 0.0003303859303871702   Iteration 36 of 100, tot loss = 4.285478999217351, l1: 9.825994826921185e-05, l2: 0.0003302879465789172   Iteration 37 of 100, tot loss = 4.246166155144975, l1: 9.732550775830482e-05, l2: 0.0003272911027935963   Iteration 38 of 100, tot loss = 4.273308982974605, l1: 9.805046197710141e-05, l2: 0.0003292804318143202   Iteration 39 of 100, tot loss = 4.317976343326079, l1: 9.903866558511837e-05, l2: 0.0003327589652960738   Iteration 40 of 100, tot loss = 4.416216829419136, l1: 0.0001006686993605399, l2: 0.00034095298069587444   Iteration 41 of 100, tot loss = 4.513036707552468, l1: 0.0001021549236079076, l2: 0.00034914874342531423   Iteration 42 of 100, tot loss = 4.547759933131082, l1: 0.0001029284525256593, l2: 0.0003518475378292524   Iteration 43 of 100, tot loss = 4.533792348795159, l1: 0.00010272732766299119, l2: 0.0003506519041852592   Iteration 44 of 100, tot loss = 4.5576829774813215, l1: 0.00010302345708623761, l2: 0.0003527448382307987   Iteration 45 of 100, tot loss = 4.58650020758311, l1: 0.00010400621354670471, l2: 0.00035464380473260664   Iteration 46 of 100, tot loss = 4.595710492652396, l1: 0.0001039255986024332, l2: 0.000355645448270598   Iteration 47 of 100, tot loss = 4.543225250345595, l1: 0.00010264269215614594, l2: 0.00035167983027194883   Iteration 48 of 100, tot loss = 4.599910847842693, l1: 0.00010390587931397022, l2: 0.00035608520253542036   Iteration 49 of 100, tot loss = 4.576959052864386, l1: 0.00010340749101988895, l2: 0.0003542884113979811   Iteration 50 of 100, tot loss = 4.559906084537506, l1: 0.00010327223193598911, l2: 0.000352718373760581   Iteration 51 of 100, tot loss = 4.556768587991303, l1: 0.00010305813767070718, l2: 0.00035261871879829055   Iteration 52 of 100, tot loss = 4.585462838411331, l1: 0.00010339310154869544, l2: 0.00035515318002301053   Iteration 53 of 100, tot loss = 4.577730972811861, l1: 0.0001030092569395355, l2: 0.0003547638375751393   Iteration 54 of 100, tot loss = 4.6141326537838685, l1: 0.00010371422252285777, l2: 0.0003576990404528462   Iteration 55 of 100, tot loss = 4.660639808394692, l1: 0.00010417788461464541, l2: 0.00036188609347763385   Iteration 56 of 100, tot loss = 4.659942075610161, l1: 0.00010414750899404421, l2: 0.00036184669547115585   Iteration 57 of 100, tot loss = 4.6365201159527425, l1: 0.00010373888564349168, l2: 0.0003599131228647342   Iteration 58 of 100, tot loss = 4.606803287719858, l1: 0.00010334159265641637, l2: 0.0003573387331361401   Iteration 59 of 100, tot loss = 4.635094782053414, l1: 0.00010390141367186176, l2: 0.0003596080607197926   Iteration 60 of 100, tot loss = 4.643084655205409, l1: 0.00010417586127005052, l2: 0.00036013260008379194   Iteration 61 of 100, tot loss = 4.623492352298049, l1: 0.00010386646830679116, l2: 0.00035848276265973195   Iteration 62 of 100, tot loss = 4.6363748754224465, l1: 0.00010403084478081925, l2: 0.00035960663902393034   Iteration 63 of 100, tot loss = 4.648334648874071, l1: 0.00010411239066068823, l2: 0.00036072107041180724   Iteration 64 of 100, tot loss = 4.704669499769807, l1: 0.00010471181656157569, l2: 0.0003657551292235439   Iteration 65 of 100, tot loss = 4.680568590530982, l1: 0.00010417430155659812, l2: 0.00036388255337862157   Iteration 66 of 100, tot loss = 4.672730097264955, l1: 0.00010418467597143415, l2: 0.00036308832998408684   Iteration 67 of 100, tot loss = 4.663357946410108, l1: 0.00010365264721774259, l2: 0.0003626831438402826   Iteration 68 of 100, tot loss = 4.649602509596768, l1: 0.00010339360511245425, l2: 0.00036156664218116714   Iteration 69 of 100, tot loss = 4.634619686914527, l1: 0.00010281702797974636, l2: 0.00036064493727188665   Iteration 70 of 100, tot loss = 4.61825441462653, l1: 0.00010219863506790716, l2: 0.00035962680308979805   Iteration 71 of 100, tot loss = 4.589168140585993, l1: 0.00010140328626041825, l2: 0.0003575135246214365   Iteration 72 of 100, tot loss = 4.6350095685985355, l1: 0.00010234657864000635, l2: 0.0003611543753423676   Iteration 73 of 100, tot loss = 4.66806102942114, l1: 0.00010306330764313765, l2: 0.00036374279273967324   Iteration 74 of 100, tot loss = 4.705087205848178, l1: 0.00010337337502081741, l2: 0.0003671353431953419   Iteration 75 of 100, tot loss = 4.7086111338933305, l1: 0.00010360721290150347, l2: 0.0003672538980996857   Iteration 76 of 100, tot loss = 4.718738833540364, l1: 0.00010384533087696917, l2: 0.0003680285502157762   Iteration 77 of 100, tot loss = 4.722516470141225, l1: 0.00010423122502843752, l2: 0.0003680204197782539   Iteration 78 of 100, tot loss = 4.729155835432884, l1: 0.00010426767920972242, l2: 0.00036864790234129684   Iteration 79 of 100, tot loss = 4.705527744715726, l1: 0.00010360909694595015, l2: 0.00036694367542933627   Iteration 80 of 100, tot loss = 4.695695458352565, l1: 0.00010340722051296324, l2: 0.00036616232318920083   Iteration 81 of 100, tot loss = 4.677719709313946, l1: 0.00010316634183659012, l2: 0.000364605626802788   Iteration 82 of 100, tot loss = 4.66292489592622, l1: 0.0001032586819857853, l2: 0.000363033805425824   Iteration 83 of 100, tot loss = 4.675329847508166, l1: 0.00010361785575970502, l2: 0.00036391512636678767   Iteration 84 of 100, tot loss = 4.662700478519712, l1: 0.00010358600199348682, l2: 0.0003626840435095974   Iteration 85 of 100, tot loss = 4.682171278841356, l1: 0.00010366359668242855, l2: 0.0003645535287967719   Iteration 86 of 100, tot loss = 4.680438275947127, l1: 0.00010348036427553039, l2: 0.00036456346103033526   Iteration 87 of 100, tot loss = 4.676846582314064, l1: 0.00010351359064927613, l2: 0.00036417106541151973   Iteration 88 of 100, tot loss = 4.657788129015402, l1: 0.00010317142951779798, l2: 0.000362607381248381   Iteration 89 of 100, tot loss = 4.648118992869773, l1: 0.00010310428303220372, l2: 0.0003617076141749289   Iteration 90 of 100, tot loss = 4.654750870333777, l1: 0.00010305851938028355, l2: 0.0003624165657028142   Iteration 91 of 100, tot loss = 4.635392648833139, l1: 0.00010277441481945801, l2: 0.00036076484820524044   Iteration 92 of 100, tot loss = 4.633856901656026, l1: 0.00010267819162317724, l2: 0.0003607074969572157   Iteration 93 of 100, tot loss = 4.646131806476141, l1: 0.00010260590277793717, l2: 0.0003620072761462921   Iteration 94 of 100, tot loss = 4.649184459067405, l1: 0.00010269580196379932, l2: 0.00036222264223281255   Iteration 95 of 100, tot loss = 4.659613919258118, l1: 0.00010281476941982921, l2: 0.0003631466204655896   Iteration 96 of 100, tot loss = 4.678492097804944, l1: 0.00010294746558277741, l2: 0.0003649017418562532   Iteration 97 of 100, tot loss = 4.68646963724156, l1: 0.00010307046205085011, l2: 0.00036557649904934053   Iteration 98 of 100, tot loss = 4.671319519986912, l1: 0.00010276867044235494, l2: 0.00036436327889309816   Iteration 99 of 100, tot loss = 4.656384430750452, l1: 0.00010255009380276221, l2: 0.0003630883468143322   Iteration 100 of 100, tot loss = 4.658229395151138, l1: 0.00010271395072777522, l2: 0.00036310898649389857
   End of epoch 1183; saving model... 

Epoch 1184 of 2000
   Iteration 1 of 100, tot loss = 6.544262409210205, l1: 0.0001456175377825275, l2: 0.0005088087054900825   Iteration 2 of 100, tot loss = 6.447387456893921, l1: 0.0001316778507316485, l2: 0.0005130608915351331   Iteration 3 of 100, tot loss = 5.526901960372925, l1: 0.00011356002748167764, l2: 0.0004391301578531663   Iteration 4 of 100, tot loss = 5.7591734528541565, l1: 0.00011505345355544705, l2: 0.0004608638846548274   Iteration 5 of 100, tot loss = 5.250110054016114, l1: 0.00011040641111321747, l2: 0.0004146045888774097   Iteration 6 of 100, tot loss = 5.119312763214111, l1: 0.00011063047107503128, l2: 0.0004013008049999674   Iteration 7 of 100, tot loss = 4.9453340257917136, l1: 0.00010838874628201925, l2: 0.0003861446540603148   Iteration 8 of 100, tot loss = 5.223197877407074, l1: 0.00011151724265801022, l2: 0.0004108025423192885   Iteration 9 of 100, tot loss = 5.2128478156195746, l1: 0.00011196588376252394, l2: 0.0004093188941220029   Iteration 10 of 100, tot loss = 5.19992504119873, l1: 0.0001123371985158883, l2: 0.0004076553042978048   Iteration 11 of 100, tot loss = 5.21449613571167, l1: 0.00011421262090814047, l2: 0.0004072369907093658   Iteration 12 of 100, tot loss = 5.289795557657878, l1: 0.00011321246711304411, l2: 0.00041576708463253453   Iteration 13 of 100, tot loss = 5.537133436936599, l1: 0.00011698951017863762, l2: 0.00043672382553967717   Iteration 14 of 100, tot loss = 5.4345709936959405, l1: 0.00011621427724354103, l2: 0.00042724281541138353   Iteration 15 of 100, tot loss = 5.29667607943217, l1: 0.00011454436219840622, l2: 0.0004151232404789577   Iteration 16 of 100, tot loss = 5.221019849181175, l1: 0.00011376227439541253, l2: 0.00040833970524545293   Iteration 17 of 100, tot loss = 5.141770559198716, l1: 0.00011180103961926172, l2: 0.00040237601120572756   Iteration 18 of 100, tot loss = 5.049695014953613, l1: 0.0001092913872627024, l2: 0.0003956781098774324   Iteration 19 of 100, tot loss = 5.074794217159874, l1: 0.00010856746743420924, l2: 0.00039891195085242783   Iteration 20 of 100, tot loss = 5.0778775930404665, l1: 0.00010855680920940358, l2: 0.0003992309473687783   Iteration 21 of 100, tot loss = 5.092714445931571, l1: 0.00010977229734029, l2: 0.00039949914346271685   Iteration 22 of 100, tot loss = 4.9373847191984, l1: 0.00010643101284751373, l2: 0.0003873074555568482   Iteration 23 of 100, tot loss = 4.970797875653142, l1: 0.00010783604232963863, l2: 0.00038924374119074935   Iteration 24 of 100, tot loss = 4.8854836374521255, l1: 0.00010633505871737725, l2: 0.0003822133009331689   Iteration 25 of 100, tot loss = 4.906367449760437, l1: 0.000106340018537594, l2: 0.00038429672189522535   Iteration 26 of 100, tot loss = 4.986191910046798, l1: 0.00010695469063648488, l2: 0.00039166449669909733   Iteration 27 of 100, tot loss = 5.002484564427976, l1: 0.00010738227933028679, l2: 0.0003928661748499575   Iteration 28 of 100, tot loss = 4.9679639637470245, l1: 0.00010747666302839727, l2: 0.00038931973176659085   Iteration 29 of 100, tot loss = 4.91474895230655, l1: 0.00010691618996949321, l2: 0.0003845587032566104   Iteration 30 of 100, tot loss = 4.905379744370778, l1: 0.00010663743899688901, l2: 0.0003839005342645881   Iteration 31 of 100, tot loss = 4.867749010362933, l1: 0.00010654221970922194, l2: 0.0003802326809231853   Iteration 32 of 100, tot loss = 4.814811740070581, l1: 0.00010597463653994055, l2: 0.0003755065372388344   Iteration 33 of 100, tot loss = 4.831962704658508, l1: 0.00010647009196073861, l2: 0.00037672617790204555   Iteration 34 of 100, tot loss = 4.827785846065073, l1: 0.00010707940547196332, l2: 0.00037569917913060635   Iteration 35 of 100, tot loss = 4.752006575039455, l1: 0.00010517891122227801, l2: 0.000370021746493876   Iteration 36 of 100, tot loss = 4.828696068790224, l1: 0.00010694263457682812, l2: 0.0003759269731947117   Iteration 37 of 100, tot loss = 4.785634810860093, l1: 0.00010593900456237984, l2: 0.0003726244770380593   Iteration 38 of 100, tot loss = 4.800158008148796, l1: 0.00010650631405664363, l2: 0.0003735094873455206   Iteration 39 of 100, tot loss = 4.882849262310908, l1: 0.00010799400367362138, l2: 0.00038029092292373   Iteration 40 of 100, tot loss = 4.898301264643669, l1: 0.00010788314684759826, l2: 0.0003819469799054787   Iteration 41 of 100, tot loss = 4.845786109203246, l1: 0.00010667743543574655, l2: 0.0003779011752978876   Iteration 42 of 100, tot loss = 4.8732335822922845, l1: 0.00010706040943443354, l2: 0.00038026294870568175   Iteration 43 of 100, tot loss = 4.847737115482952, l1: 0.00010656917051122481, l2: 0.0003782045407502284   Iteration 44 of 100, tot loss = 4.786968905817378, l1: 0.00010547875940987565, l2: 0.00037321813083095054   Iteration 45 of 100, tot loss = 4.790843711958991, l1: 0.00010578952213917445, l2: 0.00037329484896165215   Iteration 46 of 100, tot loss = 4.800209949845853, l1: 0.00010549685003238467, l2: 0.000374524144559552   Iteration 47 of 100, tot loss = 4.795001681814802, l1: 0.00010590862764226412, l2: 0.00037359154041351233   Iteration 48 of 100, tot loss = 4.784188382327557, l1: 0.00010601520693853672, l2: 0.00037240363083886524   Iteration 49 of 100, tot loss = 4.844058778821205, l1: 0.00010735623139652842, l2: 0.0003770496455681681   Iteration 50 of 100, tot loss = 4.838069212436676, l1: 0.00010740765479567926, l2: 0.00037639926566043866   Iteration 51 of 100, tot loss = 4.849790706354029, l1: 0.00010768076304777744, l2: 0.00037729830735627854   Iteration 52 of 100, tot loss = 4.849159022936454, l1: 0.00010772809743614813, l2: 0.0003771878045182138   Iteration 53 of 100, tot loss = 4.942348698400101, l1: 0.00010878276465656147, l2: 0.00038545210609643036   Iteration 54 of 100, tot loss = 4.943814306347458, l1: 0.00010876544729399029, l2: 0.00038561598463544486   Iteration 55 of 100, tot loss = 4.959507805650884, l1: 0.00010926845815530132, l2: 0.00038668232393154707   Iteration 56 of 100, tot loss = 4.9727229071514945, l1: 0.00010947349820915926, l2: 0.0003877987939111855   Iteration 57 of 100, tot loss = 4.960085051101551, l1: 0.00010958449230090257, l2: 0.00038642401468997267   Iteration 58 of 100, tot loss = 4.935896813869476, l1: 0.00010887376071156629, l2: 0.00038471592239963274   Iteration 59 of 100, tot loss = 4.92535777617309, l1: 0.00010895293364567697, l2: 0.00038358284544270737   Iteration 60 of 100, tot loss = 4.89304425517718, l1: 0.0001083342804122367, l2: 0.0003809701469435822   Iteration 61 of 100, tot loss = 4.843031574468144, l1: 0.00010747751730377786, l2: 0.0003768256419342103   Iteration 62 of 100, tot loss = 4.84844732669092, l1: 0.00010799409345406363, l2: 0.0003768506414358384   Iteration 63 of 100, tot loss = 4.879196761146424, l1: 0.00010835090304479106, l2: 0.00037956877499781847   Iteration 64 of 100, tot loss = 4.877303224056959, l1: 0.0001086535905301389, l2: 0.0003790767343616608   Iteration 65 of 100, tot loss = 4.897578206429115, l1: 0.00010890864376471235, l2: 0.0003808491793908895   Iteration 66 of 100, tot loss = 4.903598102656278, l1: 0.00010883995761987259, l2: 0.0003815198550441754   Iteration 67 of 100, tot loss = 4.874845013689639, l1: 0.00010835092933718184, l2: 0.0003791335744717256   Iteration 68 of 100, tot loss = 4.876531418632059, l1: 0.00010863662191695767, l2: 0.0003790165225355475   Iteration 69 of 100, tot loss = 4.869485405908114, l1: 0.00010871863338726816, l2: 0.0003782299098784806   Iteration 70 of 100, tot loss = 4.8915324211120605, l1: 0.00010907905385205854, l2: 0.000380074191031911   Iteration 71 of 100, tot loss = 4.881065596996899, l1: 0.0001091410855915968, l2: 0.00037896547701806615   Iteration 72 of 100, tot loss = 4.857578274276522, l1: 0.00010893580216967773, l2: 0.00037682202795015957   Iteration 73 of 100, tot loss = 4.829680397085948, l1: 0.00010846193064736164, l2: 0.00037450611165546085   Iteration 74 of 100, tot loss = 4.85657229294648, l1: 0.00010867444640049094, l2: 0.0003769827854813303   Iteration 75 of 100, tot loss = 4.876210651397705, l1: 0.00010890428980928845, l2: 0.0003787167777772993   Iteration 76 of 100, tot loss = 4.876656067998786, l1: 0.0001088487124434323, l2: 0.0003788168970985632   Iteration 77 of 100, tot loss = 4.873735811803248, l1: 0.00010902708933602523, l2: 0.0003783464947316263   Iteration 78 of 100, tot loss = 4.887687340760842, l1: 0.00010889446578706459, l2: 0.00037987427099440724   Iteration 79 of 100, tot loss = 4.848981219001963, l1: 0.0001082733640803873, l2: 0.0003766247605141606   Iteration 80 of 100, tot loss = 4.826714916527271, l1: 0.00010788173804030521, l2: 0.00037478975609701594   Iteration 81 of 100, tot loss = 4.841022787270723, l1: 0.00010794930589057642, l2: 0.0003761529755767113   Iteration 82 of 100, tot loss = 4.827825533180702, l1: 0.0001076512195229553, l2: 0.00037513133642411357   Iteration 83 of 100, tot loss = 4.807377420276044, l1: 0.00010701649273003743, l2: 0.0003737212517803692   Iteration 84 of 100, tot loss = 4.831809022596905, l1: 0.00010715654592613213, l2: 0.0003760243590000928   Iteration 85 of 100, tot loss = 4.8194243108525, l1: 0.00010721990406369407, l2: 0.00037472252973684057   Iteration 86 of 100, tot loss = 4.792362757893496, l1: 0.00010653335016815068, l2: 0.0003727029281097118   Iteration 87 of 100, tot loss = 4.809814410648126, l1: 0.00010682989694492559, l2: 0.00037415154648264974   Iteration 88 of 100, tot loss = 4.815366703000936, l1: 0.00010687952532283485, l2: 0.0003746571477346482   Iteration 89 of 100, tot loss = 4.808302768160788, l1: 0.00010689217836780506, l2: 0.0003739381011175201   Iteration 90 of 100, tot loss = 4.7960260219044155, l1: 0.00010636022408208292, l2: 0.0003732423807377927   Iteration 91 of 100, tot loss = 4.795403411100199, l1: 0.0001064566796220798, l2: 0.00037308366394187275   Iteration 92 of 100, tot loss = 4.801448623771253, l1: 0.00010634106428883767, l2: 0.0003738038002389341   Iteration 93 of 100, tot loss = 4.823810232582913, l1: 0.00010669259986619876, l2: 0.00037568842510878037   Iteration 94 of 100, tot loss = 4.814919336045042, l1: 0.00010638536642023184, l2: 0.00037510656894734523   Iteration 95 of 100, tot loss = 4.8184068240617455, l1: 0.00010625340303369356, l2: 0.00037558728133597853   Iteration 96 of 100, tot loss = 4.819387273242076, l1: 0.0001065286756632607, l2: 0.0003754100533418144   Iteration 97 of 100, tot loss = 4.80965052806225, l1: 0.00010633167695822962, l2: 0.00037463337765675345   Iteration 98 of 100, tot loss = 4.808635987797562, l1: 0.00010647099518290797, l2: 0.00037439260527857447   Iteration 99 of 100, tot loss = 4.805739011427368, l1: 0.00010650685748480487, l2: 0.0003740670454319134   Iteration 100 of 100, tot loss = 4.806867188215255, l1: 0.0001065285645017866, l2: 0.00037415815590065906
   End of epoch 1184; saving model... 

Epoch 1185 of 2000
   Iteration 1 of 100, tot loss = 4.817286014556885, l1: 0.00010194373317062855, l2: 0.00037978484760969877   Iteration 2 of 100, tot loss = 4.637450933456421, l1: 0.00011083503704867326, l2: 0.00035291005042381585   Iteration 3 of 100, tot loss = 5.174745241800944, l1: 0.00012334502874485528, l2: 0.0003941295047601064   Iteration 4 of 100, tot loss = 4.570643126964569, l1: 0.00011384478966647293, l2: 0.0003432195298955776   Iteration 5 of 100, tot loss = 4.357419157028199, l1: 0.0001052596519002691, l2: 0.000330482266144827   Iteration 6 of 100, tot loss = 4.298776268959045, l1: 0.00010237204818016228, l2: 0.0003275055787526071   Iteration 7 of 100, tot loss = 4.678221804755075, l1: 0.00010672149385624965, l2: 0.0003611006873792836   Iteration 8 of 100, tot loss = 4.78092297911644, l1: 0.00010914076392509742, l2: 0.0003689515324367676   Iteration 9 of 100, tot loss = 4.625551674101088, l1: 0.00010788204154879268, l2: 0.0003546731265184159   Iteration 10 of 100, tot loss = 4.825503516197204, l1: 0.00011055102659156546, l2: 0.0003719993299455382   Iteration 11 of 100, tot loss = 4.81565260887146, l1: 0.00010813570539043708, l2: 0.0003734295592452823   Iteration 12 of 100, tot loss = 4.759591241677602, l1: 0.00010913276067488671, l2: 0.0003668263683114977   Iteration 13 of 100, tot loss = 4.70736681498014, l1: 0.00010876417721961983, l2: 0.00036197250749235257   Iteration 14 of 100, tot loss = 4.88348959173475, l1: 0.00011178752850225595, l2: 0.0003765614331184354   Iteration 15 of 100, tot loss = 5.019333028793335, l1: 0.0001151178075815551, l2: 0.0003868154997083669   Iteration 16 of 100, tot loss = 4.890006199479103, l1: 0.00011389806331862928, l2: 0.00037510256061068503   Iteration 17 of 100, tot loss = 4.939991347930011, l1: 0.00011414728428730193, l2: 0.0003798518540433553   Iteration 18 of 100, tot loss = 4.965687235196431, l1: 0.00011363702126093105, l2: 0.00038293170493691124   Iteration 19 of 100, tot loss = 5.040127064052381, l1: 0.00011450477292160748, l2: 0.0003895079341753827   Iteration 20 of 100, tot loss = 4.965346896648407, l1: 0.00011326513995300048, l2: 0.0003832695503660943   Iteration 21 of 100, tot loss = 4.854205256416684, l1: 0.00011129805560423327, l2: 0.0003741224708002327   Iteration 22 of 100, tot loss = 4.797736200419339, l1: 0.00011020202344463377, l2: 0.0003695715971513313   Iteration 23 of 100, tot loss = 4.825872390166573, l1: 0.00011092109913158271, l2: 0.0003716661411327431   Iteration 24 of 100, tot loss = 4.874691238005956, l1: 0.00011151397635937126, l2: 0.00037595514913846273   Iteration 25 of 100, tot loss = 4.921919870376587, l1: 0.00011279979051323608, l2: 0.0003793921979377046   Iteration 26 of 100, tot loss = 4.871545672416687, l1: 0.00011226327086870487, l2: 0.0003748912976544279   Iteration 27 of 100, tot loss = 4.922146346833971, l1: 0.00011246368079877425, l2: 0.00037975095512121225   Iteration 28 of 100, tot loss = 4.885414336408887, l1: 0.00011127538683857503, l2: 0.00037726604880715186   Iteration 29 of 100, tot loss = 4.880569499114464, l1: 0.00011162802648272943, l2: 0.00037642892456115707   Iteration 30 of 100, tot loss = 4.893127115567525, l1: 0.00011175797626492568, l2: 0.00037755473604192955   Iteration 31 of 100, tot loss = 4.875456586960824, l1: 0.00011144583151044865, l2: 0.0003760998280936732   Iteration 32 of 100, tot loss = 4.829310335218906, l1: 0.0001111226697503298, l2: 0.0003718083644344006   Iteration 33 of 100, tot loss = 4.821469964403095, l1: 0.00011134859916196685, l2: 0.0003707983970261094   Iteration 34 of 100, tot loss = 4.952679879525128, l1: 0.00011261858740266795, l2: 0.0003826493998263579   Iteration 35 of 100, tot loss = 4.979917246954781, l1: 0.00011344942925331583, l2: 0.0003845422961083906   Iteration 36 of 100, tot loss = 4.957595699363285, l1: 0.00011356975179902899, l2: 0.00038218981919473864   Iteration 37 of 100, tot loss = 4.945071884103723, l1: 0.00011349566193530336, l2: 0.0003810115276546394   Iteration 38 of 100, tot loss = 4.934830809894361, l1: 0.00011288710409548673, l2: 0.0003805959790234307   Iteration 39 of 100, tot loss = 4.901575039594602, l1: 0.00011263307141444574, l2: 0.0003775244344395991   Iteration 40 of 100, tot loss = 4.911359429359436, l1: 0.00011329108474456007, l2: 0.0003778448604862206   Iteration 41 of 100, tot loss = 4.912391336952767, l1: 0.00011309847734703841, l2: 0.00037814065865127415   Iteration 42 of 100, tot loss = 4.9408986909048895, l1: 0.00011301254164122621, l2: 0.0003810773293177287   Iteration 43 of 100, tot loss = 4.865682801534963, l1: 0.00011125506776003275, l2: 0.00037531321428358816   Iteration 44 of 100, tot loss = 4.8594524318521675, l1: 0.00011149457623105263, l2: 0.0003744506681290328   Iteration 45 of 100, tot loss = 4.824071497387356, l1: 0.0001104232083081216, l2: 0.00037198394258868776   Iteration 46 of 100, tot loss = 4.780912819116012, l1: 0.00010966662968870799, l2: 0.000368424653298343   Iteration 47 of 100, tot loss = 4.796359239740575, l1: 0.0001098028819313263, l2: 0.00036983304357334493   Iteration 48 of 100, tot loss = 4.785859937469165, l1: 0.00010952162673068717, l2: 0.00036906436859377817   Iteration 49 of 100, tot loss = 4.7947904090492095, l1: 0.00010971705705742352, l2: 0.00036976198554134035   Iteration 50 of 100, tot loss = 4.766916313171387, l1: 0.00010896677536948119, l2: 0.00036772485764231534   Iteration 51 of 100, tot loss = 4.787450687558043, l1: 0.00010935974505093113, l2: 0.0003693853253863898   Iteration 52 of 100, tot loss = 4.754196107387543, l1: 0.00010889810178363965, l2: 0.0003665215103515388   Iteration 53 of 100, tot loss = 4.772383991277443, l1: 0.00010875554392630463, l2: 0.00036848285691241543   Iteration 54 of 100, tot loss = 4.763858949696576, l1: 0.0001088459455247546, l2: 0.0003675399507132911   Iteration 55 of 100, tot loss = 4.777604653618552, l1: 0.0001089356280730995, l2: 0.00036882483850190925   Iteration 56 of 100, tot loss = 4.831519216299057, l1: 0.00011014357102924675, l2: 0.00037300835148406416   Iteration 57 of 100, tot loss = 4.808384389208074, l1: 0.00010976897471680371, l2: 0.0003710694653451802   Iteration 58 of 100, tot loss = 4.816074720744429, l1: 0.00010991783855043368, l2: 0.00037168963447991953   Iteration 59 of 100, tot loss = 4.772529545476881, l1: 0.0001087753473133859, l2: 0.0003684776081896971   Iteration 60 of 100, tot loss = 4.764117368062338, l1: 0.00010885634801525157, l2: 0.0003675553896755446   Iteration 61 of 100, tot loss = 4.767027198291216, l1: 0.00010851743813798778, l2: 0.0003681852824348559   Iteration 62 of 100, tot loss = 4.73327261786307, l1: 0.00010780413105103728, l2: 0.00036552313183839884   Iteration 63 of 100, tot loss = 4.7343617507389615, l1: 0.00010760610963362047, l2: 0.0003658300662960207   Iteration 64 of 100, tot loss = 4.759912628680468, l1: 0.00010808288936914323, l2: 0.0003679083738461486   Iteration 65 of 100, tot loss = 4.7473552520458515, l1: 0.00010808236469389298, l2: 0.0003666531608010141   Iteration 66 of 100, tot loss = 4.730579542391228, l1: 0.00010755853101286586, l2: 0.0003654994233687775   Iteration 67 of 100, tot loss = 4.729611503544138, l1: 0.00010752585121871792, l2: 0.0003654352991334371   Iteration 68 of 100, tot loss = 4.730191868894241, l1: 0.00010789689419171362, l2: 0.0003651222924578606   Iteration 69 of 100, tot loss = 4.75348907277204, l1: 0.00010848621889675522, l2: 0.00036686268825745344   Iteration 70 of 100, tot loss = 4.740516941887992, l1: 0.00010815258626410339, l2: 0.0003658991078347234   Iteration 71 of 100, tot loss = 4.758264729674433, l1: 0.00010807216118260378, l2: 0.00036775431157955505   Iteration 72 of 100, tot loss = 4.758182128270467, l1: 0.00010807858416228555, l2: 0.00036773962872555584   Iteration 73 of 100, tot loss = 4.765687544051915, l1: 0.00010843516593760722, l2: 0.0003681335887673936   Iteration 74 of 100, tot loss = 4.747501395844124, l1: 0.0001077373674126795, l2: 0.0003670127724133734   Iteration 75 of 100, tot loss = 4.722078955968221, l1: 0.00010730551827388505, l2: 0.0003649023778658981   Iteration 76 of 100, tot loss = 4.707869620699632, l1: 0.00010740772966911256, l2: 0.0003633792330219876   Iteration 77 of 100, tot loss = 4.695177474579253, l1: 0.00010730945718755897, l2: 0.00036220829091004593   Iteration 78 of 100, tot loss = 4.687773111539009, l1: 0.00010738809084077962, l2: 0.0003613892210220608   Iteration 79 of 100, tot loss = 4.678943220573135, l1: 0.00010730911558715013, l2: 0.0003605852070444299   Iteration 80 of 100, tot loss = 4.704983136057853, l1: 0.0001074416228220798, l2: 0.0003630566914580413   Iteration 81 of 100, tot loss = 4.692343576454822, l1: 0.0001073173812466938, l2: 0.0003619169772180073   Iteration 82 of 100, tot loss = 4.695572626299974, l1: 0.00010731385555118322, l2: 0.0003622434076799148   Iteration 83 of 100, tot loss = 4.68505245806223, l1: 0.00010729243513196707, l2: 0.00036121281133880503   Iteration 84 of 100, tot loss = 4.692808645112174, l1: 0.00010728767102548209, l2: 0.00036199319427916113   Iteration 85 of 100, tot loss = 4.698052299723906, l1: 0.00010694701152661925, l2: 0.000362858219544732   Iteration 86 of 100, tot loss = 4.695830128913702, l1: 0.0001071343380486766, l2: 0.00036244867636881726   Iteration 87 of 100, tot loss = 4.706092390520819, l1: 0.00010730056455104623, l2: 0.0003633086755938678   Iteration 88 of 100, tot loss = 4.691256607120687, l1: 0.00010681903817881407, l2: 0.0003623066234219798   Iteration 89 of 100, tot loss = 4.676987763201253, l1: 0.00010650964747143784, l2: 0.0003611891299258562   Iteration 90 of 100, tot loss = 4.660342587365045, l1: 0.00010610639688416592, l2: 0.0003599278630443021   Iteration 91 of 100, tot loss = 4.685701558878134, l1: 0.00010657790866949094, l2: 0.00036199224852460465   Iteration 92 of 100, tot loss = 4.678085832492165, l1: 0.00010656813957093223, l2: 0.00036124044503515546   Iteration 93 of 100, tot loss = 4.674046718946067, l1: 0.00010665212809521284, l2: 0.0003607525449228643   Iteration 94 of 100, tot loss = 4.704203714715674, l1: 0.00010732436211138191, l2: 0.0003630960104117626   Iteration 95 of 100, tot loss = 4.733371250252975, l1: 0.00010767626785979557, l2: 0.00036566085810160363   Iteration 96 of 100, tot loss = 4.741108742853005, l1: 0.00010777847834712399, l2: 0.0003663323967278605   Iteration 97 of 100, tot loss = 4.76309548948229, l1: 0.0001082177123609335, l2: 0.00036809183690173685   Iteration 98 of 100, tot loss = 4.752735145237981, l1: 0.00010817932172581002, l2: 0.0003670941931682601   Iteration 99 of 100, tot loss = 4.755252715313073, l1: 0.00010824872306054176, l2: 0.0003672765488230689   Iteration 100 of 100, tot loss = 4.75358719587326, l1: 0.00010824742123077157, l2: 0.0003671112984011415
   End of epoch 1185; saving model... 

Epoch 1186 of 2000
   Iteration 1 of 100, tot loss = 4.487200736999512, l1: 0.00012558668095152825, l2: 0.0003231333976145834   Iteration 2 of 100, tot loss = 4.67583966255188, l1: 0.00010698730693547986, l2: 0.0003605966630857438   Iteration 3 of 100, tot loss = 5.222321192423503, l1: 0.00010773459022554259, l2: 0.0004144975294669469   Iteration 4 of 100, tot loss = 4.824117422103882, l1: 0.00010119439502886962, l2: 0.0003812173454207368   Iteration 5 of 100, tot loss = 4.755416202545166, l1: 9.547623776597902e-05, l2: 0.0003800653852522373   Iteration 6 of 100, tot loss = 5.1473259925842285, l1: 0.00010124680435789439, l2: 0.00041348579300877947   Iteration 7 of 100, tot loss = 5.093163081577846, l1: 0.00010056558468412342, l2: 0.00040875072175237747   Iteration 8 of 100, tot loss = 5.017797946929932, l1: 0.00010379652212577639, l2: 0.00039798327270545997   Iteration 9 of 100, tot loss = 5.261085404290093, l1: 0.00010875593822371836, l2: 0.0004173525990659578   Iteration 10 of 100, tot loss = 5.18505072593689, l1: 0.00010711907816585153, l2: 0.0004113859904464334   Iteration 11 of 100, tot loss = 5.1585260738026015, l1: 0.00010787681623531336, l2: 0.0004079757887997072   Iteration 12 of 100, tot loss = 5.195261001586914, l1: 0.00010784542973851785, l2: 0.00041168067158044625   Iteration 13 of 100, tot loss = 5.00567469230065, l1: 0.00010444961438994281, l2: 0.00039611785681560065   Iteration 14 of 100, tot loss = 4.860609582492283, l1: 0.00010187928169865959, l2: 0.00038418167865269685   Iteration 15 of 100, tot loss = 4.957835054397583, l1: 0.00010214208547646801, l2: 0.0003936414209116871   Iteration 16 of 100, tot loss = 4.824427500367165, l1: 0.00010034819342763512, l2: 0.0003820945576080703   Iteration 17 of 100, tot loss = 4.770815218196196, l1: 0.0001013268980279784, l2: 0.0003757546253516065   Iteration 18 of 100, tot loss = 4.737799525260925, l1: 0.00010180954551388923, l2: 0.00037197040849908564   Iteration 19 of 100, tot loss = 4.742978560297113, l1: 0.00010215229543207802, l2: 0.00037214556160554483   Iteration 20 of 100, tot loss = 4.698933780193329, l1: 9.974381191568682e-05, l2: 0.00037014956687926316   Iteration 21 of 100, tot loss = 4.608608177730015, l1: 9.872178201303108e-05, l2: 0.00036213903651861033   Iteration 22 of 100, tot loss = 4.611618865620006, l1: 9.998417375424073e-05, l2: 0.00036117771378485486   Iteration 23 of 100, tot loss = 4.640509833460269, l1: 0.00010127331140962349, l2: 0.00036277767313826507   Iteration 24 of 100, tot loss = 4.7181250055631, l1: 0.00010173471143086014, l2: 0.00037007779186145245   Iteration 25 of 100, tot loss = 4.757015552520752, l1: 0.00010279825437464751, l2: 0.00037290330335963516   Iteration 26 of 100, tot loss = 4.7756464664752665, l1: 0.00010412473406736703, l2: 0.0003734399153752467   Iteration 27 of 100, tot loss = 4.7042757846690995, l1: 0.00010296658419301147, l2: 0.00036746099660242044   Iteration 28 of 100, tot loss = 4.741485459463937, l1: 0.00010417415368075516, l2: 0.0003699743943538384   Iteration 29 of 100, tot loss = 4.743753811408734, l1: 0.00010401582140134294, l2: 0.00037035956208048194   Iteration 30 of 100, tot loss = 4.771750402450562, l1: 0.00010435343780651844, l2: 0.0003728216048330069   Iteration 31 of 100, tot loss = 4.761194705963135, l1: 0.0001035174424149802, l2: 0.0003726020307024761   Iteration 32 of 100, tot loss = 4.849959582090378, l1: 0.00010434027615247032, l2: 0.00038065568423917284   Iteration 33 of 100, tot loss = 4.816204677928578, l1: 0.00010415719776629527, l2: 0.0003774632720954039   Iteration 34 of 100, tot loss = 4.845154004938462, l1: 0.0001038662270302131, l2: 0.0003806491748785929   Iteration 35 of 100, tot loss = 4.839359242575509, l1: 0.00010273290213912593, l2: 0.0003812030232178846   Iteration 36 of 100, tot loss = 4.774506323867374, l1: 0.00010144891772749058, l2: 0.0003760017154693034   Iteration 37 of 100, tot loss = 4.757326377404703, l1: 0.00010137851554522845, l2: 0.00037435412358152807   Iteration 38 of 100, tot loss = 4.749638689191718, l1: 0.00010117459054239781, l2: 0.0003737892789343421   Iteration 39 of 100, tot loss = 4.765083233515422, l1: 0.00010170510713345348, l2: 0.0003748032177099958   Iteration 40 of 100, tot loss = 4.784524613618851, l1: 0.00010249548795400187, l2: 0.0003759569746762281   Iteration 41 of 100, tot loss = 4.77817121947684, l1: 0.00010321248449078511, l2: 0.0003746046387255419   Iteration 42 of 100, tot loss = 4.730494800068083, l1: 0.00010189558755276569, l2: 0.00037115389360613856   Iteration 43 of 100, tot loss = 4.780860473943311, l1: 0.00010261561988574063, l2: 0.00037547042979154907   Iteration 44 of 100, tot loss = 4.762497408823534, l1: 0.00010250903332317566, l2: 0.00037374070995032196   Iteration 45 of 100, tot loss = 4.696411318249172, l1: 0.0001015471064116961, l2: 0.00036809402760506295   Iteration 46 of 100, tot loss = 4.688508142595706, l1: 0.00010095624249673995, l2: 0.0003678945743202981   Iteration 47 of 100, tot loss = 4.674818226631651, l1: 9.997660163207911e-05, l2: 0.0003675052230778903   Iteration 48 of 100, tot loss = 4.6389832844336825, l1: 9.929019590041814e-05, l2: 0.0003646081348354831   Iteration 49 of 100, tot loss = 4.769899655361565, l1: 0.00010145411935661045, l2: 0.0003755358485825721   Iteration 50 of 100, tot loss = 4.8190576410293575, l1: 0.00010241718286124524, l2: 0.0003794885832758155   Iteration 51 of 100, tot loss = 4.8433624482622335, l1: 0.00010303815610777568, l2: 0.0003812980902890273   Iteration 52 of 100, tot loss = 4.942725305373852, l1: 0.00010411483827528382, l2: 0.00039015769443022483   Iteration 53 of 100, tot loss = 4.948342309807831, l1: 0.00010414627241464946, l2: 0.00039068796056353504   Iteration 54 of 100, tot loss = 4.959997331654584, l1: 0.00010424837219707788, l2: 0.000391751362729809   Iteration 55 of 100, tot loss = 4.9857837460257795, l1: 0.00010482983059112237, l2: 0.000393748545717575   Iteration 56 of 100, tot loss = 4.991106965712139, l1: 0.0001053347090029482, l2: 0.0003937759895831862   Iteration 57 of 100, tot loss = 4.967398267043264, l1: 0.0001049109720170555, l2: 0.00039182885653613004   Iteration 58 of 100, tot loss = 4.940416340170236, l1: 0.0001046868230708963, l2: 0.0003893548126707801   Iteration 59 of 100, tot loss = 4.94813559823117, l1: 0.00010493797810217445, l2: 0.00038987558354303966   Iteration 60 of 100, tot loss = 4.955132631460826, l1: 0.00010506388868331366, l2: 0.00039044937620928976   Iteration 61 of 100, tot loss = 5.001783367063179, l1: 0.00010589600120123368, l2: 0.0003942823371244976   Iteration 62 of 100, tot loss = 4.969062812866703, l1: 0.00010557211141032542, l2: 0.00039133417146419356   Iteration 63 of 100, tot loss = 5.001946032993377, l1: 0.00010628429201518422, l2: 0.0003939103123792891   Iteration 64 of 100, tot loss = 5.0030399188399315, l1: 0.00010580719259678517, l2: 0.0003944968008227079   Iteration 65 of 100, tot loss = 4.999024002368634, l1: 0.00010566299522626931, l2: 0.00039423940615961327   Iteration 66 of 100, tot loss = 5.026749184637358, l1: 0.00010615503742883448, l2: 0.0003965198822808099   Iteration 67 of 100, tot loss = 5.03185573264734, l1: 0.0001062933227317512, l2: 0.0003968922512774328   Iteration 68 of 100, tot loss = 5.01471867982079, l1: 0.00010571446958184584, l2: 0.0003957573992332019   Iteration 69 of 100, tot loss = 5.023332326308541, l1: 0.000106039905443575, l2: 0.0003962933278408077   Iteration 70 of 100, tot loss = 5.0191058226994105, l1: 0.00010612112554164404, l2: 0.0003957894577719604   Iteration 71 of 100, tot loss = 5.000605086205711, l1: 0.00010575678150954945, l2: 0.00039430372804341   Iteration 72 of 100, tot loss = 4.9769565264383955, l1: 0.00010551093990013922, l2: 0.0003921847135441365   Iteration 73 of 100, tot loss = 4.957317946708366, l1: 0.00010521308440605038, l2: 0.0003905187109572979   Iteration 74 of 100, tot loss = 5.0092465362033325, l1: 0.0001060701908155917, l2: 0.00039485446324908626   Iteration 75 of 100, tot loss = 4.977846616109212, l1: 0.0001056019006743251, l2: 0.0003921827610853749   Iteration 76 of 100, tot loss = 4.997720185079072, l1: 0.00010626460056971012, l2: 0.0003935074184578537   Iteration 77 of 100, tot loss = 4.9988863746841234, l1: 0.00010616513610365437, l2: 0.00039372350194706135   Iteration 78 of 100, tot loss = 5.002707505837465, l1: 0.00010619774627393314, l2: 0.0003940730044306316   Iteration 79 of 100, tot loss = 5.0091970178145395, l1: 0.0001062111037247667, l2: 0.00039470859844152096   Iteration 80 of 100, tot loss = 5.008712810277939, l1: 0.00010643885802892328, l2: 0.00039443242303605075   Iteration 81 of 100, tot loss = 5.000198782226185, l1: 0.00010665180507427898, l2: 0.0003933680733704629   Iteration 82 of 100, tot loss = 4.980863007103524, l1: 0.00010650469839009286, l2: 0.00039158160249892874   Iteration 83 of 100, tot loss = 4.971202752676355, l1: 0.00010642344888590041, l2: 0.0003906968266230514   Iteration 84 of 100, tot loss = 4.954499088582539, l1: 0.00010622113504290436, l2: 0.00038922877406765197   Iteration 85 of 100, tot loss = 4.979610384211821, l1: 0.00010659534767482375, l2: 0.00039136569068753436   Iteration 86 of 100, tot loss = 4.974827575129132, l1: 0.00010644791917340711, l2: 0.0003910348384314877   Iteration 87 of 100, tot loss = 4.95696706607424, l1: 0.00010609993733359961, l2: 0.00038959676907973195   Iteration 88 of 100, tot loss = 4.954232809218493, l1: 0.00010621856402146726, l2: 0.000389204716653198   Iteration 89 of 100, tot loss = 4.955944219332062, l1: 0.00010655493675135965, l2: 0.00038903948464731716   Iteration 90 of 100, tot loss = 4.938941412501865, l1: 0.00010609668602733614, l2: 0.0003877974547827358   Iteration 91 of 100, tot loss = 4.941468901686616, l1: 0.00010642445484556227, l2: 0.00038772243499476976   Iteration 92 of 100, tot loss = 4.929110931313557, l1: 0.00010623620567892668, l2: 0.0003866748870875555   Iteration 93 of 100, tot loss = 4.9455285841418855, l1: 0.00010666069811709496, l2: 0.00038789216013092006   Iteration 94 of 100, tot loss = 4.944740335992042, l1: 0.00010660156968174789, l2: 0.0003878724639164036   Iteration 95 of 100, tot loss = 4.925050426784314, l1: 0.00010639805944887676, l2: 0.0003861069831672419   Iteration 96 of 100, tot loss = 4.926405740280946, l1: 0.00010628949595078059, l2: 0.0003863510782290784   Iteration 97 of 100, tot loss = 4.949213084486342, l1: 0.00010646733227479944, l2: 0.00038845397580892675   Iteration 98 of 100, tot loss = 4.941679776931296, l1: 0.0001063537862248676, l2: 0.0003878141913595087   Iteration 99 of 100, tot loss = 4.940965471845685, l1: 0.000106357153664717, l2: 0.0003877393935890539   Iteration 100 of 100, tot loss = 4.944024093151093, l1: 0.00010645966151059838, l2: 0.00038794274754764044
   End of epoch 1186; saving model... 

Epoch 1187 of 2000
   Iteration 1 of 100, tot loss = 3.8780887126922607, l1: 9.076253627426922e-05, l2: 0.0002970463247038424   Iteration 2 of 100, tot loss = 5.252464413642883, l1: 0.00011916767834918573, l2: 0.00040607876144349575   Iteration 3 of 100, tot loss = 5.258884032567342, l1: 0.00011224587797187269, l2: 0.0004136425268370658   Iteration 4 of 100, tot loss = 4.227134257555008, l1: 9.234036133420886e-05, l2: 0.00033037306457117666   Iteration 5 of 100, tot loss = 4.283231282234192, l1: 9.461187510169111e-05, l2: 0.00033371125609846783   Iteration 6 of 100, tot loss = 4.274572551250458, l1: 9.192741648197018e-05, l2: 0.00033552984556687687   Iteration 7 of 100, tot loss = 4.166329537119184, l1: 9.11399937259765e-05, l2: 0.0003254929667621452   Iteration 8 of 100, tot loss = 4.126161560416222, l1: 9.079498386199703e-05, l2: 0.0003218211759303813   Iteration 9 of 100, tot loss = 4.036184933450487, l1: 8.940000528430876e-05, l2: 0.0003142184909342581   Iteration 10 of 100, tot loss = 4.291161859035492, l1: 8.965222259575967e-05, l2: 0.0003394639650650788   Iteration 11 of 100, tot loss = 4.628538337620822, l1: 9.757535305900753e-05, l2: 0.00036527848318325016   Iteration 12 of 100, tot loss = 4.527183343966802, l1: 9.671831715725905e-05, l2: 0.0003560000192616523   Iteration 13 of 100, tot loss = 4.643597098497244, l1: 0.00010033714771942378, l2: 0.0003640225650787425   Iteration 14 of 100, tot loss = 4.59502808536802, l1: 0.00010031543024524581, l2: 0.0003591873812963188   Iteration 15 of 100, tot loss = 4.434323302904764, l1: 9.779982525894108e-05, l2: 0.0003456325081060641   Iteration 16 of 100, tot loss = 4.366770349442959, l1: 9.799337817639753e-05, l2: 0.0003386836601748655   Iteration 17 of 100, tot loss = 4.2995925861246445, l1: 9.719688922118889e-05, l2: 0.00033276237210135576   Iteration 18 of 100, tot loss = 4.332080450322893, l1: 9.73426287802997e-05, l2: 0.0003358654191995609   Iteration 19 of 100, tot loss = 4.242156875760932, l1: 9.479681619651321e-05, l2: 0.00032941887337457096   Iteration 20 of 100, tot loss = 4.264257711172104, l1: 9.586969135852997e-05, l2: 0.00033055608182621656   Iteration 21 of 100, tot loss = 4.25361582778749, l1: 9.674893746185782e-05, l2: 0.00032861264653980644   Iteration 22 of 100, tot loss = 4.28268718177622, l1: 9.704615414110859e-05, l2: 0.0003312225648683538   Iteration 23 of 100, tot loss = 4.3008989821309624, l1: 9.76316378396207e-05, l2: 0.0003324582607804231   Iteration 24 of 100, tot loss = 4.2868930250406265, l1: 9.636754324067927e-05, l2: 0.0003323217603489563   Iteration 25 of 100, tot loss = 4.256996912956238, l1: 9.548624249873683e-05, l2: 0.0003302134500700049   Iteration 26 of 100, tot loss = 4.315625057770656, l1: 9.67025869333436e-05, l2: 0.00033485992053801264   Iteration 27 of 100, tot loss = 4.336182166028906, l1: 9.666072421370902e-05, l2: 0.0003369574947606048   Iteration 28 of 100, tot loss = 4.318278214761189, l1: 9.689100781023236e-05, l2: 0.00033493681543664673   Iteration 29 of 100, tot loss = 4.3439262201046125, l1: 9.78724966789114e-05, l2: 0.00033652012769317514   Iteration 30 of 100, tot loss = 4.408736582597097, l1: 9.80462195002474e-05, l2: 0.00034282744042381333   Iteration 31 of 100, tot loss = 4.4402482778795305, l1: 9.889632412758206e-05, l2: 0.0003451285055263959   Iteration 32 of 100, tot loss = 4.457602221518755, l1: 9.926462439580064e-05, l2: 0.00034649560052457673   Iteration 33 of 100, tot loss = 4.387725327954148, l1: 9.799396315374355e-05, l2: 0.0003407785722326177   Iteration 34 of 100, tot loss = 4.392513068283305, l1: 9.791131838830872e-05, l2: 0.00034133999061476276   Iteration 35 of 100, tot loss = 4.392362189292908, l1: 9.814654757584711e-05, l2: 0.0003410896729162362   Iteration 36 of 100, tot loss = 4.355243474245071, l1: 9.760038089249672e-05, l2: 0.0003379239676885643   Iteration 37 of 100, tot loss = 4.433487405648103, l1: 9.880965011984085e-05, l2: 0.0003445390912757658   Iteration 38 of 100, tot loss = 4.407461000116248, l1: 9.76466361814681e-05, l2: 0.00034309946491511685   Iteration 39 of 100, tot loss = 4.397027012629387, l1: 9.726275115658958e-05, l2: 0.0003424399513348292   Iteration 40 of 100, tot loss = 4.440551528334618, l1: 9.801121059354046e-05, l2: 0.0003460439440459595   Iteration 41 of 100, tot loss = 4.418630643588741, l1: 9.757822364208107e-05, l2: 0.0003442848422465187   Iteration 42 of 100, tot loss = 4.422323803106944, l1: 9.823928961677233e-05, l2: 0.0003439930915192235   Iteration 43 of 100, tot loss = 4.381296077439951, l1: 9.757276635608896e-05, l2: 0.0003405568424441722   Iteration 44 of 100, tot loss = 4.380849835547534, l1: 9.765982800176558e-05, l2: 0.0003404251568678195   Iteration 45 of 100, tot loss = 4.4104398171106975, l1: 9.796293250272155e-05, l2: 0.0003430810512832573   Iteration 46 of 100, tot loss = 4.45565861981848, l1: 9.892754813177896e-05, l2: 0.00034663831557617925   Iteration 47 of 100, tot loss = 4.492481685699301, l1: 9.941351047328158e-05, l2: 0.0003498346593411778   Iteration 48 of 100, tot loss = 4.514565495153268, l1: 0.0001002305403593103, l2: 0.0003512260101767121   Iteration 49 of 100, tot loss = 4.532215583081148, l1: 0.00010081862533947319, l2: 0.0003524029341452679   Iteration 50 of 100, tot loss = 4.540179574489594, l1: 0.00010123483276402113, l2: 0.0003527831258543301   Iteration 51 of 100, tot loss = 4.519633010321972, l1: 0.00010107241120981508, l2: 0.0003508908907236878   Iteration 52 of 100, tot loss = 4.556978727762516, l1: 0.00010165735336788482, l2: 0.0003540405200160771   Iteration 53 of 100, tot loss = 4.547161154027255, l1: 0.00010124560826367979, l2: 0.0003534705074172344   Iteration 54 of 100, tot loss = 4.563837163978153, l1: 0.00010106628010193904, l2: 0.0003553174358751642   Iteration 55 of 100, tot loss = 4.601182449947704, l1: 0.00010154970904378305, l2: 0.0003585685358054682   Iteration 56 of 100, tot loss = 4.621573360902922, l1: 0.0001019274352204645, l2: 0.00036022990038223464   Iteration 57 of 100, tot loss = 4.623710211954619, l1: 0.00010127133799343076, l2: 0.00036109968309654247   Iteration 58 of 100, tot loss = 4.624288515797977, l1: 0.00010145968359399282, l2: 0.0003609691676533187   Iteration 59 of 100, tot loss = 4.622166389125889, l1: 0.00010175192690409041, l2: 0.0003604647113746269   Iteration 60 of 100, tot loss = 4.605242166916529, l1: 0.00010184310976910638, l2: 0.00035868110632388076   Iteration 61 of 100, tot loss = 4.62220905061628, l1: 0.00010225857572475464, l2: 0.0003599623294755221   Iteration 62 of 100, tot loss = 4.587575045324141, l1: 0.00010143146438725037, l2: 0.00035732604045469703   Iteration 63 of 100, tot loss = 4.608916776520865, l1: 0.00010146446601581937, l2: 0.00035942721169992604   Iteration 64 of 100, tot loss = 4.580968579277396, l1: 0.00010079895582748577, l2: 0.0003572979020418643   Iteration 65 of 100, tot loss = 4.599987372985253, l1: 0.00010105227713491051, l2: 0.00035894646011561587   Iteration 66 of 100, tot loss = 4.588356113795078, l1: 0.00010109937625891303, l2: 0.0003577362348909391   Iteration 67 of 100, tot loss = 4.5998411481060195, l1: 0.00010146864473996962, l2: 0.00035851547021484835   Iteration 68 of 100, tot loss = 4.572296817513073, l1: 0.00010102619286472498, l2: 0.00035620348899101373   Iteration 69 of 100, tot loss = 4.548834785171177, l1: 0.00010031810729676668, l2: 0.0003545653713787771   Iteration 70 of 100, tot loss = 4.530479790483202, l1: 9.984980206354522e-05, l2: 0.00035319817720197276   Iteration 71 of 100, tot loss = 4.534447379515204, l1: 9.971084374472053e-05, l2: 0.0003537338942946853   Iteration 72 of 100, tot loss = 4.536246258351538, l1: 9.984551636282251e-05, l2: 0.0003537791094964228   Iteration 73 of 100, tot loss = 4.51487185693767, l1: 9.949831501341565e-05, l2: 0.0003519888706337852   Iteration 74 of 100, tot loss = 4.529166239338952, l1: 9.973476443446015e-05, l2: 0.00035318185886402763   Iteration 75 of 100, tot loss = 4.523028333981832, l1: 9.975396019096175e-05, l2: 0.0003525488721788861   Iteration 76 of 100, tot loss = 4.523301656308927, l1: 9.987003891422462e-05, l2: 0.0003524601254053107   Iteration 77 of 100, tot loss = 4.552793204010307, l1: 0.00010048996244387871, l2: 0.0003547893564765608   Iteration 78 of 100, tot loss = 4.561123351256053, l1: 0.00010091042181906792, l2: 0.0003552019116819466   Iteration 79 of 100, tot loss = 4.533671069748794, l1: 0.00010023231350461646, l2: 0.0003531347919525166   Iteration 80 of 100, tot loss = 4.536528222262859, l1: 0.00010056240712401632, l2: 0.00035309041404616435   Iteration 81 of 100, tot loss = 4.502476808465557, l1: 9.977462908819341e-05, l2: 0.0003504730506123182   Iteration 82 of 100, tot loss = 4.49854291212268, l1: 9.971584116034891e-05, l2: 0.0003501384490824066   Iteration 83 of 100, tot loss = 4.529824324401028, l1: 0.00010019035800219884, l2: 0.00035279207340096607   Iteration 84 of 100, tot loss = 4.561796203965232, l1: 0.00010087689190099592, l2: 0.00035530272734398576   Iteration 85 of 100, tot loss = 4.593371664776521, l1: 0.00010159803667407045, l2: 0.0003577391285730033   Iteration 86 of 100, tot loss = 4.599013282809147, l1: 0.00010178264423048795, l2: 0.0003581186828405196   Iteration 87 of 100, tot loss = 4.579977908353696, l1: 0.00010138224660159335, l2: 0.00035661554300887296   Iteration 88 of 100, tot loss = 4.594018785790964, l1: 0.00010155958832249532, l2: 0.00035784228905620034   Iteration 89 of 100, tot loss = 4.6070802412676, l1: 0.00010181696399761196, l2: 0.0003588910595078304   Iteration 90 of 100, tot loss = 4.606375102202097, l1: 0.00010185699154842748, l2: 0.00035878051801571724   Iteration 91 of 100, tot loss = 4.653576443483542, l1: 0.00010264598501265397, l2: 0.0003627116591535294   Iteration 92 of 100, tot loss = 4.661436999621599, l1: 0.00010256867094342496, l2: 0.00036357502912047176   Iteration 93 of 100, tot loss = 4.650330755018419, l1: 0.00010239151320532866, l2: 0.00036264156228900255   Iteration 94 of 100, tot loss = 4.663818908498643, l1: 0.00010249012035622381, l2: 0.0003638917708272818   Iteration 95 of 100, tot loss = 4.64013542250583, l1: 0.00010200769615230934, l2: 0.00036200584643091515   Iteration 96 of 100, tot loss = 4.643616709858179, l1: 0.000101716499595265, l2: 0.00036264517173852556   Iteration 97 of 100, tot loss = 4.626102237357307, l1: 0.0001011533915385071, l2: 0.00036145683257065274   Iteration 98 of 100, tot loss = 4.609829020743468, l1: 0.00010088672871108652, l2: 0.00036009617349695965   Iteration 99 of 100, tot loss = 4.574917150266243, l1: 0.00010019040696947327, l2: 0.0003573013081672752   Iteration 100 of 100, tot loss = 4.591159040927887, l1: 0.00010037838208518224, l2: 0.0003587375218194211
   End of epoch 1187; saving model... 

Epoch 1188 of 2000
   Iteration 1 of 100, tot loss = 5.842782020568848, l1: 0.00011253661068622023, l2: 0.0004717415722552687   Iteration 2 of 100, tot loss = 5.497154712677002, l1: 0.00010126750203198753, l2: 0.00044844795775134116   Iteration 3 of 100, tot loss = 5.053974151611328, l1: 9.971619874704629e-05, l2: 0.0004056812128207336   Iteration 4 of 100, tot loss = 5.1829822063446045, l1: 0.00010106027912115678, l2: 0.00041723794129211456   Iteration 5 of 100, tot loss = 5.454427814483642, l1: 0.00010845776414498687, l2: 0.0004369850270450115   Iteration 6 of 100, tot loss = 5.186267375946045, l1: 0.00010921702899698478, l2: 0.000409409719092461   Iteration 7 of 100, tot loss = 4.746319225856236, l1: 0.00010020784923524064, l2: 0.00037442408418948095   Iteration 8 of 100, tot loss = 4.667065143585205, l1: 0.00010079936919282773, l2: 0.0003659071553556714   Iteration 9 of 100, tot loss = 4.336927016576131, l1: 9.38079602848221e-05, l2: 0.00033988475075198547   Iteration 10 of 100, tot loss = 4.198363256454468, l1: 9.290294619859196e-05, l2: 0.0003269333872594871   Iteration 11 of 100, tot loss = 4.152470220219005, l1: 9.347925449467517e-05, l2: 0.0003217677731299773   Iteration 12 of 100, tot loss = 4.106422642866771, l1: 9.224184274595852e-05, l2: 0.00031840042720432393   Iteration 13 of 100, tot loss = 4.039605874281663, l1: 9.164453513221815e-05, l2: 0.00031231605680659413   Iteration 14 of 100, tot loss = 3.9644830397197177, l1: 8.977222231416298e-05, l2: 0.00030667608682831215   Iteration 15 of 100, tot loss = 4.030785290400187, l1: 9.196951480892798e-05, l2: 0.0003111090190941468   Iteration 16 of 100, tot loss = 4.078252270817757, l1: 9.38053244681214e-05, l2: 0.00031401990690937964   Iteration 17 of 100, tot loss = 4.135790025486665, l1: 9.388986612012719e-05, l2: 0.0003196891416069668   Iteration 18 of 100, tot loss = 4.234546701113383, l1: 9.525805904154872e-05, l2: 0.00032819661606077314   Iteration 19 of 100, tot loss = 4.287110918446591, l1: 9.695624306148507e-05, l2: 0.00033175485518050234   Iteration 20 of 100, tot loss = 4.274811375141144, l1: 9.649454295868054e-05, l2: 0.00033098660132964143   Iteration 21 of 100, tot loss = 4.298450413204375, l1: 9.663561165022354e-05, l2: 0.00033320943683585417   Iteration 22 of 100, tot loss = 4.212120110338384, l1: 9.515477590453388e-05, l2: 0.00032605724250474435   Iteration 23 of 100, tot loss = 4.267268958299057, l1: 9.723937356561099e-05, l2: 0.00032948752936056775   Iteration 24 of 100, tot loss = 4.3943312267462415, l1: 9.982140060553017e-05, l2: 0.0003396117293353503   Iteration 25 of 100, tot loss = 4.44328143119812, l1: 0.00010099715960677713, l2: 0.00034333099145442245   Iteration 26 of 100, tot loss = 4.479409960600046, l1: 0.00010153530148762421, l2: 0.000346405701398348   Iteration 27 of 100, tot loss = 4.511498548366405, l1: 0.00010146468061807186, l2: 0.00034968518108750385   Iteration 28 of 100, tot loss = 4.561392230646951, l1: 0.00010159409066545777, l2: 0.0003545451399986632   Iteration 29 of 100, tot loss = 4.584514363058682, l1: 0.00010171305445416285, l2: 0.00035673838935728215   Iteration 30 of 100, tot loss = 4.584559162457784, l1: 0.00010263725586507159, l2: 0.00035581866880723587   Iteration 31 of 100, tot loss = 4.646555446809338, l1: 0.00010412632916761081, l2: 0.0003605292248164093   Iteration 32 of 100, tot loss = 4.595278829336166, l1: 0.00010270430902892258, l2: 0.00035682358293342986   Iteration 33 of 100, tot loss = 4.666861389622544, l1: 0.00010370218307908738, l2: 0.00036298396519145393   Iteration 34 of 100, tot loss = 4.684089169782751, l1: 0.00010389255818582195, l2: 0.0003645163682556492   Iteration 35 of 100, tot loss = 4.783352320534842, l1: 0.00010578864894341677, l2: 0.00037254659317633403   Iteration 36 of 100, tot loss = 4.746582143836552, l1: 0.00010532581796319896, l2: 0.0003693324061815575   Iteration 37 of 100, tot loss = 4.745600178435042, l1: 0.0001055119261051201, l2: 0.0003690481004210793   Iteration 38 of 100, tot loss = 4.709535234852841, l1: 0.0001050058649286113, l2: 0.00036594766693030437   Iteration 39 of 100, tot loss = 4.719440325712546, l1: 0.00010526471436232663, l2: 0.000366679326711724   Iteration 40 of 100, tot loss = 4.7285048246383665, l1: 0.00010509794192330446, l2: 0.0003677525481180055   Iteration 41 of 100, tot loss = 4.699629021853935, l1: 0.00010473769361851737, l2: 0.0003652252159620885   Iteration 42 of 100, tot loss = 4.716008487201872, l1: 0.00010500903694524563, l2: 0.0003665918196756614   Iteration 43 of 100, tot loss = 4.750895417013834, l1: 0.00010606583389237003, l2: 0.0003690237154355706   Iteration 44 of 100, tot loss = 4.776577033779838, l1: 0.00010603085188036361, l2: 0.00037162685905720787   Iteration 45 of 100, tot loss = 4.77492692205641, l1: 0.00010579356538882065, l2: 0.00037169913379734177   Iteration 46 of 100, tot loss = 4.733695201251818, l1: 0.00010509649038557772, l2: 0.00036827303689357865   Iteration 47 of 100, tot loss = 4.700852982541348, l1: 0.00010466926323715597, l2: 0.00036541604253641783   Iteration 48 of 100, tot loss = 4.688745786746343, l1: 0.0001048998107459435, l2: 0.0003639747750033469   Iteration 49 of 100, tot loss = 4.674807626373914, l1: 0.00010412862602792376, l2: 0.0003633521442605677   Iteration 50 of 100, tot loss = 4.6575788450241085, l1: 0.00010391530537162908, l2: 0.00036184258700814096   Iteration 51 of 100, tot loss = 4.644574595432656, l1: 0.0001037562097361147, l2: 0.00036070125768709855   Iteration 52 of 100, tot loss = 4.62092290016321, l1: 0.00010377148926356592, l2: 0.00035832080846795667   Iteration 53 of 100, tot loss = 4.620523448260325, l1: 0.00010395583156869693, l2: 0.00035809652049551314   Iteration 54 of 100, tot loss = 4.622314890225728, l1: 0.0001041302193361507, l2: 0.00035810127640919137   Iteration 55 of 100, tot loss = 4.62646540294994, l1: 0.00010459948468699374, l2: 0.0003580470622347837   Iteration 56 of 100, tot loss = 4.627696501357215, l1: 0.00010458335626546094, l2: 0.0003581863009977886   Iteration 57 of 100, tot loss = 4.614326230266638, l1: 0.00010463410491642558, l2: 0.00035679852538905515   Iteration 58 of 100, tot loss = 4.610865301099317, l1: 0.0001044151522991938, l2: 0.00035667138531450825   Iteration 59 of 100, tot loss = 4.590159484895609, l1: 0.00010399592083866976, l2: 0.0003550200352161112   Iteration 60 of 100, tot loss = 4.559129369258881, l1: 0.00010360265623603482, l2: 0.00035231028814450836   Iteration 61 of 100, tot loss = 4.567406619181399, l1: 0.00010392192207738665, l2: 0.00035281874669799735   Iteration 62 of 100, tot loss = 4.560236996220004, l1: 0.0001037560993403707, l2: 0.00035226760707202277   Iteration 63 of 100, tot loss = 4.554409447170439, l1: 0.00010391807062126371, l2: 0.00035152288032732606   Iteration 64 of 100, tot loss = 4.548042628914118, l1: 0.00010401145993910177, l2: 0.0003507928092858492   Iteration 65 of 100, tot loss = 4.512071697528546, l1: 0.00010327904993363727, l2: 0.000347928126127674   Iteration 66 of 100, tot loss = 4.5095075838493575, l1: 0.0001031963747112913, l2: 0.0003477543902011927   Iteration 67 of 100, tot loss = 4.515249195383556, l1: 0.00010319746417817034, l2: 0.0003483274620575413   Iteration 68 of 100, tot loss = 4.504124066408942, l1: 0.0001025046814153457, l2: 0.0003479077319806332   Iteration 69 of 100, tot loss = 4.507085143655971, l1: 0.0001028326414993701, l2: 0.00034787587946334827   Iteration 70 of 100, tot loss = 4.49162472316197, l1: 0.00010255322782281188, l2: 0.00034660925115791283   Iteration 71 of 100, tot loss = 4.498918533325195, l1: 0.00010298318523728341, l2: 0.0003469086749787429   Iteration 72 of 100, tot loss = 4.490890426768197, l1: 0.00010313746189341246, l2: 0.00034595158730452467   Iteration 73 of 100, tot loss = 4.493623214225247, l1: 0.00010319004824651367, l2: 0.0003461722796463905   Iteration 74 of 100, tot loss = 4.477293001638876, l1: 0.00010289055289585785, l2: 0.00034483875355658094   Iteration 75 of 100, tot loss = 4.465864562988282, l1: 0.0001025111601241709, l2: 0.0003440753021277487   Iteration 76 of 100, tot loss = 4.445420738897826, l1: 0.00010220906728806624, l2: 0.0003423330124764777   Iteration 77 of 100, tot loss = 4.451699055634536, l1: 0.00010225153438385589, l2: 0.00034291837715948994   Iteration 78 of 100, tot loss = 4.45375148149637, l1: 0.00010244650757858135, l2: 0.0003429286464383133   Iteration 79 of 100, tot loss = 4.4579104864144625, l1: 0.00010256452860400564, l2: 0.00034322652577487397   Iteration 80 of 100, tot loss = 4.440132674574852, l1: 0.00010205631147073291, l2: 0.00034195696152892194   Iteration 81 of 100, tot loss = 4.419810383408158, l1: 0.00010183842028642107, l2: 0.0003401426236248679   Iteration 82 of 100, tot loss = 4.407531095714104, l1: 0.00010145658815440029, l2: 0.0003392965268065426   Iteration 83 of 100, tot loss = 4.411617827702718, l1: 0.00010143335897605922, l2: 0.0003397284290916006   Iteration 84 of 100, tot loss = 4.408968320914677, l1: 0.00010117951123469365, l2: 0.0003397173260011533   Iteration 85 of 100, tot loss = 4.388586479074815, l1: 0.00010061355655232225, l2: 0.00033824509643155207   Iteration 86 of 100, tot loss = 4.377388136331425, l1: 0.00010033759759227356, l2: 0.00033740122104740464   Iteration 87 of 100, tot loss = 4.396112340620195, l1: 0.00010056490023037012, l2: 0.00033904633858781736   Iteration 88 of 100, tot loss = 4.434185648506338, l1: 0.00010117005364339671, l2: 0.00034224851614536334   Iteration 89 of 100, tot loss = 4.436047567410416, l1: 0.00010083995208803439, l2: 0.00034276480976821793   Iteration 90 of 100, tot loss = 4.453442894087898, l1: 0.00010110141867901096, l2: 0.0003442428761950901   Iteration 91 of 100, tot loss = 4.458342748683887, l1: 0.00010105716261263056, l2: 0.0003447771172492079   Iteration 92 of 100, tot loss = 4.46356312347495, l1: 0.00010099563366131169, l2: 0.0003453606836758453   Iteration 93 of 100, tot loss = 4.484563327604724, l1: 0.00010129468661347424, l2: 0.0003471616510068998   Iteration 94 of 100, tot loss = 4.489661863509645, l1: 0.00010159796015904384, l2: 0.00034736823138875015   Iteration 95 of 100, tot loss = 4.518332579261378, l1: 0.00010205448090441917, l2: 0.00034977878232511053   Iteration 96 of 100, tot loss = 4.505421740313371, l1: 0.0001019759877181059, l2: 0.00034856619155713514   Iteration 97 of 100, tot loss = 4.5032193218309855, l1: 0.00010205296052142152, l2: 0.00034826897659588646   Iteration 98 of 100, tot loss = 4.519309961065954, l1: 0.00010213020928227818, l2: 0.0003498007916983179   Iteration 99 of 100, tot loss = 4.507814099090268, l1: 0.00010188994054609646, l2: 0.00034889147406434544   Iteration 100 of 100, tot loss = 4.503605842590332, l1: 0.00010169880566536449, l2: 0.00034866178350057453
   End of epoch 1188; saving model... 

Epoch 1189 of 2000
   Iteration 1 of 100, tot loss = 3.129291296005249, l1: 7.34291024855338e-05, l2: 0.0002395000192336738   Iteration 2 of 100, tot loss = 6.016349673271179, l1: 0.00011154828462167643, l2: 0.0004900866479147226   Iteration 3 of 100, tot loss = 5.563544352849324, l1: 0.00011352967703714967, l2: 0.00044282474361049634   Iteration 4 of 100, tot loss = 6.061644375324249, l1: 0.00012603973664226942, l2: 0.00048012469051172957   Iteration 5 of 100, tot loss = 5.865755605697632, l1: 0.00012070705270161852, l2: 0.00046586850075982513   Iteration 6 of 100, tot loss = 5.651415069897969, l1: 0.00011678917629372639, l2: 0.00044835232741509873   Iteration 7 of 100, tot loss = 5.505009412765503, l1: 0.00011547823357562135, l2: 0.00043502270792877037   Iteration 8 of 100, tot loss = 5.65362223982811, l1: 0.00012064525708410656, l2: 0.00044471696674008854   Iteration 9 of 100, tot loss = 5.495038323932224, l1: 0.00011910338960458628, l2: 0.00043040044450511533   Iteration 10 of 100, tot loss = 5.4624717950820925, l1: 0.00011855114134959877, l2: 0.00042769603605847807   Iteration 11 of 100, tot loss = 5.226665648547086, l1: 0.0001148231881829402, l2: 0.00040784337563143873   Iteration 12 of 100, tot loss = 5.1982218225797014, l1: 0.00011428086084682339, l2: 0.0004055413240469837   Iteration 13 of 100, tot loss = 5.139619662211492, l1: 0.00011516307299294007, l2: 0.00039879889608038444   Iteration 14 of 100, tot loss = 5.14724600315094, l1: 0.00011445250025384926, l2: 0.00040027210551280793   Iteration 15 of 100, tot loss = 5.0437096118927, l1: 0.00011298956378595904, l2: 0.0003913814037029321   Iteration 16 of 100, tot loss = 5.033876523375511, l1: 0.00011414507252993644, l2: 0.00038924258387851296   Iteration 17 of 100, tot loss = 4.958007223465863, l1: 0.00011262955633676884, l2: 0.0003831711698748062   Iteration 18 of 100, tot loss = 4.879095673561096, l1: 0.00011236161218322296, l2: 0.0003755479596697518   Iteration 19 of 100, tot loss = 4.8506638752786735, l1: 0.00011242504230099975, l2: 0.000372641349952717   Iteration 20 of 100, tot loss = 4.819780814647674, l1: 0.00011204765032744036, l2: 0.00036993043540860525   Iteration 21 of 100, tot loss = 4.890215703419277, l1: 0.00011329262148744117, l2: 0.0003757289537350603   Iteration 22 of 100, tot loss = 4.954671783880754, l1: 0.00011493588747477837, l2: 0.0003805312940543941   Iteration 23 of 100, tot loss = 4.970277713692707, l1: 0.00011576324987285973, l2: 0.0003812645236789451   Iteration 24 of 100, tot loss = 5.050246944030126, l1: 0.0001157793661453373, l2: 0.00038924532843035803   Iteration 25 of 100, tot loss = 5.0463086605072025, l1: 0.00011567858513444662, l2: 0.000388952280045487   Iteration 26 of 100, tot loss = 5.028066607622, l1: 0.0001161460576650615, l2: 0.00038666060236909497   Iteration 27 of 100, tot loss = 5.109975417455037, l1: 0.000117666699225083, l2: 0.0003933308433732708   Iteration 28 of 100, tot loss = 5.1591377173151285, l1: 0.00011852289234021944, l2: 0.0003973908787884284   Iteration 29 of 100, tot loss = 5.146031404363698, l1: 0.0001178598342511546, l2: 0.0003967433048869836   Iteration 30 of 100, tot loss = 5.142870100339254, l1: 0.00011684805892097453, l2: 0.0003974389503127895   Iteration 31 of 100, tot loss = 5.129575398660475, l1: 0.00011669605901847864, l2: 0.0003962614794164115   Iteration 32 of 100, tot loss = 5.06629329174757, l1: 0.00011538302828739688, l2: 0.00039124629938669386   Iteration 33 of 100, tot loss = 5.034687063910744, l1: 0.00011426943356954408, l2: 0.00038919927222499   Iteration 34 of 100, tot loss = 5.06135868324953, l1: 0.000113958211038741, l2: 0.0003921776562278597   Iteration 35 of 100, tot loss = 5.168015486853463, l1: 0.00011503978270671463, l2: 0.000401761764078401   Iteration 36 of 100, tot loss = 5.129512389500936, l1: 0.00011461712165328208, l2: 0.00039833411533941724   Iteration 37 of 100, tot loss = 5.109069128294249, l1: 0.00011411902370447344, l2: 0.0003967878878324267   Iteration 38 of 100, tot loss = 5.103765136317203, l1: 0.00011361930285745889, l2: 0.00039675720932121436   Iteration 39 of 100, tot loss = 5.062396911474375, l1: 0.0001126882545157479, l2: 0.00039355143515142397   Iteration 40 of 100, tot loss = 5.092130905389785, l1: 0.0001134628311774577, l2: 0.00039575025766680483   Iteration 41 of 100, tot loss = 5.187303828030098, l1: 0.00011489743231256242, l2: 0.0004038329483946876   Iteration 42 of 100, tot loss = 5.209005906468346, l1: 0.00011465916360889206, l2: 0.0004062414244842893   Iteration 43 of 100, tot loss = 5.1907076336616695, l1: 0.00011461308540039977, l2: 0.00040445767571813926   Iteration 44 of 100, tot loss = 5.177000918171623, l1: 0.00011420660418728154, l2: 0.00040349348505101676   Iteration 45 of 100, tot loss = 5.162631400426229, l1: 0.00011422986701493048, l2: 0.000402033270478973   Iteration 46 of 100, tot loss = 5.1547997878945395, l1: 0.00011400430673308185, l2: 0.00040147566893289837   Iteration 47 of 100, tot loss = 5.180055674086225, l1: 0.00011403303413201084, l2: 0.0004039725305467963   Iteration 48 of 100, tot loss = 5.165606652696927, l1: 0.00011430036602178006, l2: 0.0004022602967476511   Iteration 49 of 100, tot loss = 5.156722200160124, l1: 0.00011460244556418525, l2: 0.00040106977216129627   Iteration 50 of 100, tot loss = 5.151963620185852, l1: 0.0001142402934783604, l2: 0.0004009560655686073   Iteration 51 of 100, tot loss = 5.112703276615517, l1: 0.00011316093557123917, l2: 0.00039810938937543874   Iteration 52 of 100, tot loss = 5.108348387938279, l1: 0.00011335781563012951, l2: 0.00039747702108937886   Iteration 53 of 100, tot loss = 5.085722037081449, l1: 0.00011322175542206749, l2: 0.00039535044623266485   Iteration 54 of 100, tot loss = 5.041758718313994, l1: 0.0001120125136906238, l2: 0.0003921633564190146   Iteration 55 of 100, tot loss = 5.026548285917802, l1: 0.00011157626372137615, l2: 0.00039107856282498686   Iteration 56 of 100, tot loss = 5.097112489598138, l1: 0.00011284460203699252, l2: 0.00039686664578247085   Iteration 57 of 100, tot loss = 5.103284212580898, l1: 0.00011307758968701309, l2: 0.0003972508300712594   Iteration 58 of 100, tot loss = 5.076373823757829, l1: 0.00011260506195459788, l2: 0.0003950323185437873   Iteration 59 of 100, tot loss = 5.026201906850782, l1: 0.00011176155588808627, l2: 0.0003908586328734754   Iteration 60 of 100, tot loss = 5.006340738137563, l1: 0.0001117650532857321, l2: 0.00038886901869166954   Iteration 61 of 100, tot loss = 5.014252392972102, l1: 0.00011155916492170348, l2: 0.00038986607215206947   Iteration 62 of 100, tot loss = 5.0014510731543265, l1: 0.000110729389146605, l2: 0.000389415716082083   Iteration 63 of 100, tot loss = 4.994980717462207, l1: 0.00011066823098522108, l2: 0.00038882983836441466   Iteration 64 of 100, tot loss = 5.013208005577326, l1: 0.0001107739735175528, l2: 0.0003905468245193333   Iteration 65 of 100, tot loss = 5.008780409739567, l1: 0.00011102372904693206, l2: 0.00038985430974907315   Iteration 66 of 100, tot loss = 5.034816427664324, l1: 0.00011155419231341291, l2: 0.0003919274487557369   Iteration 67 of 100, tot loss = 5.037016580353922, l1: 0.00011171971888884332, l2: 0.0003919819373821617   Iteration 68 of 100, tot loss = 5.034252920571496, l1: 0.00011146022943113862, l2: 0.00039196506109232465   Iteration 69 of 100, tot loss = 4.996927914412125, l1: 0.00011095367459558369, l2: 0.0003887391153577904   Iteration 70 of 100, tot loss = 4.979134818485805, l1: 0.00011066128993531622, l2: 0.00038725219055777413   Iteration 71 of 100, tot loss = 4.969403750459913, l1: 0.00011080192529283215, l2: 0.00038613844806374323   Iteration 72 of 100, tot loss = 4.968115038341946, l1: 0.00011040531429292362, l2: 0.00038640618792366393   Iteration 73 of 100, tot loss = 4.974909958774096, l1: 0.00011054973289640046, l2: 0.00038694126124028116   Iteration 74 of 100, tot loss = 4.949861068983336, l1: 0.0001102619001825145, l2: 0.00038472420498225336   Iteration 75 of 100, tot loss = 4.940711345672607, l1: 0.00011028082105137097, l2: 0.000383790311558793   Iteration 76 of 100, tot loss = 4.950927201070283, l1: 0.00011034625051826496, l2: 0.00038474646786007245   Iteration 77 of 100, tot loss = 4.959851933764173, l1: 0.00011074994699220632, l2: 0.0003852352440242305   Iteration 78 of 100, tot loss = 4.959416633997208, l1: 0.00011095593170186755, l2: 0.0003849857289773914   Iteration 79 of 100, tot loss = 4.996256345435034, l1: 0.00011174086282579405, l2: 0.0003878847685571807   Iteration 80 of 100, tot loss = 4.963773375749588, l1: 0.00011086909489677055, l2: 0.00038550823937839597   Iteration 81 of 100, tot loss = 4.956593425185592, l1: 0.0001106531275536888, l2: 0.0003850062118690852   Iteration 82 of 100, tot loss = 5.005984533123854, l1: 0.000111187222412299, l2: 0.00038941122826455704   Iteration 83 of 100, tot loss = 5.021778347980545, l1: 0.0001115037797366729, l2: 0.0003906740526127326   Iteration 84 of 100, tot loss = 5.014600645928156, l1: 0.00011123686332818276, l2: 0.0003902231988571917   Iteration 85 of 100, tot loss = 5.041236428653493, l1: 0.00011181488354101448, l2: 0.0003923087570857366   Iteration 86 of 100, tot loss = 5.046972291414128, l1: 0.00011209284047106129, l2: 0.00039260438626475066   Iteration 87 of 100, tot loss = 5.025483043714502, l1: 0.00011175437791917965, l2: 0.0003907939242854736   Iteration 88 of 100, tot loss = 5.027964499863711, l1: 0.00011169707539176505, l2: 0.0003910993726259965   Iteration 89 of 100, tot loss = 5.045084465755505, l1: 0.00011205221675113995, l2: 0.0003924562278258604   Iteration 90 of 100, tot loss = 5.037655459509955, l1: 0.00011190776114947058, l2: 0.00039185778279918145   Iteration 91 of 100, tot loss = 5.001161617237133, l1: 0.00011108036938661986, l2: 0.00038903579035912036   Iteration 92 of 100, tot loss = 4.981255023375802, l1: 0.0001107146232322481, l2: 0.0003874108770105522   Iteration 93 of 100, tot loss = 4.963864031658377, l1: 0.00011060382526647049, l2: 0.0003857825757868047   Iteration 94 of 100, tot loss = 4.955714076123339, l1: 0.00011055776148884646, l2: 0.0003850136441116697   Iteration 95 of 100, tot loss = 4.958864104120355, l1: 0.00011048285220902854, l2: 0.0003854035563754702   Iteration 96 of 100, tot loss = 4.99184433867534, l1: 0.00011114743062989874, l2: 0.0003880370009028411   Iteration 97 of 100, tot loss = 4.963333842680626, l1: 0.00011048967550693502, l2: 0.00038584370643726174   Iteration 98 of 100, tot loss = 4.9524533578327725, l1: 0.0001104272709273239, l2: 0.000384818062563997   Iteration 99 of 100, tot loss = 4.964888406522347, l1: 0.00011055589306069481, l2: 0.00038593294565810474   Iteration 100 of 100, tot loss = 4.976267263889313, l1: 0.00011078497511334717, l2: 0.0003868417494231835
   End of epoch 1189; saving model... 

Epoch 1190 of 2000
   Iteration 1 of 100, tot loss = 3.509801149368286, l1: 7.245782762765884e-05, l2: 0.0002785222895909101   Iteration 2 of 100, tot loss = 4.020985007286072, l1: 7.337454735534266e-05, l2: 0.0003287239669589326   Iteration 3 of 100, tot loss = 4.351560513178508, l1: 7.070755721845974e-05, l2: 0.0003644485065403084   Iteration 4 of 100, tot loss = 4.55663400888443, l1: 7.958807327668183e-05, l2: 0.000376075338863302   Iteration 5 of 100, tot loss = 4.682078409194946, l1: 8.594612882006913e-05, l2: 0.000382261723279953   Iteration 6 of 100, tot loss = 4.546978076299031, l1: 8.870258655709524e-05, l2: 0.0003659952296099315   Iteration 7 of 100, tot loss = 4.475825105394636, l1: 8.699724483968956e-05, l2: 0.000360585271014965   Iteration 8 of 100, tot loss = 4.479787647724152, l1: 9.137216602539411e-05, l2: 0.00035660660432768054   Iteration 9 of 100, tot loss = 4.4385001924302845, l1: 9.144717801569236e-05, l2: 0.0003524028482691695   Iteration 10 of 100, tot loss = 4.388312339782715, l1: 9.223637680406682e-05, l2: 0.00034659486263990404   Iteration 11 of 100, tot loss = 4.6219540509310635, l1: 9.805196697760204e-05, l2: 0.00036414344371719795   Iteration 12 of 100, tot loss = 4.629056453704834, l1: 9.769433078569516e-05, l2: 0.0003652113203618986   Iteration 13 of 100, tot loss = 4.730019129239595, l1: 9.914928859171386e-05, l2: 0.00037385263175775227   Iteration 14 of 100, tot loss = 4.570701905659267, l1: 9.792234621792366e-05, l2: 0.00035914785257773474   Iteration 15 of 100, tot loss = 4.644942092895508, l1: 0.00010061304249878353, l2: 0.00036388117296155543   Iteration 16 of 100, tot loss = 4.633729636669159, l1: 0.00010013132850872353, l2: 0.0003632416419350193   Iteration 17 of 100, tot loss = 4.6424625621122475, l1: 0.00010102944275679286, l2: 0.00036321682016611756   Iteration 18 of 100, tot loss = 4.687759929233128, l1: 0.00010275493898209081, l2: 0.0003660210600679016   Iteration 19 of 100, tot loss = 4.712828234622353, l1: 0.00010329403065904779, l2: 0.00036798880089343965   Iteration 20 of 100, tot loss = 4.707585310935974, l1: 0.00010311160440323874, l2: 0.00036764693577424624   Iteration 21 of 100, tot loss = 4.781118892488026, l1: 0.00010513621410133229, l2: 0.0003729756814815725   Iteration 22 of 100, tot loss = 4.778296947479248, l1: 0.00010540609814184295, l2: 0.00037242360320471397   Iteration 23 of 100, tot loss = 4.790805837382441, l1: 0.00010528512021941741, l2: 0.0003737954697419849   Iteration 24 of 100, tot loss = 4.8760844469070435, l1: 0.00010680734309668576, l2: 0.0003808011078945128   Iteration 25 of 100, tot loss = 4.980214900970459, l1: 0.00010763669590232894, l2: 0.00039038480084855107   Iteration 26 of 100, tot loss = 4.973795817448543, l1: 0.00010816822004226896, l2: 0.0003892113684112421   Iteration 27 of 100, tot loss = 5.008746217798303, l1: 0.00010946537750661028, l2: 0.0003914092511102488   Iteration 28 of 100, tot loss = 5.047973445483616, l1: 0.00010997924593019499, l2: 0.00039481810641258823   Iteration 29 of 100, tot loss = 5.02036426807272, l1: 0.00010910193568320367, l2: 0.00039293449763449487   Iteration 30 of 100, tot loss = 5.01443707148234, l1: 0.00010943405043993456, l2: 0.000392009663725427   Iteration 31 of 100, tot loss = 5.004008585406888, l1: 0.00010893553929148061, l2: 0.0003914653268199594   Iteration 32 of 100, tot loss = 4.9898021668195724, l1: 0.00010915025950453128, l2: 0.000389829964206001   Iteration 33 of 100, tot loss = 5.017055366978501, l1: 0.00010870065505679867, l2: 0.0003930048890955125   Iteration 34 of 100, tot loss = 5.0443828947403855, l1: 0.00010876974984592594, l2: 0.00039566854684380814   Iteration 35 of 100, tot loss = 4.9739923068455285, l1: 0.0001073021958291065, l2: 0.00039009704196359964   Iteration 36 of 100, tot loss = 4.913071771462758, l1: 0.00010634258402812218, l2: 0.00038496460021835647   Iteration 37 of 100, tot loss = 4.905928837286459, l1: 0.00010623885532982672, l2: 0.0003843540349358542   Iteration 38 of 100, tot loss = 4.899132483883908, l1: 0.00010626371879066267, l2: 0.0003836495360653651   Iteration 39 of 100, tot loss = 4.854423571855594, l1: 0.00010596096389912046, l2: 0.0003794813995064499   Iteration 40 of 100, tot loss = 4.867706298828125, l1: 0.00010663459024726763, l2: 0.0003801360453508096   Iteration 41 of 100, tot loss = 4.929110433997177, l1: 0.00010711261063811324, l2: 0.000385798437580489   Iteration 42 of 100, tot loss = 4.887901805696034, l1: 0.00010662240977884115, l2: 0.0003821677753529955   Iteration 43 of 100, tot loss = 4.835873520651529, l1: 0.00010578872219369704, l2: 0.00037779863402626453   Iteration 44 of 100, tot loss = 4.836514120752161, l1: 0.00010534217777934497, l2: 0.0003783092387576207   Iteration 45 of 100, tot loss = 4.832943942811754, l1: 0.0001055714759180167, l2: 0.00037772292238918856   Iteration 46 of 100, tot loss = 4.851607286411783, l1: 0.00010531640992153173, l2: 0.000379844322868967   Iteration 47 of 100, tot loss = 4.843827161383121, l1: 0.00010558485480439057, l2: 0.0003787978658170261   Iteration 48 of 100, tot loss = 4.8151795367399854, l1: 0.00010552959633969294, l2: 0.0003759883620659821   Iteration 49 of 100, tot loss = 4.8192821230207175, l1: 0.00010568111202719963, l2: 0.00037624710477052295   Iteration 50 of 100, tot loss = 4.758744826316834, l1: 0.00010448097782500554, l2: 0.0003713935092673637   Iteration 51 of 100, tot loss = 4.736893148983226, l1: 0.00010378518281333313, l2: 0.00036990413626007663   Iteration 52 of 100, tot loss = 4.746057189427889, l1: 0.00010390382836843716, l2: 0.00037070189500809647   Iteration 53 of 100, tot loss = 4.733779205466217, l1: 0.00010404625531661605, l2: 0.00036933167012768607   Iteration 54 of 100, tot loss = 4.708831394160235, l1: 0.0001033836215356132, l2: 0.00036749952279798754   Iteration 55 of 100, tot loss = 4.729853391647339, l1: 0.0001043036466977686, l2: 0.0003686816973971542   Iteration 56 of 100, tot loss = 4.704193532466888, l1: 0.00010388559576313128, l2: 0.0003665337626443943   Iteration 57 of 100, tot loss = 4.677380210474918, l1: 0.00010295218447010899, l2: 0.0003647858416355264   Iteration 58 of 100, tot loss = 4.683314553622542, l1: 0.0001034557624671865, l2: 0.00036487569766528586   Iteration 59 of 100, tot loss = 4.684894820391121, l1: 0.00010357956883943465, l2: 0.00036490991809286997   Iteration 60 of 100, tot loss = 4.694231239954631, l1: 0.00010420298773775964, l2: 0.0003652201414903781   Iteration 61 of 100, tot loss = 4.681144128080274, l1: 0.00010400135668861054, l2: 0.00036411306111821444   Iteration 62 of 100, tot loss = 4.69834190799344, l1: 0.00010453640225610035, l2: 0.0003652977933187128   Iteration 63 of 100, tot loss = 4.6983089749775235, l1: 0.00010409952757606608, l2: 0.00036573137451399355   Iteration 64 of 100, tot loss = 4.725508339703083, l1: 0.00010470232018633396, l2: 0.00036784851795346185   Iteration 65 of 100, tot loss = 4.699423573567317, l1: 0.00010412526921404955, l2: 0.0003658170923769761   Iteration 66 of 100, tot loss = 4.712293209451618, l1: 0.00010436333137807775, l2: 0.0003668659932869063   Iteration 67 of 100, tot loss = 4.723124557466649, l1: 0.0001047211648147456, l2: 0.00036759129482140737   Iteration 68 of 100, tot loss = 4.721013612606946, l1: 0.0001047622873182477, l2: 0.00036733907772406167   Iteration 69 of 100, tot loss = 4.737693340882011, l1: 0.00010482557231823549, l2: 0.0003689437652465658   Iteration 70 of 100, tot loss = 4.729973571641104, l1: 0.00010470988875437927, l2: 0.0003682874721042546   Iteration 71 of 100, tot loss = 4.714340508823663, l1: 0.00010456184694268019, l2: 0.000366872207681373   Iteration 72 of 100, tot loss = 4.707340237167147, l1: 0.00010447564414385447, l2: 0.00036625838321116235   Iteration 73 of 100, tot loss = 4.732145528270774, l1: 0.00010478568792802422, l2: 0.00036842886865623805   Iteration 74 of 100, tot loss = 4.716313703640087, l1: 0.0001043884816730546, l2: 0.00036724289246784474   Iteration 75 of 100, tot loss = 4.727776495615641, l1: 0.00010436098741289849, l2: 0.0003684166663636764   Iteration 76 of 100, tot loss = 4.709666434087251, l1: 0.00010390862026772658, l2: 0.00036705802707920636   Iteration 77 of 100, tot loss = 4.687582192482886, l1: 0.00010346366852664014, l2: 0.00036529455463970445   Iteration 78 of 100, tot loss = 4.675086125349387, l1: 0.00010329722947342537, l2: 0.0003642113868129225   Iteration 79 of 100, tot loss = 4.676352760459803, l1: 0.00010330456427277855, l2: 0.0003643307158475907   Iteration 80 of 100, tot loss = 4.661273920536042, l1: 0.00010319614921172615, l2: 0.0003629312468547141   Iteration 81 of 100, tot loss = 4.628277197296237, l1: 0.00010271456662456241, l2: 0.0003601131570183408   Iteration 82 of 100, tot loss = 4.630067547646965, l1: 0.00010274809232569391, l2: 0.00036025866663804643   Iteration 83 of 100, tot loss = 4.624289973672614, l1: 0.00010248657562475405, l2: 0.0003599424259835316   Iteration 84 of 100, tot loss = 4.639908956629889, l1: 0.00010254575023455323, l2: 0.0003614451497491072   Iteration 85 of 100, tot loss = 4.6351325217415305, l1: 0.00010246499415884233, l2: 0.00036104826225816987   Iteration 86 of 100, tot loss = 4.6289900860121085, l1: 0.00010201412611465013, l2: 0.0003608848862715477   Iteration 87 of 100, tot loss = 4.605713286619077, l1: 0.00010166310860130444, l2: 0.0003589082240988768   Iteration 88 of 100, tot loss = 4.614737755872986, l1: 0.00010166596356694671, l2: 0.0003598078159980518   Iteration 89 of 100, tot loss = 4.615194991733251, l1: 0.00010189106463902012, l2: 0.0003596284384891558   Iteration 90 of 100, tot loss = 4.624316524134742, l1: 0.00010200372376453338, l2: 0.0003604279323351673   Iteration 91 of 100, tot loss = 4.661143513826223, l1: 0.00010249921959277324, l2: 0.00036361513519915495   Iteration 92 of 100, tot loss = 4.669189112341923, l1: 0.0001028079780593069, l2: 0.0003641109363778251   Iteration 93 of 100, tot loss = 4.6562689337679135, l1: 0.00010271191398891049, l2: 0.00036291498252721404   Iteration 94 of 100, tot loss = 4.644825748940732, l1: 0.00010240511774293524, l2: 0.0003620774604063085   Iteration 95 of 100, tot loss = 4.625607336194892, l1: 0.00010211551684494081, l2: 0.0003604452199253597   Iteration 96 of 100, tot loss = 4.638870811710755, l1: 0.00010225102065912021, l2: 0.00036163606394741993   Iteration 97 of 100, tot loss = 4.652685564817841, l1: 0.00010239434987786658, l2: 0.00036287420974161054   Iteration 98 of 100, tot loss = 4.701548309958711, l1: 0.0001031426525477567, l2: 0.0003670121819892784   Iteration 99 of 100, tot loss = 4.716454685336411, l1: 0.0001033127828674024, l2: 0.0003683326890541598   Iteration 100 of 100, tot loss = 4.721085335016251, l1: 0.0001034064050327288, l2: 0.0003687021316727623
   End of epoch 1190; saving model... 

Epoch 1191 of 2000
   Iteration 1 of 100, tot loss = 3.058566093444824, l1: 7.667557656532153e-05, l2: 0.00022918103786651045   Iteration 2 of 100, tot loss = 2.9397754669189453, l1: 8.183298996300437e-05, l2: 0.00021214455045992509   Iteration 3 of 100, tot loss = 3.255462884902954, l1: 8.258651845001926e-05, l2: 0.0002429597710336869   Iteration 4 of 100, tot loss = 3.360796809196472, l1: 8.353170596819837e-05, l2: 0.0002525479758332949   Iteration 5 of 100, tot loss = 3.648931121826172, l1: 8.695826109033078e-05, l2: 0.0002779348491458222   Iteration 6 of 100, tot loss = 3.684157689412435, l1: 9.084700771685068e-05, l2: 0.0002775687583683369   Iteration 7 of 100, tot loss = 3.999763216291155, l1: 9.62961868416252e-05, l2: 0.00030368013332398344   Iteration 8 of 100, tot loss = 3.925305873155594, l1: 9.6740143817442e-05, l2: 0.00029579044166894164   Iteration 9 of 100, tot loss = 4.507375213834974, l1: 0.00010753766966647365, l2: 0.0003431998435795928   Iteration 10 of 100, tot loss = 4.83693425655365, l1: 0.00011109176484751515, l2: 0.0003726016540895216   Iteration 11 of 100, tot loss = 4.587919625368985, l1: 0.0001048400309545369, l2: 0.0003539519252213226   Iteration 12 of 100, tot loss = 4.431767861048381, l1: 0.00010233482710949222, l2: 0.0003408419518867352   Iteration 13 of 100, tot loss = 4.466074650104229, l1: 0.0001019969391703713, l2: 0.0003446105189281158   Iteration 14 of 100, tot loss = 4.383980734007699, l1: 0.00010063091524768555, l2: 0.00033776715177477205   Iteration 15 of 100, tot loss = 4.4445933183034265, l1: 0.0001023894309279664, l2: 0.00034206989512313156   Iteration 16 of 100, tot loss = 4.790640369057655, l1: 0.00010647590420376218, l2: 0.0003725881279024179   Iteration 17 of 100, tot loss = 4.6997145905214195, l1: 0.00010500999569023192, l2: 0.0003649614595547866   Iteration 18 of 100, tot loss = 4.723236891958448, l1: 0.00010481242033064418, l2: 0.0003675112651156572   Iteration 19 of 100, tot loss = 4.630974405690243, l1: 0.00010367803636319495, l2: 0.00035941940012363424   Iteration 20 of 100, tot loss = 4.657266390323639, l1: 0.0001048445619744598, l2: 0.000360882071981905   Iteration 21 of 100, tot loss = 4.778550363722301, l1: 0.00010719457817945763, l2: 0.00037066045513797904   Iteration 22 of 100, tot loss = 4.804983713410118, l1: 0.00010711540165754162, l2: 0.00037338296599707314   Iteration 23 of 100, tot loss = 4.773100427959276, l1: 0.00010632952208929372, l2: 0.00037098051815637916   Iteration 24 of 100, tot loss = 4.903184503316879, l1: 0.00010780015312169174, l2: 0.00038251829573709983   Iteration 25 of 100, tot loss = 4.789553203582764, l1: 0.00010574561383691616, l2: 0.000373209704994224   Iteration 26 of 100, tot loss = 4.717041217363798, l1: 0.00010484497271625933, l2: 0.00036685914752664615   Iteration 27 of 100, tot loss = 4.677985553388242, l1: 0.00010377123746568234, l2: 0.0003640273164448984   Iteration 28 of 100, tot loss = 4.648586290223258, l1: 0.00010262776582489355, l2: 0.0003622308613557834   Iteration 29 of 100, tot loss = 4.770469155804864, l1: 0.00010451997927859714, l2: 0.00037252693386860834   Iteration 30 of 100, tot loss = 4.833732096354167, l1: 0.00010549341059231664, l2: 0.0003778797976944285   Iteration 31 of 100, tot loss = 4.859689020341443, l1: 0.0001060990960526866, l2: 0.00037986980497671833   Iteration 32 of 100, tot loss = 4.85093092918396, l1: 0.00010517226689898962, l2: 0.0003799208247983188   Iteration 33 of 100, tot loss = 4.869991027947628, l1: 0.0001056436847262155, l2: 0.0003813554168587099   Iteration 34 of 100, tot loss = 4.813376938595491, l1: 0.00010457573433300508, l2: 0.0003767619581434273   Iteration 35 of 100, tot loss = 4.815286915642875, l1: 0.00010432008722481052, l2: 0.00037720860356265414   Iteration 36 of 100, tot loss = 4.825793577565087, l1: 0.00010486127666808898, l2: 0.00037771807991602045   Iteration 37 of 100, tot loss = 4.855929123388754, l1: 0.00010560682946831819, l2: 0.0003799860828990318   Iteration 38 of 100, tot loss = 5.001876611458628, l1: 0.00010791162113539605, l2: 0.00039227603893347184   Iteration 39 of 100, tot loss = 4.961263179779053, l1: 0.0001074856434290548, l2: 0.00038864067345200916   Iteration 40 of 100, tot loss = 4.959270644187927, l1: 0.00010737854545368463, l2: 0.0003885485177306691   Iteration 41 of 100, tot loss = 4.965854098157185, l1: 0.00010805568636874895, l2: 0.00038852972204217733   Iteration 42 of 100, tot loss = 4.966174318676903, l1: 0.00010723125636840927, l2: 0.000389386174598864   Iteration 43 of 100, tot loss = 4.937600551649582, l1: 0.00010712695811255185, l2: 0.00038663309634673994   Iteration 44 of 100, tot loss = 4.959940222176638, l1: 0.00010777220309550599, l2: 0.000388221818866441   Iteration 45 of 100, tot loss = 4.908405616548326, l1: 0.00010662729279526199, l2: 0.0003842132686663212   Iteration 46 of 100, tot loss = 4.98090772525124, l1: 0.00010794389638012153, l2: 0.00039014687702191344   Iteration 47 of 100, tot loss = 5.038718086607913, l1: 0.00010869325868701166, l2: 0.0003951785499552027   Iteration 48 of 100, tot loss = 5.038691913088162, l1: 0.00010929251357083558, l2: 0.0003945766769296218   Iteration 49 of 100, tot loss = 5.071797258999883, l1: 0.0001098006033892173, l2: 0.00039737912195459085   Iteration 50 of 100, tot loss = 5.0448261785507205, l1: 0.00010984582069795579, l2: 0.00039463679626351223   Iteration 51 of 100, tot loss = 5.041911148557476, l1: 0.00011005976800030719, l2: 0.0003941313460053803   Iteration 52 of 100, tot loss = 5.0335001441148615, l1: 0.00010959901262070231, l2: 0.00039375100151608843   Iteration 53 of 100, tot loss = 5.000386017673421, l1: 0.00010908625191868336, l2: 0.00039095234950644155   Iteration 54 of 100, tot loss = 4.944155523070583, l1: 0.00010783979525041543, l2: 0.0003865757567432916   Iteration 55 of 100, tot loss = 4.934780500151894, l1: 0.00010759824687159959, l2: 0.00038587980221067977   Iteration 56 of 100, tot loss = 4.887402055518968, l1: 0.00010652993159965263, l2: 0.0003822102730087603   Iteration 57 of 100, tot loss = 4.919817964235942, l1: 0.00010680807176150235, l2: 0.00038517372357953146   Iteration 58 of 100, tot loss = 4.963294002516516, l1: 0.0001078088376304769, l2: 0.000388520561531989   Iteration 59 of 100, tot loss = 4.950930318589938, l1: 0.00010766530409701942, l2: 0.00038742772658687024   Iteration 60 of 100, tot loss = 4.954353751738866, l1: 0.0001080598745829775, l2: 0.0003873754986367809   Iteration 61 of 100, tot loss = 4.968872037090239, l1: 0.00010830271049435098, l2: 0.00038858449189603083   Iteration 62 of 100, tot loss = 4.930002087546933, l1: 0.00010768622233891003, l2: 0.00038531398510507816   Iteration 63 of 100, tot loss = 4.928292454235137, l1: 0.00010780242946422068, l2: 0.0003850268145065962   Iteration 64 of 100, tot loss = 4.950516982004046, l1: 0.0001080805080277969, l2: 0.0003869711893003114   Iteration 65 of 100, tot loss = 4.936894136208754, l1: 0.00010828675703333619, l2: 0.0003854026559005993   Iteration 66 of 100, tot loss = 4.89404612418377, l1: 0.00010767973793862797, l2: 0.00038172487367057437   Iteration 67 of 100, tot loss = 4.891186747977983, l1: 0.0001072089319528296, l2: 0.000381909742162549   Iteration 68 of 100, tot loss = 4.903145080103593, l1: 0.00010717612172704642, l2: 0.0003831383852657917   Iteration 69 of 100, tot loss = 4.9658529222875405, l1: 0.00010813645723459743, l2: 0.0003884488343855069   Iteration 70 of 100, tot loss = 4.993204299041203, l1: 0.00010858894992062622, l2: 0.000390731479274109   Iteration 71 of 100, tot loss = 4.957799520291073, l1: 0.00010818524191256048, l2: 0.00038759470945978405   Iteration 72 of 100, tot loss = 4.938026092118687, l1: 0.00010779193624128109, l2: 0.00038601067252683506   Iteration 73 of 100, tot loss = 4.942459369358951, l1: 0.00010801248278505526, l2: 0.00038623345369147776   Iteration 74 of 100, tot loss = 4.950942788575147, l1: 0.00010809791230123075, l2: 0.0003869963660002434   Iteration 75 of 100, tot loss = 4.964514695803325, l1: 0.0001083232527889777, l2: 0.0003881282159515346   Iteration 76 of 100, tot loss = 4.947580822204289, l1: 0.00010777139241805631, l2: 0.00038698668906459046   Iteration 77 of 100, tot loss = 4.9326048098601305, l1: 0.00010750244360064858, l2: 0.00038575803669768253   Iteration 78 of 100, tot loss = 4.90169296050683, l1: 0.00010678434759588876, l2: 0.00038338494778145105   Iteration 79 of 100, tot loss = 4.926354524455493, l1: 0.00010688444220975242, l2: 0.00038575100943835194   Iteration 80 of 100, tot loss = 4.904496605694294, l1: 0.00010664675633051957, l2: 0.0003838029033431667   Iteration 81 of 100, tot loss = 4.928503373522817, l1: 0.00010736847198805426, l2: 0.0003854818640342902   Iteration 82 of 100, tot loss = 4.896086586684715, l1: 0.00010658885970237529, l2: 0.0003830197978148623   Iteration 83 of 100, tot loss = 4.914706676839346, l1: 0.00010709386346623279, l2: 0.00038437680351822805   Iteration 84 of 100, tot loss = 4.920149551970618, l1: 0.00010749662734659588, l2: 0.00038451832699051705   Iteration 85 of 100, tot loss = 4.897850158635308, l1: 0.00010683869319002363, l2: 0.0003829463218982496   Iteration 86 of 100, tot loss = 4.9062777106152025, l1: 0.00010719071754063534, l2: 0.000383437052944426   Iteration 87 of 100, tot loss = 4.899624806711043, l1: 0.00010721869233978281, l2: 0.0003827437877202749   Iteration 88 of 100, tot loss = 4.903223150155761, l1: 0.00010721201492329287, l2: 0.00038311029939822305   Iteration 89 of 100, tot loss = 4.888886812027921, l1: 0.00010691184626226418, l2: 0.0003819768341569481   Iteration 90 of 100, tot loss = 4.873696535163456, l1: 0.00010668138642763046, l2: 0.000380688266563488   Iteration 91 of 100, tot loss = 4.8673325436455865, l1: 0.00010664535307864003, l2: 0.0003800879008389989   Iteration 92 of 100, tot loss = 4.882236984760865, l1: 0.00010668419034355675, l2: 0.0003815395078993828   Iteration 93 of 100, tot loss = 4.894347510030193, l1: 0.00010688808603150901, l2: 0.00038254666514569513   Iteration 94 of 100, tot loss = 4.887249673934693, l1: 0.00010670908999058388, l2: 0.0003820158774342804   Iteration 95 of 100, tot loss = 4.877889805091055, l1: 0.0001065169518896207, l2: 0.000381272028684371   Iteration 96 of 100, tot loss = 4.867360737174749, l1: 0.00010630086865148769, l2: 0.0003804352049883164   Iteration 97 of 100, tot loss = 4.846615062546484, l1: 0.00010615806792143819, l2: 0.00037850343805891415   Iteration 98 of 100, tot loss = 4.8390337484223505, l1: 0.00010603677245463976, l2: 0.000377866602265181   Iteration 99 of 100, tot loss = 4.854303431029272, l1: 0.00010610803127003072, l2: 0.00037932231189739525   Iteration 100 of 100, tot loss = 4.849403780698776, l1: 0.00010596968048048438, l2: 0.0003789706973475404
   End of epoch 1191; saving model... 

Epoch 1192 of 2000
   Iteration 1 of 100, tot loss = 2.698902130126953, l1: 7.644218567293137e-05, l2: 0.00019344802421983331   Iteration 2 of 100, tot loss = 5.229351758956909, l1: 0.00010984150867443532, l2: 0.00041309365042252466   Iteration 3 of 100, tot loss = 4.966302871704102, l1: 0.00010304259194526821, l2: 0.000393587673897855   Iteration 4 of 100, tot loss = 5.096932649612427, l1: 0.00011010110029019415, l2: 0.00039959214700502343   Iteration 5 of 100, tot loss = 4.938203144073486, l1: 0.00010840721806744113, l2: 0.00038541308313142506   Iteration 6 of 100, tot loss = 5.034547805786133, l1: 0.00010935903628706001, l2: 0.0003940957418914574   Iteration 7 of 100, tot loss = 4.811608825411115, l1: 0.00010481644858373329, l2: 0.0003763444318402825   Iteration 8 of 100, tot loss = 4.582036644220352, l1: 9.921435821524938e-05, l2: 0.0003589893058233429   Iteration 9 of 100, tot loss = 4.53019282552931, l1: 0.00010052695102381727, l2: 0.0003524923296127882   Iteration 10 of 100, tot loss = 4.399786400794983, l1: 9.906299783324357e-05, l2: 0.00034091564011760054   Iteration 11 of 100, tot loss = 4.470651604912498, l1: 0.00010116430712514557, l2: 0.0003459008505821905   Iteration 12 of 100, tot loss = 4.813882410526276, l1: 0.00010712630167593791, l2: 0.0003742619398205231   Iteration 13 of 100, tot loss = 4.984374614862295, l1: 0.00010901064104893102, l2: 0.00038942682126966806   Iteration 14 of 100, tot loss = 5.029245802334377, l1: 0.00010903150619664561, l2: 0.00039389307702159774   Iteration 15 of 100, tot loss = 4.972664753595988, l1: 0.00010811037912693185, l2: 0.0003891560986327628   Iteration 16 of 100, tot loss = 5.219593748450279, l1: 0.00011152589763696596, l2: 0.00041043348028324544   Iteration 17 of 100, tot loss = 5.1957149926353905, l1: 0.00011145926584825193, l2: 0.0004081122358055676   Iteration 18 of 100, tot loss = 5.196246345837911, l1: 0.00011131594247773237, l2: 0.0004083086953162112   Iteration 19 of 100, tot loss = 5.274597255807174, l1: 0.00011249622816409867, l2: 0.00041496350068451936   Iteration 20 of 100, tot loss = 5.253103268146515, l1: 0.00011280086746410234, l2: 0.00041250946233049033   Iteration 21 of 100, tot loss = 5.186630158197312, l1: 0.00011104899524417262, l2: 0.00040761402314750566   Iteration 22 of 100, tot loss = 5.120214646512812, l1: 0.00010992379396735818, l2: 0.00040209767228754407   Iteration 23 of 100, tot loss = 5.183951139450073, l1: 0.0001109987024392467, l2: 0.00040739641332512963   Iteration 24 of 100, tot loss = 5.156878759463628, l1: 0.00011012684368931029, l2: 0.0004055610346161605   Iteration 25 of 100, tot loss = 5.119785089492797, l1: 0.0001103614158637356, l2: 0.00040161709417589007   Iteration 26 of 100, tot loss = 5.217135566931504, l1: 0.00011242297184183441, l2: 0.00040929058741312474   Iteration 27 of 100, tot loss = 5.17564011503149, l1: 0.00011116894125330469, l2: 0.000406395072652096   Iteration 28 of 100, tot loss = 5.174771640981946, l1: 0.00011046141103828891, l2: 0.0004070157540679377   Iteration 29 of 100, tot loss = 5.082763277251145, l1: 0.00010928948766628184, l2: 0.00039898684039583494   Iteration 30 of 100, tot loss = 5.042655785878499, l1: 0.00010836381855673001, l2: 0.00039590176020283253   Iteration 31 of 100, tot loss = 5.05994487577869, l1: 0.00010934025586691624, l2: 0.00039665423138367554   Iteration 32 of 100, tot loss = 5.026238419115543, l1: 0.00010954703782317665, l2: 0.0003930768034479115   Iteration 33 of 100, tot loss = 4.948897722995643, l1: 0.00010826325634490897, l2: 0.000386626515220004   Iteration 34 of 100, tot loss = 4.880754632108352, l1: 0.00010756191894709926, l2: 0.00038051354332470935   Iteration 35 of 100, tot loss = 4.859956502914429, l1: 0.00010719250286846155, l2: 0.0003788031463045627   Iteration 36 of 100, tot loss = 4.833851986461216, l1: 0.00010745603412942728, l2: 0.00037592916406639334   Iteration 37 of 100, tot loss = 4.778457209870622, l1: 0.00010665169906428059, l2: 0.00037119402180463577   Iteration 38 of 100, tot loss = 4.8421587379355175, l1: 0.00010820968188789648, l2: 0.0003760061919915882   Iteration 39 of 100, tot loss = 4.820426133962778, l1: 0.00010760981678732862, l2: 0.0003744327968422276   Iteration 40 of 100, tot loss = 4.79194473028183, l1: 0.00010728636962085148, l2: 0.00037190810362517366   Iteration 41 of 100, tot loss = 4.7845917911064335, l1: 0.00010731501979313268, l2: 0.0003711441599213087   Iteration 42 of 100, tot loss = 4.772001073473976, l1: 0.00010774608701813988, l2: 0.0003694540213300137   Iteration 43 of 100, tot loss = 4.726773311925489, l1: 0.00010676338917069505, l2: 0.00036591394306268805   Iteration 44 of 100, tot loss = 4.720084965229034, l1: 0.00010694258716367503, l2: 0.00036506591012849555   Iteration 45 of 100, tot loss = 4.718395079506768, l1: 0.00010680866891764001, l2: 0.0003650308399099029   Iteration 46 of 100, tot loss = 4.7121094983557, l1: 0.00010681556325479228, l2: 0.0003643953867772918   Iteration 47 of 100, tot loss = 4.685305544670592, l1: 0.00010593664611657547, l2: 0.00036259390849699365   Iteration 48 of 100, tot loss = 4.697133898735046, l1: 0.00010602612921199277, l2: 0.00036368726068758406   Iteration 49 of 100, tot loss = 4.6775013135404, l1: 0.00010570492950322733, l2: 0.0003620452023044761   Iteration 50 of 100, tot loss = 4.636524081230164, l1: 0.00010471781402884517, l2: 0.0003589345945511013   Iteration 51 of 100, tot loss = 4.652835420533722, l1: 0.00010512968105567601, l2: 0.00036015386143060143   Iteration 52 of 100, tot loss = 4.689899852642646, l1: 0.00010567121779996132, l2: 0.000363318767295613   Iteration 53 of 100, tot loss = 4.66279244872759, l1: 0.0001051479556959096, l2: 0.00036113128946865645   Iteration 54 of 100, tot loss = 4.670085253538908, l1: 0.0001053954244896347, l2: 0.0003616131003514898   Iteration 55 of 100, tot loss = 4.6658201130953705, l1: 0.00010514469831832685, l2: 0.0003614373118828305   Iteration 56 of 100, tot loss = 4.640625583274024, l1: 0.00010494501404017293, l2: 0.00035911754327701474   Iteration 57 of 100, tot loss = 4.628882755313003, l1: 0.00010492698422974187, l2: 0.0003579612900482556   Iteration 58 of 100, tot loss = 4.602406242798114, l1: 0.00010451802424133097, l2: 0.0003557225989362482   Iteration 59 of 100, tot loss = 4.573623968383013, l1: 0.00010401338603499472, l2: 0.0003533490096866074   Iteration 60 of 100, tot loss = 4.539375694592794, l1: 0.00010319315018326354, l2: 0.0003507444183924235   Iteration 61 of 100, tot loss = 4.570177445646192, l1: 0.00010371709826448169, l2: 0.0003533006448214721   Iteration 62 of 100, tot loss = 4.576949780987155, l1: 0.00010395800094439408, l2: 0.00035373697615033315   Iteration 63 of 100, tot loss = 4.542696192151024, l1: 0.00010302170186251815, l2: 0.0003512479163454993   Iteration 64 of 100, tot loss = 4.54916625097394, l1: 0.0001032558408269324, l2: 0.0003516607830533758   Iteration 65 of 100, tot loss = 4.5422923344832205, l1: 0.00010316808016236442, l2: 0.0003510611521330877   Iteration 66 of 100, tot loss = 4.540317950826703, l1: 0.00010287643498017904, l2: 0.0003511553588781901   Iteration 67 of 100, tot loss = 4.529604965181493, l1: 0.0001030763697640192, l2: 0.0003498841256056148   Iteration 68 of 100, tot loss = 4.549253537374384, l1: 0.00010343590753002311, l2: 0.00035148944495149944   Iteration 69 of 100, tot loss = 4.529376893803693, l1: 0.00010307450766972117, l2: 0.00034986318032547456   Iteration 70 of 100, tot loss = 4.496715038163321, l1: 0.00010224092548014597, l2: 0.0003474305770526241   Iteration 71 of 100, tot loss = 4.497149249197731, l1: 0.0001022121115812284, l2: 0.00034750281164134053   Iteration 72 of 100, tot loss = 4.492196748654048, l1: 0.00010175569473681712, l2: 0.00034746397836392536   Iteration 73 of 100, tot loss = 4.51386804776649, l1: 0.0001020089464823473, l2: 0.0003493778564654648   Iteration 74 of 100, tot loss = 4.538901809099558, l1: 0.00010262727340198801, l2: 0.0003512629053737918   Iteration 75 of 100, tot loss = 4.531036275227865, l1: 0.00010250341971792901, l2: 0.00035060020551706356   Iteration 76 of 100, tot loss = 4.513727743374674, l1: 0.00010245623333934131, l2: 0.00034891653903239213   Iteration 77 of 100, tot loss = 4.528627956068361, l1: 0.00010247878715556187, l2: 0.000350384006326619   Iteration 78 of 100, tot loss = 4.530762168077322, l1: 0.00010242976312815116, l2: 0.00035064645131709625   Iteration 79 of 100, tot loss = 4.546385967278782, l1: 0.00010260374154866071, l2: 0.0003520348527442216   Iteration 80 of 100, tot loss = 4.525217255949974, l1: 0.0001021934581331152, l2: 0.00035032826490351   Iteration 81 of 100, tot loss = 4.537197015903614, l1: 0.00010241317849485549, l2: 0.00035130652098881975   Iteration 82 of 100, tot loss = 4.513028578060429, l1: 0.00010199407285942537, l2: 0.0003493087825495977   Iteration 83 of 100, tot loss = 4.505772343601089, l1: 0.00010179332518846313, l2: 0.0003487839067567993   Iteration 84 of 100, tot loss = 4.520094144911993, l1: 0.00010208450207539413, l2: 0.0003499249096854501   Iteration 85 of 100, tot loss = 4.518180667652803, l1: 0.00010199794932679437, l2: 0.0003498201148913187   Iteration 86 of 100, tot loss = 4.510399275047835, l1: 0.00010209234908891863, l2: 0.00034894757609013036   Iteration 87 of 100, tot loss = 4.5223796504667435, l1: 0.00010258633796936544, l2: 0.0003496516250400408   Iteration 88 of 100, tot loss = 4.529036348516291, l1: 0.00010288915812204571, l2: 0.0003500144752747887   Iteration 89 of 100, tot loss = 4.513133908925432, l1: 0.00010227348615076613, l2: 0.00034903990349397483   Iteration 90 of 100, tot loss = 4.492903216679891, l1: 0.00010170632696018502, l2: 0.0003475839935062039   Iteration 91 of 100, tot loss = 4.506842681339809, l1: 0.00010207873068853527, l2: 0.000348605535691604   Iteration 92 of 100, tot loss = 4.504513362179631, l1: 0.00010215588067892341, l2: 0.0003482954536751925   Iteration 93 of 100, tot loss = 4.496569656556653, l1: 0.00010215135680678081, l2: 0.00034750560711648676   Iteration 94 of 100, tot loss = 4.4841144693658705, l1: 0.00010194156907636305, l2: 0.00034646987617808453   Iteration 95 of 100, tot loss = 4.491192797610634, l1: 0.0001017431709011649, l2: 0.0003473761068706058   Iteration 96 of 100, tot loss = 4.473762661218643, l1: 0.00010138660366010299, l2: 0.00034598966067278525   Iteration 97 of 100, tot loss = 4.476523581239366, l1: 0.00010150389211911218, l2: 0.0003461484643078611   Iteration 98 of 100, tot loss = 4.501339888086124, l1: 0.00010190910588604237, l2: 0.00034822488164443677   Iteration 99 of 100, tot loss = 4.485183949422354, l1: 0.00010182700634227284, l2: 0.0003466913871986867   Iteration 100 of 100, tot loss = 4.503222453594208, l1: 0.00010222622218861943, l2: 0.0003480960220622364
   End of epoch 1192; saving model... 

Epoch 1193 of 2000
   Iteration 1 of 100, tot loss = 5.823589324951172, l1: 0.00011877349606947973, l2: 0.00046358542749658227   Iteration 2 of 100, tot loss = 5.392749786376953, l1: 0.00011943793651880696, l2: 0.0004198370297672227   Iteration 3 of 100, tot loss = 5.559254010518392, l1: 0.00011512869969010353, l2: 0.00044079669169150293   Iteration 4 of 100, tot loss = 4.747177362442017, l1: 9.932229295372963e-05, l2: 0.00037539543700404465   Iteration 5 of 100, tot loss = 4.720568084716797, l1: 0.00010337480489397421, l2: 0.00036868200404569504   Iteration 6 of 100, tot loss = 5.387469291687012, l1: 0.00011289176351662415, l2: 0.0004258551634848118   Iteration 7 of 100, tot loss = 5.1391521862574985, l1: 0.00010712653082529349, l2: 0.00040678868702213677   Iteration 8 of 100, tot loss = 5.247422873973846, l1: 0.00010879563069465803, l2: 0.0004159466516284738   Iteration 9 of 100, tot loss = 5.2299023734198675, l1: 0.00011132126302173775, l2: 0.0004116689734574821   Iteration 10 of 100, tot loss = 5.3589019775390625, l1: 0.00011536604215507395, l2: 0.0004205241508316249   Iteration 11 of 100, tot loss = 5.222199353304776, l1: 0.00011553903724151579, l2: 0.0004066808932376179   Iteration 12 of 100, tot loss = 5.003499825795491, l1: 0.00011032675623331063, l2: 0.0003900232210677738   Iteration 13 of 100, tot loss = 4.7729827257303095, l1: 0.00010540288796899124, l2: 0.00037189537901073124   Iteration 14 of 100, tot loss = 4.863423092024667, l1: 0.00010583486333156802, l2: 0.0003805074395911236   Iteration 15 of 100, tot loss = 4.918584616978963, l1: 0.00010641054104780779, l2: 0.0003854479155658434   Iteration 16 of 100, tot loss = 4.836476594209671, l1: 0.00010467225865795626, l2: 0.00037897539732512087   Iteration 17 of 100, tot loss = 4.904633886673871, l1: 0.0001051843210007064, l2: 0.00038527906358735086   Iteration 18 of 100, tot loss = 4.8408950964609785, l1: 0.00010444900949045809, l2: 0.00037964049584439234   Iteration 19 of 100, tot loss = 4.79360650715075, l1: 0.00010252028005197644, l2: 0.000376840365597194   Iteration 20 of 100, tot loss = 4.817501401901245, l1: 0.00010331856392440387, l2: 0.000378431573335547   Iteration 21 of 100, tot loss = 4.8139227004278276, l1: 0.00010243652927567295, l2: 0.00037895573768764734   Iteration 22 of 100, tot loss = 4.841810616579923, l1: 0.00010386833938272585, l2: 0.0003803127209804106   Iteration 23 of 100, tot loss = 4.904046162315037, l1: 0.00010491710228329201, l2: 0.0003854875141532039   Iteration 24 of 100, tot loss = 4.817234069108963, l1: 0.00010314146826810126, l2: 0.0003785819390031975   Iteration 25 of 100, tot loss = 4.7088381290435795, l1: 0.00010181249119341374, l2: 0.00036907132249325514   Iteration 26 of 100, tot loss = 4.902606239685645, l1: 0.00010419325497843182, l2: 0.0003860673683588035   Iteration 27 of 100, tot loss = 4.894976960288154, l1: 0.00010417008053520005, l2: 0.00038532761499044245   Iteration 28 of 100, tot loss = 4.7952381031853815, l1: 0.0001027875394876381, l2: 0.0003767362702222142   Iteration 29 of 100, tot loss = 4.797690851935025, l1: 0.00010352085719832445, l2: 0.0003762482264047039   Iteration 30 of 100, tot loss = 4.7185572226842245, l1: 0.00010267853358527645, l2: 0.0003691771868034266   Iteration 31 of 100, tot loss = 4.790691137313843, l1: 0.0001032854690434291, l2: 0.00037578364220192475   Iteration 32 of 100, tot loss = 4.787833951413631, l1: 0.00010359335215071042, l2: 0.0003751900408133224   Iteration 33 of 100, tot loss = 4.729131322918517, l1: 0.00010286806862109877, l2: 0.00037004506113148773   Iteration 34 of 100, tot loss = 4.701623797416687, l1: 0.00010249109425999718, l2: 0.0003676712829688126   Iteration 35 of 100, tot loss = 4.6562608582632885, l1: 0.0001017464029635968, l2: 0.000363879680350822   Iteration 36 of 100, tot loss = 4.583802428510454, l1: 0.00010052769670841776, l2: 0.00035785254335173196   Iteration 37 of 100, tot loss = 4.529828155362928, l1: 9.944115260401993e-05, l2: 0.00035354166017920787   Iteration 38 of 100, tot loss = 4.493360004926982, l1: 9.84375194667863e-05, l2: 0.00035089847867646694   Iteration 39 of 100, tot loss = 4.456680475137173, l1: 9.790480077917424e-05, l2: 0.0003477632443462379   Iteration 40 of 100, tot loss = 4.534903961420059, l1: 9.885324889182811e-05, l2: 0.0003546371455740882   Iteration 41 of 100, tot loss = 4.51762333730372, l1: 9.86755217238125e-05, l2: 0.0003530868104085463   Iteration 42 of 100, tot loss = 4.5504390228362315, l1: 9.956745357374617e-05, l2: 0.00035547644594251845   Iteration 43 of 100, tot loss = 4.616538818492446, l1: 0.00010035729318651436, l2: 0.0003612965868574805   Iteration 44 of 100, tot loss = 4.579747237942436, l1: 0.00010002818089560606, l2: 0.0003579465413746551   Iteration 45 of 100, tot loss = 4.607002782821655, l1: 0.0001009972481470969, l2: 0.0003597030283546903   Iteration 46 of 100, tot loss = 4.544113646382871, l1: 9.961470301356673e-05, l2: 0.00035479666004666006   Iteration 47 of 100, tot loss = 4.548183126652495, l1: 9.95937043626327e-05, l2: 0.0003552246068485398   Iteration 48 of 100, tot loss = 4.508594805995624, l1: 9.907136980776461e-05, l2: 0.00035178810897681007   Iteration 49 of 100, tot loss = 4.522735824390334, l1: 9.967242755121742e-05, l2: 0.0003526011527615732   Iteration 50 of 100, tot loss = 4.540460705757141, l1: 0.0001001966742478544, l2: 0.0003538493943051435   Iteration 51 of 100, tot loss = 4.5167146290049835, l1: 9.986081703591124e-05, l2: 0.0003518106438123676   Iteration 52 of 100, tot loss = 4.490812269540934, l1: 9.944465644086729e-05, l2: 0.00034963656854905333   Iteration 53 of 100, tot loss = 4.530503790333586, l1: 0.00010039149581746831, l2: 0.000352658881072201   Iteration 54 of 100, tot loss = 4.535788717093291, l1: 0.00010092625443239403, l2: 0.00035265261518630994   Iteration 55 of 100, tot loss = 4.523709024082531, l1: 0.00010117172185511497, l2: 0.0003511991785754534   Iteration 56 of 100, tot loss = 4.5311485316072195, l1: 0.00010099429703456866, l2: 0.0003521205549727061   Iteration 57 of 100, tot loss = 4.513171677003827, l1: 0.000100118446553416, l2: 0.00035119872017387756   Iteration 58 of 100, tot loss = 4.5382769313351865, l1: 0.00010035484410764184, l2: 0.0003534728482542238   Iteration 59 of 100, tot loss = 4.5390052674180374, l1: 0.00010013752719298621, l2: 0.00035376299884943766   Iteration 60 of 100, tot loss = 4.511073426405589, l1: 9.936509498705466e-05, l2: 0.00035174224685761144   Iteration 61 of 100, tot loss = 4.512475846243686, l1: 9.919586281299774e-05, l2: 0.00035205172097165383   Iteration 62 of 100, tot loss = 4.511364871455777, l1: 9.912410666490154e-05, l2: 0.00035201238009199923   Iteration 63 of 100, tot loss = 4.469270414776272, l1: 9.856648772718414e-05, l2: 0.00034836055327480334   Iteration 64 of 100, tot loss = 4.4466076493263245, l1: 9.826013717884052e-05, l2: 0.00034640062699509144   Iteration 65 of 100, tot loss = 4.438560324448805, l1: 9.831554310109752e-05, l2: 0.00034554048884624184   Iteration 66 of 100, tot loss = 4.452744382800478, l1: 9.882824396658124e-05, l2: 0.0003464461939681011   Iteration 67 of 100, tot loss = 4.459001178172097, l1: 9.886034501111829e-05, l2: 0.000347039772418272   Iteration 68 of 100, tot loss = 4.446009376469781, l1: 9.869447025908706e-05, l2: 0.0003459064672129256   Iteration 69 of 100, tot loss = 4.444611984750499, l1: 9.885550347119705e-05, l2: 0.00034560569527430755   Iteration 70 of 100, tot loss = 4.448587151936122, l1: 9.872022128547542e-05, l2: 0.0003461384944135456   Iteration 71 of 100, tot loss = 4.444157264602016, l1: 9.865422815520605e-05, l2: 0.0003457614989265878   Iteration 72 of 100, tot loss = 4.4288659824265375, l1: 9.833499987305711e-05, l2: 0.0003445515990784366   Iteration 73 of 100, tot loss = 4.429911463227991, l1: 9.846799994566898e-05, l2: 0.0003445231471743358   Iteration 74 of 100, tot loss = 4.432075552038245, l1: 9.874260669331872e-05, l2: 0.00034446494926767727   Iteration 75 of 100, tot loss = 4.413426272074381, l1: 9.804432193050161e-05, l2: 0.00034329830610658975   Iteration 76 of 100, tot loss = 4.451481731314408, l1: 9.887994144121062e-05, l2: 0.0003462682325334754   Iteration 77 of 100, tot loss = 4.450332672565015, l1: 9.874657496459202e-05, l2: 0.00034628669310258337   Iteration 78 of 100, tot loss = 4.42341746428074, l1: 9.809961701625396e-05, l2: 0.00034424213029127807   Iteration 79 of 100, tot loss = 4.424413584455659, l1: 9.827644943113768e-05, l2: 0.00034416491026500876   Iteration 80 of 100, tot loss = 4.402257525920868, l1: 9.799886433938809e-05, l2: 0.00034222688955196643   Iteration 81 of 100, tot loss = 4.441279211162049, l1: 9.889938541792943e-05, l2: 0.00034522853704042916   Iteration 82 of 100, tot loss = 4.4211472011194, l1: 9.849344483305553e-05, l2: 0.00034362127633180424   Iteration 83 of 100, tot loss = 4.396412303648799, l1: 9.824488199272484e-05, l2: 0.0003413963494527838   Iteration 84 of 100, tot loss = 4.379639600004468, l1: 9.800771169379697e-05, l2: 0.00033995624956636067   Iteration 85 of 100, tot loss = 4.372551601073321, l1: 9.789471527362955e-05, l2: 0.0003393604459501255   Iteration 86 of 100, tot loss = 4.379216269005177, l1: 9.817591621127619e-05, l2: 0.0003397457112441771   Iteration 87 of 100, tot loss = 4.406836402827296, l1: 9.873949155049538e-05, l2: 0.0003419441497823138   Iteration 88 of 100, tot loss = 4.457606892694127, l1: 9.937681546613352e-05, l2: 0.00034638387511553117   Iteration 89 of 100, tot loss = 4.465274454502577, l1: 9.965816223464142e-05, l2: 0.00034686928439043   Iteration 90 of 100, tot loss = 4.467732506328159, l1: 9.97625995094293e-05, l2: 0.0003470106524117808   Iteration 91 of 100, tot loss = 4.461871165495652, l1: 9.977647793705698e-05, l2: 0.00034641063977467856   Iteration 92 of 100, tot loss = 4.4710295174432835, l1: 9.98177456890319e-05, l2: 0.00034728520768112503   Iteration 93 of 100, tot loss = 4.481366626677975, l1: 9.992179606812104e-05, l2: 0.0003482148682470784   Iteration 94 of 100, tot loss = 4.497240069064688, l1: 0.00010028953573132736, l2: 0.00034943447331278683   Iteration 95 of 100, tot loss = 4.527224663684243, l1: 0.00010069465639198346, l2: 0.00035202781237833396   Iteration 96 of 100, tot loss = 4.54363722850879, l1: 0.00010107293583890471, l2: 0.0003532907896139174   Iteration 97 of 100, tot loss = 4.549564044500134, l1: 0.00010091295631898846, l2: 0.0003540434509446153   Iteration 98 of 100, tot loss = 4.552703840391977, l1: 0.00010089404765624619, l2: 0.0003543763389699256   Iteration 99 of 100, tot loss = 4.605222820031522, l1: 0.00010162629706713353, l2: 0.00035889598736751145   Iteration 100 of 100, tot loss = 4.603388183116913, l1: 0.0001017086935826228, l2: 0.0003586301270115655
   End of epoch 1193; saving model... 

Epoch 1194 of 2000
   Iteration 1 of 100, tot loss = 3.024134397506714, l1: 4.7352998080896214e-05, l2: 0.0002550604403950274   Iteration 2 of 100, tot loss = 3.4240182638168335, l1: 6.992591261223424e-05, l2: 0.00027247592515777797   Iteration 3 of 100, tot loss = 4.253785212834676, l1: 8.407088049959081e-05, l2: 0.00034130764349053305   Iteration 4 of 100, tot loss = 4.317649781703949, l1: 8.659898139740108e-05, l2: 0.00034516600135248154   Iteration 5 of 100, tot loss = 4.478662157058716, l1: 9.057244533323683e-05, l2: 0.00035729377996176483   Iteration 6 of 100, tot loss = 4.442261099815369, l1: 9.280561607738491e-05, l2: 0.000351420501829125   Iteration 7 of 100, tot loss = 4.494746174131121, l1: 9.564786523696966e-05, l2: 0.00035382675871785196   Iteration 8 of 100, tot loss = 4.600523084402084, l1: 9.718384490042808e-05, l2: 0.00036286847171140835   Iteration 9 of 100, tot loss = 4.59461866484748, l1: 9.334960446317887e-05, l2: 0.0003661122690472338   Iteration 10 of 100, tot loss = 4.554457116127014, l1: 9.328853884653654e-05, l2: 0.0003621571813710034   Iteration 11 of 100, tot loss = 4.74886120449413, l1: 9.784343968600628e-05, l2: 0.0003770426847040653   Iteration 12 of 100, tot loss = 4.964775582154592, l1: 0.00010077356728288578, l2: 0.0003957039928839852   Iteration 13 of 100, tot loss = 4.928940387872549, l1: 0.00010208181657407504, l2: 0.0003908122218070695   Iteration 14 of 100, tot loss = 4.960329617772784, l1: 0.00010293252307747025, l2: 0.0003931004383567987   Iteration 15 of 100, tot loss = 4.848954931894938, l1: 9.973552708591645e-05, l2: 0.00038515996614781517   Iteration 16 of 100, tot loss = 4.94738706946373, l1: 0.00010231259375359514, l2: 0.0003924261127394857   Iteration 17 of 100, tot loss = 4.893583437975715, l1: 0.00010160010167644086, l2: 0.00038775824085699725   Iteration 18 of 100, tot loss = 4.897747463650173, l1: 0.00010267150355502963, l2: 0.0003871032434593265   Iteration 19 of 100, tot loss = 4.717222213745117, l1: 9.914869339186943e-05, l2: 0.00037257352845412433   Iteration 20 of 100, tot loss = 4.702282810211182, l1: 0.00010027205989899812, l2: 0.00036995622103859206   Iteration 21 of 100, tot loss = 4.82904711223784, l1: 0.000103186148494604, l2: 0.0003797185619034627   Iteration 22 of 100, tot loss = 4.845802502198652, l1: 0.00010325555376766715, l2: 0.0003813246958915525   Iteration 23 of 100, tot loss = 4.831814123236614, l1: 0.0001029033856352528, l2: 0.0003802780275177413   Iteration 24 of 100, tot loss = 4.724050720532735, l1: 0.00010092185448229429, l2: 0.00037148321795636247   Iteration 25 of 100, tot loss = 4.790208950042724, l1: 0.00010230565079837106, l2: 0.0003767152459477074   Iteration 26 of 100, tot loss = 4.873380019114568, l1: 0.00010346575086609497, l2: 0.00038387225252406026   Iteration 27 of 100, tot loss = 4.869624190860325, l1: 0.00010305506361031871, l2: 0.00038390735803491054   Iteration 28 of 100, tot loss = 4.927772215434483, l1: 0.00010446552875821778, l2: 0.00038831169435330333   Iteration 29 of 100, tot loss = 4.874375392650736, l1: 0.00010374672161435291, l2: 0.00038369081875285263   Iteration 30 of 100, tot loss = 4.946728038787842, l1: 0.00010490984289693492, l2: 0.00038976296224670174   Iteration 31 of 100, tot loss = 4.941050760207638, l1: 0.00010557301521335068, l2: 0.0003885320613570061   Iteration 32 of 100, tot loss = 4.924806758761406, l1: 0.00010530126280627883, l2: 0.0003871794135648088   Iteration 33 of 100, tot loss = 4.874497709852276, l1: 0.00010383147541274825, l2: 0.0003836182959000298   Iteration 34 of 100, tot loss = 4.934495399979984, l1: 0.00010413064260587013, l2: 0.00038931889888344277   Iteration 35 of 100, tot loss = 4.91784645489284, l1: 0.00010429493236837775, l2: 0.0003874897152335117   Iteration 36 of 100, tot loss = 4.984527554776934, l1: 0.00010537879734329181, l2: 0.00039307395996325393   Iteration 37 of 100, tot loss = 5.047697383004266, l1: 0.00010705784537807435, l2: 0.0003977118957098699   Iteration 38 of 100, tot loss = 4.961951098944011, l1: 0.00010550874924918294, l2: 0.0003906863633969088   Iteration 39 of 100, tot loss = 4.920317649841309, l1: 0.00010505871036095927, l2: 0.0003869730577347442   Iteration 40 of 100, tot loss = 4.898493254184723, l1: 0.00010526133282837691, l2: 0.0003845879959044396   Iteration 41 of 100, tot loss = 4.935177175010123, l1: 0.00010580901945969544, l2: 0.00038770870245260575   Iteration 42 of 100, tot loss = 4.892977884837559, l1: 0.00010504472811589949, l2: 0.0003842530642855092   Iteration 43 of 100, tot loss = 4.837049811385398, l1: 0.00010399983173471789, l2: 0.0003797051530307007   Iteration 44 of 100, tot loss = 4.886774328621951, l1: 0.00010464950264577055, l2: 0.00038402793325076345   Iteration 45 of 100, tot loss = 4.958558130264282, l1: 0.00010577925673310852, l2: 0.0003900765594961639   Iteration 46 of 100, tot loss = 4.968959595846093, l1: 0.00010587111991417655, l2: 0.0003910248429430952   Iteration 47 of 100, tot loss = 4.984686866719672, l1: 0.00010631506851728153, l2: 0.00039215362057996357   Iteration 48 of 100, tot loss = 4.982058351238568, l1: 0.000106315527091283, l2: 0.000391890310462865   Iteration 49 of 100, tot loss = 4.963882597125306, l1: 0.00010613308961405799, l2: 0.00039025517260746044   Iteration 50 of 100, tot loss = 4.912064161300659, l1: 0.00010487692998140119, l2: 0.0003863294886832591   Iteration 51 of 100, tot loss = 4.911525651520374, l1: 0.00010499070948058302, l2: 0.00038616185794999896   Iteration 52 of 100, tot loss = 4.92262615607335, l1: 0.0001052418573146077, l2: 0.0003870207610116967   Iteration 53 of 100, tot loss = 4.892721846418561, l1: 0.00010475816953606587, l2: 0.00038451401784220924   Iteration 54 of 100, tot loss = 4.89213204825366, l1: 0.0001049284123668999, l2: 0.0003842847949676474   Iteration 55 of 100, tot loss = 4.882467317581177, l1: 0.0001043241430042227, l2: 0.00038392259144033727   Iteration 56 of 100, tot loss = 4.8825361260346005, l1: 0.00010429428766656201, l2: 0.0003839593273369246   Iteration 57 of 100, tot loss = 4.882755727098699, l1: 0.00010425268160281376, l2: 0.00038402289366084067   Iteration 58 of 100, tot loss = 4.847921634542531, l1: 0.00010356330024259133, l2: 0.0003812288656451434   Iteration 59 of 100, tot loss = 4.861868987649174, l1: 0.00010387660635603687, l2: 0.00038231029488458064   Iteration 60 of 100, tot loss = 4.917601044972738, l1: 0.00010475763556314633, l2: 0.0003870024708400403   Iteration 61 of 100, tot loss = 4.87998851791757, l1: 0.00010402666282702665, l2: 0.0003839721906783258   Iteration 62 of 100, tot loss = 4.874263736509508, l1: 0.00010434842824320038, l2: 0.0003830779473167739   Iteration 63 of 100, tot loss = 4.88537831911965, l1: 0.00010468228535342311, l2: 0.0003838555485200477   Iteration 64 of 100, tot loss = 4.893537443131208, l1: 0.00010484420067768951, l2: 0.0003845095455972114   Iteration 65 of 100, tot loss = 4.865803227057824, l1: 0.00010399300646475659, l2: 0.00038258731801761315   Iteration 66 of 100, tot loss = 4.868182781970862, l1: 0.00010384764011012686, l2: 0.0003829706396517753   Iteration 67 of 100, tot loss = 4.817233859603085, l1: 0.00010277683791919931, l2: 0.0003789465495649336   Iteration 68 of 100, tot loss = 4.808297940913369, l1: 0.00010277311850907555, l2: 0.00037805667713709063   Iteration 69 of 100, tot loss = 4.808491290479466, l1: 0.0001028117446854805, l2: 0.00037803738572013873   Iteration 70 of 100, tot loss = 4.8054275563785005, l1: 0.00010274010169918516, l2: 0.0003778026551507147   Iteration 71 of 100, tot loss = 4.815771060930172, l1: 0.00010284561906534602, l2: 0.00037873148775502093   Iteration 72 of 100, tot loss = 4.854523498151037, l1: 0.00010354902062519816, l2: 0.0003819033303847795   Iteration 73 of 100, tot loss = 4.857695728132169, l1: 0.00010354210527323518, l2: 0.0003822274691080802   Iteration 74 of 100, tot loss = 4.877796957621703, l1: 0.00010419468782020959, l2: 0.0003835850098717807   Iteration 75 of 100, tot loss = 4.879241127967834, l1: 0.00010425609924520056, l2: 0.00038366801493490736   Iteration 76 of 100, tot loss = 4.874234423825615, l1: 0.00010415102090184136, l2: 0.00038327242279927686   Iteration 77 of 100, tot loss = 4.893292859003141, l1: 0.00010415951739512868, l2: 0.0003851697694770266   Iteration 78 of 100, tot loss = 4.89366726233409, l1: 0.00010433650972211185, l2: 0.00038503021754037874   Iteration 79 of 100, tot loss = 4.9285876102085355, l1: 0.000104693051658223, l2: 0.0003881657104114   Iteration 80 of 100, tot loss = 4.934805248677731, l1: 0.00010489124288142193, l2: 0.0003885892834659899   Iteration 81 of 100, tot loss = 4.930884065451445, l1: 0.00010494878361099342, l2: 0.00038813962421471965   Iteration 82 of 100, tot loss = 4.998374088508327, l1: 0.00010594486584744939, l2: 0.000393892544330234   Iteration 83 of 100, tot loss = 5.011647366615663, l1: 0.00010595409224491499, l2: 0.000395210646022762   Iteration 84 of 100, tot loss = 5.0059168721948355, l1: 0.00010591230567938293, l2: 0.00039467938283147913   Iteration 85 of 100, tot loss = 5.003100045989542, l1: 0.00010612677775926012, l2: 0.00039418322849087416   Iteration 86 of 100, tot loss = 5.0027411025623945, l1: 0.00010641090836522148, l2: 0.00039386320358337183   Iteration 87 of 100, tot loss = 4.982125363130679, l1: 0.00010607960090662043, l2: 0.000392132937164185   Iteration 88 of 100, tot loss = 4.961120636625723, l1: 0.00010547431470926809, l2: 0.0003906377509834287   Iteration 89 of 100, tot loss = 4.942255747452211, l1: 0.00010525379936522124, l2: 0.00038897177728358656   Iteration 90 of 100, tot loss = 4.934402480390337, l1: 0.0001051838272865603, l2: 0.0003882564224315704   Iteration 91 of 100, tot loss = 4.900710166155637, l1: 0.00010469356125763075, l2: 0.00038537745710058075   Iteration 92 of 100, tot loss = 4.877728778383006, l1: 0.00010449475010555825, l2: 0.0003832781294477172   Iteration 93 of 100, tot loss = 4.8703966038201445, l1: 0.00010431063386814429, l2: 0.00038272902833908715   Iteration 94 of 100, tot loss = 4.85879714184619, l1: 0.00010401626748213684, l2: 0.0003818634484443774   Iteration 95 of 100, tot loss = 4.878122834155434, l1: 0.00010429483334423582, l2: 0.0003835174515475764   Iteration 96 of 100, tot loss = 4.878304548561573, l1: 0.00010431232309808063, l2: 0.00038351813357924885   Iteration 97 of 100, tot loss = 4.862978775476672, l1: 0.00010391185187660253, l2: 0.000382386027360047   Iteration 98 of 100, tot loss = 4.873033822799216, l1: 0.00010411946789886593, l2: 0.00038318391604234977   Iteration 99 of 100, tot loss = 4.865213752997042, l1: 0.00010420719987414119, l2: 0.000382314177142983   Iteration 100 of 100, tot loss = 4.852680187225342, l1: 0.00010390875657321885, l2: 0.00038135926384711636
   End of epoch 1194; saving model... 

Epoch 1195 of 2000
   Iteration 1 of 100, tot loss = 7.253379821777344, l1: 0.00013750427751801908, l2: 0.0005878337542526424   Iteration 2 of 100, tot loss = 5.648592948913574, l1: 0.00011177692067576572, l2: 0.00045308240805752575   Iteration 3 of 100, tot loss = 5.589078903198242, l1: 0.00011315060934672753, l2: 0.00044575730377497774   Iteration 4 of 100, tot loss = 4.795704901218414, l1: 9.583249811839778e-05, l2: 0.0003837380099867005   Iteration 5 of 100, tot loss = 5.65772500038147, l1: 0.00011456686625024303, l2: 0.0004512056388193741   Iteration 6 of 100, tot loss = 5.637075225512187, l1: 0.00011490976127485435, l2: 0.00044879775911491987   Iteration 7 of 100, tot loss = 5.37155784879412, l1: 0.00010851462242758966, l2: 0.0004286411629956482   Iteration 8 of 100, tot loss = 5.035132199525833, l1: 0.00010073898329210351, l2: 0.00040277423431689385   Iteration 9 of 100, tot loss = 5.043846633699205, l1: 0.00010497011882964418, l2: 0.00039941454532607977   Iteration 10 of 100, tot loss = 5.216207242012024, l1: 0.00010740565448941197, l2: 0.00041421506757615133   Iteration 11 of 100, tot loss = 5.055813247507269, l1: 0.00010564747373626398, l2: 0.00039993384879463434   Iteration 12 of 100, tot loss = 4.98324312766393, l1: 0.00010518184687195269, l2: 0.0003931424616894219   Iteration 13 of 100, tot loss = 5.001236567130456, l1: 0.00010615195983868594, l2: 0.0003939716944632192   Iteration 14 of 100, tot loss = 4.977476852280753, l1: 0.0001074455677943271, l2: 0.00039030211441318637   Iteration 15 of 100, tot loss = 5.12549532254537, l1: 0.00011028703859968421, l2: 0.0004022624916008984   Iteration 16 of 100, tot loss = 5.002451673150063, l1: 0.00010946488214358396, l2: 0.0003907802838511998   Iteration 17 of 100, tot loss = 5.054685831069946, l1: 0.00011176667206146864, l2: 0.00039370191071237275   Iteration 18 of 100, tot loss = 4.919855899280972, l1: 0.00010960507259167368, l2: 0.000382380517193168   Iteration 19 of 100, tot loss = 4.8288414227335075, l1: 0.00010761768671248941, l2: 0.0003752664557204729   Iteration 20 of 100, tot loss = 4.971743500232696, l1: 0.00010899537683144445, l2: 0.0003881789707520511   Iteration 21 of 100, tot loss = 4.946385258720035, l1: 0.00010819697196157427, l2: 0.00038644155262902914   Iteration 22 of 100, tot loss = 4.92920323935422, l1: 0.00010859316783105235, l2: 0.00038432715452720663   Iteration 23 of 100, tot loss = 4.887363340543664, l1: 0.00010817077782121487, l2: 0.0003805655546982408   Iteration 24 of 100, tot loss = 4.787515193223953, l1: 0.00010575038731985842, l2: 0.0003730011309623175   Iteration 25 of 100, tot loss = 4.804157934188843, l1: 0.00010648703537299297, l2: 0.00037392875703517347   Iteration 26 of 100, tot loss = 4.799096024953402, l1: 0.00010589945122774225, l2: 0.0003740101512028979   Iteration 27 of 100, tot loss = 4.863791333304511, l1: 0.00010656932176465893, l2: 0.0003798098127676726   Iteration 28 of 100, tot loss = 4.818765648773739, l1: 0.00010562045840093301, l2: 0.0003762561084710926   Iteration 29 of 100, tot loss = 4.821884788315872, l1: 0.00010494075819412407, l2: 0.00037724772269500355   Iteration 30 of 100, tot loss = 4.791466490427653, l1: 0.00010427249132286913, l2: 0.00037487415975192564   Iteration 31 of 100, tot loss = 4.779330915020358, l1: 0.00010446115852720405, l2: 0.0003734719343370788   Iteration 32 of 100, tot loss = 4.767679199576378, l1: 0.00010473463282778539, l2: 0.0003720332883858646   Iteration 33 of 100, tot loss = 4.864326867190274, l1: 0.00010580396481214628, l2: 0.0003806287233160357   Iteration 34 of 100, tot loss = 4.877523422241211, l1: 0.00010558629465402406, l2: 0.00038216604811810507   Iteration 35 of 100, tot loss = 4.840598978315081, l1: 0.00010439086524586726, l2: 0.0003796690330740863   Iteration 36 of 100, tot loss = 4.851486709382799, l1: 0.00010470438903414308, l2: 0.0003804442826044073   Iteration 37 of 100, tot loss = 4.860637071970347, l1: 0.00010556820809535959, l2: 0.0003804954990423662   Iteration 38 of 100, tot loss = 4.84668146936517, l1: 0.0001054021144419063, l2: 0.0003792660323884583   Iteration 39 of 100, tot loss = 4.827898245591384, l1: 0.00010516291392074588, l2: 0.0003776269108922674   Iteration 40 of 100, tot loss = 4.838130724430084, l1: 0.00010465353288964252, l2: 0.00037915954053460157   Iteration 41 of 100, tot loss = 4.846481358132711, l1: 0.00010484291949357679, l2: 0.00037980521709669564   Iteration 42 of 100, tot loss = 4.832763013385591, l1: 0.00010478538795377106, l2: 0.0003784909138284136   Iteration 43 of 100, tot loss = 4.811913318412248, l1: 0.00010410593211756435, l2: 0.0003770853999899267   Iteration 44 of 100, tot loss = 4.790632193738764, l1: 0.00010378190075822535, l2: 0.00037528131856974636   Iteration 45 of 100, tot loss = 4.780822001563179, l1: 0.00010299956880367568, l2: 0.00037508263065117516   Iteration 46 of 100, tot loss = 4.810358669446862, l1: 0.00010340140634861714, l2: 0.0003776344601646997   Iteration 47 of 100, tot loss = 4.813048738114377, l1: 0.00010390189453709297, l2: 0.00037740297866935664   Iteration 48 of 100, tot loss = 4.799133529265721, l1: 0.00010375497739308533, l2: 0.000376158375426409   Iteration 49 of 100, tot loss = 4.794043696656519, l1: 0.00010390158029028443, l2: 0.00037550279004641864   Iteration 50 of 100, tot loss = 4.755208415985107, l1: 0.000103206988205784, l2: 0.0003723138538771309   Iteration 51 of 100, tot loss = 4.8096090017580515, l1: 0.0001035901774479962, l2: 0.000377370722822425   Iteration 52 of 100, tot loss = 4.817548559262202, l1: 0.00010387025556086043, l2: 0.0003778846000316732   Iteration 53 of 100, tot loss = 4.818142989896378, l1: 0.00010338786942499935, l2: 0.00037842642934333954   Iteration 54 of 100, tot loss = 4.7981639791418, l1: 0.000103574915893087, l2: 0.000376241481852183   Iteration 55 of 100, tot loss = 4.781798041950573, l1: 0.00010325088177606548, l2: 0.00037492892244534397   Iteration 56 of 100, tot loss = 4.806089307580676, l1: 0.00010352765437866245, l2: 0.00037708127664310657   Iteration 57 of 100, tot loss = 4.822778283504018, l1: 0.00010409750042558778, l2: 0.00037818032867283346   Iteration 58 of 100, tot loss = 4.794935078456484, l1: 0.00010371792092788885, l2: 0.0003757755879758343   Iteration 59 of 100, tot loss = 4.769538673303895, l1: 0.00010315902866334339, l2: 0.0003737948400311772   Iteration 60 of 100, tot loss = 4.773946885267893, l1: 0.0001034530787364929, l2: 0.00037394161117845217   Iteration 61 of 100, tot loss = 4.761462121713357, l1: 0.00010356016903896206, l2: 0.0003725860444886121   Iteration 62 of 100, tot loss = 4.7658124931396975, l1: 0.00010393720713768891, l2: 0.0003726440427858653   Iteration 63 of 100, tot loss = 4.794416794701228, l1: 0.00010415170283291474, l2: 0.00037528997742652243   Iteration 64 of 100, tot loss = 4.856894079595804, l1: 0.00010494553538364926, l2: 0.00038074387271080923   Iteration 65 of 100, tot loss = 4.8265759468078615, l1: 0.00010455397428953662, l2: 0.000378103620515993   Iteration 66 of 100, tot loss = 4.812253041700884, l1: 0.00010429706212149044, l2: 0.00037692824182820254   Iteration 67 of 100, tot loss = 4.846933379102109, l1: 0.00010495127767103779, l2: 0.00037974206020068653   Iteration 68 of 100, tot loss = 4.848309243426604, l1: 0.00010514097656970775, l2: 0.000379689948150532   Iteration 69 of 100, tot loss = 4.8451092277748, l1: 0.00010532127408373196, l2: 0.0003791896489691799   Iteration 70 of 100, tot loss = 4.876897859573364, l1: 0.00010608821056458899, l2: 0.00038160157606138714   Iteration 71 of 100, tot loss = 4.848192466816432, l1: 0.00010551350515271851, l2: 0.000379305742197329   Iteration 72 of 100, tot loss = 4.844050013356739, l1: 0.00010561983026491362, l2: 0.00037878517165760667   Iteration 73 of 100, tot loss = 4.852791684947602, l1: 0.00010545468965524566, l2: 0.00037982447885378737   Iteration 74 of 100, tot loss = 4.836853671718288, l1: 0.00010513510234296838, l2: 0.00037855026489155163   Iteration 75 of 100, tot loss = 4.81759996732076, l1: 0.00010495902674544292, l2: 0.0003768009702131773   Iteration 76 of 100, tot loss = 4.792843442214163, l1: 0.00010463324068903028, l2: 0.0003746511038594977   Iteration 77 of 100, tot loss = 4.78360400261817, l1: 0.00010432007656600507, l2: 0.00037404032372688126   Iteration 78 of 100, tot loss = 4.831646980383457, l1: 0.00010521945095649407, l2: 0.000377945246724173   Iteration 79 of 100, tot loss = 4.819561925115464, l1: 0.00010498520912480234, l2: 0.0003769709831811016   Iteration 80 of 100, tot loss = 4.78656575679779, l1: 0.0001043698742705601, l2: 0.00037428670111694373   Iteration 81 of 100, tot loss = 4.794340269065198, l1: 0.00010463358081818306, l2: 0.00037480044619893127   Iteration 82 of 100, tot loss = 4.786666038559704, l1: 0.00010471084298354449, l2: 0.0003739557606622395   Iteration 83 of 100, tot loss = 4.776600518858577, l1: 0.00010427259516597905, l2: 0.0003733874567930239   Iteration 84 of 100, tot loss = 4.809652728693826, l1: 0.00010476201866064609, l2: 0.00037620325400937525   Iteration 85 of 100, tot loss = 4.838684393377865, l1: 0.00010511569643687621, l2: 0.0003787527429213857   Iteration 86 of 100, tot loss = 4.835393925045812, l1: 0.0001049949968187003, l2: 0.0003785443956541374   Iteration 87 of 100, tot loss = 4.845343970704353, l1: 0.00010529020880636673, l2: 0.0003792441887620451   Iteration 88 of 100, tot loss = 4.87279059399258, l1: 0.00010554326246909278, l2: 0.0003817357970877889   Iteration 89 of 100, tot loss = 4.8844607037104915, l1: 0.00010588891447614617, l2: 0.00038255715619751743   Iteration 90 of 100, tot loss = 4.88304308520423, l1: 0.00010564573325003343, l2: 0.00038265857535103956   Iteration 91 of 100, tot loss = 4.848718564588945, l1: 0.00010490723998173253, l2: 0.0003799646165334507   Iteration 92 of 100, tot loss = 4.821474834628727, l1: 0.00010434731694144632, l2: 0.00037780016661955693   Iteration 93 of 100, tot loss = 4.815609611490721, l1: 0.00010414290587231266, l2: 0.0003774180555821306   Iteration 94 of 100, tot loss = 4.8324993341527085, l1: 0.00010432534687192566, l2: 0.0003789245869909869   Iteration 95 of 100, tot loss = 4.821325758883828, l1: 0.0001041220756489661, l2: 0.00037801050072486855   Iteration 96 of 100, tot loss = 4.822032456596692, l1: 0.00010439188398928916, l2: 0.00037781136203799787   Iteration 97 of 100, tot loss = 4.844960940252874, l1: 0.00010480918706551952, l2: 0.00037968690781950106   Iteration 98 of 100, tot loss = 4.827648301513827, l1: 0.00010451110890188625, l2: 0.00037825372211435545   Iteration 99 of 100, tot loss = 4.816216016056562, l1: 0.00010446210889671364, l2: 0.0003771594936625486   Iteration 100 of 100, tot loss = 4.835535950660706, l1: 0.00010480723121872871, l2: 0.0003787463650223799
   End of epoch 1195; saving model... 

Epoch 1196 of 2000
   Iteration 1 of 100, tot loss = 3.2445621490478516, l1: 9.449887875234708e-05, l2: 0.00022995735344011337   Iteration 2 of 100, tot loss = 5.302505731582642, l1: 0.00011277546218479984, l2: 0.0004174751302343793   Iteration 3 of 100, tot loss = 5.815209070841472, l1: 0.00012198252807138488, l2: 0.0004595383895017828   Iteration 4 of 100, tot loss = 5.509916663169861, l1: 0.00011389383507776074, l2: 0.00043709784586098976   Iteration 5 of 100, tot loss = 5.398629188537598, l1: 0.00011503138230182231, l2: 0.00042483154975343497   Iteration 6 of 100, tot loss = 5.385900338490804, l1: 0.0001178254751721397, l2: 0.0004207645615679212   Iteration 7 of 100, tot loss = 5.041842188153948, l1: 0.00011462055824397664, l2: 0.00038956366396243017   Iteration 8 of 100, tot loss = 5.043928265571594, l1: 0.00011552777141332626, l2: 0.0003888650571752805   Iteration 9 of 100, tot loss = 5.194428655836317, l1: 0.00011663528696064734, l2: 0.00040280757821165025   Iteration 10 of 100, tot loss = 5.418262577056884, l1: 0.00011886313441209496, l2: 0.0004229631245834753   Iteration 11 of 100, tot loss = 5.373831922357732, l1: 0.00011993867519777268, l2: 0.0004174445189577951   Iteration 12 of 100, tot loss = 5.280509869257609, l1: 0.0001174413428088883, l2: 0.00041060964576900005   Iteration 13 of 100, tot loss = 5.431291103363037, l1: 0.0001209596873825201, l2: 0.0004221694230531844   Iteration 14 of 100, tot loss = 5.40020169530596, l1: 0.00012038463631012877, l2: 0.0004196355335547456   Iteration 15 of 100, tot loss = 5.364033285776774, l1: 0.00011840805285222208, l2: 0.0004179952763176213   Iteration 16 of 100, tot loss = 5.270634412765503, l1: 0.00011811993590526981, l2: 0.00040894350604503416   Iteration 17 of 100, tot loss = 5.277588760151582, l1: 0.00011773140560013845, l2: 0.00041002747157643386   Iteration 18 of 100, tot loss = 5.204417864481608, l1: 0.0001172426014414264, l2: 0.0004031991864192403   Iteration 19 of 100, tot loss = 5.208302071219997, l1: 0.00011625738146616833, l2: 0.00040457282662636747   Iteration 20 of 100, tot loss = 5.091905808448791, l1: 0.00011500536020321306, l2: 0.0003941852213756647   Iteration 21 of 100, tot loss = 5.151274408612933, l1: 0.00011610933073652199, l2: 0.0003990181132740829   Iteration 22 of 100, tot loss = 5.122176625511863, l1: 0.0001161391866679134, l2: 0.00039607847958739677   Iteration 23 of 100, tot loss = 5.043286935142849, l1: 0.00011554098518504559, l2: 0.0003887877113732469   Iteration 24 of 100, tot loss = 4.959408750136693, l1: 0.00011395972038978168, l2: 0.00038198115726117976   Iteration 25 of 100, tot loss = 5.030935869216919, l1: 0.00011450129939476028, l2: 0.0003885922895278782   Iteration 26 of 100, tot loss = 5.108380822034983, l1: 0.00011445043716006554, l2: 0.00039638764820455643   Iteration 27 of 100, tot loss = 5.089757468965319, l1: 0.00011339383544745269, l2: 0.000395581915250255   Iteration 28 of 100, tot loss = 5.0593012656484335, l1: 0.00011358695987187925, l2: 0.0003923431702008072   Iteration 29 of 100, tot loss = 5.210536619712567, l1: 0.00011576169771583879, l2: 0.00040529196748734805   Iteration 30 of 100, tot loss = 5.232211915651957, l1: 0.00011611925559312416, l2: 0.0004071019395875434   Iteration 31 of 100, tot loss = 5.22046289905425, l1: 0.00011634035294146968, l2: 0.0004057059394976785   Iteration 32 of 100, tot loss = 5.2101200595498085, l1: 0.00011546823770913761, l2: 0.00040554376937507186   Iteration 33 of 100, tot loss = 5.194063786304358, l1: 0.0001149193236470279, l2: 0.0004044870570810004   Iteration 34 of 100, tot loss = 5.161236503544976, l1: 0.00011422414493555312, l2: 0.00040189950702958465   Iteration 35 of 100, tot loss = 5.178396081924438, l1: 0.00011494191795853631, l2: 0.0004028976911545864   Iteration 36 of 100, tot loss = 5.151182684633467, l1: 0.00011402920507761236, l2: 0.00040108906412367814   Iteration 37 of 100, tot loss = 5.101267543998924, l1: 0.00011307141382555553, l2: 0.000397055341415359   Iteration 38 of 100, tot loss = 5.07679197662755, l1: 0.00011220502404428676, l2: 0.00039547417499737715   Iteration 39 of 100, tot loss = 5.164245691054907, l1: 0.0001136330006724725, l2: 0.0004027915696123949   Iteration 40 of 100, tot loss = 5.113287061452866, l1: 0.00011238794686505572, l2: 0.0003989407603512518   Iteration 41 of 100, tot loss = 5.08616822521861, l1: 0.00011195059086298343, l2: 0.00039666623200812354   Iteration 42 of 100, tot loss = 5.085180992171878, l1: 0.0001120649590337139, l2: 0.00039645314072480513   Iteration 43 of 100, tot loss = 5.045669971510422, l1: 0.00011148940613439177, l2: 0.0003930775917048544   Iteration 44 of 100, tot loss = 5.077270632440394, l1: 0.0001124247309317897, l2: 0.00039530233268372035   Iteration 45 of 100, tot loss = 5.080950032340156, l1: 0.00011254615658092209, l2: 0.00039554884683133825   Iteration 46 of 100, tot loss = 5.041807718898939, l1: 0.00011174699861642338, l2: 0.00039243377345290196   Iteration 47 of 100, tot loss = 5.026356326772811, l1: 0.00011152646905603878, l2: 0.00039110916352117474   Iteration 48 of 100, tot loss = 4.997881884376208, l1: 0.00011081229104092927, l2: 0.0003889758972945856   Iteration 49 of 100, tot loss = 4.990434203829084, l1: 0.00011098467610058926, l2: 0.00038805874468454595   Iteration 50 of 100, tot loss = 4.97082492351532, l1: 0.00011027935965103097, l2: 0.0003868031332967803   Iteration 51 of 100, tot loss = 4.97138253847758, l1: 0.00010991869731144249, l2: 0.0003872195574884102   Iteration 52 of 100, tot loss = 4.966539479218996, l1: 0.0001100032477552304, l2: 0.0003866507009101602   Iteration 53 of 100, tot loss = 4.925269981600204, l1: 0.00010921707363658638, l2: 0.00038330992497064454   Iteration 54 of 100, tot loss = 4.952986337520458, l1: 0.00010992056195391342, l2: 0.00038537807294805707   Iteration 55 of 100, tot loss = 4.928935610164295, l1: 0.0001096715388650244, l2: 0.0003832220232156529   Iteration 56 of 100, tot loss = 4.924615898302624, l1: 0.00010952430836498803, l2: 0.0003829372820161682   Iteration 57 of 100, tot loss = 4.897130556273878, l1: 0.0001091636540398835, l2: 0.00038054940217223604   Iteration 58 of 100, tot loss = 4.882085602858971, l1: 0.00010902215280080342, l2: 0.0003791864081245751   Iteration 59 of 100, tot loss = 4.887816792827541, l1: 0.00010892050624410701, l2: 0.00037986117413156995   Iteration 60 of 100, tot loss = 4.862316119670868, l1: 0.00010833130218088627, l2: 0.0003779003110442621   Iteration 61 of 100, tot loss = 4.845939499432923, l1: 0.00010822829086673217, l2: 0.0003763656604264054   Iteration 62 of 100, tot loss = 4.84019805923585, l1: 0.00010827961042166627, l2: 0.00037574019675488555   Iteration 63 of 100, tot loss = 4.8255431462848, l1: 0.00010803127952385694, l2: 0.0003745230362891027   Iteration 64 of 100, tot loss = 4.813074804842472, l1: 0.00010794901356803166, l2: 0.0003733584676410828   Iteration 65 of 100, tot loss = 4.794824989025409, l1: 0.0001075263479903627, l2: 0.00037195615139073475   Iteration 66 of 100, tot loss = 4.808282902746489, l1: 0.00010758812089933957, l2: 0.0003732401695963221   Iteration 67 of 100, tot loss = 4.778443880935214, l1: 0.00010720033019628209, l2: 0.00037064405836672313   Iteration 68 of 100, tot loss = 4.820303583846373, l1: 0.00010753735649118694, l2: 0.00037449300276314127   Iteration 69 of 100, tot loss = 4.800903555275737, l1: 0.00010752199106745125, l2: 0.00037256836557112956   Iteration 70 of 100, tot loss = 4.817651265008109, l1: 0.0001074646508121597, l2: 0.0003743004768953792   Iteration 71 of 100, tot loss = 4.818726183662952, l1: 0.00010753970230031023, l2: 0.00037433291682120886   Iteration 72 of 100, tot loss = 4.8214373389879865, l1: 0.0001076131199321632, l2: 0.0003745306146609235   Iteration 73 of 100, tot loss = 4.806823096863211, l1: 0.00010732558738497686, l2: 0.00037335672285025045   Iteration 74 of 100, tot loss = 4.827236233530818, l1: 0.00010764504514628316, l2: 0.0003750785788823221   Iteration 75 of 100, tot loss = 4.840360488891601, l1: 0.00010813454437690477, l2: 0.00037590150527345637   Iteration 76 of 100, tot loss = 4.807427669826307, l1: 0.0001075453730759603, l2: 0.00037319739452581607   Iteration 77 of 100, tot loss = 4.803108568315382, l1: 0.00010729238911389748, l2: 0.0003730184683960246   Iteration 78 of 100, tot loss = 4.856525561748406, l1: 0.00010807076456004348, l2: 0.00037758179193284985   Iteration 79 of 100, tot loss = 4.843313352971137, l1: 0.00010738174175538558, l2: 0.00037694959389193196   Iteration 80 of 100, tot loss = 4.813725945353508, l1: 0.00010670551082512248, l2: 0.0003746670839973376   Iteration 81 of 100, tot loss = 4.815573848324058, l1: 0.00010677576596837169, l2: 0.0003747816190747314   Iteration 82 of 100, tot loss = 4.85156167135006, l1: 0.00010722705461908268, l2: 0.00037792911302303987   Iteration 83 of 100, tot loss = 4.830436028629901, l1: 0.00010694580123526976, l2: 0.00037609780202367267   Iteration 84 of 100, tot loss = 4.813730759280069, l1: 0.00010685462846248854, l2: 0.00037451844781615016   Iteration 85 of 100, tot loss = 4.796496051900527, l1: 0.0001064520441372331, l2: 0.0003731975616092848   Iteration 86 of 100, tot loss = 4.776330088460168, l1: 0.00010594160605335695, l2: 0.0003716914035003567   Iteration 87 of 100, tot loss = 4.769710266727141, l1: 0.00010602090165859367, l2: 0.0003709501254669359   Iteration 88 of 100, tot loss = 4.765067886222493, l1: 0.00010611722336761886, l2: 0.000370389565763286   Iteration 89 of 100, tot loss = 4.774176988708839, l1: 0.00010638349446575315, l2: 0.0003710342052829546   Iteration 90 of 100, tot loss = 4.80850043296814, l1: 0.00010662352085799082, l2: 0.0003742265235309282   Iteration 91 of 100, tot loss = 4.800041612688002, l1: 0.00010652374052429817, l2: 0.00037348042175066297   Iteration 92 of 100, tot loss = 4.798732145972874, l1: 0.00010651458480582653, l2: 0.00037335863057903583   Iteration 93 of 100, tot loss = 4.8089687644794425, l1: 0.00010677557754685842, l2: 0.0003741212999410627   Iteration 94 of 100, tot loss = 4.801602946951034, l1: 0.00010660333584305494, l2: 0.00037355695964466347   Iteration 95 of 100, tot loss = 4.785034987801, l1: 0.00010628880087384268, l2: 0.0003722146988270412   Iteration 96 of 100, tot loss = 4.764922233919303, l1: 0.00010582012873783242, l2: 0.00037067209556577535   Iteration 97 of 100, tot loss = 4.7753200703060505, l1: 0.00010607915809834088, l2: 0.00037145284985638577   Iteration 98 of 100, tot loss = 4.771599168680152, l1: 0.0001062319321618463, l2: 0.0003709279855164433   Iteration 99 of 100, tot loss = 4.766193320052793, l1: 0.00010608021304248879, l2: 0.0003705391193789458   Iteration 100 of 100, tot loss = 4.752133052349091, l1: 0.00010610469777020625, l2: 0.0003691086078470107
   End of epoch 1196; saving model... 

Epoch 1197 of 2000
   Iteration 1 of 100, tot loss = 4.281337261199951, l1: 0.00010342618043068796, l2: 0.00032470750738866627   Iteration 2 of 100, tot loss = 5.252897500991821, l1: 0.00012103859626222402, l2: 0.00040425114275421947   Iteration 3 of 100, tot loss = 5.2007425626118975, l1: 0.00012187001508815835, l2: 0.0003982042447508623   Iteration 4 of 100, tot loss = 5.349156379699707, l1: 0.0001235131931025535, l2: 0.0004114024486625567   Iteration 5 of 100, tot loss = 5.354940891265869, l1: 0.00012181945203337818, l2: 0.0004136746400035918   Iteration 6 of 100, tot loss = 5.1831347942352295, l1: 0.00011466802728439991, l2: 0.00040364545323730755   Iteration 7 of 100, tot loss = 5.215956892286028, l1: 0.00011481582207904597, l2: 0.0004067798685615084   Iteration 8 of 100, tot loss = 4.942550212144852, l1: 0.00011083621484431205, l2: 0.00038341880826919805   Iteration 9 of 100, tot loss = 4.9748140176137285, l1: 0.00010968228404332574, l2: 0.0003877991225130649   Iteration 10 of 100, tot loss = 5.089892935752869, l1: 0.00010959859646391124, l2: 0.0003993906997493468   Iteration 11 of 100, tot loss = 5.285545804283836, l1: 0.00011345137450420721, l2: 0.0004151032070777464   Iteration 12 of 100, tot loss = 5.269045452276866, l1: 0.00011338551606362064, l2: 0.0004135190307958207   Iteration 13 of 100, tot loss = 5.228504602725689, l1: 0.00011123807948584166, l2: 0.0004116123813079097   Iteration 14 of 100, tot loss = 5.176823530878339, l1: 0.00011172344862383657, l2: 0.0004059589066725623   Iteration 15 of 100, tot loss = 5.118793249130249, l1: 0.00011128388481059422, l2: 0.00040059544165463497   Iteration 16 of 100, tot loss = 5.030722796916962, l1: 0.0001095797242669505, l2: 0.00039349255621345947   Iteration 17 of 100, tot loss = 4.957225406871123, l1: 0.00010872954418814247, l2: 0.0003869929975788931   Iteration 18 of 100, tot loss = 4.922188970777723, l1: 0.00010871275703215765, l2: 0.000383506141790551   Iteration 19 of 100, tot loss = 5.049598794234426, l1: 0.00011004362711805458, l2: 0.00039491625456752156   Iteration 20 of 100, tot loss = 5.0832366943359375, l1: 0.00011029821871488821, l2: 0.00039802545288694093   Iteration 21 of 100, tot loss = 5.1374001275925405, l1: 0.00011111464361116911, l2: 0.0004026253709092825   Iteration 22 of 100, tot loss = 5.159359064969149, l1: 0.00011098173655971715, l2: 0.0004049541715754789   Iteration 23 of 100, tot loss = 5.111150554988695, l1: 0.00011126438640412348, l2: 0.00039985067142760784   Iteration 24 of 100, tot loss = 5.130650321642558, l1: 0.00011179889861523407, l2: 0.00040126613445560605   Iteration 25 of 100, tot loss = 5.1740375328063966, l1: 0.0001136256693280302, l2: 0.0004037780856015161   Iteration 26 of 100, tot loss = 5.056440564302298, l1: 0.00011132645704492461, l2: 0.0003943176007641551   Iteration 27 of 100, tot loss = 4.958638297186957, l1: 0.00011016127987036102, l2: 0.0003857025515538192   Iteration 28 of 100, tot loss = 4.88891864674432, l1: 0.00010896100021844697, l2: 0.0003799308662045015   Iteration 29 of 100, tot loss = 4.839561495287665, l1: 0.00010864831071150296, l2: 0.0003753078409976422   Iteration 30 of 100, tot loss = 4.8659015655517575, l1: 0.00010909992452070582, l2: 0.000377490234192616   Iteration 31 of 100, tot loss = 4.870131400323683, l1: 0.00010877866952796467, l2: 0.0003782344729481866   Iteration 32 of 100, tot loss = 4.873697176575661, l1: 0.00010770197798137815, l2: 0.00037966774243614054   Iteration 33 of 100, tot loss = 4.885757807529334, l1: 0.00010805290951270074, l2: 0.00038052287444853306   Iteration 34 of 100, tot loss = 4.868656495038201, l1: 0.00010817536777347628, l2: 0.000378690285502029   Iteration 35 of 100, tot loss = 4.821978419167655, l1: 0.00010720763270260899, l2: 0.0003749902131468324   Iteration 36 of 100, tot loss = 4.829024910926819, l1: 0.00010729342456114764, l2: 0.0003756090700335335   Iteration 37 of 100, tot loss = 4.80502919893007, l1: 0.0001059119736915769, l2: 0.00037459095010244464   Iteration 38 of 100, tot loss = 4.803352556730571, l1: 0.00010627338298562743, l2: 0.000374061876898754   Iteration 39 of 100, tot loss = 4.787345543885842, l1: 0.00010600863914497985, l2: 0.00037272591987111344   Iteration 40 of 100, tot loss = 4.74393784403801, l1: 0.00010589726471152972, l2: 0.0003684965238790028   Iteration 41 of 100, tot loss = 4.734418502668055, l1: 0.0001060475889705519, l2: 0.0003673942658834432   Iteration 42 of 100, tot loss = 4.717461648441496, l1: 0.00010579121685406703, l2: 0.0003659549533733211   Iteration 43 of 100, tot loss = 4.739283290020255, l1: 0.00010639122140060482, l2: 0.00036753711241925525   Iteration 44 of 100, tot loss = 4.755034441297704, l1: 0.00010548297053570754, l2: 0.00037002047875747934   Iteration 45 of 100, tot loss = 4.745588784747653, l1: 0.00010539062374543088, l2: 0.000369168259203434   Iteration 46 of 100, tot loss = 4.797403123067773, l1: 0.00010608048749260058, l2: 0.0003736598288093734   Iteration 47 of 100, tot loss = 4.875975542880119, l1: 0.00010774566203992358, l2: 0.0003798518966427667   Iteration 48 of 100, tot loss = 4.8700548360745115, l1: 0.00010753527377952803, l2: 0.0003794702145872482   Iteration 49 of 100, tot loss = 4.848184108734131, l1: 0.00010725919165285019, l2: 0.0003775592236447015   Iteration 50 of 100, tot loss = 4.834258117675781, l1: 0.0001069039181311382, l2: 0.0003765218978514895   Iteration 51 of 100, tot loss = 4.866224625531365, l1: 0.00010711185064328853, l2: 0.0003795106159112252   Iteration 52 of 100, tot loss = 4.7956313880590296, l1: 0.00010572273521784855, l2: 0.0003738404075892714   Iteration 53 of 100, tot loss = 4.795408795464714, l1: 0.00010600130993242481, l2: 0.0003735395730473101   Iteration 54 of 100, tot loss = 4.76029520785367, l1: 0.00010558632702331697, l2: 0.0003704431968529847   Iteration 55 of 100, tot loss = 4.742818297039379, l1: 0.00010566940122358077, l2: 0.0003686124313389882   Iteration 56 of 100, tot loss = 4.73030250625951, l1: 0.0001057300744053334, l2: 0.0003673001792776631   Iteration 57 of 100, tot loss = 4.683225428848936, l1: 0.00010474475002091934, l2: 0.00036357779575618015   Iteration 58 of 100, tot loss = 4.661087013524154, l1: 0.00010395831078971366, l2: 0.00036215039340433953   Iteration 59 of 100, tot loss = 4.664595824176982, l1: 0.00010430366233715361, l2: 0.00036215592275365746   Iteration 60 of 100, tot loss = 4.652071954806646, l1: 0.00010397458469621293, l2: 0.000361232613552905   Iteration 61 of 100, tot loss = 4.636400979073321, l1: 0.00010328378964198341, l2: 0.00036035631101036475   Iteration 62 of 100, tot loss = 4.6486773087132365, l1: 0.00010329113836398333, l2: 0.00036157659539434636   Iteration 63 of 100, tot loss = 4.660490669901409, l1: 0.00010341695076296297, l2: 0.00036263211915779506   Iteration 64 of 100, tot loss = 4.682924157008529, l1: 0.00010395732300594318, l2: 0.00036433509581002   Iteration 65 of 100, tot loss = 4.664700293540955, l1: 0.00010381061555772949, l2: 0.0003626594165465436   Iteration 66 of 100, tot loss = 4.724649478088725, l1: 0.00010441811079517444, l2: 0.00036804683953629467   Iteration 67 of 100, tot loss = 4.728195871879805, l1: 0.00010449393660415645, l2: 0.0003683256528795516   Iteration 68 of 100, tot loss = 4.733365491909139, l1: 0.00010431691487824805, l2: 0.00036901963668402885   Iteration 69 of 100, tot loss = 4.7116129173748735, l1: 0.0001038389724482735, l2: 0.0003673223218883968   Iteration 70 of 100, tot loss = 4.738219978128161, l1: 0.00010399543165736499, l2: 0.0003698265689308755   Iteration 71 of 100, tot loss = 4.717910607096175, l1: 0.00010321877249532944, l2: 0.0003685722909261092   Iteration 72 of 100, tot loss = 4.687730284200774, l1: 0.00010275637689321431, l2: 0.00036601665447556623   Iteration 73 of 100, tot loss = 4.694810349647313, l1: 0.00010278034819312368, l2: 0.00036670068998132156   Iteration 74 of 100, tot loss = 4.6538527849558236, l1: 0.00010207089906396357, l2: 0.00036331438251009613   Iteration 75 of 100, tot loss = 4.665200271606445, l1: 0.00010255161580668452, l2: 0.0003639684146037325   Iteration 76 of 100, tot loss = 4.6515346671405595, l1: 0.00010269539229499826, l2: 0.0003624580776407751   Iteration 77 of 100, tot loss = 4.632653453133323, l1: 0.0001024768373406607, l2: 0.0003607885111894386   Iteration 78 of 100, tot loss = 4.629151888382741, l1: 0.0001024885621471838, l2: 0.00036042663011287985   Iteration 79 of 100, tot loss = 4.639834760110589, l1: 0.00010254936661354643, l2: 0.0003614341130972357   Iteration 80 of 100, tot loss = 4.655157482624054, l1: 0.00010299472291990242, l2: 0.0003625210285463254   Iteration 81 of 100, tot loss = 4.663826489154204, l1: 0.00010322375120817868, l2: 0.0003631589005422422   Iteration 82 of 100, tot loss = 4.6441424619860765, l1: 0.00010309657959513105, l2: 0.000361317669314037   Iteration 83 of 100, tot loss = 4.659474944493857, l1: 0.00010333603100903074, l2: 0.00036261146586579103   Iteration 84 of 100, tot loss = 4.67467277674448, l1: 0.00010353268654788346, l2: 0.0003639345935274226   Iteration 85 of 100, tot loss = 4.6726321024053235, l1: 0.00010335233352688032, l2: 0.0003639108788989046   Iteration 86 of 100, tot loss = 4.682813985403194, l1: 0.00010351944564358429, l2: 0.0003647619552455496   Iteration 87 of 100, tot loss = 4.686690777197652, l1: 0.00010392254053035633, l2: 0.00036474653946546215   Iteration 88 of 100, tot loss = 4.709883416240865, l1: 0.00010427273508122943, l2: 0.0003667156088340562   Iteration 89 of 100, tot loss = 4.700538686152255, l1: 0.00010432578207829476, l2: 0.00036572808860225623   Iteration 90 of 100, tot loss = 4.684261496861776, l1: 0.00010418477302462432, l2: 0.00036424137845945855   Iteration 91 of 100, tot loss = 4.678852626255581, l1: 0.00010411364388886255, l2: 0.0003637716204188969   Iteration 92 of 100, tot loss = 4.675617865894152, l1: 0.00010405045739626867, l2: 0.00036351133072141397   Iteration 93 of 100, tot loss = 4.678130052422964, l1: 0.00010429234574868104, l2: 0.00036352066077812703   Iteration 94 of 100, tot loss = 4.6932021962835435, l1: 0.00010444492646127672, l2: 0.0003648752939857622   Iteration 95 of 100, tot loss = 4.708589031821803, l1: 0.00010479973712599052, l2: 0.00036605916653857813   Iteration 96 of 100, tot loss = 4.700451634824276, l1: 0.00010465502278596735, l2: 0.0003653901412690175   Iteration 97 of 100, tot loss = 4.7181007419664835, l1: 0.00010503078108028144, l2: 0.000366779293603331   Iteration 98 of 100, tot loss = 4.709198489481089, l1: 0.0001048734139491286, l2: 0.00036604643552278984   Iteration 99 of 100, tot loss = 4.720885589869336, l1: 0.00010492902951264481, l2: 0.00036715953040519973   Iteration 100 of 100, tot loss = 4.709392666816711, l1: 0.00010467080104717752, l2: 0.0003662684664595872
   End of epoch 1197; saving model... 

Epoch 1198 of 2000
   Iteration 1 of 100, tot loss = 3.7894062995910645, l1: 9.63782295002602e-05, l2: 0.0002825623960234225   Iteration 2 of 100, tot loss = 4.4724791049957275, l1: 9.8922861070605e-05, l2: 0.0003483250620774925   Iteration 3 of 100, tot loss = 4.526628176371257, l1: 9.797534100168075e-05, l2: 0.0003546874892587463   Iteration 4 of 100, tot loss = 5.398483991622925, l1: 0.00011488634299894329, l2: 0.000424962054239586   Iteration 5 of 100, tot loss = 5.547789096832275, l1: 0.0001144215595559217, l2: 0.00044035734608769415   Iteration 6 of 100, tot loss = 5.1000068585077925, l1: 0.00010355260747019202, l2: 0.00040644807450007647   Iteration 7 of 100, tot loss = 4.995168311255319, l1: 0.0001072150917025283, l2: 0.00039230173154334934   Iteration 8 of 100, tot loss = 4.928484886884689, l1: 0.00010837426816578954, l2: 0.0003844742168439552   Iteration 9 of 100, tot loss = 4.935150014029609, l1: 0.00010702321014832705, l2: 0.00038649178653334576   Iteration 10 of 100, tot loss = 4.686944103240966, l1: 0.00010341898450860754, l2: 0.0003652754225186072   Iteration 11 of 100, tot loss = 4.436349023472179, l1: 9.861903394762935e-05, l2: 0.0003450158665972677   Iteration 12 of 100, tot loss = 4.367091039816539, l1: 9.650454012444243e-05, l2: 0.00034020456223515794   Iteration 13 of 100, tot loss = 4.5220773953657885, l1: 9.826609255889288e-05, l2: 0.0003539416424106233   Iteration 14 of 100, tot loss = 4.882428424698966, l1: 0.00010252968786517158, l2: 0.00038571314611804804   Iteration 15 of 100, tot loss = 4.846925497055054, l1: 9.972616389859467e-05, l2: 0.00038496637910914917   Iteration 16 of 100, tot loss = 4.8066621869802475, l1: 0.00010073854400616256, l2: 0.000379927669200697   Iteration 17 of 100, tot loss = 4.721900897867539, l1: 0.00010048244011533611, l2: 0.0003717076452791362   Iteration 18 of 100, tot loss = 4.802614463700189, l1: 0.00010250351543719362, l2: 0.00037775792508101504   Iteration 19 of 100, tot loss = 4.8159315837057015, l1: 0.00010261573755231343, l2: 0.0003789774146801057   Iteration 20 of 100, tot loss = 4.7671184062957765, l1: 0.00010185559513047338, l2: 0.0003748562397959176   Iteration 21 of 100, tot loss = 4.706348510015578, l1: 0.00010097868861131636, l2: 0.0003696561567873384   Iteration 22 of 100, tot loss = 4.6843107613650234, l1: 0.00010045107641913505, l2: 0.0003679799933293411   Iteration 23 of 100, tot loss = 4.680550803308901, l1: 0.00010026768153614324, l2: 0.0003677873939285865   Iteration 24 of 100, tot loss = 4.753397981325786, l1: 0.00010113276463622849, l2: 0.0003742070287747386   Iteration 25 of 100, tot loss = 4.704318561553955, l1: 0.00010090745054185391, l2: 0.0003695244010305032   Iteration 26 of 100, tot loss = 4.694264246867253, l1: 0.00010026993438189563, l2: 0.0003691564863682008   Iteration 27 of 100, tot loss = 4.760109018396448, l1: 0.00010124407385268973, l2: 0.00037476682498688914   Iteration 28 of 100, tot loss = 4.881714582443237, l1: 0.00010369131268817, l2: 0.00038448014103258695   Iteration 29 of 100, tot loss = 4.82491947864664, l1: 0.00010210310148099309, l2: 0.00038038884191211826   Iteration 30 of 100, tot loss = 4.774438230196635, l1: 0.00010077247231189782, l2: 0.00037667134650594863   Iteration 31 of 100, tot loss = 4.732788962702597, l1: 0.00010045140938487865, l2: 0.0003728274825862759   Iteration 32 of 100, tot loss = 4.883283749222755, l1: 0.00010287325494573452, l2: 0.000385455115520017   Iteration 33 of 100, tot loss = 4.87241268157959, l1: 0.00010267451668341616, l2: 0.00038456674674031973   Iteration 34 of 100, tot loss = 4.836505679523244, l1: 0.00010127220668758074, l2: 0.0003823783571373069   Iteration 35 of 100, tot loss = 4.874171202523367, l1: 0.00010201807797005001, l2: 0.00038539903762284665   Iteration 36 of 100, tot loss = 4.9131888548533125, l1: 0.00010278941034711251, l2: 0.0003885294714160005   Iteration 37 of 100, tot loss = 4.944097802445695, l1: 0.00010413721680084268, l2: 0.00039027255882723003   Iteration 38 of 100, tot loss = 4.959193330062063, l1: 0.00010418699792736381, l2: 0.0003917323295976722   Iteration 39 of 100, tot loss = 4.911685161101512, l1: 0.00010367515353256096, l2: 0.00038749335679965903   Iteration 40 of 100, tot loss = 4.876581102609634, l1: 0.00010312689764759852, l2: 0.00038453120723715985   Iteration 41 of 100, tot loss = 4.948624116618459, l1: 0.00010347485081959947, l2: 0.00039138755599241247   Iteration 42 of 100, tot loss = 4.951258562860035, l1: 0.00010369021479813714, l2: 0.0003914356368893225   Iteration 43 of 100, tot loss = 4.964466178139975, l1: 0.00010397333591204505, l2: 0.0003924732763850845   Iteration 44 of 100, tot loss = 4.991851703687147, l1: 0.00010372773803365735, l2: 0.000395457427657675   Iteration 45 of 100, tot loss = 5.0221379544999865, l1: 0.00010414249320294605, l2: 0.0003980712971598324   Iteration 46 of 100, tot loss = 5.007356078728385, l1: 0.00010407473501286206, l2: 0.00039666086756725514   Iteration 47 of 100, tot loss = 4.992793930337784, l1: 0.00010406370161713022, l2: 0.0003952156861332503   Iteration 48 of 100, tot loss = 5.032292599479358, l1: 0.00010504421417560177, l2: 0.00039818503986073967   Iteration 49 of 100, tot loss = 4.95982089577889, l1: 0.00010357027676203573, l2: 0.0003924118069817825   Iteration 50 of 100, tot loss = 4.979408733844757, l1: 0.00010394293756689877, l2: 0.00039399793080519886   Iteration 51 of 100, tot loss = 4.948792950779784, l1: 0.00010344078627360217, l2: 0.00039143850410576253   Iteration 52 of 100, tot loss = 4.947341228906925, l1: 0.00010334814130776346, l2: 0.00039138597695944976   Iteration 53 of 100, tot loss = 4.969472424039301, l1: 0.00010375315180698515, l2: 0.0003931940865992867   Iteration 54 of 100, tot loss = 4.987905747360653, l1: 0.00010442680946585756, l2: 0.00039436376203041246   Iteration 55 of 100, tot loss = 4.962607026100159, l1: 0.00010421596009771085, l2: 0.0003920447393532165   Iteration 56 of 100, tot loss = 4.979105155382838, l1: 0.00010458617063185167, l2: 0.0003933243419201712   Iteration 57 of 100, tot loss = 5.041761555169758, l1: 0.00010544468642093993, l2: 0.00039873146625611476   Iteration 58 of 100, tot loss = 5.012763058317119, l1: 0.000105130654415477, l2: 0.0003961456481945412   Iteration 59 of 100, tot loss = 5.02138513831769, l1: 0.00010531308362260461, l2: 0.0003968254274341388   Iteration 60 of 100, tot loss = 5.030285042524338, l1: 0.0001052551740334214, l2: 0.00039777332713128997   Iteration 61 of 100, tot loss = 5.0594663405027545, l1: 0.00010557796450171498, l2: 0.00040036866576868856   Iteration 62 of 100, tot loss = 5.077879492313631, l1: 0.00010569053530719341, l2: 0.00040209740954225945   Iteration 63 of 100, tot loss = 5.0738152795367775, l1: 0.00010542670405277657, l2: 0.00040195481981047324   Iteration 64 of 100, tot loss = 5.085243569687009, l1: 0.00010587701717668097, l2: 0.0004026473361591343   Iteration 65 of 100, tot loss = 5.047885504135719, l1: 0.00010561523835909052, l2: 0.0003991733083519368   Iteration 66 of 100, tot loss = 5.002920907555205, l1: 0.00010475946460660039, l2: 0.0003955326226952655   Iteration 67 of 100, tot loss = 4.993287162994271, l1: 0.00010462444941965001, l2: 0.000394704263203833   Iteration 68 of 100, tot loss = 4.987772904774722, l1: 0.00010492730892592055, l2: 0.00039384997751981514   Iteration 69 of 100, tot loss = 4.982734851215197, l1: 0.00010470618039160372, l2: 0.0003935673007947406   Iteration 70 of 100, tot loss = 4.988600599765777, l1: 0.00010489655937167949, l2: 0.0003939634968576554   Iteration 71 of 100, tot loss = 4.985364153351583, l1: 0.00010475597420582732, l2: 0.00039378043717514514   Iteration 72 of 100, tot loss = 4.987001523375511, l1: 0.00010499595757412155, l2: 0.0003937041908809786   Iteration 73 of 100, tot loss = 4.9716187748190475, l1: 0.00010458803803449471, l2: 0.0003925738355849729   Iteration 74 of 100, tot loss = 4.94114200972222, l1: 0.00010415219052496518, l2: 0.00038996200660098895   Iteration 75 of 100, tot loss = 4.94454477151235, l1: 0.00010427748105333497, l2: 0.00039017699212611965   Iteration 76 of 100, tot loss = 4.933626593727815, l1: 0.0001038857328814246, l2: 0.0003894769227037231   Iteration 77 of 100, tot loss = 4.920378999276594, l1: 0.00010377526604162238, l2: 0.00038826262994893135   Iteration 78 of 100, tot loss = 4.951758682727814, l1: 0.00010419728184784524, l2: 0.0003909785828466575   Iteration 79 of 100, tot loss = 4.934475903269611, l1: 0.00010387653126116087, l2: 0.0003895710553617299   Iteration 80 of 100, tot loss = 4.93221268504858, l1: 0.00010392000503998134, l2: 0.00038930126011109677   Iteration 81 of 100, tot loss = 4.9452211518346525, l1: 0.00010414993619319212, l2: 0.00039037217527633695   Iteration 82 of 100, tot loss = 4.930250933984431, l1: 0.00010384605660120298, l2: 0.0003891790332911513   Iteration 83 of 100, tot loss = 4.932926251227597, l1: 0.00010392713914162491, l2: 0.0003893654823306028   Iteration 84 of 100, tot loss = 4.939080821616309, l1: 0.00010406403063305853, l2: 0.0003898440473200199   Iteration 85 of 100, tot loss = 4.9139521893332985, l1: 0.0001037670548505369, l2: 0.00038762815991717883   Iteration 86 of 100, tot loss = 4.91859877525374, l1: 0.0001041062454913667, l2: 0.00038775362783019583   Iteration 87 of 100, tot loss = 4.899265104326709, l1: 0.00010386365273466933, l2: 0.00038606285344375746   Iteration 88 of 100, tot loss = 4.889305945147168, l1: 0.00010369399181151742, l2: 0.0003852365981319136   Iteration 89 of 100, tot loss = 4.889820684207959, l1: 0.00010373767733827673, l2: 0.0003852443866041918   Iteration 90 of 100, tot loss = 4.889613587326473, l1: 0.00010389880432436864, l2: 0.00038506254996819835   Iteration 91 of 100, tot loss = 4.875921984295268, l1: 0.00010362973477970296, l2: 0.0003839624592468941   Iteration 92 of 100, tot loss = 4.8856638838415565, l1: 0.00010395875498310805, l2: 0.00038460762927728786   Iteration 93 of 100, tot loss = 4.860990340991687, l1: 0.00010334210118569273, l2: 0.00038275692889827394   Iteration 94 of 100, tot loss = 4.886541119281282, l1: 0.000103768747841117, l2: 0.00038488536015489794   Iteration 95 of 100, tot loss = 4.909035096670452, l1: 0.00010428456758075443, l2: 0.00038661893800293144   Iteration 96 of 100, tot loss = 4.944490101188421, l1: 0.00010498638403078075, l2: 0.0003894626224791864   Iteration 97 of 100, tot loss = 4.941184649762419, l1: 0.00010494561145831015, l2: 0.00038917284979744207   Iteration 98 of 100, tot loss = 4.939513056862111, l1: 0.00010507253021814348, l2: 0.000388878771835672   Iteration 99 of 100, tot loss = 4.9413395084515965, l1: 0.00010504829712009829, l2: 0.0003890856501359417   Iteration 100 of 100, tot loss = 4.93619007229805, l1: 0.00010513730499951635, l2: 0.0003884816984646022
   End of epoch 1198; saving model... 

Epoch 1199 of 2000
   Iteration 1 of 100, tot loss = 3.557795524597168, l1: 7.488825940527022e-05, l2: 0.0002808912831824273   Iteration 2 of 100, tot loss = 3.832730293273926, l1: 8.569977944716811e-05, l2: 0.00029757324955426157   Iteration 3 of 100, tot loss = 4.217700004577637, l1: 9.51575687698399e-05, l2: 0.00032661242100099724   Iteration 4 of 100, tot loss = 4.354687929153442, l1: 9.391509956913069e-05, l2: 0.00034155367757193744   Iteration 5 of 100, tot loss = 4.388048458099365, l1: 9.263368701795116e-05, l2: 0.00034617114579305053   Iteration 6 of 100, tot loss = 4.450536410013835, l1: 9.591914567863569e-05, l2: 0.0003491344881088783   Iteration 7 of 100, tot loss = 4.600120748792376, l1: 0.00010108175047207624, l2: 0.0003589303191152534   Iteration 8 of 100, tot loss = 4.73573237657547, l1: 0.0001047194964485243, l2: 0.00036885373629047535   Iteration 9 of 100, tot loss = 4.841484705607097, l1: 0.00010438341269036755, l2: 0.00037976505377122923   Iteration 10 of 100, tot loss = 4.967421436309815, l1: 0.00010524079552851618, l2: 0.00039150134252849964   Iteration 11 of 100, tot loss = 5.034753149205988, l1: 0.00010593530698150227, l2: 0.0003975400018548085   Iteration 12 of 100, tot loss = 5.054559866587321, l1: 0.00010725368520070333, l2: 0.00039820229236890253   Iteration 13 of 100, tot loss = 4.932524534372183, l1: 0.00010626959892061468, l2: 0.00038698284725586954   Iteration 14 of 100, tot loss = 4.825700112751552, l1: 0.00010560692524969844, l2: 0.00037696307949123105   Iteration 15 of 100, tot loss = 4.855606873830159, l1: 0.00010601733811199665, l2: 0.00037954334208431344   Iteration 16 of 100, tot loss = 4.890771001577377, l1: 0.00010752614525699755, l2: 0.00038155094807734713   Iteration 17 of 100, tot loss = 4.9057575955110435, l1: 0.00010781540442942915, l2: 0.00038276034790803403   Iteration 18 of 100, tot loss = 4.91029339366489, l1: 0.0001070270474075288, l2: 0.00038400228368118405   Iteration 19 of 100, tot loss = 4.855600319410625, l1: 0.00010561715316725895, l2: 0.00037994287068661496   Iteration 20 of 100, tot loss = 4.806667006015777, l1: 0.00010555945118539967, l2: 0.0003751072421437129   Iteration 21 of 100, tot loss = 4.792057752609253, l1: 0.00010638335342740728, l2: 0.00037282241447385224   Iteration 22 of 100, tot loss = 4.702051487835971, l1: 0.00010503889676659706, l2: 0.0003651662449225445   Iteration 23 of 100, tot loss = 4.671911509140678, l1: 0.00010460741475032157, l2: 0.00036258372837556123   Iteration 24 of 100, tot loss = 4.690698504447937, l1: 0.00010530044407156917, l2: 0.0003637693989730906   Iteration 25 of 100, tot loss = 4.683731117248535, l1: 0.00010543508076807484, l2: 0.0003629380231723189   Iteration 26 of 100, tot loss = 4.767584580641526, l1: 0.00010598197639830268, l2: 0.0003707764755325535   Iteration 27 of 100, tot loss = 4.7682191177650735, l1: 0.00010608768422604987, l2: 0.0003707342203361569   Iteration 28 of 100, tot loss = 4.836166518075125, l1: 0.00010711894408034693, l2: 0.00037649770092684776   Iteration 29 of 100, tot loss = 4.746830093449559, l1: 0.00010497309568595013, l2: 0.0003697099068788169   Iteration 30 of 100, tot loss = 4.816602524121603, l1: 0.0001063728894223459, l2: 0.00037528735750432435   Iteration 31 of 100, tot loss = 4.730565963252898, l1: 0.0001042113279606471, l2: 0.00036884526273370873   Iteration 32 of 100, tot loss = 4.7608122527599335, l1: 0.00010396491541087016, l2: 0.000372116305243253   Iteration 33 of 100, tot loss = 4.884076580856785, l1: 0.00010587704607380807, l2: 0.0003825306057203279   Iteration 34 of 100, tot loss = 4.815734063877779, l1: 0.00010485065489346572, l2: 0.00037672274543985943   Iteration 35 of 100, tot loss = 4.805304908752442, l1: 0.0001053959433712797, l2: 0.00037513454195245037   Iteration 36 of 100, tot loss = 4.748738944530487, l1: 0.00010508714462452594, l2: 0.000369786744866158   Iteration 37 of 100, tot loss = 4.689279092324747, l1: 0.00010367953133961262, l2: 0.0003652483723566842   Iteration 38 of 100, tot loss = 4.746042728424072, l1: 0.00010414113373707024, l2: 0.0003704631335016242   Iteration 39 of 100, tot loss = 4.722175793770032, l1: 0.00010395267702570448, l2: 0.0003682648970518643   Iteration 40 of 100, tot loss = 4.674900937080383, l1: 0.00010307268394171842, l2: 0.00036441740485315677   Iteration 41 of 100, tot loss = 4.6310237035518735, l1: 0.00010227268161010793, l2: 0.0003608296836141451   Iteration 42 of 100, tot loss = 4.6599644763129096, l1: 0.00010291948819940444, l2: 0.00036307695492204014   Iteration 43 of 100, tot loss = 4.599854469299316, l1: 0.00010147304385819278, l2: 0.0003585123983590842   Iteration 44 of 100, tot loss = 4.64563699202104, l1: 0.00010221250092647757, l2: 0.0003623511934578842   Iteration 45 of 100, tot loss = 4.618678087658353, l1: 0.0001021189134917222, l2: 0.0003597488904940999   Iteration 46 of 100, tot loss = 4.648311941520028, l1: 0.00010225014446642372, l2: 0.0003625810451289315   Iteration 47 of 100, tot loss = 4.628593424533276, l1: 0.00010177216887662307, l2: 0.0003610871691365746   Iteration 48 of 100, tot loss = 4.623992512623469, l1: 0.00010203317363751314, l2: 0.00036036607283070526   Iteration 49 of 100, tot loss = 4.642594785106425, l1: 0.00010270387755721161, l2: 0.00036155559628612687   Iteration 50 of 100, tot loss = 4.650853681564331, l1: 0.00010312710262951441, l2: 0.0003619582607643679   Iteration 51 of 100, tot loss = 4.659810206469367, l1: 0.00010296118425150566, l2: 0.0003630198320309978   Iteration 52 of 100, tot loss = 4.645334601402283, l1: 0.00010291091656164589, l2: 0.00036162253896831174   Iteration 53 of 100, tot loss = 4.6589118759587125, l1: 0.00010336734168341313, l2: 0.0003625238413114171   Iteration 54 of 100, tot loss = 4.649010720076384, l1: 0.00010259972704160545, l2: 0.0003623013401886931   Iteration 55 of 100, tot loss = 4.694329582561146, l1: 0.00010345590496647425, l2: 0.0003659770485352386   Iteration 56 of 100, tot loss = 4.662117208753314, l1: 0.00010285980845635225, l2: 0.0003633519079033119   Iteration 57 of 100, tot loss = 4.7351686745359185, l1: 0.00010407584838838757, l2: 0.00036944101514548   Iteration 58 of 100, tot loss = 4.7829263045870025, l1: 0.00010495372006171865, l2: 0.00037333890690331766   Iteration 59 of 100, tot loss = 4.789151102809583, l1: 0.00010508678888535064, l2: 0.00037382831782205187   Iteration 60 of 100, tot loss = 4.832742373148601, l1: 0.00010554942430947752, l2: 0.0003777248098534377   Iteration 61 of 100, tot loss = 4.849385652385775, l1: 0.00010540621480656422, l2: 0.0003795323468321079   Iteration 62 of 100, tot loss = 4.851950299355291, l1: 0.00010516717954869232, l2: 0.0003800278469908892   Iteration 63 of 100, tot loss = 4.857377075013661, l1: 0.00010550893623278373, l2: 0.00038022876810467256   Iteration 64 of 100, tot loss = 4.841151349246502, l1: 0.00010545066822942317, l2: 0.00037866446359657857   Iteration 65 of 100, tot loss = 4.809718054991502, l1: 0.00010495048974497387, l2: 0.00037602131286313615   Iteration 66 of 100, tot loss = 4.791927846995267, l1: 0.00010476570693726623, l2: 0.000374427074734051   Iteration 67 of 100, tot loss = 4.752759887211359, l1: 0.00010411452196639225, l2: 0.00037116146365826977   Iteration 68 of 100, tot loss = 4.802466501207912, l1: 0.00010492351119704407, l2: 0.00037532313559923377   Iteration 69 of 100, tot loss = 4.803069988886516, l1: 0.0001047269597884092, l2: 0.00037558003564102006   Iteration 70 of 100, tot loss = 4.780127171107701, l1: 0.00010455723219950284, l2: 0.0003734554814790109   Iteration 71 of 100, tot loss = 4.788575790297817, l1: 0.00010505789283394131, l2: 0.0003737996832031468   Iteration 72 of 100, tot loss = 4.765605469544728, l1: 0.00010489343720918341, l2: 0.00037166710677006957   Iteration 73 of 100, tot loss = 4.809183270963904, l1: 0.00010574135715253524, l2: 0.00037517696710730815   Iteration 74 of 100, tot loss = 4.789410082069603, l1: 0.00010537215004316406, l2: 0.00037356885523705214   Iteration 75 of 100, tot loss = 4.849093736012777, l1: 0.00010630144330207258, l2: 0.0003786079273171102   Iteration 76 of 100, tot loss = 4.824656721792723, l1: 0.00010564495158560978, l2: 0.0003768207176395471   Iteration 77 of 100, tot loss = 4.828475558912599, l1: 0.00010579045102803049, l2: 0.0003770571018723384   Iteration 78 of 100, tot loss = 4.798687091240516, l1: 0.00010535007650338602, l2: 0.00037451862982169795   Iteration 79 of 100, tot loss = 4.7960589505449125, l1: 0.00010526903824478424, l2: 0.000374336853878325   Iteration 80 of 100, tot loss = 4.788581937551498, l1: 0.0001051234079113783, l2: 0.0003737347831702209   Iteration 81 of 100, tot loss = 4.767873142972404, l1: 0.00010446510045037513, l2: 0.00037232221102048816   Iteration 82 of 100, tot loss = 4.777338071567256, l1: 0.00010461980052307964, l2: 0.0003731140042385427   Iteration 83 of 100, tot loss = 4.790321605751314, l1: 0.00010475494671109447, l2: 0.00037427721120815454   Iteration 84 of 100, tot loss = 4.7581959281648905, l1: 0.00010401772054874094, l2: 0.0003718018696028074   Iteration 85 of 100, tot loss = 4.737451581393971, l1: 0.00010362971918178065, l2: 0.0003701154363464893   Iteration 86 of 100, tot loss = 4.736429103585177, l1: 0.00010350909749747167, l2: 0.000370133810150058   Iteration 87 of 100, tot loss = 4.722337166468303, l1: 0.00010335580862311903, l2: 0.0003688779056417467   Iteration 88 of 100, tot loss = 4.697199862111699, l1: 0.00010268837819570432, l2: 0.0003670316054922296   Iteration 89 of 100, tot loss = 4.70955551608225, l1: 0.00010279512448658842, l2: 0.0003681604250457159   Iteration 90 of 100, tot loss = 4.698618851767646, l1: 0.00010226573091737615, l2: 0.00036759615225795037   Iteration 91 of 100, tot loss = 4.718046649471744, l1: 0.00010256238729797898, l2: 0.0003692422759051404   Iteration 92 of 100, tot loss = 4.729422247928122, l1: 0.00010300318812685715, l2: 0.00036993903457819806   Iteration 93 of 100, tot loss = 4.726976220325757, l1: 0.00010277854611343573, l2: 0.0003699190739069074   Iteration 94 of 100, tot loss = 4.707350472186474, l1: 0.00010232420808801606, l2: 0.00036841083721733317   Iteration 95 of 100, tot loss = 4.692372736177947, l1: 0.0001020428207611028, l2: 0.0003671944512078833   Iteration 96 of 100, tot loss = 4.674412662784259, l1: 0.00010152677873520588, l2: 0.0003659144858829677   Iteration 97 of 100, tot loss = 4.697159462368365, l1: 0.00010176810513060904, l2: 0.0003679478390996834   Iteration 98 of 100, tot loss = 4.702746775685524, l1: 0.00010183726927997279, l2: 0.000368437406663992   Iteration 99 of 100, tot loss = 4.701884789900347, l1: 0.00010192714523667508, l2: 0.00036826133234144163   Iteration 100 of 100, tot loss = 4.726834073066711, l1: 0.00010239956864097622, l2: 0.0003702838375465944
   End of epoch 1199; saving model... 

Epoch 1200 of 2000
   Iteration 1 of 100, tot loss = 5.500208854675293, l1: 0.00012931918899994344, l2: 0.0004207017191220075   Iteration 2 of 100, tot loss = 4.72830867767334, l1: 0.00011563171574380249, l2: 0.00035719916922971606   Iteration 3 of 100, tot loss = 5.185043493906657, l1: 0.0001148712641831177, l2: 0.0004036331083625555   Iteration 4 of 100, tot loss = 4.598594486713409, l1: 0.00010721624312282074, l2: 0.0003526432192302309   Iteration 5 of 100, tot loss = 4.343313074111938, l1: 0.0001028234968543984, l2: 0.0003315078211016953   Iteration 6 of 100, tot loss = 4.019834756851196, l1: 9.662900993134826e-05, l2: 0.00030535447391836595   Iteration 7 of 100, tot loss = 4.247491291591099, l1: 0.00010061494582000055, l2: 0.000324134186874809   Iteration 8 of 100, tot loss = 4.0501501858234406, l1: 9.630432759877294e-05, l2: 0.0003087106942984974   Iteration 9 of 100, tot loss = 4.070613463719686, l1: 9.727044784489812e-05, l2: 0.00030979090337900235   Iteration 10 of 100, tot loss = 4.133241295814514, l1: 9.821726416703314e-05, l2: 0.0003151068711304106   Iteration 11 of 100, tot loss = 4.245135849172419, l1: 9.79244703723287e-05, l2: 0.0003265891216208481   Iteration 12 of 100, tot loss = 4.467618962128957, l1: 0.00010270983693771996, l2: 0.0003440520680063249   Iteration 13 of 100, tot loss = 4.541874170303345, l1: 0.00010531804470631939, l2: 0.0003488693787053657   Iteration 14 of 100, tot loss = 4.514431084905352, l1: 0.0001048020770083115, l2: 0.0003466410385694222   Iteration 15 of 100, tot loss = 4.681287018458049, l1: 0.0001049446402854907, l2: 0.0003631840683131789   Iteration 16 of 100, tot loss = 4.605996355414391, l1: 0.0001047181362991978, l2: 0.00035588150694820797   Iteration 17 of 100, tot loss = 4.571803331375122, l1: 0.00010345158336670411, l2: 0.00035372875743846903   Iteration 18 of 100, tot loss = 4.548562990294562, l1: 0.00010213300993200392, l2: 0.0003527232968028531   Iteration 19 of 100, tot loss = 4.468700684999165, l1: 0.00010060450011606965, l2: 0.00034626557546863824   Iteration 20 of 100, tot loss = 4.472755765914917, l1: 0.00010052168945549056, l2: 0.00034675389324547725   Iteration 21 of 100, tot loss = 4.501582667941139, l1: 0.00010201675745303787, l2: 0.00034814151530597535   Iteration 22 of 100, tot loss = 4.488700866699219, l1: 0.0001005002136067064, l2: 0.00034836987933059305   Iteration 23 of 100, tot loss = 4.435368745223336, l1: 9.960931904219171e-05, l2: 0.00034392756187235534   Iteration 24 of 100, tot loss = 4.373500029246013, l1: 9.857117432450953e-05, l2: 0.00033877883409635007   Iteration 25 of 100, tot loss = 4.352235460281372, l1: 9.793711855309084e-05, l2: 0.0003372864326229319   Iteration 26 of 100, tot loss = 4.3570884466171265, l1: 9.865448401587155e-05, l2: 0.00033705436586527724   Iteration 27 of 100, tot loss = 4.461531206413552, l1: 9.995430284309098e-05, l2: 0.0003461988219189354   Iteration 28 of 100, tot loss = 4.497079942907606, l1: 0.00010081099104094651, l2: 0.0003488970079550719   Iteration 29 of 100, tot loss = 4.534436891818869, l1: 0.00010102535703355961, l2: 0.00035241833568855737   Iteration 30 of 100, tot loss = 4.537482126553853, l1: 0.00010063676842643569, l2: 0.0003531114474753849   Iteration 31 of 100, tot loss = 4.648237312993696, l1: 0.0001027560004243447, l2: 0.00036206773356459436   Iteration 32 of 100, tot loss = 4.649371661245823, l1: 0.00010331642442906741, l2: 0.00036162074457024573   Iteration 33 of 100, tot loss = 4.65823448788036, l1: 0.00010324945712151627, l2: 0.0003625739945246923   Iteration 34 of 100, tot loss = 4.6572944767334885, l1: 0.00010348194936180815, l2: 0.0003622475004008533   Iteration 35 of 100, tot loss = 4.633727897916521, l1: 0.00010314986804067822, l2: 0.0003602229237523196   Iteration 36 of 100, tot loss = 4.733583192030589, l1: 0.00010458780707267579, l2: 0.0003687705138872843   Iteration 37 of 100, tot loss = 4.765881815472165, l1: 0.00010515878396266064, l2: 0.0003714294002133396   Iteration 38 of 100, tot loss = 4.762232460473713, l1: 0.00010479021511855535, l2: 0.00037143303358645524   Iteration 39 of 100, tot loss = 4.757521806619106, l1: 0.00010459833515013974, l2: 0.0003711538481114146   Iteration 40 of 100, tot loss = 4.756844824552536, l1: 0.00010465306568221421, l2: 0.00037103141912666617   Iteration 41 of 100, tot loss = 4.8315092237984265, l1: 0.00010571822722118756, l2: 0.00037743269697274647   Iteration 42 of 100, tot loss = 4.834067225456238, l1: 0.00010578667137278466, l2: 0.0003776200523827269   Iteration 43 of 100, tot loss = 4.835240691207176, l1: 0.00010580902152321149, l2: 0.0003777150491788594   Iteration 44 of 100, tot loss = 4.805627817457372, l1: 0.00010525124542420434, l2: 0.00037531153760607015   Iteration 45 of 100, tot loss = 4.838169399897257, l1: 0.00010629839493453296, l2: 0.00037751854623719634   Iteration 46 of 100, tot loss = 4.852594328963238, l1: 0.00010654108447614935, l2: 0.0003787183494474131   Iteration 47 of 100, tot loss = 4.836790039184246, l1: 0.00010659896237329837, l2: 0.00037708004256660873   Iteration 48 of 100, tot loss = 4.8412327617406845, l1: 0.00010670514575394918, l2: 0.0003774181310897499   Iteration 49 of 100, tot loss = 4.847037398085302, l1: 0.00010690837653059208, l2: 0.0003777953647005809   Iteration 50 of 100, tot loss = 4.874894585609436, l1: 0.00010739855468273163, l2: 0.00038009090494597333   Iteration 51 of 100, tot loss = 4.858063665090823, l1: 0.0001070143433239804, l2: 0.00037879202436135313   Iteration 52 of 100, tot loss = 4.831427303644327, l1: 0.00010670670837078852, l2: 0.00037643602291399683   Iteration 53 of 100, tot loss = 4.8846801137024505, l1: 0.00010707504256026609, l2: 0.0003813929690535725   Iteration 54 of 100, tot loss = 4.863701714409722, l1: 0.00010690249648716956, l2: 0.00037946767541278086   Iteration 55 of 100, tot loss = 4.89192685213956, l1: 0.00010710085638989272, l2: 0.00038209182914050125   Iteration 56 of 100, tot loss = 4.85902892266001, l1: 0.00010697828007063695, l2: 0.00037892461270010766   Iteration 57 of 100, tot loss = 4.8464189788751435, l1: 0.00010684077253181226, l2: 0.00037780112593042617   Iteration 58 of 100, tot loss = 4.848468891505537, l1: 0.00010642895107719533, l2: 0.00037841793854223116   Iteration 59 of 100, tot loss = 4.831496671094733, l1: 0.00010625359486843759, l2: 0.0003768960729249264   Iteration 60 of 100, tot loss = 4.80555560986201, l1: 0.00010566441245221844, l2: 0.00037489114959801857   Iteration 61 of 100, tot loss = 4.780687410323346, l1: 0.00010497353245317172, l2: 0.00037309520956144105   Iteration 62 of 100, tot loss = 4.805727089605024, l1: 0.00010532548901298264, l2: 0.0003752472210532775   Iteration 63 of 100, tot loss = 4.812025766524058, l1: 0.0001055596011509705, l2: 0.00037564297663121826   Iteration 64 of 100, tot loss = 4.807570748031139, l1: 0.00010577852799542597, l2: 0.00037497854782486684   Iteration 65 of 100, tot loss = 4.825672721862793, l1: 0.00010577972246280226, l2: 0.0003767875511235056   Iteration 66 of 100, tot loss = 4.839311932072495, l1: 0.00010621618882093269, l2: 0.00037771500655887365   Iteration 67 of 100, tot loss = 4.86207579854709, l1: 0.00010689211574490808, l2: 0.0003793154662447189   Iteration 68 of 100, tot loss = 4.8760241971296425, l1: 0.00010710945111093358, l2: 0.0003804929711965515   Iteration 69 of 100, tot loss = 4.852925017260123, l1: 0.00010695606254992764, l2: 0.00037833644170820225   Iteration 70 of 100, tot loss = 4.8360305547714235, l1: 0.00010660881692144487, l2: 0.0003769942411703856   Iteration 71 of 100, tot loss = 4.836725775624664, l1: 0.00010665254386610837, l2: 0.00037702003598022996   Iteration 72 of 100, tot loss = 4.906919045580758, l1: 0.00010778239862298101, l2: 0.00038290950821343966   Iteration 73 of 100, tot loss = 4.879147114819044, l1: 0.00010718462258347705, l2: 0.00038073009138365517   Iteration 74 of 100, tot loss = 4.879100538588859, l1: 0.00010723058496935394, l2: 0.0003806794718272883   Iteration 75 of 100, tot loss = 4.88426251411438, l1: 0.00010715658174982915, l2: 0.00038126967226465545   Iteration 76 of 100, tot loss = 4.864185897927535, l1: 0.00010676377563581092, l2: 0.00037965481667350486   Iteration 77 of 100, tot loss = 4.8407018184661865, l1: 0.00010643572117165699, l2: 0.00037763446328296723   Iteration 78 of 100, tot loss = 4.840952772360581, l1: 0.00010639000594472655, l2: 0.0003777052738885873   Iteration 79 of 100, tot loss = 4.895914980127841, l1: 0.00010731954571844043, l2: 0.0003822719550446925   Iteration 80 of 100, tot loss = 4.86939851641655, l1: 0.00010658249480002268, l2: 0.00038035735960875174   Iteration 81 of 100, tot loss = 4.872141749770553, l1: 0.00010686896393401836, l2: 0.0003803452138392324   Iteration 82 of 100, tot loss = 4.895022124778934, l1: 0.0001074289288581207, l2: 0.00038207328641253364   Iteration 83 of 100, tot loss = 4.929149926426899, l1: 0.00010797052663654168, l2: 0.0003849444689975311   Iteration 84 of 100, tot loss = 4.906882098742893, l1: 0.00010758862736977226, l2: 0.00038309958541677113   Iteration 85 of 100, tot loss = 4.880635297999662, l1: 0.00010717954141642515, l2: 0.0003808839910882799   Iteration 86 of 100, tot loss = 4.879687539366788, l1: 0.0001067000034391788, l2: 0.00038126875344544734   Iteration 87 of 100, tot loss = 4.871638887230007, l1: 0.0001062692044588507, l2: 0.0003808946873933805   Iteration 88 of 100, tot loss = 4.862708110700954, l1: 0.00010609715044748472, l2: 0.0003801736638706643   Iteration 89 of 100, tot loss = 4.860384397292405, l1: 0.0001062426104663552, l2: 0.00037979583250714486   Iteration 90 of 100, tot loss = 4.847577381134033, l1: 0.00010614782800581048, l2: 0.00037860991336250056   Iteration 91 of 100, tot loss = 4.851454850081559, l1: 0.00010622785191446962, l2: 0.0003789176362960671   Iteration 92 of 100, tot loss = 4.8566964864730835, l1: 0.00010664011140603259, l2: 0.00037902954088918784   Iteration 93 of 100, tot loss = 4.84169316035445, l1: 0.00010615416073461904, l2: 0.00037801515884316897   Iteration 94 of 100, tot loss = 4.838923418775518, l1: 0.00010637590724961247, l2: 0.0003775164380947009   Iteration 95 of 100, tot loss = 4.8184405577810185, l1: 0.00010593647545503795, l2: 0.00037590758369524815   Iteration 96 of 100, tot loss = 4.80774799734354, l1: 0.00010581292588085489, l2: 0.0003749618770901482   Iteration 97 of 100, tot loss = 4.803566050283687, l1: 0.00010573335947407668, l2: 0.00037462324862363604   Iteration 98 of 100, tot loss = 4.802452050909704, l1: 0.00010583977486281324, l2: 0.00037440543323731507   Iteration 99 of 100, tot loss = 4.7847384057863795, l1: 0.00010554485348308449, l2: 0.00037292899011817735   Iteration 100 of 100, tot loss = 4.792507123947144, l1: 0.00010590476409561234, l2: 0.000373345951520605
   End of epoch 1200; saving model... 

Epoch 1201 of 2000
   Iteration 1 of 100, tot loss = 4.820450305938721, l1: 7.485481182811782e-05, l2: 0.00040719020762480795   Iteration 2 of 100, tot loss = 4.724663257598877, l1: 8.956069723353721e-05, l2: 0.0003829056367976591   Iteration 3 of 100, tot loss = 4.756561915079753, l1: 0.00010092894808622077, l2: 0.00037472725186186534   Iteration 4 of 100, tot loss = 5.583984851837158, l1: 0.00011901275684067514, l2: 0.0004393857452669181   Iteration 5 of 100, tot loss = 4.86948528289795, l1: 0.00010463834842084907, l2: 0.0003823101927991956   Iteration 6 of 100, tot loss = 4.8627370198567705, l1: 0.00010433422661056586, l2: 0.00038193948663926375   Iteration 7 of 100, tot loss = 4.848904405321393, l1: 0.00010500188311977711, l2: 0.0003798885695037565   Iteration 8 of 100, tot loss = 4.741668611764908, l1: 0.00010264554794048308, l2: 0.00037152132426854223   Iteration 9 of 100, tot loss = 4.7767184310489235, l1: 0.00010275782106974575, l2: 0.0003749140305444598   Iteration 10 of 100, tot loss = 4.828254580497742, l1: 0.00010376733662269544, l2: 0.00037905812787357717   Iteration 11 of 100, tot loss = 4.69108789617365, l1: 0.00010274023191992786, l2: 0.00036636856516865504   Iteration 12 of 100, tot loss = 4.889268080393474, l1: 0.00010542958837807721, l2: 0.0003834972267213743   Iteration 13 of 100, tot loss = 4.875135201674241, l1: 0.00010397470620004102, l2: 0.0003835388197330758   Iteration 14 of 100, tot loss = 4.728347999708993, l1: 0.00010038664394025025, l2: 0.0003724481606955773   Iteration 15 of 100, tot loss = 4.773121150334676, l1: 0.00010005223011830822, l2: 0.0003772598885310193   Iteration 16 of 100, tot loss = 4.709484398365021, l1: 9.855942971626064e-05, l2: 0.00037238901313685346   Iteration 17 of 100, tot loss = 4.716182596543256, l1: 9.812855783178854e-05, l2: 0.00037348970620180755   Iteration 18 of 100, tot loss = 4.689437362882826, l1: 9.783943136830608e-05, l2: 0.0003711043100338429   Iteration 19 of 100, tot loss = 4.607571074837132, l1: 9.735978605213429e-05, l2: 0.00036339732545648555   Iteration 20 of 100, tot loss = 4.814082503318787, l1: 0.00010016015985456762, l2: 0.00038124809725559317   Iteration 21 of 100, tot loss = 4.784586565835135, l1: 0.00010029479155838046, l2: 0.00037816387166025205   Iteration 22 of 100, tot loss = 4.7363526387648145, l1: 9.923413678982548e-05, l2: 0.0003744011330788701   Iteration 23 of 100, tot loss = 4.704206093497898, l1: 9.791533628468524e-05, l2: 0.00037250527922246283   Iteration 24 of 100, tot loss = 4.664296845595042, l1: 9.839143088659814e-05, l2: 0.0003680382596940035   Iteration 25 of 100, tot loss = 4.805660839080811, l1: 0.00010141201986698434, l2: 0.0003791540692327544   Iteration 26 of 100, tot loss = 4.783488585398747, l1: 0.00010070166647962581, l2: 0.0003776471975680369   Iteration 27 of 100, tot loss = 4.708871523539226, l1: 9.93759059821497e-05, l2: 0.00037151125110944524   Iteration 28 of 100, tot loss = 4.750273261751447, l1: 0.00010061623626305456, l2: 0.00037441109400658334   Iteration 29 of 100, tot loss = 4.6722971981969375, l1: 9.950502160414732e-05, l2: 0.0003677247024887916   Iteration 30 of 100, tot loss = 4.648361070950826, l1: 9.869728528428822e-05, l2: 0.0003661388264542135   Iteration 31 of 100, tot loss = 4.583006758843699, l1: 9.730057096138836e-05, l2: 0.0003610001093024508   Iteration 32 of 100, tot loss = 4.563425689935684, l1: 9.745860052134958e-05, l2: 0.0003588839731492044   Iteration 33 of 100, tot loss = 4.477182099313447, l1: 9.604679430732645e-05, l2: 0.0003516714200624406   Iteration 34 of 100, tot loss = 4.4594160528743965, l1: 9.553365185531541e-05, l2: 0.0003504079577396624   Iteration 35 of 100, tot loss = 4.42546135357448, l1: 9.554236239637249e-05, l2: 0.0003470037772785872   Iteration 36 of 100, tot loss = 4.497620390521155, l1: 9.645872998791876e-05, l2: 0.00035330331350754324   Iteration 37 of 100, tot loss = 4.562789330611357, l1: 9.756432730166518e-05, l2: 0.00035871460961454824   Iteration 38 of 100, tot loss = 4.563375065201207, l1: 9.728668575055628e-05, l2: 0.0003590508246193885   Iteration 39 of 100, tot loss = 4.5791392754285765, l1: 9.754638831975918e-05, l2: 0.00036036754280734714   Iteration 40 of 100, tot loss = 4.587855178117752, l1: 9.805332438190817e-05, l2: 0.00036073219671379776   Iteration 41 of 100, tot loss = 4.594760795918907, l1: 9.796877646117425e-05, l2: 0.00036150730632404545   Iteration 42 of 100, tot loss = 4.586020622934614, l1: 9.75823695625877e-05, l2: 0.0003610196962697609   Iteration 43 of 100, tot loss = 4.58108477814253, l1: 9.801625746212965e-05, l2: 0.00036009222399010214   Iteration 44 of 100, tot loss = 4.6224135756492615, l1: 9.865195360444804e-05, l2: 0.0003635894073258069   Iteration 45 of 100, tot loss = 4.622312360339695, l1: 9.925576119308567e-05, l2: 0.00036297547824991246   Iteration 46 of 100, tot loss = 4.614920237789983, l1: 9.934467198036145e-05, l2: 0.0003621473550320724   Iteration 47 of 100, tot loss = 4.659425212981853, l1: 9.996984397033923e-05, l2: 0.000365972680540042   Iteration 48 of 100, tot loss = 4.62204463283221, l1: 9.939338656295149e-05, l2: 0.00036281108017040725   Iteration 49 of 100, tot loss = 4.689874600390999, l1: 9.997233762393934e-05, l2: 0.00036901512601393826   Iteration 50 of 100, tot loss = 4.6929801559448245, l1: 9.969371247279923e-05, l2: 0.0003696043067611754   Iteration 51 of 100, tot loss = 4.676381447735955, l1: 9.893733580361632e-05, l2: 0.0003687008124246609   Iteration 52 of 100, tot loss = 4.6615170790598945, l1: 9.910669184766727e-05, l2: 0.0003670450191398581   Iteration 53 of 100, tot loss = 4.653656995521401, l1: 9.899124999273622e-05, l2: 0.0003663744521546968   Iteration 54 of 100, tot loss = 4.679082296512745, l1: 0.00010000267792309858, l2: 0.0003679055538399283   Iteration 55 of 100, tot loss = 4.684977453405207, l1: 0.00010010655883392742, l2: 0.0003683911879885603   Iteration 56 of 100, tot loss = 4.6928898351533075, l1: 0.00010039885020367885, l2: 0.0003688901347881516   Iteration 57 of 100, tot loss = 4.725328596014726, l1: 0.00010084379149096397, l2: 0.00037168906930131477   Iteration 58 of 100, tot loss = 4.757578742915187, l1: 0.00010137410127158389, l2: 0.0003743837743090337   Iteration 59 of 100, tot loss = 4.798471846822965, l1: 0.00010239062032916868, l2: 0.0003774565653441379   Iteration 60 of 100, tot loss = 4.825264390309652, l1: 0.00010291592898283852, l2: 0.00037961051129968836   Iteration 61 of 100, tot loss = 4.851664722942915, l1: 0.0001035295363031824, l2: 0.00038163693761071344   Iteration 62 of 100, tot loss = 4.848765873139905, l1: 0.0001034576074626755, l2: 0.00038141898121759894   Iteration 63 of 100, tot loss = 4.880475044250488, l1: 0.00010359643685718304, l2: 0.000384451068904517   Iteration 64 of 100, tot loss = 4.872339583933353, l1: 0.00010367748365069929, l2: 0.00038355647620846867   Iteration 65 of 100, tot loss = 4.851254067054161, l1: 0.00010283038601091203, l2: 0.00038229502209175664   Iteration 66 of 100, tot loss = 4.860477844874064, l1: 0.00010303287969939198, l2: 0.0003830149062471746   Iteration 67 of 100, tot loss = 4.858525902477663, l1: 0.00010323142427594901, l2: 0.00038262116705387164   Iteration 68 of 100, tot loss = 4.876579067286323, l1: 0.00010371327960670255, l2: 0.0003839446285650518   Iteration 69 of 100, tot loss = 4.870693918587505, l1: 0.00010355614216203558, l2: 0.00038351325117443025   Iteration 70 of 100, tot loss = 4.895548520769392, l1: 0.0001042714035325584, l2: 0.0003852834496813427   Iteration 71 of 100, tot loss = 4.8700471058697765, l1: 0.00010385901346830585, l2: 0.0003831456983084856   Iteration 72 of 100, tot loss = 4.905105286174351, l1: 0.00010402343746641741, l2: 0.0003864870921259151   Iteration 73 of 100, tot loss = 4.891805547557465, l1: 0.00010380532634596035, l2: 0.00038537522957322174   Iteration 74 of 100, tot loss = 4.8701869635968595, l1: 0.00010329139297175209, l2: 0.00038372730459495624   Iteration 75 of 100, tot loss = 4.908106578191122, l1: 0.00010373005027455898, l2: 0.0003870806092163548   Iteration 76 of 100, tot loss = 4.922701085868635, l1: 0.0001039441984080201, l2: 0.0003883259121625758   Iteration 77 of 100, tot loss = 4.909454868985461, l1: 0.00010374702658548092, l2: 0.0003871984623732384   Iteration 78 of 100, tot loss = 4.89332592793, l1: 0.000103841127127406, l2: 0.0003854914675451791   Iteration 79 of 100, tot loss = 4.8815495484991915, l1: 0.00010406193615392712, l2: 0.0003840930207670796   Iteration 80 of 100, tot loss = 4.874197885394096, l1: 0.00010430212191749887, l2: 0.000383117668570776   Iteration 81 of 100, tot loss = 4.895215714419329, l1: 0.00010457977690815892, l2: 0.00038494179646892724   Iteration 82 of 100, tot loss = 4.904590010643005, l1: 0.00010493336634592596, l2: 0.0003855256365524128   Iteration 83 of 100, tot loss = 4.93646886262549, l1: 0.00010527051721050134, l2: 0.00038837637076945416   Iteration 84 of 100, tot loss = 4.944857009819576, l1: 0.00010540324213555625, l2: 0.0003890824607099473   Iteration 85 of 100, tot loss = 4.923572963826796, l1: 0.00010473888584837207, l2: 0.00038761841218836385   Iteration 86 of 100, tot loss = 4.9332593956659005, l1: 0.00010460668416398707, l2: 0.0003887192570525871   Iteration 87 of 100, tot loss = 4.928024881187526, l1: 0.00010468204408394301, l2: 0.00038812044541613767   Iteration 88 of 100, tot loss = 4.9115397117354656, l1: 0.00010444973370547153, l2: 0.00038670423881822814   Iteration 89 of 100, tot loss = 4.901898619833957, l1: 0.00010459494014765041, l2: 0.00038559492327550196   Iteration 90 of 100, tot loss = 4.9185069931877985, l1: 0.00010477869695427621, l2: 0.0003870720038927781   Iteration 91 of 100, tot loss = 4.901602986094716, l1: 0.00010451832413609437, l2: 0.0003856419760588004   Iteration 92 of 100, tot loss = 4.912232222764389, l1: 0.00010469170339764905, l2: 0.0003865315205906012   Iteration 93 of 100, tot loss = 4.887386537367298, l1: 0.00010405770734250446, l2: 0.0003846809482364665   Iteration 94 of 100, tot loss = 4.91755048772122, l1: 0.00010459690159397259, l2: 0.0003871581490223415   Iteration 95 of 100, tot loss = 4.9316448613217005, l1: 0.00010468097867775022, l2: 0.00038848350967256057   Iteration 96 of 100, tot loss = 4.940544342001279, l1: 0.0001046386921643716, l2: 0.0003894157445453554   Iteration 97 of 100, tot loss = 4.919353730899772, l1: 0.00010421133719291058, l2: 0.0003877240386143442   Iteration 98 of 100, tot loss = 4.910710398031741, l1: 0.00010432001014420648, l2: 0.000386751032070903   Iteration 99 of 100, tot loss = 4.886395211171622, l1: 0.00010374497967381783, l2: 0.00038489454362372106   Iteration 100 of 100, tot loss = 4.929236133098602, l1: 0.00010438185287057422, l2: 0.00038854176294989885
   End of epoch 1201; saving model... 

Epoch 1202 of 2000
   Iteration 1 of 100, tot loss = 3.8120594024658203, l1: 9.046929335454479e-05, l2: 0.000290736643364653   Iteration 2 of 100, tot loss = 4.155741930007935, l1: 0.00010431728514959104, l2: 0.0003112569102086127   Iteration 3 of 100, tot loss = 4.58188533782959, l1: 0.00010939414399520804, l2: 0.0003487944001487146   Iteration 4 of 100, tot loss = 5.09766149520874, l1: 0.00011573866140679456, l2: 0.00039402749825967476   Iteration 5 of 100, tot loss = 5.23690938949585, l1: 0.00011721737100742758, l2: 0.0004064735723659396   Iteration 6 of 100, tot loss = 5.183216333389282, l1: 0.00011989819176960737, l2: 0.00039842344995122403   Iteration 7 of 100, tot loss = 4.969291482652936, l1: 0.00011412381718400866, l2: 0.00038280533460368005   Iteration 8 of 100, tot loss = 5.110862910747528, l1: 0.00011748026736313477, l2: 0.00039360603113891557   Iteration 9 of 100, tot loss = 4.980451186498006, l1: 0.00011491548907037618, l2: 0.0003831296376625283   Iteration 10 of 100, tot loss = 5.109160590171814, l1: 0.00011437899302109144, l2: 0.00039653707353863864   Iteration 11 of 100, tot loss = 5.082230979746038, l1: 0.00011632251037305899, l2: 0.00039190059231424874   Iteration 12 of 100, tot loss = 5.191861569881439, l1: 0.00011829160151440495, l2: 0.0004008945591825371   Iteration 13 of 100, tot loss = 5.237046003341675, l1: 0.00011807781881473672, l2: 0.00040562678791152744   Iteration 14 of 100, tot loss = 5.209339124815805, l1: 0.00011826979555605379, l2: 0.0004026641212736389   Iteration 15 of 100, tot loss = 5.052088769276937, l1: 0.00011383771779946983, l2: 0.0003913711630351221   Iteration 16 of 100, tot loss = 5.117375552654266, l1: 0.00011531101336004212, l2: 0.00039642654610361205   Iteration 17 of 100, tot loss = 5.104892730712891, l1: 0.0001147597619873362, l2: 0.00039572951551663745   Iteration 18 of 100, tot loss = 5.206709464391072, l1: 0.00011707598645848015, l2: 0.0004035949669843022   Iteration 19 of 100, tot loss = 5.4333320667869165, l1: 0.00012032434116056385, l2: 0.0004230088739668166   Iteration 20 of 100, tot loss = 5.381518697738647, l1: 0.00011980676317762118, l2: 0.00041834511430351997   Iteration 21 of 100, tot loss = 5.245208388283139, l1: 0.00011747206528005856, l2: 0.00040704878171839354   Iteration 22 of 100, tot loss = 5.3050888993523335, l1: 0.00011737240889818746, l2: 0.0004131364886151542   Iteration 23 of 100, tot loss = 5.287366504254549, l1: 0.00011593818983427532, l2: 0.00041279846703653914   Iteration 24 of 100, tot loss = 5.266061335802078, l1: 0.00011681187121818463, l2: 0.0004097942686106156   Iteration 25 of 100, tot loss = 5.303293142318726, l1: 0.00011768591357395053, l2: 0.00041264340456109494   Iteration 26 of 100, tot loss = 5.236262138073261, l1: 0.00011562764652458449, l2: 0.00040799857043356705   Iteration 27 of 100, tot loss = 5.4126565721299915, l1: 0.00011772236811574894, l2: 0.0004235432904722445   Iteration 28 of 100, tot loss = 5.413656370980399, l1: 0.00011719052885642409, l2: 0.00042417510901162004   Iteration 29 of 100, tot loss = 5.331351082900475, l1: 0.00011620679940498466, l2: 0.00041692831015599697   Iteration 30 of 100, tot loss = 5.352306524912517, l1: 0.00011539725504311112, l2: 0.00041983339857930936   Iteration 31 of 100, tot loss = 5.293454270209035, l1: 0.00011364458013333441, l2: 0.00041570084830445627   Iteration 32 of 100, tot loss = 5.309421323239803, l1: 0.0001135427523877297, l2: 0.00041739938114915276   Iteration 33 of 100, tot loss = 5.269056912624475, l1: 0.0001121214544665153, l2: 0.00041478423835625023   Iteration 34 of 100, tot loss = 5.199508091982673, l1: 0.00011096376243478837, l2: 0.00040898704858537875   Iteration 35 of 100, tot loss = 5.163699054718018, l1: 0.00010968032667213785, l2: 0.0004066895805798205   Iteration 36 of 100, tot loss = 5.097963763607873, l1: 0.00010882685374882486, l2: 0.0004009695237780559   Iteration 37 of 100, tot loss = 5.073463691247476, l1: 0.00010816261803139807, l2: 0.0003991837513387656   Iteration 38 of 100, tot loss = 5.081237623566075, l1: 0.00010844334974535741, l2: 0.0003996804126042039   Iteration 39 of 100, tot loss = 5.017291967685406, l1: 0.00010758124139661399, l2: 0.0003941479554692379   Iteration 40 of 100, tot loss = 5.0053991138935086, l1: 0.00010676555248210207, l2: 0.0003937743585993303   Iteration 41 of 100, tot loss = 4.9756568990102625, l1: 0.00010612427871731087, l2: 0.0003914414105011241   Iteration 42 of 100, tot loss = 4.95441210269928, l1: 0.00010625141762180387, l2: 0.00038918979165221873   Iteration 43 of 100, tot loss = 4.943844102149786, l1: 0.00010626292486301471, l2: 0.00038812148518762874   Iteration 44 of 100, tot loss = 4.951189349998128, l1: 0.00010643850096543743, l2: 0.0003886804337195248   Iteration 45 of 100, tot loss = 4.927965105904473, l1: 0.00010607276765060508, l2: 0.00038672374309195825   Iteration 46 of 100, tot loss = 4.944404244422913, l1: 0.00010581330286146587, l2: 0.0003886271224473603   Iteration 47 of 100, tot loss = 4.927677372668652, l1: 0.0001059544597507177, l2: 0.00038681327807469333   Iteration 48 of 100, tot loss = 4.94001420835654, l1: 0.00010595192058341733, l2: 0.0003880495008464398   Iteration 49 of 100, tot loss = 4.9463658673422675, l1: 0.00010613722005640442, l2: 0.0003884993674713472   Iteration 50 of 100, tot loss = 4.930480980873108, l1: 0.0001056890191102866, l2: 0.000387359079613816   Iteration 51 of 100, tot loss = 4.903799856410307, l1: 0.00010553147976501278, l2: 0.0003848485063702124   Iteration 52 of 100, tot loss = 4.886136829853058, l1: 0.0001054947356001671, l2: 0.00038311894800594577   Iteration 53 of 100, tot loss = 4.8884982657882405, l1: 0.00010544639783647824, l2: 0.00038340342981204884   Iteration 54 of 100, tot loss = 4.893176780806647, l1: 0.00010566338446719951, l2: 0.0003836542943005312   Iteration 55 of 100, tot loss = 4.870456314086914, l1: 0.00010547954443609341, l2: 0.0003815660874930803   Iteration 56 of 100, tot loss = 4.86135253735951, l1: 0.00010481860122776457, l2: 0.0003813166526275121   Iteration 57 of 100, tot loss = 4.821449923933598, l1: 0.0001042378166937596, l2: 0.0003779071756932688   Iteration 58 of 100, tot loss = 4.780864596366882, l1: 0.00010352095780725558, l2: 0.0003745655021389368   Iteration 59 of 100, tot loss = 4.78749360472469, l1: 0.00010378989815254206, l2: 0.0003749594626103733   Iteration 60 of 100, tot loss = 4.792932418982188, l1: 0.00010427924741331177, l2: 0.0003750139944410572   Iteration 61 of 100, tot loss = 4.767065767382012, l1: 0.00010386337674047309, l2: 0.00037284319997062814   Iteration 62 of 100, tot loss = 4.763118097859044, l1: 0.00010387806903435699, l2: 0.0003724337410887763   Iteration 63 of 100, tot loss = 4.761978550562783, l1: 0.00010367853581787102, l2: 0.00037251931933006123   Iteration 64 of 100, tot loss = 4.774049066007137, l1: 0.0001040335782818147, l2: 0.0003733713288056606   Iteration 65 of 100, tot loss = 4.761053961973924, l1: 0.00010374622797826305, l2: 0.000372359168697865   Iteration 66 of 100, tot loss = 4.770931095787973, l1: 0.00010374633028263678, l2: 0.00037334677957308787   Iteration 67 of 100, tot loss = 4.744217716046234, l1: 0.00010310398164307768, l2: 0.00037131779032252007   Iteration 68 of 100, tot loss = 4.746888286927167, l1: 0.00010332400260642102, l2: 0.0003713648259102175   Iteration 69 of 100, tot loss = 4.766770252283068, l1: 0.00010352484573756911, l2: 0.00037315217976330143   Iteration 70 of 100, tot loss = 4.796353966849191, l1: 0.00010381890975362954, l2: 0.0003758164869006058   Iteration 71 of 100, tot loss = 4.828745868844046, l1: 0.00010435406152859107, l2: 0.00037852052536065046   Iteration 72 of 100, tot loss = 4.827385617627038, l1: 0.00010473429549367413, l2: 0.00037800426657162863   Iteration 73 of 100, tot loss = 4.861438437683941, l1: 0.00010529118054825494, l2: 0.0003808526633616715   Iteration 74 of 100, tot loss = 4.8623921033498405, l1: 0.00010564403207870389, l2: 0.000380595178285148   Iteration 75 of 100, tot loss = 4.863499673207601, l1: 0.00010579993225595293, l2: 0.0003805500348486627   Iteration 76 of 100, tot loss = 4.886267599306609, l1: 0.00010607347705540873, l2: 0.0003825532830409142   Iteration 77 of 100, tot loss = 4.87369119966185, l1: 0.00010603197256682211, l2: 0.0003813371474646008   Iteration 78 of 100, tot loss = 4.859636774429908, l1: 0.00010588640067838311, l2: 0.00038007727711467456   Iteration 79 of 100, tot loss = 4.881487070759641, l1: 0.00010649958429998082, l2: 0.0003816491234944426   Iteration 80 of 100, tot loss = 4.8709026128053665, l1: 0.00010617970851853898, l2: 0.0003809105533946422   Iteration 81 of 100, tot loss = 4.878078157519117, l1: 0.00010618430099350883, l2: 0.00038162351533465867   Iteration 82 of 100, tot loss = 4.85633573299501, l1: 0.00010581978854244836, l2: 0.00037981378533536705   Iteration 83 of 100, tot loss = 4.831907559590167, l1: 0.00010553945352271458, l2: 0.00037765130295463084   Iteration 84 of 100, tot loss = 4.864106672150748, l1: 0.00010577651322646056, l2: 0.00038063415455066467   Iteration 85 of 100, tot loss = 4.858895997440114, l1: 0.0001055366087016677, l2: 0.0003803529917015968   Iteration 86 of 100, tot loss = 4.869171735852263, l1: 0.00010600030488371145, l2: 0.0003809168695223106   Iteration 87 of 100, tot loss = 4.860211164101787, l1: 0.00010544348555120597, l2: 0.000380577631602492   Iteration 88 of 100, tot loss = 4.878376668149775, l1: 0.00010573543696326934, l2: 0.00038210223051878114   Iteration 89 of 100, tot loss = 4.864535224571656, l1: 0.00010515020195076901, l2: 0.000381303321324247   Iteration 90 of 100, tot loss = 4.878575955496894, l1: 0.00010554210396852189, l2: 0.0003823154922833459   Iteration 91 of 100, tot loss = 4.859354362382994, l1: 0.0001054246743848665, l2: 0.0003805107623850415   Iteration 92 of 100, tot loss = 4.838492613771687, l1: 0.00010483369030977053, l2: 0.0003790155716832606   Iteration 93 of 100, tot loss = 4.81346369558765, l1: 0.00010456121980427934, l2: 0.0003767851503848809   Iteration 94 of 100, tot loss = 4.783184930365136, l1: 0.00010417304207642682, l2: 0.0003741454516359149   Iteration 95 of 100, tot loss = 4.766249974150407, l1: 0.00010417336146163411, l2: 0.0003724516367870628   Iteration 96 of 100, tot loss = 4.764498855918646, l1: 0.0001042166517587854, l2: 0.00037223323442958645   Iteration 97 of 100, tot loss = 4.7496882111755845, l1: 0.00010383517310964231, l2: 0.0003711336486452   Iteration 98 of 100, tot loss = 4.749652506137381, l1: 0.00010384217655103254, l2: 0.000371123074431552   Iteration 99 of 100, tot loss = 4.766525638223898, l1: 0.00010423653124363603, l2: 0.0003724160330485308   Iteration 100 of 100, tot loss = 4.78686533331871, l1: 0.00010469273118360434, l2: 0.0003739938030776102
   End of epoch 1202; saving model... 

Epoch 1203 of 2000
   Iteration 1 of 100, tot loss = 2.1747491359710693, l1: 5.068386235507205e-05, l2: 0.0001667910546530038   Iteration 2 of 100, tot loss = 2.698145270347595, l1: 7.00930213497486e-05, l2: 0.00019972150039393455   Iteration 3 of 100, tot loss = 3.315948247909546, l1: 8.192834502551705e-05, l2: 0.0002496664722760518   Iteration 4 of 100, tot loss = 3.1174229979515076, l1: 7.995913802005816e-05, l2: 0.0002317831531399861   Iteration 5 of 100, tot loss = 3.3054272174835204, l1: 8.291981503134593e-05, l2: 0.00024762290413491426   Iteration 6 of 100, tot loss = 3.3758832216262817, l1: 8.75628502399195e-05, l2: 0.00025002546802473563   Iteration 7 of 100, tot loss = 3.3692340510232106, l1: 8.445125838209475e-05, l2: 0.0002524721452833286   Iteration 8 of 100, tot loss = 3.6884532272815704, l1: 8.881123085302534e-05, l2: 0.0002800340917019639   Iteration 9 of 100, tot loss = 3.9370632436540394, l1: 9.468103639341684e-05, l2: 0.0002990252881621321   Iteration 10 of 100, tot loss = 4.046294093132019, l1: 9.411457358510233e-05, l2: 0.0003105148352915421   Iteration 11 of 100, tot loss = 4.018697110089389, l1: 9.442486241989008e-05, l2: 0.0003074448481625454   Iteration 12 of 100, tot loss = 4.15615592400233, l1: 9.573460799098636e-05, l2: 0.00031988098392806325   Iteration 13 of 100, tot loss = 4.226836479627169, l1: 9.737065892505388e-05, l2: 0.0003253129901937567   Iteration 14 of 100, tot loss = 4.211520825113569, l1: 9.754877412758236e-05, l2: 0.0003236033101399828   Iteration 15 of 100, tot loss = 4.143247175216675, l1: 9.688240679679438e-05, l2: 0.0003174423114008581   Iteration 16 of 100, tot loss = 4.121792688965797, l1: 9.737954087540857e-05, l2: 0.0003147997294945526   Iteration 17 of 100, tot loss = 4.142762759152581, l1: 9.812885143807816e-05, l2: 0.0003161474245751057   Iteration 18 of 100, tot loss = 4.099307590060764, l1: 9.608842730004754e-05, l2: 0.00031384233201000217   Iteration 19 of 100, tot loss = 4.113710579119231, l1: 9.741207108354981e-05, l2: 0.0003139589871831336   Iteration 20 of 100, tot loss = 4.111809873580933, l1: 9.827864305407274e-05, l2: 0.00031290234401240016   Iteration 21 of 100, tot loss = 4.1431047802879695, l1: 9.858077863186953e-05, l2: 0.0003157296998237836   Iteration 22 of 100, tot loss = 4.127817901698026, l1: 9.84057273440571e-05, l2: 0.0003143760636290112   Iteration 23 of 100, tot loss = 4.055646771970003, l1: 9.647199856218599e-05, l2: 0.00030909267977973366   Iteration 24 of 100, tot loss = 4.045475383599599, l1: 9.600587206174775e-05, l2: 0.0003085416671334921   Iteration 25 of 100, tot loss = 4.053419437408447, l1: 9.624266676837579e-05, l2: 0.000309099277947098   Iteration 26 of 100, tot loss = 4.061081060996423, l1: 9.617886294458563e-05, l2: 0.00030992924271581264   Iteration 27 of 100, tot loss = 3.980608419135765, l1: 9.427897115259569e-05, l2: 0.0003037818699980293   Iteration 28 of 100, tot loss = 4.015515199729374, l1: 9.441749755621589e-05, l2: 0.0003071340215683449   Iteration 29 of 100, tot loss = 4.07794921973656, l1: 9.525966504889797e-05, l2: 0.00031253525632000045   Iteration 30 of 100, tot loss = 4.135739350318909, l1: 9.674426631439322e-05, l2: 0.00031682966849378623   Iteration 31 of 100, tot loss = 4.2132249724480415, l1: 9.80959865104993e-05, l2: 0.00032322650968713025   Iteration 32 of 100, tot loss = 4.3327570632100105, l1: 9.963808031443477e-05, l2: 0.0003336376244078565   Iteration 33 of 100, tot loss = 4.3396744222352, l1: 0.00010037486814932588, l2: 0.0003335925730149911   Iteration 34 of 100, tot loss = 4.288713265867794, l1: 9.87599873684196e-05, l2: 0.00033011133799685496   Iteration 35 of 100, tot loss = 4.339467913763864, l1: 9.892052569609535e-05, l2: 0.0003350262638247971   Iteration 36 of 100, tot loss = 4.313374565707313, l1: 9.788276747713098e-05, l2: 0.0003334546870771899   Iteration 37 of 100, tot loss = 4.257429883286759, l1: 9.70393508347066e-05, l2: 0.0003287036355735885   Iteration 38 of 100, tot loss = 4.280160452190199, l1: 9.774274507148412e-05, l2: 0.0003302732978849427   Iteration 39 of 100, tot loss = 4.254533779926789, l1: 9.72893503785599e-05, l2: 0.0003281640253053644   Iteration 40 of 100, tot loss = 4.318886291980744, l1: 9.774121399459545e-05, l2: 0.00033414741192245857   Iteration 41 of 100, tot loss = 4.39137475083514, l1: 9.925398138343221e-05, l2: 0.00033988349037472067   Iteration 42 of 100, tot loss = 4.400672674179077, l1: 9.94628458318489e-05, l2: 0.00034060441802943214   Iteration 43 of 100, tot loss = 4.416802373043327, l1: 9.966133151211516e-05, l2: 0.00034201890162495507   Iteration 44 of 100, tot loss = 4.4007915312593635, l1: 9.942594947047341e-05, l2: 0.00034065319976600057   Iteration 45 of 100, tot loss = 4.417144664128622, l1: 9.96926604582566e-05, l2: 0.0003420218016900536   Iteration 46 of 100, tot loss = 4.42506494211114, l1: 0.00010022972152079961, l2: 0.00034227676919924664   Iteration 47 of 100, tot loss = 4.408284780826975, l1: 0.00010061798813230199, l2: 0.00034021048625792436   Iteration 48 of 100, tot loss = 4.445180878043175, l1: 0.00010094140202454582, l2: 0.000343576681795336   Iteration 49 of 100, tot loss = 4.450145093762145, l1: 0.0001008067661225178, l2: 0.00034420773961904404   Iteration 50 of 100, tot loss = 4.425428595542908, l1: 0.0001005919723684201, l2: 0.0003419508834485896   Iteration 51 of 100, tot loss = 4.441529409558165, l1: 0.00010083629668395802, l2: 0.0003433166406751957   Iteration 52 of 100, tot loss = 4.436953374972711, l1: 0.00010083060155571169, l2: 0.00034286473265428166   Iteration 53 of 100, tot loss = 4.409317160552403, l1: 0.00010026742344258687, l2: 0.00034066428949854354   Iteration 54 of 100, tot loss = 4.386572811338636, l1: 9.949336648181391e-05, l2: 0.00033916391166039157   Iteration 55 of 100, tot loss = 4.354948603023182, l1: 9.879648039879447e-05, l2: 0.0003366983768170361   Iteration 56 of 100, tot loss = 4.355038034064429, l1: 9.908677241453136e-05, l2: 0.0003364170279382961   Iteration 57 of 100, tot loss = 4.345929706305788, l1: 9.911902516410409e-05, l2: 0.0003354739422783288   Iteration 58 of 100, tot loss = 4.366369214551202, l1: 9.961073524679924e-05, l2: 0.00033702618308999605   Iteration 59 of 100, tot loss = 4.346324144783667, l1: 9.92603759109674e-05, l2: 0.0003353720358217735   Iteration 60 of 100, tot loss = 4.373346074422201, l1: 9.992373003721392e-05, l2: 0.00033741087439314774   Iteration 61 of 100, tot loss = 4.401781590258489, l1: 0.00010024692904257566, l2: 0.00033993122728968986   Iteration 62 of 100, tot loss = 4.408656689428514, l1: 0.00010043723077321755, l2: 0.0003404284352361555   Iteration 63 of 100, tot loss = 4.376510608763922, l1: 9.989733810052423e-05, l2: 0.0003377537197795593   Iteration 64 of 100, tot loss = 4.383123379200697, l1: 9.975662453598488e-05, l2: 0.0003385557106412307   Iteration 65 of 100, tot loss = 4.385115634478056, l1: 9.976410645042331e-05, l2: 0.0003387474544489613   Iteration 66 of 100, tot loss = 4.38346423886039, l1: 9.979956214947654e-05, l2: 0.0003385468589840457   Iteration 67 of 100, tot loss = 4.3922563986991765, l1: 0.00010005074836411027, l2: 0.0003391748890211222   Iteration 68 of 100, tot loss = 4.394455681828892, l1: 0.00010039369697230595, l2: 0.00033905186865013093   Iteration 69 of 100, tot loss = 4.393778742223546, l1: 0.00010052521837929237, l2: 0.00033885265356334656   Iteration 70 of 100, tot loss = 4.4499018226351055, l1: 0.0001014699912047945, l2: 0.0003435201884713024   Iteration 71 of 100, tot loss = 4.481001467771933, l1: 0.00010206534824019271, l2: 0.00034603479639692627   Iteration 72 of 100, tot loss = 4.475760218169954, l1: 0.00010198652590689663, l2: 0.0003455894936147767   Iteration 73 of 100, tot loss = 4.500344580166961, l1: 0.00010254140883486412, l2: 0.0003474930469552693   Iteration 74 of 100, tot loss = 4.51034718913001, l1: 0.00010260692923455624, l2: 0.0003484277871934496   Iteration 75 of 100, tot loss = 4.5117419211069745, l1: 0.00010252681357087568, l2: 0.00034864737613437077   Iteration 76 of 100, tot loss = 4.530183468994341, l1: 0.00010293727944755167, l2: 0.0003500810650477529   Iteration 77 of 100, tot loss = 4.504651178013194, l1: 0.00010275300987164624, l2: 0.0003477121057372855   Iteration 78 of 100, tot loss = 4.490431614411183, l1: 0.00010265457034946825, l2: 0.0003463885889057285   Iteration 79 of 100, tot loss = 4.4846403568605835, l1: 0.00010264083198595887, l2: 0.00034582320194768046   Iteration 80 of 100, tot loss = 4.474847853183746, l1: 0.00010238790891889949, l2: 0.0003450968744800775   Iteration 81 of 100, tot loss = 4.49798556316046, l1: 0.00010270921816692952, l2: 0.0003470893362522654   Iteration 82 of 100, tot loss = 4.50604220134456, l1: 0.00010308348921570564, l2: 0.0003475207292755907   Iteration 83 of 100, tot loss = 4.488114328269499, l1: 0.00010294307542947985, l2: 0.00034586835567732845   Iteration 84 of 100, tot loss = 4.458283814645949, l1: 0.00010232473912418798, l2: 0.00034350364079554786   Iteration 85 of 100, tot loss = 4.489744100851171, l1: 0.00010276351933819516, l2: 0.00034621088964152425   Iteration 86 of 100, tot loss = 4.4801162384277164, l1: 0.00010269950541632445, l2: 0.0003453121171332896   Iteration 87 of 100, tot loss = 4.479662285453972, l1: 0.00010244904098371258, l2: 0.0003455171860679556   Iteration 88 of 100, tot loss = 4.503360589796847, l1: 0.00010257185807528334, l2: 0.0003477641988535073   Iteration 89 of 100, tot loss = 4.50113625338908, l1: 0.00010262423043212528, l2: 0.000347489392675271   Iteration 90 of 100, tot loss = 4.484385632144081, l1: 0.0001023955025529075, l2: 0.0003460430585417069   Iteration 91 of 100, tot loss = 4.508855849831969, l1: 0.00010280754349378685, l2: 0.00034807803963853435   Iteration 92 of 100, tot loss = 4.501313459614049, l1: 0.00010277111204745977, l2: 0.00034736023213633376   Iteration 93 of 100, tot loss = 4.503141327570844, l1: 0.00010272673128100153, l2: 0.0003475873998331247   Iteration 94 of 100, tot loss = 4.50704628482778, l1: 0.00010267439830250681, l2: 0.0003480302285095875   Iteration 95 of 100, tot loss = 4.503971055934303, l1: 0.00010260434746190808, l2: 0.00034779275669471213   Iteration 96 of 100, tot loss = 4.519187342375517, l1: 0.0001028211664409658, l2: 0.0003490975665651301   Iteration 97 of 100, tot loss = 4.4973441190326335, l1: 0.00010249896014598918, l2: 0.00034723545051991135   Iteration 98 of 100, tot loss = 4.501047089391825, l1: 0.00010241852719538694, l2: 0.00034768618058237457   Iteration 99 of 100, tot loss = 4.48783598403738, l1: 0.00010217640932879852, l2: 0.0003466071879063178   Iteration 100 of 100, tot loss = 4.481864923238755, l1: 0.00010175493844144512, l2: 0.00034643155260710045
   End of epoch 1203; saving model... 

Epoch 1204 of 2000
   Iteration 1 of 100, tot loss = 4.255710124969482, l1: 5.875406714039855e-05, l2: 0.0003668169374577701   Iteration 2 of 100, tot loss = 3.8858489990234375, l1: 7.01796652720077e-05, l2: 0.0003184052329743281   Iteration 3 of 100, tot loss = 4.400374889373779, l1: 8.166822590283118e-05, l2: 0.0003583692596293986   Iteration 4 of 100, tot loss = 4.928448438644409, l1: 8.663680728204781e-05, l2: 0.00040620802610646933   Iteration 5 of 100, tot loss = 5.116627502441406, l1: 9.830523122218437e-05, l2: 0.00041335750720463695   Iteration 6 of 100, tot loss = 5.030529260635376, l1: 9.393661396946602e-05, l2: 0.0004091162991244346   Iteration 7 of 100, tot loss = 4.976101125989642, l1: 9.62892512948851e-05, l2: 0.0004013208506096687   Iteration 8 of 100, tot loss = 4.87504780292511, l1: 9.779939819054562e-05, l2: 0.0003897053720720578   Iteration 9 of 100, tot loss = 5.362709257337782, l1: 0.00010846264477651048, l2: 0.00042780826657286123   Iteration 10 of 100, tot loss = 5.329843425750733, l1: 0.00010793268775159959, l2: 0.000425051644560881   Iteration 11 of 100, tot loss = 5.423855521462181, l1: 0.00011002740443059751, l2: 0.0004323581365910782   Iteration 12 of 100, tot loss = 5.116238832473755, l1: 0.00010415977976663271, l2: 0.00040746409285929985   Iteration 13 of 100, tot loss = 4.967691880006057, l1: 0.00010333299360354431, l2: 0.0003934361851021934   Iteration 14 of 100, tot loss = 5.101950492177691, l1: 0.00010582299130744235, l2: 0.00040437204838131687   Iteration 15 of 100, tot loss = 4.936716858545939, l1: 0.00010329406480498922, l2: 0.000390377613560607   Iteration 16 of 100, tot loss = 5.056268349289894, l1: 0.00010390554621153569, l2: 0.0004017212813778315   Iteration 17 of 100, tot loss = 4.95554921206306, l1: 0.00010168763251653324, l2: 0.00039386728038901794   Iteration 18 of 100, tot loss = 4.855008310741848, l1: 0.00010061791726911906, l2: 0.0003848829057662644   Iteration 19 of 100, tot loss = 4.808096898229499, l1: 9.962804688257165e-05, l2: 0.00038118163540371155   Iteration 20 of 100, tot loss = 4.7524551272392275, l1: 9.92494928141241e-05, l2: 0.00037599601346300916   Iteration 21 of 100, tot loss = 4.689312015260969, l1: 9.964673777825997e-05, l2: 0.0003692844572166602   Iteration 22 of 100, tot loss = 4.610436634583906, l1: 9.914424679829443e-05, l2: 0.0003618994107290002   Iteration 23 of 100, tot loss = 4.628128300542417, l1: 0.00010004807273103127, l2: 0.0003627647523029262   Iteration 24 of 100, tot loss = 4.537719905376434, l1: 9.79939136414032e-05, l2: 0.0003557780722379296   Iteration 25 of 100, tot loss = 4.531014595031738, l1: 9.775122904102317e-05, l2: 0.0003553502255817875   Iteration 26 of 100, tot loss = 4.5677801462320184, l1: 9.835257668992899e-05, l2: 0.0003584254328206253   Iteration 27 of 100, tot loss = 4.541542927424113, l1: 9.771671641458481e-05, l2: 0.00035643757103200725   Iteration 28 of 100, tot loss = 4.55933963400977, l1: 9.845813331464472e-05, l2: 0.00035747582660405897   Iteration 29 of 100, tot loss = 4.566673895408368, l1: 9.854327862708958e-05, l2: 0.00035812410853144807   Iteration 30 of 100, tot loss = 4.587300181388855, l1: 9.907993480737787e-05, l2: 0.000359650080887756   Iteration 31 of 100, tot loss = 4.53539296888536, l1: 9.803798328052395e-05, l2: 0.000355501311376781   Iteration 32 of 100, tot loss = 4.5033915266394615, l1: 9.73877437218107e-05, l2: 0.0003529514065121475   Iteration 33 of 100, tot loss = 4.564464113929055, l1: 9.858327625393444e-05, l2: 0.0003578631329935778   Iteration 34 of 100, tot loss = 4.546345815939062, l1: 9.780381119258785e-05, l2: 0.00035683076823582216   Iteration 35 of 100, tot loss = 4.683961452756609, l1: 9.969276132843723e-05, l2: 0.00036870338164070353   Iteration 36 of 100, tot loss = 4.682552079359691, l1: 0.00010014975477032649, l2: 0.00036810545093936124   Iteration 37 of 100, tot loss = 4.745930600810695, l1: 0.00010135189842077198, l2: 0.00037324115928149203   Iteration 38 of 100, tot loss = 4.727064251899719, l1: 0.00010109872039863963, l2: 0.0003716077025226121   Iteration 39 of 100, tot loss = 4.696291837936792, l1: 0.00010114493516784233, l2: 0.00036848424627887417   Iteration 40 of 100, tot loss = 4.773309862613678, l1: 0.00010207158402408822, l2: 0.0003752593995159259   Iteration 41 of 100, tot loss = 4.75601337014175, l1: 0.00010233573008984577, l2: 0.00037326560484915515   Iteration 42 of 100, tot loss = 4.814558778490339, l1: 0.0001035031076580275, l2: 0.00037795276806545664   Iteration 43 of 100, tot loss = 4.808057862658833, l1: 0.00010390102270744243, l2: 0.00037690476131311423   Iteration 44 of 100, tot loss = 4.8624420382759785, l1: 0.00010527263533400732, l2: 0.00038097156588760714   Iteration 45 of 100, tot loss = 4.837149635950724, l1: 0.00010486184918489825, l2: 0.0003788531120840667   Iteration 46 of 100, tot loss = 4.798294196958127, l1: 0.0001044362835154575, l2: 0.00037539313400528675   Iteration 47 of 100, tot loss = 4.736625440577243, l1: 0.00010304072279819942, l2: 0.000370621819301766   Iteration 48 of 100, tot loss = 4.765006832778454, l1: 0.00010346883664169582, l2: 0.00037303184520472615   Iteration 49 of 100, tot loss = 4.699627012622599, l1: 0.000102294172541647, l2: 0.00036766852726755967   Iteration 50 of 100, tot loss = 4.700184643268585, l1: 0.00010243418262689374, l2: 0.00036758428075700067   Iteration 51 of 100, tot loss = 4.716295029602799, l1: 0.00010216063331908492, l2: 0.0003694688692157084   Iteration 52 of 100, tot loss = 4.737714898127776, l1: 0.0001020421994046326, l2: 0.00037172929002666095   Iteration 53 of 100, tot loss = 4.729461087370819, l1: 0.00010235089411133163, l2: 0.0003705952141625051   Iteration 54 of 100, tot loss = 4.8020715823879945, l1: 0.0001038898152290602, l2: 0.00037631734273788797   Iteration 55 of 100, tot loss = 4.858479163863442, l1: 0.0001050530340183865, l2: 0.0003807948816260746   Iteration 56 of 100, tot loss = 4.821830253515925, l1: 0.00010454889681048891, l2: 0.0003776341276794223   Iteration 57 of 100, tot loss = 4.808099359796758, l1: 0.00010390831780489207, l2: 0.0003769016175443029   Iteration 58 of 100, tot loss = 4.807673731754566, l1: 0.00010406577310842012, l2: 0.00037670159897594394   Iteration 59 of 100, tot loss = 4.827485421956596, l1: 0.00010450376714165402, l2: 0.0003782447746420137   Iteration 60 of 100, tot loss = 4.803605792919795, l1: 0.00010420888465887402, l2: 0.00037615169433896276   Iteration 61 of 100, tot loss = 4.816572210827812, l1: 0.00010423404658423187, l2: 0.0003774231739893754   Iteration 62 of 100, tot loss = 4.824305663185735, l1: 0.00010436676881997095, l2: 0.00037806379646832694   Iteration 63 of 100, tot loss = 4.825690176751879, l1: 0.00010442040331240388, l2: 0.0003781486128222212   Iteration 64 of 100, tot loss = 4.833114540204406, l1: 0.00010456358199917304, l2: 0.0003787478705135072   Iteration 65 of 100, tot loss = 4.839327619625972, l1: 0.00010440837728450647, l2: 0.00037952438321483964   Iteration 66 of 100, tot loss = 4.835687852267063, l1: 0.00010468155420367663, l2: 0.0003788872300267967   Iteration 67 of 100, tot loss = 4.820226076823562, l1: 0.00010457914311755031, l2: 0.0003774434636710142   Iteration 68 of 100, tot loss = 4.813109767787597, l1: 0.00010454765812456197, l2: 0.0003767633179985055   Iteration 69 of 100, tot loss = 4.783212304115295, l1: 0.00010435204445536289, l2: 0.00037396918533899674   Iteration 70 of 100, tot loss = 4.794273800509316, l1: 0.0001044821173958813, l2: 0.0003749452623846342   Iteration 71 of 100, tot loss = 4.839295758327967, l1: 0.00010531281810139083, l2: 0.00037861675686862086   Iteration 72 of 100, tot loss = 4.813067401448886, l1: 0.0001049531674652826, l2: 0.00037635357158352336   Iteration 73 of 100, tot loss = 4.841085615223402, l1: 0.0001053778954609361, l2: 0.00037873066470986997   Iteration 74 of 100, tot loss = 4.83412235330891, l1: 0.00010547166193892072, l2: 0.00037794057218864483   Iteration 75 of 100, tot loss = 4.839294298489889, l1: 0.00010572794592007995, l2: 0.00037820148252649233   Iteration 76 of 100, tot loss = 4.858274734333942, l1: 0.00010595566843556681, l2: 0.00037987180344077246   Iteration 77 of 100, tot loss = 4.85722697090793, l1: 0.00010613633951704417, l2: 0.0003795863561787304   Iteration 78 of 100, tot loss = 4.8520521980065565, l1: 0.00010557979243085123, l2: 0.00037962542550163704   Iteration 79 of 100, tot loss = 4.848788151258154, l1: 0.00010546378196729936, l2: 0.00037941503122263247   Iteration 80 of 100, tot loss = 4.87907273620367, l1: 0.0001059969359630486, l2: 0.0003819103357272979   Iteration 81 of 100, tot loss = 4.870924859871099, l1: 0.00010586902651904197, l2: 0.00038122345794374584   Iteration 82 of 100, tot loss = 4.83812519253754, l1: 0.00010534120608609533, l2: 0.0003784713116169214   Iteration 83 of 100, tot loss = 4.864879526287677, l1: 0.00010563424614493747, l2: 0.00038085370445525813   Iteration 84 of 100, tot loss = 4.867736826340358, l1: 0.0001057392001668978, l2: 0.0003810344802505348   Iteration 85 of 100, tot loss = 4.89053834326127, l1: 0.00010547965939622372, l2: 0.0003835741725839291   Iteration 86 of 100, tot loss = 4.881064945875212, l1: 0.00010535395665291892, l2: 0.00038275253550531233   Iteration 87 of 100, tot loss = 4.8932409492032285, l1: 0.00010584325196111747, l2: 0.00038348084047032605   Iteration 88 of 100, tot loss = 4.884834374893796, l1: 0.00010569832499062282, l2: 0.0003827851099810108   Iteration 89 of 100, tot loss = 4.881840870621499, l1: 0.00010570894157987486, l2: 0.0003824751426660707   Iteration 90 of 100, tot loss = 4.878651161988576, l1: 0.00010585166099998686, l2: 0.000382013452488334   Iteration 91 of 100, tot loss = 4.855751067727477, l1: 0.00010535925077300027, l2: 0.00038021585319106786   Iteration 92 of 100, tot loss = 4.853060275316238, l1: 0.00010526047544376722, l2: 0.0003800455495207722   Iteration 93 of 100, tot loss = 4.852560852163581, l1: 0.00010545587921965747, l2: 0.00037980020371383374   Iteration 94 of 100, tot loss = 4.835007615545963, l1: 0.00010515287152536094, l2: 0.0003783478878563557   Iteration 95 of 100, tot loss = 4.840641221247221, l1: 0.00010547248285116726, l2: 0.00037859163751004655   Iteration 96 of 100, tot loss = 4.82293385391434, l1: 0.0001053023570420919, l2: 0.00037699102669345547   Iteration 97 of 100, tot loss = 4.801506466472272, l1: 0.00010492292239712359, l2: 0.00037522772265557373   Iteration 98 of 100, tot loss = 4.826277417795999, l1: 0.00010559803172791548, l2: 0.00037702970818214934   Iteration 99 of 100, tot loss = 4.803298322841375, l1: 0.00010527675035656571, l2: 0.0003750530800475231   Iteration 100 of 100, tot loss = 4.807787498235703, l1: 0.0001055520562658785, l2: 0.0003752266917581437
   End of epoch 1204; saving model... 

Epoch 1205 of 2000
   Iteration 1 of 100, tot loss = 5.147802829742432, l1: 9.732446778798476e-05, l2: 0.00041745579801499844   Iteration 2 of 100, tot loss = 4.927708148956299, l1: 0.00011094721776316874, l2: 0.00038182357093319297   Iteration 3 of 100, tot loss = 4.913678963979085, l1: 0.0001101156425041457, l2: 0.00038125223363749683   Iteration 4 of 100, tot loss = 5.001171946525574, l1: 0.00010908800140896346, l2: 0.00039102917799027637   Iteration 5 of 100, tot loss = 5.221460247039795, l1: 0.00011406244448153302, l2: 0.00040808356716297566   Iteration 6 of 100, tot loss = 5.060160954793294, l1: 0.00011332183930790052, l2: 0.00039269424451049417   Iteration 7 of 100, tot loss = 4.884568555014474, l1: 0.0001111845309164242, l2: 0.00037727231392636895   Iteration 8 of 100, tot loss = 5.160282492637634, l1: 0.00011371897835488198, l2: 0.00040230925515061244   Iteration 9 of 100, tot loss = 5.147505813174778, l1: 0.00011206014945249383, l2: 0.0004026904126577493   Iteration 10 of 100, tot loss = 5.076614713668823, l1: 0.0001078468696505297, l2: 0.0003998145868536085   Iteration 11 of 100, tot loss = 4.9813923402266065, l1: 0.00010587007537568834, l2: 0.0003922691447025334   Iteration 12 of 100, tot loss = 4.892344792683919, l1: 0.00010505563356370355, l2: 0.000384178832367373   Iteration 13 of 100, tot loss = 4.8845672974219685, l1: 0.0001051185038075066, l2: 0.000383338212626628   Iteration 14 of 100, tot loss = 4.837214367730277, l1: 0.00010614163745360981, l2: 0.0003775797875797642   Iteration 15 of 100, tot loss = 4.893240197499593, l1: 0.0001074755205384766, l2: 0.0003818484915730854   Iteration 16 of 100, tot loss = 4.94251549243927, l1: 0.0001084857185560395, l2: 0.0003857658230117522   Iteration 17 of 100, tot loss = 4.983781365787282, l1: 0.00010949510718038415, l2: 0.0003888830234406187   Iteration 18 of 100, tot loss = 4.960304339726766, l1: 0.00011019863126825335, l2: 0.0003858317957363195   Iteration 19 of 100, tot loss = 4.989982655173854, l1: 0.00011149649492979638, l2: 0.0003875017619544738   Iteration 20 of 100, tot loss = 4.864330327510833, l1: 0.00010911132267210633, l2: 0.00037732170239905826   Iteration 21 of 100, tot loss = 4.800465061551049, l1: 0.00010827803773628104, l2: 0.0003717684615237106   Iteration 22 of 100, tot loss = 4.814033334905451, l1: 0.00010817388928527097, l2: 0.0003732294393227097   Iteration 23 of 100, tot loss = 4.853492674620255, l1: 0.00010784616366119893, l2: 0.0003775031003422792   Iteration 24 of 100, tot loss = 4.764293144146602, l1: 0.00010644133484068637, l2: 0.0003699879768343332   Iteration 25 of 100, tot loss = 4.773007402420044, l1: 0.00010696427198126912, l2: 0.0003703364648390561   Iteration 26 of 100, tot loss = 4.844444247392508, l1: 0.00010818039691702534, l2: 0.0003762640240333545   Iteration 27 of 100, tot loss = 4.775168039180614, l1: 0.00010739188831023596, l2: 0.0003701249122429915   Iteration 28 of 100, tot loss = 4.77637654542923, l1: 0.00010677628571491888, l2: 0.00037086136476968283   Iteration 29 of 100, tot loss = 4.725395260186031, l1: 0.00010560114003902558, l2: 0.0003669383817207839   Iteration 30 of 100, tot loss = 4.683539485931396, l1: 0.000105107250904742, l2: 0.00036324669393555573   Iteration 31 of 100, tot loss = 4.706757760817005, l1: 0.00010603444811309717, l2: 0.00036464132473713927   Iteration 32 of 100, tot loss = 4.7277016043663025, l1: 0.00010607329704725998, l2: 0.00036669685960077913   Iteration 33 of 100, tot loss = 4.785516146457557, l1: 0.00010650166002574913, l2: 0.0003720499499291746   Iteration 34 of 100, tot loss = 4.774130077923045, l1: 0.00010586066448869293, l2: 0.000371552339535864   Iteration 35 of 100, tot loss = 4.7981271607535225, l1: 0.00010679188375693879, l2: 0.00037302082969940136   Iteration 36 of 100, tot loss = 4.846747994422913, l1: 0.00010614777400203618, l2: 0.00037852702209622494   Iteration 37 of 100, tot loss = 4.899092919117695, l1: 0.00010719657321051518, l2: 0.00038271271655134653   Iteration 38 of 100, tot loss = 4.839095485837836, l1: 0.00010602737567847056, l2: 0.00037788217129051   Iteration 39 of 100, tot loss = 4.800161557319837, l1: 0.0001052573572498006, l2: 0.00037475879705594614   Iteration 40 of 100, tot loss = 4.832809436321258, l1: 0.00010585934323898982, l2: 0.0003774215987505158   Iteration 41 of 100, tot loss = 4.776000400868858, l1: 0.00010492671004258005, l2: 0.00037267332805729494   Iteration 42 of 100, tot loss = 4.743477809996832, l1: 0.0001046191921757002, l2: 0.00036972858652006835   Iteration 43 of 100, tot loss = 4.749979961750119, l1: 0.00010524919936108555, l2: 0.0003697487943924877   Iteration 44 of 100, tot loss = 4.753536917946556, l1: 0.00010500951297713486, l2: 0.00037034417685142466   Iteration 45 of 100, tot loss = 4.73167732556661, l1: 0.00010471455526486452, l2: 0.0003684531755021049   Iteration 46 of 100, tot loss = 4.756730737893478, l1: 0.00010488675552946718, l2: 0.00037078631652073693   Iteration 47 of 100, tot loss = 4.8049305601322905, l1: 0.00010539390991362645, l2: 0.00037509914329077334   Iteration 48 of 100, tot loss = 4.787573233246803, l1: 0.00010520334399188869, l2: 0.0003735539764117372   Iteration 49 of 100, tot loss = 4.79647083185157, l1: 0.00010564885184178319, l2: 0.0003739982280329022   Iteration 50 of 100, tot loss = 4.825741801261902, l1: 0.0001065686420770362, l2: 0.0003760055347811431   Iteration 51 of 100, tot loss = 4.7946962515513105, l1: 0.0001055886010969213, l2: 0.00037388102089821855   Iteration 52 of 100, tot loss = 4.77230827166484, l1: 0.00010479245716291749, l2: 0.0003724383669368063   Iteration 53 of 100, tot loss = 4.77859558699266, l1: 0.00010502969530105749, l2: 0.0003728298607660141   Iteration 54 of 100, tot loss = 4.767964199737266, l1: 0.00010496639225931614, l2: 0.00037183002536443784   Iteration 55 of 100, tot loss = 4.81205563545227, l1: 0.00010574220048676414, l2: 0.00037546336111104626   Iteration 56 of 100, tot loss = 4.795679973704474, l1: 0.00010549906846790691, l2: 0.0003740689270281499   Iteration 57 of 100, tot loss = 4.824212638955367, l1: 0.00010619064829498121, l2: 0.00037623061430885603   Iteration 58 of 100, tot loss = 4.8168567747905335, l1: 0.00010586629042367788, l2: 0.0003758193854772454   Iteration 59 of 100, tot loss = 4.846535395767729, l1: 0.00010662746581917834, l2: 0.0003780260715649416   Iteration 60 of 100, tot loss = 4.843043371041616, l1: 0.00010646119462762727, l2: 0.00037784313996477674   Iteration 61 of 100, tot loss = 4.8425696130658755, l1: 0.0001062816064389705, l2: 0.00037797535273444946   Iteration 62 of 100, tot loss = 4.855953889508402, l1: 0.00010663505453257747, l2: 0.00037896033186810994   Iteration 63 of 100, tot loss = 4.835999530459207, l1: 0.00010651118409152823, l2: 0.0003770887662309207   Iteration 64 of 100, tot loss = 4.826932724565268, l1: 0.00010643934643894681, l2: 0.000376253923150216   Iteration 65 of 100, tot loss = 4.819266631053044, l1: 0.00010626430823601997, l2: 0.0003756623522629245   Iteration 66 of 100, tot loss = 4.7825872500737505, l1: 0.00010564894408603185, l2: 0.0003726097784237936   Iteration 67 of 100, tot loss = 4.772524125540435, l1: 0.00010508771088980346, l2: 0.0003721646994088234   Iteration 68 of 100, tot loss = 4.755117146407857, l1: 0.00010451351146698203, l2: 0.00037099820089197774   Iteration 69 of 100, tot loss = 4.735606798227282, l1: 0.00010428322829007495, l2: 0.00036927744909864515   Iteration 70 of 100, tot loss = 4.737113581384931, l1: 0.00010460937373863999, l2: 0.00036910198229764187   Iteration 71 of 100, tot loss = 4.732531483744232, l1: 0.0001047825985231509, l2: 0.00036847054741909386   Iteration 72 of 100, tot loss = 4.707878639300664, l1: 0.00010419177836714273, l2: 0.00036659608283823926   Iteration 73 of 100, tot loss = 4.701772294632376, l1: 0.00010428246640327565, l2: 0.0003658947608615463   Iteration 74 of 100, tot loss = 4.721000004459071, l1: 0.00010441768417139601, l2: 0.0003676823140955782   Iteration 75 of 100, tot loss = 4.703376537958781, l1: 0.00010400048277612465, l2: 0.00036633716915578893   Iteration 76 of 100, tot loss = 4.714512144264422, l1: 0.00010391096709913415, l2: 0.00036754024557395545   Iteration 77 of 100, tot loss = 4.730508181955908, l1: 0.00010430756966733108, l2: 0.0003687432464801815   Iteration 78 of 100, tot loss = 4.693864258436056, l1: 0.00010365772467570749, l2: 0.00036572869915592793   Iteration 79 of 100, tot loss = 4.6775342920158485, l1: 0.00010338115896933854, l2: 0.0003643722681968202   Iteration 80 of 100, tot loss = 4.68224340826273, l1: 0.00010354071414440114, l2: 0.00036468362504820105   Iteration 81 of 100, tot loss = 4.658445901340908, l1: 0.00010315266420402173, l2: 0.0003626919241929657   Iteration 82 of 100, tot loss = 4.630340481676707, l1: 0.0001026788722811034, l2: 0.0003603551742299914   Iteration 83 of 100, tot loss = 4.637618907962937, l1: 0.00010286855301153509, l2: 0.00036089333613734153   Iteration 84 of 100, tot loss = 4.676612892321178, l1: 0.00010347262464381943, l2: 0.0003641886628645894   Iteration 85 of 100, tot loss = 4.675749070504132, l1: 0.00010345205677474629, l2: 0.0003641228487728821   Iteration 86 of 100, tot loss = 4.675705370514891, l1: 0.00010347266462029142, l2: 0.0003640978706413122   Iteration 87 of 100, tot loss = 4.691936486068813, l1: 0.00010374323367080972, l2: 0.00036545041309610736   Iteration 88 of 100, tot loss = 4.677394413135269, l1: 0.0001036259556134279, l2: 0.0003641134838041828   Iteration 89 of 100, tot loss = 4.6846814329704545, l1: 0.00010387922819145285, l2: 0.0003645889131742101   Iteration 90 of 100, tot loss = 4.689489143424564, l1: 0.0001038723732360975, l2: 0.000365076538906174   Iteration 91 of 100, tot loss = 4.699434407464751, l1: 0.00010403304226961305, l2: 0.00036591039647496625   Iteration 92 of 100, tot loss = 4.685509263173394, l1: 0.00010369482108375371, l2: 0.00036485610310236274   Iteration 93 of 100, tot loss = 4.657342568520577, l1: 0.00010325408545203797, l2: 0.00036248016926605174   Iteration 94 of 100, tot loss = 4.660614826577775, l1: 0.00010340243638796543, l2: 0.0003626590442628243   Iteration 95 of 100, tot loss = 4.655074186074106, l1: 0.00010328642295606091, l2: 0.00036222099335741646   Iteration 96 of 100, tot loss = 4.645219051589568, l1: 0.00010306723875904329, l2: 0.0003614546641680742   Iteration 97 of 100, tot loss = 4.653294124554113, l1: 0.00010316532336087831, l2: 0.0003621640872299076   Iteration 98 of 100, tot loss = 4.641810904960243, l1: 0.00010298908711440757, l2: 0.0003611920016867641   Iteration 99 of 100, tot loss = 4.654943457757584, l1: 0.00010312630826839001, l2: 0.00036236803553589254   Iteration 100 of 100, tot loss = 4.684701601266861, l1: 0.00010329092499887338, l2: 0.00036517923304927533
   End of epoch 1205; saving model... 

Epoch 1206 of 2000
   Iteration 1 of 100, tot loss = 7.207358360290527, l1: 0.00015437213005498052, l2: 0.0005663636839017272   Iteration 2 of 100, tot loss = 6.385796785354614, l1: 0.0001418417159584351, l2: 0.0004967379354638979   Iteration 3 of 100, tot loss = 6.125851949055989, l1: 0.00012671968579525128, l2: 0.0004858654865529388   Iteration 4 of 100, tot loss = 5.285366117954254, l1: 0.00011234079647692852, l2: 0.00041619579860707745   Iteration 5 of 100, tot loss = 5.169723081588745, l1: 0.00010691653587855399, l2: 0.0004100557649508119   Iteration 6 of 100, tot loss = 4.870431542396545, l1: 9.817523641686421e-05, l2: 0.00038886791056332487   Iteration 7 of 100, tot loss = 4.91502731187003, l1: 9.933391317775073e-05, l2: 0.0003921688101919634   Iteration 8 of 100, tot loss = 5.036713749170303, l1: 0.00010226263248114265, l2: 0.0004014087317045778   Iteration 9 of 100, tot loss = 5.149480581283569, l1: 0.00010767142319107532, l2: 0.00040727662336495187   Iteration 10 of 100, tot loss = 5.057919239997863, l1: 0.00010607887707010377, l2: 0.00039971303776837886   Iteration 11 of 100, tot loss = 4.985082171180031, l1: 0.00010509066917372614, l2: 0.0003934175421653146   Iteration 12 of 100, tot loss = 5.101261556148529, l1: 0.00010585348445601994, l2: 0.0004042726650368422   Iteration 13 of 100, tot loss = 5.107475812618549, l1: 0.00010572477941213247, l2: 0.00040502279611805885   Iteration 14 of 100, tot loss = 5.034003036362784, l1: 0.00010618260759136839, l2: 0.0003972176886496267   Iteration 15 of 100, tot loss = 4.985103686650594, l1: 0.00010647248006231772, l2: 0.0003920378803741187   Iteration 16 of 100, tot loss = 5.065130427479744, l1: 0.00010746894054136646, l2: 0.0003990440945926821   Iteration 17 of 100, tot loss = 5.140429482740514, l1: 0.00010899150712408728, l2: 0.00040505143218016364   Iteration 18 of 100, tot loss = 5.242910610304938, l1: 0.0001101703292079037, l2: 0.00041412072379090305   Iteration 19 of 100, tot loss = 5.299167218961213, l1: 0.00011093126388579166, l2: 0.0004189854509221684   Iteration 20 of 100, tot loss = 5.266226351261139, l1: 0.00010988570447807433, l2: 0.00041673692321637646   Iteration 21 of 100, tot loss = 5.266451483681088, l1: 0.00011074083245364905, l2: 0.0004159043088466639   Iteration 22 of 100, tot loss = 5.337684945626692, l1: 0.00011189735116865698, l2: 0.0004218711374877867   Iteration 23 of 100, tot loss = 5.213053433791451, l1: 0.00010895331228232902, l2: 0.0004123520262984802   Iteration 24 of 100, tot loss = 5.267685651779175, l1: 0.00010970945792602531, l2: 0.0004170591031045963   Iteration 25 of 100, tot loss = 5.253663635253906, l1: 0.00010938655264908448, l2: 0.00041597980773076413   Iteration 26 of 100, tot loss = 5.207238399065458, l1: 0.0001093350219889544, l2: 0.0004113888157567439   Iteration 27 of 100, tot loss = 5.17115921444363, l1: 0.00010928511430931933, l2: 0.0004078308044915536   Iteration 28 of 100, tot loss = 5.232003143855503, l1: 0.0001111122559710306, l2: 0.0004120880548725836   Iteration 29 of 100, tot loss = 5.218610040072737, l1: 0.0001109131995251338, l2: 0.00041094780116791613   Iteration 30 of 100, tot loss = 5.224648682276408, l1: 0.0001110884317313321, l2: 0.0004113764327485114   Iteration 31 of 100, tot loss = 5.194628761660669, l1: 0.00011027734111731632, l2: 0.00040918553056525846   Iteration 32 of 100, tot loss = 5.14676309376955, l1: 0.00010948324529636011, l2: 0.0004051930600326159   Iteration 33 of 100, tot loss = 5.099369583707867, l1: 0.0001090421849794714, l2: 0.00040089476952385723   Iteration 34 of 100, tot loss = 5.034707462086397, l1: 0.00010825293600860546, l2: 0.00039521780597758205   Iteration 35 of 100, tot loss = 5.026915795462472, l1: 0.00010825453105748499, l2: 0.00039443704382782535   Iteration 36 of 100, tot loss = 4.96604006157981, l1: 0.000107625910863539, l2: 0.0003889780911979162   Iteration 37 of 100, tot loss = 4.887321369068043, l1: 0.00010594227077908235, l2: 0.0003827898621533972   Iteration 38 of 100, tot loss = 4.8832386543876245, l1: 0.00010636583704725031, l2: 0.0003819580245027809   Iteration 39 of 100, tot loss = 4.940070726932624, l1: 0.00010769353488845249, l2: 0.0003863135352730751   Iteration 40 of 100, tot loss = 4.898796999454499, l1: 0.00010714665395425982, l2: 0.00038273304380709305   Iteration 41 of 100, tot loss = 4.883750275867741, l1: 0.0001061483095194179, l2: 0.00038222671648283015   Iteration 42 of 100, tot loss = 4.86215565318153, l1: 0.00010588835521914353, l2: 0.0003803272085774335   Iteration 43 of 100, tot loss = 4.852331593979237, l1: 0.00010606429248920989, l2: 0.00037916886571491527   Iteration 44 of 100, tot loss = 4.846511103890159, l1: 0.00010570003169877137, l2: 0.0003789510771060701   Iteration 45 of 100, tot loss = 4.852714347839355, l1: 0.0001059918713710633, l2: 0.00037927956242735187   Iteration 46 of 100, tot loss = 4.852495131285294, l1: 0.0001062356724382287, l2: 0.0003790138403216944   Iteration 47 of 100, tot loss = 4.8734792851387185, l1: 0.00010637924648843647, l2: 0.0003809686824481221   Iteration 48 of 100, tot loss = 4.895406941572825, l1: 0.00010704064705654066, l2: 0.00038250004702907364   Iteration 49 of 100, tot loss = 4.859744840738725, l1: 0.00010658512035995361, l2: 0.00037938936397300234   Iteration 50 of 100, tot loss = 4.876444482803345, l1: 0.00010669799892639276, l2: 0.0003809464490041137   Iteration 51 of 100, tot loss = 4.877565757901061, l1: 0.00010657017100682282, l2: 0.00038118640476764707   Iteration 52 of 100, tot loss = 4.839817230518047, l1: 0.00010587945920493579, l2: 0.0003781022639416803   Iteration 53 of 100, tot loss = 4.972081148399497, l1: 0.0001068208915074927, l2: 0.0003903872242311613   Iteration 54 of 100, tot loss = 4.917041376784995, l1: 0.00010559088109908366, l2: 0.000386113257533267   Iteration 55 of 100, tot loss = 4.92653978954662, l1: 0.0001056618059175724, l2: 0.00038699217442296106   Iteration 56 of 100, tot loss = 4.935693242720196, l1: 0.00010586727687820842, l2: 0.0003877020496604798   Iteration 57 of 100, tot loss = 4.896542754089623, l1: 0.0001052855909061659, l2: 0.00038436868661001586   Iteration 58 of 100, tot loss = 4.860161801864361, l1: 0.0001048195918601464, l2: 0.00038119659037529974   Iteration 59 of 100, tot loss = 4.855922751507516, l1: 0.00010469721678804244, l2: 0.0003808950603312134   Iteration 60 of 100, tot loss = 4.844996845722198, l1: 0.0001048993096749958, l2: 0.0003796003766183276   Iteration 61 of 100, tot loss = 4.858903310338005, l1: 0.00010521111197122678, l2: 0.0003806792204859896   Iteration 62 of 100, tot loss = 4.847990885857613, l1: 0.00010532707523041961, l2: 0.0003794720149955772   Iteration 63 of 100, tot loss = 4.819258958574325, l1: 0.00010484348621546469, l2: 0.0003770824109706732   Iteration 64 of 100, tot loss = 4.81642846390605, l1: 0.00010496138764892748, l2: 0.0003766814602386148   Iteration 65 of 100, tot loss = 4.829527777891893, l1: 0.0001056749601467835, l2: 0.00037727781902783766   Iteration 66 of 100, tot loss = 4.82918943419601, l1: 0.0001056626242092479, l2: 0.00037725632093056584   Iteration 67 of 100, tot loss = 4.807652779479525, l1: 0.00010511284747722198, l2: 0.00037565243208030266   Iteration 68 of 100, tot loss = 4.838616251945496, l1: 0.00010563258584785645, l2: 0.0003782290415769881   Iteration 69 of 100, tot loss = 4.840872868247654, l1: 0.00010585474508158201, l2: 0.00037823254377513695   Iteration 70 of 100, tot loss = 4.810279791695731, l1: 0.00010509947195324847, l2: 0.0003759285092071098   Iteration 71 of 100, tot loss = 4.7897029964017195, l1: 0.00010438848925921672, l2: 0.0003745818121644052   Iteration 72 of 100, tot loss = 4.784718483686447, l1: 0.00010432952775671665, l2: 0.0003741423224710161   Iteration 73 of 100, tot loss = 4.817466608465534, l1: 0.00010524503507951198, l2: 0.0003765016279505464   Iteration 74 of 100, tot loss = 4.787569996472952, l1: 0.0001050485713946998, l2: 0.00037370843032526   Iteration 75 of 100, tot loss = 4.782103974024455, l1: 0.00010497560811927543, l2: 0.00037323479113789896   Iteration 76 of 100, tot loss = 4.784865733824279, l1: 0.0001048808641682722, l2: 0.0003736057111190779   Iteration 77 of 100, tot loss = 4.754248371371975, l1: 0.00010457484493056289, l2: 0.00037084999395981915   Iteration 78 of 100, tot loss = 4.7549668825589695, l1: 0.00010474015522422866, l2: 0.00037075653455805225   Iteration 79 of 100, tot loss = 4.782523692408694, l1: 0.00010526983118601308, l2: 0.00037298253936327616   Iteration 80 of 100, tot loss = 4.780003952980041, l1: 0.00010534329830989009, l2: 0.00037265709834173323   Iteration 81 of 100, tot loss = 4.758508911839238, l1: 0.00010516194419728385, l2: 0.00037068894842961505   Iteration 82 of 100, tot loss = 4.7920439766674505, l1: 0.00010569655533777777, l2: 0.0003735078441933161   Iteration 83 of 100, tot loss = 4.762809158807777, l1: 0.00010528013484302841, l2: 0.0003710007827998285   Iteration 84 of 100, tot loss = 4.756637513637543, l1: 0.00010505675596678408, l2: 0.000370606997291491   Iteration 85 of 100, tot loss = 4.74170560836792, l1: 0.00010488714435589773, l2: 0.0003692834182669792   Iteration 86 of 100, tot loss = 4.732024126274641, l1: 0.00010485755419087878, l2: 0.00036834486012975144   Iteration 87 of 100, tot loss = 4.710715762500105, l1: 0.00010456512697193579, l2: 0.00036650645093665857   Iteration 88 of 100, tot loss = 4.698313745585355, l1: 0.00010419932203314437, l2: 0.00036563205420944456   Iteration 89 of 100, tot loss = 4.692466119701943, l1: 0.00010433311914577767, l2: 0.00036491349478791146   Iteration 90 of 100, tot loss = 4.726656458112928, l1: 0.0001046183785042053, l2: 0.0003680472690676753   Iteration 91 of 100, tot loss = 4.707589345973926, l1: 0.0001043663695991899, l2: 0.0003663925666702338   Iteration 92 of 100, tot loss = 4.730411931224491, l1: 0.00010447993427869338, l2: 0.00036856126050976025   Iteration 93 of 100, tot loss = 4.722567253215338, l1: 0.00010460528096945215, l2: 0.00036765144614037126   Iteration 94 of 100, tot loss = 4.710678803159835, l1: 0.00010419737690368648, l2: 0.00036687050537983985   Iteration 95 of 100, tot loss = 4.699633834236547, l1: 0.00010369928035958621, l2: 0.0003662641051433686   Iteration 96 of 100, tot loss = 4.690975857277711, l1: 0.0001035794941799395, l2: 0.00036551809368271887   Iteration 97 of 100, tot loss = 4.686202607204005, l1: 0.00010339666367088416, l2: 0.00036522359890292023   Iteration 98 of 100, tot loss = 4.689566600079439, l1: 0.00010337488752806426, l2: 0.00036558177430843174   Iteration 99 of 100, tot loss = 4.719341323833273, l1: 0.00010362220259272785, l2: 0.00036831193170251534   Iteration 100 of 100, tot loss = 4.7045715403556825, l1: 0.00010339292603021022, l2: 0.00036706422979477794
   End of epoch 1206; saving model... 

Epoch 1207 of 2000
   Iteration 1 of 100, tot loss = 6.4498419761657715, l1: 0.00012402456195559353, l2: 0.0005209596129134297   Iteration 2 of 100, tot loss = 4.42919921875, l1: 9.625451639294624e-05, l2: 0.0003466653943178244   Iteration 3 of 100, tot loss = 4.8172688484191895, l1: 0.0001086165818075339, l2: 0.0003731102963987117   Iteration 4 of 100, tot loss = 4.552310228347778, l1: 0.00010386781650595367, l2: 0.0003513632000249345   Iteration 5 of 100, tot loss = 4.748007965087891, l1: 0.00010521234653424472, l2: 0.0003695884457556531   Iteration 6 of 100, tot loss = 4.364234805107117, l1: 9.678714074349652e-05, l2: 0.00033963633904932067   Iteration 7 of 100, tot loss = 4.314065149852207, l1: 9.861384855217434e-05, l2: 0.0003327926658260237   Iteration 8 of 100, tot loss = 4.166702091693878, l1: 9.761321507539833e-05, l2: 0.0003190569914295338   Iteration 9 of 100, tot loss = 4.125547938876682, l1: 9.785036268618164e-05, l2: 0.00031470443031543656   Iteration 10 of 100, tot loss = 4.277958917617798, l1: 0.00010283249357598834, l2: 0.00032496339408680794   Iteration 11 of 100, tot loss = 4.387023969130083, l1: 0.00010316652068550783, l2: 0.0003355358706109903   Iteration 12 of 100, tot loss = 4.323171297709147, l1: 0.00010119916441908572, l2: 0.0003311179607408121   Iteration 13 of 100, tot loss = 4.493200082045335, l1: 0.00010340782542838357, l2: 0.0003459121799096465   Iteration 14 of 100, tot loss = 4.438557522637503, l1: 0.00010144198209413194, l2: 0.00034241376644266505   Iteration 15 of 100, tot loss = 4.786175759633382, l1: 0.00010632906826989105, l2: 0.0003722885080302755   Iteration 16 of 100, tot loss = 4.823892563581467, l1: 0.00010675900875867228, l2: 0.0003756302467081696   Iteration 17 of 100, tot loss = 4.7647095708286065, l1: 0.0001041792202158831, l2: 0.0003722917360445375   Iteration 18 of 100, tot loss = 4.776036964522468, l1: 0.00010603962639126823, l2: 0.0003715640697110858   Iteration 19 of 100, tot loss = 4.669573256843968, l1: 0.0001039230601715022, l2: 0.0003630342643622211   Iteration 20 of 100, tot loss = 4.709488487243652, l1: 0.00010445642292324919, l2: 0.0003664924246550072   Iteration 21 of 100, tot loss = 4.642389081773304, l1: 0.00010191767432843335, l2: 0.00036232123204085623   Iteration 22 of 100, tot loss = 4.804155273870989, l1: 0.00010415734554540408, l2: 0.0003762581797094423   Iteration 23 of 100, tot loss = 4.830020500266033, l1: 0.0001044698613480685, l2: 0.0003785321877449346   Iteration 24 of 100, tot loss = 4.866189847389857, l1: 0.00010430455965130629, l2: 0.00038231442461741   Iteration 25 of 100, tot loss = 4.850008916854859, l1: 0.00010422158389701508, l2: 0.0003807793074520305   Iteration 26 of 100, tot loss = 4.81066214121305, l1: 0.00010395229890911232, l2: 0.00037711391465004103   Iteration 27 of 100, tot loss = 4.8509639987239135, l1: 0.00010383464851420959, l2: 0.00038126175005109635   Iteration 28 of 100, tot loss = 4.7261015857969015, l1: 0.00010131560117900205, l2: 0.0003712945563165704   Iteration 29 of 100, tot loss = 4.813673677115605, l1: 0.00010288909269829987, l2: 0.00037847827304082375   Iteration 30 of 100, tot loss = 4.752559272448222, l1: 0.00010101836135921379, l2: 0.00037423756414985593   Iteration 31 of 100, tot loss = 4.722254030166134, l1: 0.00010034998112748707, l2: 0.00037187542048125195   Iteration 32 of 100, tot loss = 4.688761308789253, l1: 9.983696099880035e-05, l2: 0.00036903916839037265   Iteration 33 of 100, tot loss = 4.688632098111239, l1: 9.967455755968606e-05, l2: 0.0003691886515959843   Iteration 34 of 100, tot loss = 4.679048748577342, l1: 9.991031487623011e-05, l2: 0.00036799455954357707   Iteration 35 of 100, tot loss = 4.730142348153251, l1: 0.00010006730673402281, l2: 0.00037294692655060705   Iteration 36 of 100, tot loss = 4.761561115582784, l1: 0.00010046278839581646, l2: 0.000375693321782617   Iteration 37 of 100, tot loss = 4.778213668513942, l1: 0.00010117480249735652, l2: 0.0003766465632833314   Iteration 38 of 100, tot loss = 4.741056373244838, l1: 0.00010099154563871899, l2: 0.0003731140905553107   Iteration 39 of 100, tot loss = 4.802849506720518, l1: 0.00010216949736693491, l2: 0.00037811545208126353   Iteration 40 of 100, tot loss = 4.825828164815903, l1: 0.00010287513050570851, l2: 0.0003797076855335035   Iteration 41 of 100, tot loss = 4.762523197546238, l1: 0.00010155953758640396, l2: 0.0003746927816281663   Iteration 42 of 100, tot loss = 4.7743778342292424, l1: 0.00010209032448723779, l2: 0.00037534745778040294   Iteration 43 of 100, tot loss = 4.799004432766936, l1: 0.00010269866233588694, l2: 0.00037720177893196555   Iteration 44 of 100, tot loss = 4.7627450823783875, l1: 0.00010160464460733982, l2: 0.0003746698618091283   Iteration 45 of 100, tot loss = 4.769781361685859, l1: 0.0001023799742041673, l2: 0.0003745981600433071   Iteration 46 of 100, tot loss = 4.7559099974839585, l1: 0.00010257305304707108, l2: 0.00037301794430316914   Iteration 47 of 100, tot loss = 4.721869301288686, l1: 0.0001024935721788813, l2: 0.0003696933555592624   Iteration 48 of 100, tot loss = 4.69608661532402, l1: 0.00010202393089760638, l2: 0.0003675847284891158   Iteration 49 of 100, tot loss = 4.674897383670418, l1: 0.00010166562587611985, l2: 0.0003658241100255305   Iteration 50 of 100, tot loss = 4.669445538520813, l1: 0.00010197954456089065, l2: 0.0003649650076113176   Iteration 51 of 100, tot loss = 4.711609340181537, l1: 0.00010292676098270378, l2: 0.0003682341711657286   Iteration 52 of 100, tot loss = 4.714025116883791, l1: 0.00010329853103030473, l2: 0.00036810397810208646   Iteration 53 of 100, tot loss = 4.697414776064315, l1: 0.00010304766397413639, l2: 0.00036669381109466075   Iteration 54 of 100, tot loss = 4.709000516820837, l1: 0.00010328260557596675, l2: 0.00036761744303038103   Iteration 55 of 100, tot loss = 4.758543994209983, l1: 0.00010410320934500883, l2: 0.00037175118751210076   Iteration 56 of 100, tot loss = 4.846858526979174, l1: 0.00010549854388435571, l2: 0.00037918730686214985   Iteration 57 of 100, tot loss = 4.879650760115239, l1: 0.00010632652188896348, l2: 0.00038163855162820894   Iteration 58 of 100, tot loss = 4.905423896066074, l1: 0.00010636279066916202, l2: 0.00038417959663563343   Iteration 59 of 100, tot loss = 4.894085205207436, l1: 0.0001060338286383046, l2: 0.00038337468954446424   Iteration 60 of 100, tot loss = 4.887811398506164, l1: 0.00010606856897841984, l2: 0.0003827125681709731   Iteration 61 of 100, tot loss = 4.941010420439674, l1: 0.00010715269464064297, l2: 0.00038694834456393557   Iteration 62 of 100, tot loss = 4.93507327571992, l1: 0.00010740216912968355, l2: 0.0003861051559544003   Iteration 63 of 100, tot loss = 4.890669909734575, l1: 0.00010635328553256667, l2: 0.00038271370292585784   Iteration 64 of 100, tot loss = 4.8740537986159325, l1: 0.0001061583985233483, l2: 0.0003812469786907968   Iteration 65 of 100, tot loss = 4.836295997179471, l1: 0.00010547633769769723, l2: 0.00037815325922565537   Iteration 66 of 100, tot loss = 4.8186202627239805, l1: 0.00010533213015040735, l2: 0.0003765298934013117   Iteration 67 of 100, tot loss = 4.793244586062076, l1: 0.00010492347352208444, l2: 0.00037440098203117474   Iteration 68 of 100, tot loss = 4.78567151111715, l1: 0.00010510032697151045, l2: 0.000373466821068124   Iteration 69 of 100, tot loss = 4.782841596050539, l1: 0.00010518897503008729, l2: 0.00037309518149308184   Iteration 70 of 100, tot loss = 4.783964432988848, l1: 0.0001055095542791865, l2: 0.0003728868858680861   Iteration 71 of 100, tot loss = 4.784629281138031, l1: 0.00010542646922543317, l2: 0.0003730364558475525   Iteration 72 of 100, tot loss = 4.787506649891536, l1: 0.00010539348280265888, l2: 0.0003733571792407828   Iteration 73 of 100, tot loss = 4.801458708227497, l1: 0.00010552187664522024, l2: 0.00037462399163752257   Iteration 74 of 100, tot loss = 4.8334706828400895, l1: 0.00010556364552915486, l2: 0.00037778342074661113   Iteration 75 of 100, tot loss = 4.816747992833456, l1: 0.00010529701631943074, l2: 0.00037637778082474444   Iteration 76 of 100, tot loss = 4.796520634701378, l1: 0.00010513835738828148, l2: 0.0003745137040057575   Iteration 77 of 100, tot loss = 4.795292618986848, l1: 0.00010505425121050098, l2: 0.00037447500842783675   Iteration 78 of 100, tot loss = 4.789689082365769, l1: 0.00010491366773725857, l2: 0.0003740552381667583   Iteration 79 of 100, tot loss = 4.768085709101038, l1: 0.00010460897064598313, l2: 0.0003721995979711071   Iteration 80 of 100, tot loss = 4.751784986257553, l1: 0.00010438639269523264, l2: 0.00037079210369483917   Iteration 81 of 100, tot loss = 4.796626650256875, l1: 0.00010491158006774595, l2: 0.00037475108349750415   Iteration 82 of 100, tot loss = 4.791865145287862, l1: 0.00010476370254941796, l2: 0.0003744228103264837   Iteration 83 of 100, tot loss = 4.767668623522104, l1: 0.00010442074427012275, l2: 0.0003723461165672157   Iteration 84 of 100, tot loss = 4.773632341907138, l1: 0.00010476247401432677, l2: 0.0003726007584018156   Iteration 85 of 100, tot loss = 4.776218490039601, l1: 0.00010499945496763651, l2: 0.0003726223919383141   Iteration 86 of 100, tot loss = 4.746936903443447, l1: 0.00010474032696524667, l2: 0.00036995336124037264   Iteration 87 of 100, tot loss = 4.732918742059291, l1: 0.00010445432204066711, l2: 0.00036883754991918225   Iteration 88 of 100, tot loss = 4.719795828515833, l1: 0.00010447213344377815, l2: 0.0003675074470844596   Iteration 89 of 100, tot loss = 4.755962795086122, l1: 0.00010522939886909574, l2: 0.00037036687824051055   Iteration 90 of 100, tot loss = 4.744403272204929, l1: 0.00010509013700357172, l2: 0.0003693501876518389   Iteration 91 of 100, tot loss = 4.729551137148679, l1: 0.00010483988120500766, l2: 0.0003681152299907478   Iteration 92 of 100, tot loss = 4.724450360173765, l1: 0.00010484659805629433, l2: 0.0003675984358067027   Iteration 93 of 100, tot loss = 4.737212811746905, l1: 0.00010505730081418459, l2: 0.00036866397833794617   Iteration 94 of 100, tot loss = 4.730525453039941, l1: 0.00010452649234564965, l2: 0.00036852605099990427   Iteration 95 of 100, tot loss = 4.72955883929604, l1: 0.000104438390323929, l2: 0.00036851749181024434   Iteration 96 of 100, tot loss = 4.726590345303218, l1: 0.00010433787773915053, l2: 0.0003683211550651322   Iteration 97 of 100, tot loss = 4.710129922198266, l1: 0.00010403881354897149, l2: 0.0003669741768275925   Iteration 98 of 100, tot loss = 4.6896703097285055, l1: 0.00010356580368148124, l2: 0.0003654012255036814   Iteration 99 of 100, tot loss = 4.709080195186114, l1: 0.00010408377589427424, l2: 0.000366824241563786   Iteration 100 of 100, tot loss = 4.703746194839478, l1: 0.00010427263376186602, l2: 0.00036610198374546597
   End of epoch 1207; saving model... 

Epoch 1208 of 2000
   Iteration 1 of 100, tot loss = 3.3509373664855957, l1: 5.293787762639113e-05, l2: 0.00028215584461577237   Iteration 2 of 100, tot loss = 3.1378753185272217, l1: 6.001936890243087e-05, l2: 0.0002537681648391299   Iteration 3 of 100, tot loss = 3.4181528091430664, l1: 6.274681679011944e-05, l2: 0.00027906846662517637   Iteration 4 of 100, tot loss = 4.034623622894287, l1: 7.41095818739268e-05, l2: 0.00032935278068180196   Iteration 5 of 100, tot loss = 3.7660430431365968, l1: 7.34728055249434e-05, l2: 0.0003031315020052716   Iteration 6 of 100, tot loss = 3.8702942927678428, l1: 7.859222205297556e-05, l2: 0.00030843721227332327   Iteration 7 of 100, tot loss = 3.7913010460989818, l1: 7.829788072350701e-05, l2: 0.00030083222788692055   Iteration 8 of 100, tot loss = 3.951438069343567, l1: 7.978982057466055e-05, l2: 0.00031535399284621235   Iteration 9 of 100, tot loss = 3.8157251411014133, l1: 7.896253333051896e-05, l2: 0.0003026099859400549   Iteration 10 of 100, tot loss = 3.9589855909347533, l1: 8.042866647883784e-05, l2: 0.0003154699006699957   Iteration 11 of 100, tot loss = 4.339338194240224, l1: 8.87439815745562e-05, l2: 0.00034518984532703394   Iteration 12 of 100, tot loss = 4.371281365553538, l1: 8.783628527453402e-05, l2: 0.00034929186100877513   Iteration 13 of 100, tot loss = 4.393581702159001, l1: 8.797870479891292e-05, l2: 0.0003513794766219619   Iteration 14 of 100, tot loss = 4.424148542540414, l1: 8.94343546471126e-05, l2: 0.0003529805097579291   Iteration 15 of 100, tot loss = 4.371869627634684, l1: 8.94824991216107e-05, l2: 0.0003477044743097698   Iteration 16 of 100, tot loss = 4.272668361663818, l1: 8.858409341883089e-05, l2: 0.0003386827511349111   Iteration 17 of 100, tot loss = 4.371442738701315, l1: 9.098133608965916e-05, l2: 0.0003461629462168168   Iteration 18 of 100, tot loss = 4.336150778664483, l1: 9.031311633912587e-05, l2: 0.00034330196972910524   Iteration 19 of 100, tot loss = 4.262601814771953, l1: 8.817472457610022e-05, l2: 0.0003380854652373512   Iteration 20 of 100, tot loss = 4.381804049015045, l1: 9.068414492503507e-05, l2: 0.0003474962672044057   Iteration 21 of 100, tot loss = 4.308144035793486, l1: 8.988434395481211e-05, l2: 0.00034093006702494763   Iteration 22 of 100, tot loss = 4.437698288397356, l1: 9.255146853154821e-05, l2: 0.0003512183664662933   Iteration 23 of 100, tot loss = 4.556633503540702, l1: 9.489317634016398e-05, l2: 0.0003607701788575429   Iteration 24 of 100, tot loss = 4.517666717370351, l1: 9.517308126305579e-05, l2: 0.0003565935949154664   Iteration 25 of 100, tot loss = 4.524702110290527, l1: 9.564038846292533e-05, l2: 0.0003568298276513815   Iteration 26 of 100, tot loss = 4.477010516019968, l1: 9.536823213588483e-05, l2: 0.00035233282463965367   Iteration 27 of 100, tot loss = 4.467583576838176, l1: 9.636576645856689e-05, l2: 0.0003503925954983397   Iteration 28 of 100, tot loss = 4.483691207000187, l1: 9.647447495808592e-05, l2: 0.00035189464948156716   Iteration 29 of 100, tot loss = 4.627930534297023, l1: 9.8751771192524e-05, l2: 0.0003640412855772944   Iteration 30 of 100, tot loss = 4.663433416684469, l1: 0.00010020890974071032, l2: 0.00036613443565632525   Iteration 31 of 100, tot loss = 4.634576297575427, l1: 9.936620652649341e-05, l2: 0.00036409142710674074   Iteration 32 of 100, tot loss = 4.747649393975735, l1: 0.00010128998803793365, l2: 0.0003734749529940018   Iteration 33 of 100, tot loss = 4.772557352528428, l1: 0.00010229054863554087, l2: 0.0003749651893458301   Iteration 34 of 100, tot loss = 4.810042444397421, l1: 0.00010253509315268329, l2: 0.00037846915374222374   Iteration 35 of 100, tot loss = 4.856549787521362, l1: 0.0001034768320096191, l2: 0.00038217814830464444   Iteration 36 of 100, tot loss = 4.8063266409768, l1: 0.00010258983082975546, l2: 0.0003780428350081719   Iteration 37 of 100, tot loss = 4.873873156470221, l1: 0.00010428062081258316, l2: 0.00038310669628412435   Iteration 38 of 100, tot loss = 4.911245684874685, l1: 0.00010479471137315206, l2: 0.00038632985830956484   Iteration 39 of 100, tot loss = 4.8841768105824785, l1: 0.00010435475148066568, l2: 0.0003840629304669654   Iteration 40 of 100, tot loss = 4.880701464414597, l1: 0.0001042572887854476, l2: 0.00038381285812647545   Iteration 41 of 100, tot loss = 4.861559117712626, l1: 0.00010459605015589991, l2: 0.0003815598621901988   Iteration 42 of 100, tot loss = 4.835793398675465, l1: 0.00010463747362553564, l2: 0.0003789418665941672   Iteration 43 of 100, tot loss = 4.848663191462672, l1: 0.00010536550105952787, l2: 0.0003795008189288553   Iteration 44 of 100, tot loss = 4.82129932533611, l1: 0.00010493294881929283, l2: 0.0003771969849450133   Iteration 45 of 100, tot loss = 4.784043947855632, l1: 0.00010443810242577456, l2: 0.00037396629405621855   Iteration 46 of 100, tot loss = 4.775705855825673, l1: 0.00010453371111867929, l2: 0.0003730368765030542   Iteration 47 of 100, tot loss = 4.833482843764285, l1: 0.00010493354389798193, l2: 0.000378414742064048   Iteration 48 of 100, tot loss = 4.818445881207784, l1: 0.00010451606347790705, l2: 0.00037732852676223655   Iteration 49 of 100, tot loss = 4.825805119105747, l1: 0.00010504417558858761, l2: 0.00037753633912462664   Iteration 50 of 100, tot loss = 4.8213888454437255, l1: 0.00010479389929969329, l2: 0.00037734498793724926   Iteration 51 of 100, tot loss = 4.850446897394517, l1: 0.00010554493165531597, l2: 0.0003794997601824648   Iteration 52 of 100, tot loss = 4.822374770274529, l1: 0.0001053641111026921, l2: 0.0003768733684130264   Iteration 53 of 100, tot loss = 4.899616407898237, l1: 0.00010654084799857371, l2: 0.0003834207954636406   Iteration 54 of 100, tot loss = 4.880262476426584, l1: 0.00010635215726829806, l2: 0.00038167409285152744   Iteration 55 of 100, tot loss = 4.848596850308505, l1: 0.00010587329961708747, l2: 0.00037898638798982245   Iteration 56 of 100, tot loss = 4.8029758376734595, l1: 0.00010507818342375685, l2: 0.0003752194027453827   Iteration 57 of 100, tot loss = 4.803423308489616, l1: 0.00010493699850038658, l2: 0.00037540533409347726   Iteration 58 of 100, tot loss = 4.771471040002231, l1: 0.00010417323956689557, l2: 0.000372973866339227   Iteration 59 of 100, tot loss = 4.758277941558321, l1: 0.00010393397413713116, l2: 0.0003718938219747743   Iteration 60 of 100, tot loss = 4.755449104309082, l1: 0.00010402246934972936, l2: 0.00037152244331082327   Iteration 61 of 100, tot loss = 4.72895103986146, l1: 0.00010371375543906994, l2: 0.0003691813507743302   Iteration 62 of 100, tot loss = 4.723041103732202, l1: 0.00010375393069426213, l2: 0.0003685501818132076   Iteration 63 of 100, tot loss = 4.69753218454028, l1: 0.00010317668894126583, l2: 0.00036657653150281737   Iteration 64 of 100, tot loss = 4.711062513291836, l1: 0.0001033461595056906, l2: 0.00036776009346795036   Iteration 65 of 100, tot loss = 4.686248507866493, l1: 0.00010299898673950408, l2: 0.0003656258658828357   Iteration 66 of 100, tot loss = 4.6948082808292275, l1: 0.00010307738629115082, l2: 0.0003664034434636547   Iteration 67 of 100, tot loss = 4.674874448064548, l1: 0.0001028427569549218, l2: 0.00036464468966961017   Iteration 68 of 100, tot loss = 4.683359833324657, l1: 0.00010341904587275006, l2: 0.00036491693882614043   Iteration 69 of 100, tot loss = 4.687136373658111, l1: 0.00010350372447620443, l2: 0.0003652099137822085   Iteration 70 of 100, tot loss = 4.67349157673972, l1: 0.00010370218000649142, l2: 0.00036364697817979115   Iteration 71 of 100, tot loss = 4.703668349225756, l1: 0.00010412915583747491, l2: 0.0003662376802309718   Iteration 72 of 100, tot loss = 4.7278408043914375, l1: 0.00010453905234600016, l2: 0.0003682450292722529   Iteration 73 of 100, tot loss = 4.736635714361112, l1: 0.00010457158455272742, l2: 0.0003690919878713991   Iteration 74 of 100, tot loss = 4.71643703370481, l1: 0.00010399394174070235, l2: 0.00036764976244444986   Iteration 75 of 100, tot loss = 4.688178828557333, l1: 0.00010326363844797015, l2: 0.000365554245072417   Iteration 76 of 100, tot loss = 4.6557426358524125, l1: 0.00010284772734418144, l2: 0.0003627265371170851   Iteration 77 of 100, tot loss = 4.64275236563249, l1: 0.00010292948567226844, l2: 0.0003613457516479826   Iteration 78 of 100, tot loss = 4.656337569921445, l1: 0.00010307016334711359, l2: 0.00036256359453545883   Iteration 79 of 100, tot loss = 4.682861276819736, l1: 0.00010377810118554891, l2: 0.0003645080272506192   Iteration 80 of 100, tot loss = 4.6866931229829785, l1: 0.00010387399024693877, l2: 0.0003647953222753131   Iteration 81 of 100, tot loss = 4.681609038953428, l1: 0.00010352852063707686, l2: 0.00036463238328356113   Iteration 82 of 100, tot loss = 4.662211668200609, l1: 0.00010346924398440822, l2: 0.0003627519231153334   Iteration 83 of 100, tot loss = 4.662767835410245, l1: 0.00010348566638092589, l2: 0.0003627911171488778   Iteration 84 of 100, tot loss = 4.667664720898583, l1: 0.00010381338298819693, l2: 0.0003629530886731421   Iteration 85 of 100, tot loss = 4.657387696995455, l1: 0.00010376176214776933, l2: 0.00036197700708940185   Iteration 86 of 100, tot loss = 4.674274935278782, l1: 0.00010399658741905938, l2: 0.00036343090543469286   Iteration 87 of 100, tot loss = 4.670857333588875, l1: 0.0001041915644215816, l2: 0.0003628941683029092   Iteration 88 of 100, tot loss = 4.667487011714415, l1: 0.00010425981236039661, l2: 0.00036248888830992985   Iteration 89 of 100, tot loss = 4.690119494213147, l1: 0.00010444167210050764, l2: 0.00036457027674893317   Iteration 90 of 100, tot loss = 4.735612506336636, l1: 0.00010515891756060429, l2: 0.00036840233240380054   Iteration 91 of 100, tot loss = 4.714214442850469, l1: 0.00010490402504788957, l2: 0.0003665174185848478   Iteration 92 of 100, tot loss = 4.708201431709787, l1: 0.00010472083854635812, l2: 0.0003660993039058825   Iteration 93 of 100, tot loss = 4.7252945771781345, l1: 0.00010505377256151248, l2: 0.0003674756846658545   Iteration 94 of 100, tot loss = 4.738017612315239, l1: 0.00010518217319994174, l2: 0.00036861958769224387   Iteration 95 of 100, tot loss = 4.7372112399653385, l1: 0.00010513965627117278, l2: 0.00036858146731423114   Iteration 96 of 100, tot loss = 4.7172108963131905, l1: 0.00010498199245982202, l2: 0.00036673909683789435   Iteration 97 of 100, tot loss = 4.694729455967539, l1: 0.00010454371973893787, l2: 0.00036492922565942995   Iteration 98 of 100, tot loss = 4.71208253685309, l1: 0.00010486746099076653, l2: 0.00036634079270345175   Iteration 99 of 100, tot loss = 4.692720449332035, l1: 0.00010456559980274037, l2: 0.0003647064451172929   Iteration 100 of 100, tot loss = 4.695889427661895, l1: 0.00010466606574482284, l2: 0.0003649228771973867
   End of epoch 1208; saving model... 

Epoch 1209 of 2000
   Iteration 1 of 100, tot loss = 5.109196186065674, l1: 0.0001105960036511533, l2: 0.00040032362448982894   Iteration 2 of 100, tot loss = 5.817163944244385, l1: 0.00012062277892255224, l2: 0.00046109363029245287   Iteration 3 of 100, tot loss = 5.369462013244629, l1: 0.00011176135255179058, l2: 0.00042518485376300913   Iteration 4 of 100, tot loss = 5.317027807235718, l1: 0.00010850282524188515, l2: 0.00042319995554862544   Iteration 5 of 100, tot loss = 5.162449932098388, l1: 0.00010700384445954114, l2: 0.0004092411487363279   Iteration 6 of 100, tot loss = 4.977699915568034, l1: 0.00010853636801281634, l2: 0.0003892336244462058   Iteration 7 of 100, tot loss = 5.00387293951852, l1: 0.00010445453413662367, l2: 0.0003959327670080321   Iteration 8 of 100, tot loss = 4.88572096824646, l1: 0.00010288998146279482, l2: 0.00038568212403333746   Iteration 9 of 100, tot loss = 4.6375815868377686, l1: 9.766192953166965e-05, l2: 0.00036609623768729053   Iteration 10 of 100, tot loss = 4.655554699897766, l1: 9.880285106191877e-05, l2: 0.0003667526281788014   Iteration 11 of 100, tot loss = 4.608340285041115, l1: 0.00010054541152468036, l2: 0.00036028862682128835   Iteration 12 of 100, tot loss = 4.709510505199432, l1: 0.00010281344505832142, l2: 0.0003681376147142146   Iteration 13 of 100, tot loss = 4.608973924930279, l1: 0.00010167891042118963, l2: 0.00035921849037269846   Iteration 14 of 100, tot loss = 4.843278595379421, l1: 0.00010534977705641982, l2: 0.0003789780883900156   Iteration 15 of 100, tot loss = 4.8473901907602945, l1: 0.00010584335468593054, l2: 0.0003788956702919677   Iteration 16 of 100, tot loss = 4.881281211972237, l1: 0.0001069961610937753, l2: 0.0003811319629676291   Iteration 17 of 100, tot loss = 4.91452182040495, l1: 0.00010783328509526601, l2: 0.0003836188980665825   Iteration 18 of 100, tot loss = 4.8347212473551435, l1: 0.00010520099709133824, l2: 0.00037827112909225334   Iteration 19 of 100, tot loss = 4.77047408254523, l1: 0.00010425295362150983, l2: 0.00037279445601406654   Iteration 20 of 100, tot loss = 4.730424416065216, l1: 0.00010387074617028702, l2: 0.0003691716970934067   Iteration 21 of 100, tot loss = 4.675591321218581, l1: 0.00010273905458494223, l2: 0.0003648200786084912   Iteration 22 of 100, tot loss = 4.685718980702487, l1: 0.00010249780428818089, l2: 0.0003660740945964459   Iteration 23 of 100, tot loss = 4.919495012449182, l1: 0.00010482529171934837, l2: 0.00038712420918391615   Iteration 24 of 100, tot loss = 5.006846179564794, l1: 0.00010645740773422101, l2: 0.0003942272093505987   Iteration 25 of 100, tot loss = 5.039432706832886, l1: 0.00010700892540626228, l2: 0.00039693434548098593   Iteration 26 of 100, tot loss = 5.007047368929936, l1: 0.0001050574456502755, l2: 0.00039564729209254996   Iteration 27 of 100, tot loss = 4.971245809837624, l1: 0.00010429810612953993, l2: 0.00039282647614729486   Iteration 28 of 100, tot loss = 5.018113383225033, l1: 0.0001053683156767095, l2: 0.00039644302315926846   Iteration 29 of 100, tot loss = 5.033215054150285, l1: 0.00010629658911433392, l2: 0.0003970249165612241   Iteration 30 of 100, tot loss = 5.063706803321838, l1: 0.00010670279613502013, l2: 0.00039966788463061673   Iteration 31 of 100, tot loss = 5.102000536457185, l1: 0.00010726862608627866, l2: 0.0004029314286233256   Iteration 32 of 100, tot loss = 5.104952074587345, l1: 0.00010681491858122172, l2: 0.00040368029067394673   Iteration 33 of 100, tot loss = 5.161685459541552, l1: 0.00010756670830728994, l2: 0.0004086018390918263   Iteration 34 of 100, tot loss = 5.189005353871514, l1: 0.00010807143510593211, l2: 0.00041082910277456157   Iteration 35 of 100, tot loss = 5.192133528845651, l1: 0.00010770749504445121, l2: 0.00041150585956139753   Iteration 36 of 100, tot loss = 5.190297729439205, l1: 0.00010729023294212918, l2: 0.00041173954256616224   Iteration 37 of 100, tot loss = 5.179835673925039, l1: 0.00010727584653854924, l2: 0.0004107077229245742   Iteration 38 of 100, tot loss = 5.2112027406692505, l1: 0.00010831707396918271, l2: 0.0004128032018534692   Iteration 39 of 100, tot loss = 5.201922337214152, l1: 0.0001082174050493333, l2: 0.00041197483071114105   Iteration 40 of 100, tot loss = 5.153352522850037, l1: 0.00010743425555119757, l2: 0.0004079009991983185   Iteration 41 of 100, tot loss = 5.18205935780595, l1: 0.00010796465553207004, l2: 0.00041024128249108157   Iteration 42 of 100, tot loss = 5.186479659307571, l1: 0.00010818948621384888, l2: 0.0004104584815130303   Iteration 43 of 100, tot loss = 5.195712377858716, l1: 0.0001089400165107881, l2: 0.00041063122355195067   Iteration 44 of 100, tot loss = 5.261773651296442, l1: 0.00010963260600957173, l2: 0.000416544762463838   Iteration 45 of 100, tot loss = 5.20649299621582, l1: 0.0001083204107999336, l2: 0.00041232889246505997   Iteration 46 of 100, tot loss = 5.199410158654918, l1: 0.00010883335159198158, l2: 0.0004111076679850078   Iteration 47 of 100, tot loss = 5.154894341813757, l1: 0.00010857478899081198, l2: 0.000406914648842661   Iteration 48 of 100, tot loss = 5.112098793188731, l1: 0.00010755818410264813, l2: 0.00040365169904058956   Iteration 49 of 100, tot loss = 5.1134953304212925, l1: 0.00010784518539998205, l2: 0.0004035043512347477   Iteration 50 of 100, tot loss = 5.118132514953613, l1: 0.00010768987056508194, l2: 0.0004041233845055103   Iteration 51 of 100, tot loss = 5.104030403436399, l1: 0.00010771433645619683, l2: 0.00040268870732546144   Iteration 52 of 100, tot loss = 5.0502343865541315, l1: 0.00010661887636860438, l2: 0.0003984045657629255   Iteration 53 of 100, tot loss = 5.083143211760611, l1: 0.00010759460523983744, l2: 0.0004007197191108475   Iteration 54 of 100, tot loss = 5.06991121062526, l1: 0.00010729239807847921, l2: 0.00039969872617533566   Iteration 55 of 100, tot loss = 5.078892460736362, l1: 0.00010753175143194808, l2: 0.0004003574973797764   Iteration 56 of 100, tot loss = 5.054107683045523, l1: 0.00010733682626908246, l2: 0.000398073944909681   Iteration 57 of 100, tot loss = 5.054819767935234, l1: 0.00010751248217338958, l2: 0.0003979694975944432   Iteration 58 of 100, tot loss = 5.038831825914054, l1: 0.00010675766943363826, l2: 0.0003971255162092536   Iteration 59 of 100, tot loss = 5.039674249746032, l1: 0.0001067099494660289, l2: 0.000397257478483479   Iteration 60 of 100, tot loss = 4.992995063463847, l1: 0.00010600986861390993, l2: 0.0003932896407301693   Iteration 61 of 100, tot loss = 4.954506151011733, l1: 0.00010542208522073467, l2: 0.0003900285328349068   Iteration 62 of 100, tot loss = 4.952440842505424, l1: 0.000105399342564558, l2: 0.0003898447448004698   Iteration 63 of 100, tot loss = 4.948165253987388, l1: 0.00010530172492883774, l2: 0.00038951480325074894   Iteration 64 of 100, tot loss = 5.01268757507205, l1: 0.00010641909182140807, l2: 0.00039484966873715166   Iteration 65 of 100, tot loss = 5.001564220281748, l1: 0.00010635346563783689, l2: 0.00039380295963313143   Iteration 66 of 100, tot loss = 5.044822067925424, l1: 0.00010712854771444461, l2: 0.0003973536619345065   Iteration 67 of 100, tot loss = 5.051715135574341, l1: 0.00010681936094700706, l2: 0.0003983521553402794   Iteration 68 of 100, tot loss = 5.049213889767142, l1: 0.00010680503424630254, l2: 0.00039811635703218226   Iteration 69 of 100, tot loss = 5.0459431323452275, l1: 0.00010673038786331165, l2: 0.0003978639277562067   Iteration 70 of 100, tot loss = 5.032886624336243, l1: 0.00010674419120602709, l2: 0.0003965444738111858   Iteration 71 of 100, tot loss = 5.01836977878087, l1: 0.00010656050341220206, l2: 0.00039527647674385404   Iteration 72 of 100, tot loss = 5.037552247444789, l1: 0.00010680441836383479, l2: 0.0003969508086609292   Iteration 73 of 100, tot loss = 5.0468549042531885, l1: 0.00010713578805037174, l2: 0.00039754970412229327   Iteration 74 of 100, tot loss = 5.052675430839126, l1: 0.00010707592180650052, l2: 0.0003981916226031309   Iteration 75 of 100, tot loss = 5.04105045636495, l1: 0.00010678226331947371, l2: 0.0003973227840227385   Iteration 76 of 100, tot loss = 5.032912163357985, l1: 0.00010677836689865217, l2: 0.00039651285126302883   Iteration 77 of 100, tot loss = 5.017102114565961, l1: 0.0001065605088528669, l2: 0.00039514970425939697   Iteration 78 of 100, tot loss = 4.990390970156743, l1: 0.00010618125610889342, l2: 0.00039285784255181893   Iteration 79 of 100, tot loss = 4.972337976286683, l1: 0.00010603795439974535, l2: 0.00039119584485151676   Iteration 80 of 100, tot loss = 4.937406545877456, l1: 0.00010540439097894705, l2: 0.00038833626531413755   Iteration 81 of 100, tot loss = 4.9605081876118975, l1: 0.00010604718597648941, l2: 0.0003900036343695297   Iteration 82 of 100, tot loss = 4.95522569447029, l1: 0.00010625459777302017, l2: 0.00038926797335479043   Iteration 83 of 100, tot loss = 4.948495968278632, l1: 0.00010638594242017604, l2: 0.0003884636564881552   Iteration 84 of 100, tot loss = 4.957080863770985, l1: 0.00010658141668716867, l2: 0.00038912667133401903   Iteration 85 of 100, tot loss = 4.978407859802246, l1: 0.00010685444362667006, l2: 0.0003909863442566027   Iteration 86 of 100, tot loss = 4.973203875297724, l1: 0.00010695049507397248, l2: 0.0003903698941643978   Iteration 87 of 100, tot loss = 4.951278867392705, l1: 0.00010644236334329257, l2: 0.0003886855250537588   Iteration 88 of 100, tot loss = 4.95631206035614, l1: 0.00010661504670265872, l2: 0.0003890161611211211   Iteration 89 of 100, tot loss = 4.922360363970982, l1: 0.00010589819226225608, l2: 0.0003863378458007584   Iteration 90 of 100, tot loss = 4.901171628634135, l1: 0.00010533482685180692, l2: 0.0003847823378034971   Iteration 91 of 100, tot loss = 4.884408125510583, l1: 0.00010523257059413571, l2: 0.00038320824371492184   Iteration 92 of 100, tot loss = 4.89004812810732, l1: 0.00010559010084050342, l2: 0.0003834147138822237   Iteration 93 of 100, tot loss = 4.891309484358756, l1: 0.0001057380036937864, l2: 0.00038339294632771604   Iteration 94 of 100, tot loss = 4.87333775073924, l1: 0.00010559526491944896, l2: 0.0003817385117377234   Iteration 95 of 100, tot loss = 4.880018379813746, l1: 0.00010582565538291083, l2: 0.0003821761842118576   Iteration 96 of 100, tot loss = 4.856604874134064, l1: 0.00010560030796114006, l2: 0.0003800601810629208   Iteration 97 of 100, tot loss = 4.8467851938660615, l1: 0.00010551293040013671, l2: 0.00037916559040709636   Iteration 98 of 100, tot loss = 4.848599801258165, l1: 0.00010564226724863128, l2: 0.0003792177143326143   Iteration 99 of 100, tot loss = 4.8560942953283135, l1: 0.00010583216116900058, l2: 0.00037977726973659084   Iteration 100 of 100, tot loss = 4.847594754695892, l1: 0.00010575609383522532, l2: 0.00037900338298641145
   End of epoch 1209; saving model... 

Epoch 1210 of 2000
   Iteration 1 of 100, tot loss = 3.5976850986480713, l1: 8.877085201675072e-05, l2: 0.0002709976688493043   Iteration 2 of 100, tot loss = 4.393179774284363, l1: 8.249733946286142e-05, l2: 0.0003568206448107958   Iteration 3 of 100, tot loss = 4.273315827051799, l1: 8.41445410818172e-05, l2: 0.00034318704274483025   Iteration 4 of 100, tot loss = 4.595052182674408, l1: 9.065939048014116e-05, l2: 0.00036884583096252754   Iteration 5 of 100, tot loss = 4.7894988536834715, l1: 9.544232161715627e-05, l2: 0.0003835075651295483   Iteration 6 of 100, tot loss = 4.744733452796936, l1: 0.00010050752219588806, l2: 0.00037396582774817944   Iteration 7 of 100, tot loss = 4.300474047660828, l1: 9.269266800921676e-05, l2: 0.0003373547411423975   Iteration 8 of 100, tot loss = 4.150340661406517, l1: 9.074116860574577e-05, l2: 0.00032429289967694785   Iteration 9 of 100, tot loss = 4.648286912176344, l1: 9.632248353833954e-05, l2: 0.0003685062127058498   Iteration 10 of 100, tot loss = 4.547697079181671, l1: 9.436129985260778e-05, l2: 0.000360408412234392   Iteration 11 of 100, tot loss = 4.6842416091398755, l1: 9.772679781731726e-05, l2: 0.00037069736572448164   Iteration 12 of 100, tot loss = 4.696147054433823, l1: 9.930706437444314e-05, l2: 0.0003703076448194527   Iteration 13 of 100, tot loss = 4.664400366636423, l1: 9.912892822355319e-05, l2: 0.0003673111104245226   Iteration 14 of 100, tot loss = 4.728673807212284, l1: 0.00010240355706108468, l2: 0.0003704638272340942   Iteration 15 of 100, tot loss = 4.700341423352559, l1: 0.00010125438178268572, l2: 0.0003687797647823269   Iteration 16 of 100, tot loss = 4.666654236614704, l1: 9.969958455258165e-05, l2: 0.0003669658444778179   Iteration 17 of 100, tot loss = 4.527522823389838, l1: 9.776620701064959e-05, l2: 0.0003549860814339755   Iteration 18 of 100, tot loss = 4.682451996538374, l1: 0.00010153269330557022, l2: 0.00036671251129721187   Iteration 19 of 100, tot loss = 4.71927357347388, l1: 0.00010215292844039045, l2: 0.00036977443347783074   Iteration 20 of 100, tot loss = 4.7826316177845, l1: 0.00010389080562163144, l2: 0.00037437236023833974   Iteration 21 of 100, tot loss = 4.783194865499224, l1: 0.00010466296849439718, l2: 0.0003736565219393621   Iteration 22 of 100, tot loss = 4.840015568516471, l1: 0.00010523732006814416, l2: 0.00037876424109774894   Iteration 23 of 100, tot loss = 4.817225502884907, l1: 0.00010546641557163599, l2: 0.0003762561385013649   Iteration 24 of 100, tot loss = 4.757586504022281, l1: 0.00010468314364212954, l2: 0.0003710755105809464   Iteration 25 of 100, tot loss = 4.8704663133621215, l1: 0.00010589761164737865, l2: 0.0003811490221414715   Iteration 26 of 100, tot loss = 4.768094140749711, l1: 0.00010464855250365172, l2: 0.0003721608635924685   Iteration 27 of 100, tot loss = 4.754315945837233, l1: 0.00010384558010156508, l2: 0.00037158601669404933   Iteration 28 of 100, tot loss = 4.65426430957658, l1: 0.00010149987390052826, l2: 0.00036392655913784565   Iteration 29 of 100, tot loss = 4.670998799389806, l1: 0.00010190616878138951, l2: 0.00036519371346442094   Iteration 30 of 100, tot loss = 4.659939650694529, l1: 0.00010101232461844726, l2: 0.000364981642148147   Iteration 31 of 100, tot loss = 4.699654329207636, l1: 0.00010171170839795752, l2: 0.00036825372597142574   Iteration 32 of 100, tot loss = 4.724063377827406, l1: 0.00010164709976834274, l2: 0.00037075923864904325   Iteration 33 of 100, tot loss = 4.699137517900178, l1: 0.00010151685593441609, l2: 0.00036839689621984053   Iteration 34 of 100, tot loss = 4.67347968676511, l1: 0.0001013375668288277, l2: 0.00036601040183621296   Iteration 35 of 100, tot loss = 4.606807684898376, l1: 9.977701068107438e-05, l2: 0.0003609037575578051   Iteration 36 of 100, tot loss = 4.617166827122371, l1: 9.959323920662023e-05, l2: 0.00036212344356398616   Iteration 37 of 100, tot loss = 4.551306579564069, l1: 9.827498011086514e-05, l2: 0.00035685567756347055   Iteration 38 of 100, tot loss = 4.549498718035848, l1: 9.784308205876426e-05, l2: 0.0003571067902253401   Iteration 39 of 100, tot loss = 4.494320487364744, l1: 9.644726639798579e-05, l2: 0.0003529847827172862   Iteration 40 of 100, tot loss = 4.537720474600792, l1: 9.692642779555172e-05, l2: 0.0003568456191715086   Iteration 41 of 100, tot loss = 4.600951366308259, l1: 9.817849396069238e-05, l2: 0.0003619166421164509   Iteration 42 of 100, tot loss = 4.5941354632377625, l1: 9.840805155579888e-05, l2: 0.0003610054946399205   Iteration 43 of 100, tot loss = 4.572946024495502, l1: 9.805731492950905e-05, l2: 0.0003592372878026867   Iteration 44 of 100, tot loss = 4.615905574776909, l1: 9.836866113553036e-05, l2: 0.00036322189639163713   Iteration 45 of 100, tot loss = 4.604929243193732, l1: 9.869841596810147e-05, l2: 0.0003617945085150293   Iteration 46 of 100, tot loss = 4.594058770200481, l1: 9.806282310898456e-05, l2: 0.0003613430539110635   Iteration 47 of 100, tot loss = 4.5983098288799855, l1: 9.746403752905415e-05, l2: 0.0003623669447520986   Iteration 48 of 100, tot loss = 4.575978669027488, l1: 9.733722132902282e-05, l2: 0.00036026064511437045   Iteration 49 of 100, tot loss = 4.5857356348816225, l1: 9.77416815918072e-05, l2: 0.0003608318811344287   Iteration 50 of 100, tot loss = 4.6043290257453915, l1: 9.777188184671103e-05, l2: 0.00036266101960791275   Iteration 51 of 100, tot loss = 4.609500700352239, l1: 9.841280567058016e-05, l2: 0.0003625372628448531   Iteration 52 of 100, tot loss = 4.599359221183336, l1: 9.788311777251343e-05, l2: 0.00036205280356136005   Iteration 53 of 100, tot loss = 4.598498787520067, l1: 9.827778873995896e-05, l2: 0.0003615720897697721   Iteration 54 of 100, tot loss = 4.582042426974685, l1: 9.860611828992626e-05, l2: 0.00035959812440624874   Iteration 55 of 100, tot loss = 4.601744649626992, l1: 9.899702735393392e-05, l2: 0.0003611774382244965   Iteration 56 of 100, tot loss = 4.593730234674045, l1: 9.886369675119308e-05, l2: 0.00036050932742780006   Iteration 57 of 100, tot loss = 4.603495871811583, l1: 9.935062923448226e-05, l2: 0.00036099895844113474   Iteration 58 of 100, tot loss = 4.593497109824214, l1: 9.919499317565451e-05, l2: 0.0003601547186604123   Iteration 59 of 100, tot loss = 4.636728268558696, l1: 0.00010020166284787484, l2: 0.0003634711643148182   Iteration 60 of 100, tot loss = 4.652257909377416, l1: 0.00010013406851309507, l2: 0.000365091723263807   Iteration 61 of 100, tot loss = 4.631877158508926, l1: 9.98561086131214e-05, l2: 0.000363331608002868   Iteration 62 of 100, tot loss = 4.648423681336064, l1: 0.00010012390386339487, l2: 0.0003647184651438898   Iteration 63 of 100, tot loss = 4.648373938742138, l1: 0.00010054855455463339, l2: 0.0003642888399236466   Iteration 64 of 100, tot loss = 4.6721957344561815, l1: 0.00010076605315134657, l2: 0.000366453521337462   Iteration 65 of 100, tot loss = 4.657193710253789, l1: 0.00010069818123995972, l2: 0.00036502119104485386   Iteration 66 of 100, tot loss = 4.64518490524003, l1: 0.00010055106119552599, l2: 0.0003639674305855861   Iteration 67 of 100, tot loss = 4.628136508500398, l1: 0.00010018656052016555, l2: 0.00036262709160036507   Iteration 68 of 100, tot loss = 4.610475008978563, l1: 9.995397072891458e-05, l2: 0.0003610935311692575   Iteration 69 of 100, tot loss = 4.5975044171015425, l1: 9.97387565355208e-05, l2: 0.0003600116860057376   Iteration 70 of 100, tot loss = 4.628698480129242, l1: 0.00010016746195365808, l2: 0.00036270238696098594   Iteration 71 of 100, tot loss = 4.601359177643145, l1: 9.944249920319097e-05, l2: 0.0003606934195899234   Iteration 72 of 100, tot loss = 4.631567628847228, l1: 0.00010031912971600023, l2: 0.0003628376339798302   Iteration 73 of 100, tot loss = 4.646913409233093, l1: 0.00010065479174755475, l2: 0.0003640365502589752   Iteration 74 of 100, tot loss = 4.6176422109475, l1: 9.989045724363973e-05, l2: 0.00036187376492700815   Iteration 75 of 100, tot loss = 4.628049546877543, l1: 0.00010019298497354612, l2: 0.0003626119704373802   Iteration 76 of 100, tot loss = 4.624118812774357, l1: 0.00010000312642277047, l2: 0.0003624087554558891   Iteration 77 of 100, tot loss = 4.64283270185644, l1: 0.00010037818039516471, l2: 0.00036390509033823606   Iteration 78 of 100, tot loss = 4.626852733966632, l1: 0.00010022164198558014, l2: 0.0003624636318328647   Iteration 79 of 100, tot loss = 4.603073148787776, l1: 9.963459446156822e-05, l2: 0.0003606727207890016   Iteration 80 of 100, tot loss = 4.597569344937801, l1: 9.950224348358461e-05, l2: 0.0003602546914407867   Iteration 81 of 100, tot loss = 4.598492461958049, l1: 9.986250361292565e-05, l2: 0.000359986743273179   Iteration 82 of 100, tot loss = 4.616612008432063, l1: 0.00010030453773740702, l2: 0.0003613566643652143   Iteration 83 of 100, tot loss = 4.608484367290175, l1: 0.00010024070982044248, l2: 0.0003606077283270186   Iteration 84 of 100, tot loss = 4.605542394376936, l1: 0.00010019818849589986, l2: 0.00036035605219131826   Iteration 85 of 100, tot loss = 4.620406286856707, l1: 0.00010057029807392289, l2: 0.0003614703318282195   Iteration 86 of 100, tot loss = 4.607528607512629, l1: 0.00010006425885834223, l2: 0.0003606886029321982   Iteration 87 of 100, tot loss = 4.61134712038369, l1: 0.00010014163975332242, l2: 0.0003609930728120601   Iteration 88 of 100, tot loss = 4.619035291400823, l1: 0.00010019567654043468, l2: 0.0003617078534510008   Iteration 89 of 100, tot loss = 4.601488195108564, l1: 0.00010003033204473753, l2: 0.0003601184882958723   Iteration 90 of 100, tot loss = 4.599999090035756, l1: 0.00010001685776741295, l2: 0.0003599830519912454   Iteration 91 of 100, tot loss = 4.610705932418068, l1: 0.0001001070321273381, l2: 0.0003609635621235082   Iteration 92 of 100, tot loss = 4.60750480449718, l1: 0.0001000392075698271, l2: 0.0003607112738119094   Iteration 93 of 100, tot loss = 4.6240332062526415, l1: 0.00010010379311654927, l2: 0.0003622995283622395   Iteration 94 of 100, tot loss = 4.619263158199635, l1: 9.996868813802993e-05, l2: 0.00036195762832574743   Iteration 95 of 100, tot loss = 4.581969215995387, l1: 9.926012407001843e-05, l2: 0.00035893679818591886   Iteration 96 of 100, tot loss = 4.555334913233916, l1: 9.868322577707052e-05, l2: 0.0003568502663711115   Iteration 97 of 100, tot loss = 4.556287581158667, l1: 9.89384301197019e-05, l2: 0.00035669032888446177   Iteration 98 of 100, tot loss = 4.5631105778168655, l1: 9.89958097102781e-05, l2: 0.00035731524918929255   Iteration 99 of 100, tot loss = 4.5646488570203685, l1: 9.89467323851193e-05, l2: 0.0003575181544874795   Iteration 100 of 100, tot loss = 4.570995175838471, l1: 9.895325489196693e-05, l2: 0.0003581462633883348
   End of epoch 1210; saving model... 

Epoch 1211 of 2000
   Iteration 1 of 100, tot loss = 3.513361930847168, l1: 9.787594899535179e-05, l2: 0.00025346025358885527   Iteration 2 of 100, tot loss = 3.1754621267318726, l1: 7.406027725664899e-05, l2: 0.00024348593433387578   Iteration 3 of 100, tot loss = 2.9268669287363687, l1: 6.770152685930952e-05, l2: 0.0002249851628827552   Iteration 4 of 100, tot loss = 3.6585587859153748, l1: 8.14863797131693e-05, l2: 0.00028436949651222676   Iteration 5 of 100, tot loss = 3.663183832168579, l1: 8.117432735161856e-05, l2: 0.0002851440571248531   Iteration 6 of 100, tot loss = 3.76677930355072, l1: 8.32457299111411e-05, l2: 0.0002934322013364484   Iteration 7 of 100, tot loss = 3.620600598199027, l1: 8.289080869872123e-05, l2: 0.00027916925526889306   Iteration 8 of 100, tot loss = 3.761379688978195, l1: 8.501859883836005e-05, l2: 0.00029111937328707427   Iteration 9 of 100, tot loss = 3.9989716476864285, l1: 8.985141984238807e-05, l2: 0.00031004575107039674   Iteration 10 of 100, tot loss = 4.172598242759705, l1: 8.972060968517325e-05, l2: 0.00032753921987023206   Iteration 11 of 100, tot loss = 4.098611788316206, l1: 8.942230223593387e-05, l2: 0.00032043888297101313   Iteration 12 of 100, tot loss = 4.3876798550287885, l1: 9.408124727391017e-05, l2: 0.0003446867437257121   Iteration 13 of 100, tot loss = 4.501575800088736, l1: 9.77007752785889e-05, l2: 0.00035245680751708837   Iteration 14 of 100, tot loss = 4.4957809788840155, l1: 9.690470947784238e-05, l2: 0.0003526733905476119   Iteration 15 of 100, tot loss = 4.514992618560791, l1: 9.505143049561108e-05, l2: 0.00035644783444392186   Iteration 16 of 100, tot loss = 4.5468869805336, l1: 9.624680978959077e-05, l2: 0.00035844189187628217   Iteration 17 of 100, tot loss = 4.489753737169154, l1: 9.491923607557135e-05, l2: 0.0003540561404353117   Iteration 18 of 100, tot loss = 4.482274783982171, l1: 9.485755435889587e-05, l2: 0.00035336992813326005   Iteration 19 of 100, tot loss = 4.392328789359645, l1: 9.348847111380707e-05, l2: 0.00034574441087897867   Iteration 20 of 100, tot loss = 4.42316460609436, l1: 9.510495183349122e-05, l2: 0.0003472115109616425   Iteration 21 of 100, tot loss = 4.4319864227658226, l1: 9.56516703895648e-05, l2: 0.00034754697463497343   Iteration 22 of 100, tot loss = 4.433564944700762, l1: 9.595410433989441e-05, l2: 0.00034740239326228305   Iteration 23 of 100, tot loss = 4.478994929272195, l1: 9.769087678843947e-05, l2: 0.0003502086183289066   Iteration 24 of 100, tot loss = 4.521859606107076, l1: 9.695412063592812e-05, l2: 0.00035523184184664086   Iteration 25 of 100, tot loss = 4.511348247528076, l1: 9.690001199487597e-05, l2: 0.00035423481429461387   Iteration 26 of 100, tot loss = 4.514638332220224, l1: 9.638845334008623e-05, l2: 0.00035507538156637637   Iteration 27 of 100, tot loss = 4.525643719567193, l1: 9.658799454983737e-05, l2: 0.00035597637987747375   Iteration 28 of 100, tot loss = 4.487315237522125, l1: 9.602275609462854e-05, l2: 0.00035270876989151087   Iteration 29 of 100, tot loss = 4.554929873038983, l1: 9.779989391220091e-05, l2: 0.0003576930968596696   Iteration 30 of 100, tot loss = 4.608307973543803, l1: 9.889676269570676e-05, l2: 0.00036193403696718935   Iteration 31 of 100, tot loss = 4.680897720398441, l1: 9.99740274686877e-05, l2: 0.00036811574611948024   Iteration 32 of 100, tot loss = 4.6895662769675255, l1: 0.00010020627496487577, l2: 0.0003687503535729775   Iteration 33 of 100, tot loss = 4.64868890877926, l1: 9.996741441268982e-05, l2: 0.0003649014767936685   Iteration 34 of 100, tot loss = 4.693296152002671, l1: 0.00010097658185473206, l2: 0.00036835303334930143   Iteration 35 of 100, tot loss = 4.631043250220163, l1: 9.990712735868458e-05, l2: 0.0003631971980212256   Iteration 36 of 100, tot loss = 4.63117210732566, l1: 9.988925861155924e-05, l2: 0.000363227952928153   Iteration 37 of 100, tot loss = 4.6013106977617415, l1: 9.984459265487621e-05, l2: 0.00036028647721380095   Iteration 38 of 100, tot loss = 4.560097430881701, l1: 9.972837965745527e-05, l2: 0.00035628136346962203   Iteration 39 of 100, tot loss = 4.566506593655317, l1: 0.00010001562138010438, l2: 0.0003566350381170662   Iteration 40 of 100, tot loss = 4.557240986824036, l1: 0.00010060583317681449, l2: 0.0003551182660885388   Iteration 41 of 100, tot loss = 4.589726320127162, l1: 0.00010137022320300386, l2: 0.0003576024095442646   Iteration 42 of 100, tot loss = 4.599125067392985, l1: 0.00010149283497455708, l2: 0.00035841967266086224   Iteration 43 of 100, tot loss = 4.543637774711431, l1: 0.00010039712956207696, l2: 0.0003539666487445492   Iteration 44 of 100, tot loss = 4.520324452356859, l1: 0.00010052782338541213, l2: 0.000351504622482356   Iteration 45 of 100, tot loss = 4.506180005603366, l1: 0.00010043505107104364, l2: 0.00035018295036732324   Iteration 46 of 100, tot loss = 4.510704750600069, l1: 0.00010067175136772794, l2: 0.00035039872454945   Iteration 47 of 100, tot loss = 4.488165409006971, l1: 0.00010068658253232493, l2: 0.0003481299595961823   Iteration 48 of 100, tot loss = 4.508537222941716, l1: 0.00010088065876819503, l2: 0.00034997306435495074   Iteration 49 of 100, tot loss = 4.514195267035037, l1: 0.00010079882278140368, l2: 0.0003506207042159893   Iteration 50 of 100, tot loss = 4.501279058456421, l1: 0.00010075735626742243, l2: 0.00034937054995680226   Iteration 51 of 100, tot loss = 4.471055124320236, l1: 0.00010035585671407626, l2: 0.0003467496560416276   Iteration 52 of 100, tot loss = 4.498886777804448, l1: 0.00010082163442768014, l2: 0.00034906704324664763   Iteration 53 of 100, tot loss = 4.53720261015982, l1: 0.0001009629733469513, l2: 0.00035275728798708615   Iteration 54 of 100, tot loss = 4.5374885046923605, l1: 0.00010062114867010948, l2: 0.00035312770207248696   Iteration 55 of 100, tot loss = 4.540913945978338, l1: 0.00010064286497336898, l2: 0.00035344852975950663   Iteration 56 of 100, tot loss = 4.524617139782224, l1: 0.00010056558676296845, l2: 0.00035189612728052974   Iteration 57 of 100, tot loss = 4.506852501317074, l1: 0.00010051034756511319, l2: 0.00035017490281780627   Iteration 58 of 100, tot loss = 4.494523973300539, l1: 0.00010021294417975727, l2: 0.00034923945299694957   Iteration 59 of 100, tot loss = 4.512305983042313, l1: 0.00010023695999811703, l2: 0.0003509936384142424   Iteration 60 of 100, tot loss = 4.50551882982254, l1: 0.00010011181814964706, l2: 0.0003504400648428903   Iteration 61 of 100, tot loss = 4.576430254295224, l1: 0.00010109750530587082, l2: 0.0003565455203375886   Iteration 62 of 100, tot loss = 4.555550294537698, l1: 0.00010098028348781349, l2: 0.00035457474610490363   Iteration 63 of 100, tot loss = 4.55804063024975, l1: 0.00010115201404828223, l2: 0.0003546520493396129   Iteration 64 of 100, tot loss = 4.542090188711882, l1: 0.00010107378045631776, l2: 0.0003531352388108644   Iteration 65 of 100, tot loss = 4.55053815474877, l1: 0.00010133977439881374, l2: 0.00035371404172530256   Iteration 66 of 100, tot loss = 4.555804711399657, l1: 0.00010130074191362259, l2: 0.0003542797298216487   Iteration 67 of 100, tot loss = 4.570370521118392, l1: 0.00010148227206559907, l2: 0.00035555478000367033   Iteration 68 of 100, tot loss = 4.584245306604049, l1: 0.00010163423030462582, l2: 0.00035679030042956583   Iteration 69 of 100, tot loss = 4.563471393308777, l1: 0.00010131936711411032, l2: 0.00035502777250983036   Iteration 70 of 100, tot loss = 4.562490606307984, l1: 0.00010157025348494894, l2: 0.0003546788074475314   Iteration 71 of 100, tot loss = 4.569848376260677, l1: 0.00010164273746187051, l2: 0.00035534210053836585   Iteration 72 of 100, tot loss = 4.571448855929905, l1: 0.00010157723085689617, l2: 0.0003555676553838162   Iteration 73 of 100, tot loss = 4.590594690139979, l1: 0.00010190592609881742, l2: 0.0003571535439919425   Iteration 74 of 100, tot loss = 4.586210682585433, l1: 0.00010175931055454311, l2: 0.000356861759084151   Iteration 75 of 100, tot loss = 4.596336256663005, l1: 0.0001012997697883596, l2: 0.0003583338573419799   Iteration 76 of 100, tot loss = 4.6137733145764, l1: 0.00010164586140155351, l2: 0.0003597314713069385   Iteration 77 of 100, tot loss = 4.606493596906786, l1: 0.00010128661658684421, l2: 0.0003593627445841232   Iteration 78 of 100, tot loss = 4.578226768053495, l1: 0.00010074728132353928, l2: 0.0003570753967953631   Iteration 79 of 100, tot loss = 4.5998871054830435, l1: 0.00010134693214159955, l2: 0.0003586417803455356   Iteration 80 of 100, tot loss = 4.645010524988175, l1: 0.0001019143329358485, l2: 0.00036258672171243236   Iteration 81 of 100, tot loss = 4.648147547686541, l1: 0.00010200767006429004, l2: 0.00036280708697216334   Iteration 82 of 100, tot loss = 4.6795211303524855, l1: 0.00010256863118606484, l2: 0.00036538348418927363   Iteration 83 of 100, tot loss = 4.688738604626024, l1: 0.00010270330317161349, l2: 0.00036617055979237546   Iteration 84 of 100, tot loss = 4.690002872830346, l1: 0.0001026908284984529, l2: 0.00036630946141529073   Iteration 85 of 100, tot loss = 4.751097118153291, l1: 0.00010352457620148711, l2: 0.00037158513835439566   Iteration 86 of 100, tot loss = 4.717477479646372, l1: 0.00010284599764333486, l2: 0.00036890175312670855   Iteration 87 of 100, tot loss = 4.754217479420804, l1: 0.00010331804953929004, l2: 0.0003721037017845217   Iteration 88 of 100, tot loss = 4.744956975633448, l1: 0.00010329186207772264, l2: 0.00037120383885022335   Iteration 89 of 100, tot loss = 4.740309024125002, l1: 0.00010307818221018567, l2: 0.0003709527235504324   Iteration 90 of 100, tot loss = 4.775297138426039, l1: 0.00010365778972401231, l2: 0.00037387192747296975   Iteration 91 of 100, tot loss = 4.770216061518743, l1: 0.00010357548416033132, l2: 0.0003734461254450482   Iteration 92 of 100, tot loss = 4.7577798340631565, l1: 0.00010327688889209757, l2: 0.00037250109807760253   Iteration 93 of 100, tot loss = 4.756308624821324, l1: 0.00010323143789954033, l2: 0.00037239942829855665   Iteration 94 of 100, tot loss = 4.758370117938265, l1: 0.00010326030907039085, l2: 0.0003725767062550926   Iteration 95 of 100, tot loss = 4.754959174206382, l1: 0.00010338459990299798, l2: 0.0003721113212553686   Iteration 96 of 100, tot loss = 4.759948723018169, l1: 0.00010333102587386141, l2: 0.00037266385030913324   Iteration 97 of 100, tot loss = 4.727250569874478, l1: 0.0001027891091335104, l2: 0.00036993595168712196   Iteration 98 of 100, tot loss = 4.715798885238414, l1: 0.00010287496241995095, l2: 0.0003687049298902394   Iteration 99 of 100, tot loss = 4.7172329991754856, l1: 0.00010311707489444131, l2: 0.0003686062288276541   Iteration 100 of 100, tot loss = 4.734028946161271, l1: 0.000103188143693842, l2: 0.00037021475458459463
   End of epoch 1211; saving model... 

Epoch 1212 of 2000
   Iteration 1 of 100, tot loss = 4.923676490783691, l1: 0.00010888615361182019, l2: 0.00038348149973899126   Iteration 2 of 100, tot loss = 4.844837427139282, l1: 0.0001066521908796858, l2: 0.0003778315440285951   Iteration 3 of 100, tot loss = 4.506354808807373, l1: 9.153912348362307e-05, l2: 0.0003590963509244223   Iteration 4 of 100, tot loss = 5.327467441558838, l1: 0.00010310525249224156, l2: 0.0004296414990676567   Iteration 5 of 100, tot loss = 5.524360847473145, l1: 0.00010423322237329558, l2: 0.00044820286566391585   Iteration 6 of 100, tot loss = 5.455447991689046, l1: 0.00011001590731514928, l2: 0.00043552889352819574   Iteration 7 of 100, tot loss = 5.264200074332101, l1: 0.00010606215905032254, l2: 0.00042035784489209097   Iteration 8 of 100, tot loss = 5.286417305469513, l1: 0.00010667033984645968, l2: 0.0004219713882775977   Iteration 9 of 100, tot loss = 5.077869865629408, l1: 0.00010373138745004933, l2: 0.00040405559896801907   Iteration 10 of 100, tot loss = 5.015676760673523, l1: 0.00010190116809098982, l2: 0.00039966650947462765   Iteration 11 of 100, tot loss = 5.1862217512997715, l1: 0.00010203512912002307, l2: 0.00041658704603006214   Iteration 12 of 100, tot loss = 5.123988211154938, l1: 0.00010186829119144629, l2: 0.0004105305318565418   Iteration 13 of 100, tot loss = 5.170808150218083, l1: 0.00010460340686572286, l2: 0.000412477408714879   Iteration 14 of 100, tot loss = 5.0528810024261475, l1: 0.00010305095773739075, l2: 0.00040223714313469827   Iteration 15 of 100, tot loss = 4.927165857950846, l1: 0.00010116282113206884, l2: 0.0003915537662881737   Iteration 16 of 100, tot loss = 4.940690785646439, l1: 0.00010225574305877672, l2: 0.00039181333613669267   Iteration 17 of 100, tot loss = 4.774868081597721, l1: 9.990508859867558e-05, l2: 0.00037758171931593954   Iteration 18 of 100, tot loss = 4.991390771336025, l1: 0.00010358886356698349, l2: 0.00039555021551980946   Iteration 19 of 100, tot loss = 5.031414044530768, l1: 0.00010582397762367404, l2: 0.0003973174308684017   Iteration 20 of 100, tot loss = 5.080458199977874, l1: 0.00010625055292621255, l2: 0.00040179527204600165   Iteration 21 of 100, tot loss = 5.021223034177508, l1: 0.00010429033309698017, l2: 0.00039783197419074854   Iteration 22 of 100, tot loss = 4.997992981563915, l1: 0.00010407612121939151, l2: 0.00039572318002518097   Iteration 23 of 100, tot loss = 5.093972112821496, l1: 0.00010542964130012399, l2: 0.0004039675727456241   Iteration 24 of 100, tot loss = 5.037009249130885, l1: 0.00010411774474050617, l2: 0.00039958318241891294   Iteration 25 of 100, tot loss = 4.9898511505126955, l1: 0.00010421533021144569, l2: 0.0003947697865078226   Iteration 26 of 100, tot loss = 4.915508371133071, l1: 0.00010298938706490354, l2: 0.00038856145189269085   Iteration 27 of 100, tot loss = 4.865663749200326, l1: 0.0001022880985854297, l2: 0.0003842782780002044   Iteration 28 of 100, tot loss = 4.862284975392478, l1: 0.00010251259144362328, l2: 0.0003837159077063136   Iteration 29 of 100, tot loss = 4.827932078262855, l1: 0.0001012077010618041, l2: 0.0003815855081233292   Iteration 30 of 100, tot loss = 4.8373249212900795, l1: 0.00010090000165898042, l2: 0.00038283249353601907   Iteration 31 of 100, tot loss = 4.801162596671812, l1: 0.00010032774838280954, l2: 0.0003797885142229197   Iteration 32 of 100, tot loss = 4.776953890919685, l1: 0.00010072069767375069, l2: 0.00037697469497288694   Iteration 33 of 100, tot loss = 4.754506140044241, l1: 0.0001007505777648255, l2: 0.00037470003933942115   Iteration 34 of 100, tot loss = 4.746836255578434, l1: 0.00010090298875344589, l2: 0.00037378063964698576   Iteration 35 of 100, tot loss = 4.7306356021336144, l1: 0.00010044632425498484, l2: 0.0003726172381513087   Iteration 36 of 100, tot loss = 4.690470569663578, l1: 0.00010031745235513274, l2: 0.00036872960643247806   Iteration 37 of 100, tot loss = 4.68569421123814, l1: 0.0001008607607611732, l2: 0.00036770866230457415   Iteration 38 of 100, tot loss = 4.701359077503807, l1: 0.00010131610389706973, l2: 0.0003688198056280319   Iteration 39 of 100, tot loss = 4.679473455135639, l1: 0.00010117464896607905, l2: 0.00036677269814297173   Iteration 40 of 100, tot loss = 4.644561779499054, l1: 0.00010077573588205269, l2: 0.00036368044384289534   Iteration 41 of 100, tot loss = 4.717938748801627, l1: 0.00010174133288872814, l2: 0.0003700525437413556   Iteration 42 of 100, tot loss = 4.733821675890968, l1: 0.00010185626489969546, l2: 0.00037152590465709743   Iteration 43 of 100, tot loss = 4.699390123056811, l1: 0.0001007420358418617, l2: 0.0003691969783218621   Iteration 44 of 100, tot loss = 4.653834901072762, l1: 9.935248329962964e-05, l2: 0.00036603100869053213   Iteration 45 of 100, tot loss = 4.622332832548353, l1: 9.912892120256503e-05, l2: 0.0003631043639163383   Iteration 46 of 100, tot loss = 4.667601497276969, l1: 9.999146870699326e-05, l2: 0.0003667686823578349   Iteration 47 of 100, tot loss = 4.725710143434241, l1: 0.00010123368167811311, l2: 0.00037133733356777736   Iteration 48 of 100, tot loss = 4.705328087011973, l1: 0.00010087042505801946, l2: 0.0003696623840975614   Iteration 49 of 100, tot loss = 4.73563526114639, l1: 0.00010131400074674368, l2: 0.00037224952527084294   Iteration 50 of 100, tot loss = 4.70071120262146, l1: 0.00010107995760336053, l2: 0.0003689911623951048   Iteration 51 of 100, tot loss = 4.704870298797009, l1: 0.00010121989230266955, l2: 0.0003692671370135584   Iteration 52 of 100, tot loss = 4.692507092769329, l1: 0.0001015564974015713, l2: 0.00036769421198047127   Iteration 53 of 100, tot loss = 4.697570323944092, l1: 0.00010158739431337278, l2: 0.0003681696390179601   Iteration 54 of 100, tot loss = 4.703886738529912, l1: 0.00010190651873590132, l2: 0.000368482156234138   Iteration 55 of 100, tot loss = 4.691411365162242, l1: 0.00010212392087189734, l2: 0.0003670172167899595   Iteration 56 of 100, tot loss = 4.697881383555276, l1: 0.0001027281500682875, l2: 0.0003670599892627381   Iteration 57 of 100, tot loss = 4.72572528270253, l1: 0.00010350663931029358, l2: 0.0003690658905134912   Iteration 58 of 100, tot loss = 4.7038635919834, l1: 0.00010355635477653464, l2: 0.0003668300058303305   Iteration 59 of 100, tot loss = 4.7069357330516235, l1: 0.0001032187218528233, l2: 0.00036747485297629454   Iteration 60 of 100, tot loss = 4.7360261162122095, l1: 0.00010359698274745218, l2: 0.00037000563000522867   Iteration 61 of 100, tot loss = 4.717617062271619, l1: 0.00010347050848678441, l2: 0.0003682911989076796   Iteration 62 of 100, tot loss = 4.708549603339164, l1: 0.00010359300986531326, l2: 0.0003672619516897436   Iteration 63 of 100, tot loss = 4.767159882045927, l1: 0.00010442383681370374, l2: 0.0003722921530771557   Iteration 64 of 100, tot loss = 4.755862329155207, l1: 0.00010424919031493118, l2: 0.0003713370444984321   Iteration 65 of 100, tot loss = 4.7304294732900765, l1: 0.00010337675882440704, l2: 0.00036966619059407654   Iteration 66 of 100, tot loss = 4.702562274354877, l1: 0.00010294506583622106, l2: 0.000367311163713704   Iteration 67 of 100, tot loss = 4.6960164468679855, l1: 0.00010273910778941267, l2: 0.00036686253925062606   Iteration 68 of 100, tot loss = 4.689366200390984, l1: 0.00010270109118316802, l2: 0.0003662355306647334   Iteration 69 of 100, tot loss = 4.673468662344891, l1: 0.00010219977311730938, l2: 0.0003651470946404489   Iteration 70 of 100, tot loss = 4.69878169127873, l1: 0.00010241121204411944, l2: 0.00036746695903795105   Iteration 71 of 100, tot loss = 4.669684275774888, l1: 0.00010181670330266441, l2: 0.00036515172613574797   Iteration 72 of 100, tot loss = 4.653813504510456, l1: 0.00010182361079892467, l2: 0.000363557741366094   Iteration 73 of 100, tot loss = 4.6605262135806145, l1: 0.00010202328505254453, l2: 0.00036402933789123716   Iteration 74 of 100, tot loss = 4.622365400597856, l1: 0.00010144648837701204, l2: 0.0003607900533195887   Iteration 75 of 100, tot loss = 4.609305438995361, l1: 0.00010080227613798343, l2: 0.0003601282691427817   Iteration 76 of 100, tot loss = 4.60892853611394, l1: 0.00010099098649094997, l2: 0.00035990186810714046   Iteration 77 of 100, tot loss = 4.616947818112064, l1: 0.00010126207010441024, l2: 0.0003604327132713727   Iteration 78 of 100, tot loss = 4.612417746812869, l1: 0.00010131660602406229, l2: 0.0003599251698081692   Iteration 79 of 100, tot loss = 4.607775645919993, l1: 0.00010161291633082754, l2: 0.0003591646496324411   Iteration 80 of 100, tot loss = 4.6333102822303776, l1: 0.0001018743686927337, l2: 0.00036145666090305897   Iteration 81 of 100, tot loss = 4.656673372527699, l1: 0.00010217185972803157, l2: 0.0003634954789756901   Iteration 82 of 100, tot loss = 4.6867361010574715, l1: 0.00010274270230516644, l2: 0.0003659309096713891   Iteration 83 of 100, tot loss = 4.700214747922966, l1: 0.00010298245865395798, l2: 0.00036703901826852866   Iteration 84 of 100, tot loss = 4.688072204589844, l1: 0.0001028492608730587, l2: 0.0003659579614620833   Iteration 85 of 100, tot loss = 4.672837582756491, l1: 0.00010255573848981465, l2: 0.00036472802166827024   Iteration 86 of 100, tot loss = 4.67462214203768, l1: 0.00010286202843925039, l2: 0.00036460018769545523   Iteration 87 of 100, tot loss = 4.660356116020816, l1: 0.00010279338123931163, l2: 0.00036324223205072526   Iteration 88 of 100, tot loss = 4.682705456560308, l1: 0.00010318182735731666, l2: 0.0003650887199248907   Iteration 89 of 100, tot loss = 4.692300180370888, l1: 0.00010325221927395729, l2: 0.000365977800448603   Iteration 90 of 100, tot loss = 4.703010871675279, l1: 0.00010357977165161476, l2: 0.0003667213171461804   Iteration 91 of 100, tot loss = 4.713125407040774, l1: 0.00010366547632237108, l2: 0.0003676470660138875   Iteration 92 of 100, tot loss = 4.711146271747092, l1: 0.00010354701569904928, l2: 0.0003675676131894326   Iteration 93 of 100, tot loss = 4.717448967759327, l1: 0.00010380469823865751, l2: 0.0003679402001071922   Iteration 94 of 100, tot loss = 4.6960706203541855, l1: 0.000103420512266197, l2: 0.0003661865514314218   Iteration 95 of 100, tot loss = 4.70475852363988, l1: 0.00010373895971784613, l2: 0.00036673689441828937   Iteration 96 of 100, tot loss = 4.735575020313263, l1: 0.00010385213768889419, l2: 0.00036970536636241985   Iteration 97 of 100, tot loss = 4.728722444514639, l1: 0.00010376234988762382, l2: 0.00036910989639210064   Iteration 98 of 100, tot loss = 4.716616910331103, l1: 0.00010337372964850332, l2: 0.0003682879630977535   Iteration 99 of 100, tot loss = 4.7267292990829, l1: 0.00010347908762997403, l2: 0.0003691938436164455   Iteration 100 of 100, tot loss = 4.726595680713654, l1: 0.0001034537572923, l2: 0.0003692058123124298
   End of epoch 1212; saving model... 

Epoch 1213 of 2000
   Iteration 1 of 100, tot loss = 2.5720791816711426, l1: 6.609123374801129e-05, l2: 0.00019111669098492712   Iteration 2 of 100, tot loss = 3.554446220397949, l1: 8.875444837030955e-05, l2: 0.00026669016369851306   Iteration 3 of 100, tot loss = 3.9267040888468423, l1: 9.178659577931587e-05, l2: 0.00030088380541807663   Iteration 4 of 100, tot loss = 3.7816100120544434, l1: 8.818773130769841e-05, l2: 0.0002899732680816669   Iteration 5 of 100, tot loss = 3.6527124404907227, l1: 8.209984080167487e-05, l2: 0.0002831714024068788   Iteration 6 of 100, tot loss = 3.560279130935669, l1: 8.01486009246825e-05, l2: 0.0002758793149647924   Iteration 7 of 100, tot loss = 3.5367095129830495, l1: 7.931511624649699e-05, l2: 0.0002743558371938499   Iteration 8 of 100, tot loss = 3.9546884894371033, l1: 8.930467265599873e-05, l2: 0.00030616418189310934   Iteration 9 of 100, tot loss = 3.968956788380941, l1: 8.961199718113575e-05, l2: 0.00030728368599536933   Iteration 10 of 100, tot loss = 3.9950984001159666, l1: 9.018123819259927e-05, l2: 0.00030932860245229675   Iteration 11 of 100, tot loss = 4.027821800925515, l1: 9.064395403997464e-05, l2: 0.0003121382257879966   Iteration 12 of 100, tot loss = 4.131014704704285, l1: 9.231498491620489e-05, l2: 0.0003207864853417656   Iteration 13 of 100, tot loss = 4.518677381368784, l1: 9.631048026046931e-05, l2: 0.0003555572620825842   Iteration 14 of 100, tot loss = 4.518984692437308, l1: 9.78580090824315e-05, l2: 0.0003540404629477832   Iteration 15 of 100, tot loss = 4.666247717539469, l1: 0.00010054516363500928, l2: 0.00036607961228583006   Iteration 16 of 100, tot loss = 4.672425299882889, l1: 0.00010237337619400932, l2: 0.0003648691572379903   Iteration 17 of 100, tot loss = 4.730712273541619, l1: 0.0001036732441314277, l2: 0.0003693979875499602   Iteration 18 of 100, tot loss = 4.798543161816067, l1: 0.00010566240356032115, l2: 0.00037419191519600444   Iteration 19 of 100, tot loss = 4.747315921281514, l1: 0.00010451747450707971, l2: 0.00037021411959645584   Iteration 20 of 100, tot loss = 4.68433336019516, l1: 0.00010324794966436457, l2: 0.0003651853890914936   Iteration 21 of 100, tot loss = 4.675280309858776, l1: 0.00010229133580911107, l2: 0.000365236697169686   Iteration 22 of 100, tot loss = 4.647206924178383, l1: 0.00010325377231559038, l2: 0.0003614669225168076   Iteration 23 of 100, tot loss = 4.644694504530533, l1: 0.00010411191317910814, l2: 0.00036035754032048357   Iteration 24 of 100, tot loss = 4.686385740836461, l1: 0.00010427521950380954, l2: 0.0003643633554020198   Iteration 25 of 100, tot loss = 4.604862995147705, l1: 0.00010221177988569253, l2: 0.0003582745196763426   Iteration 26 of 100, tot loss = 4.563350521601164, l1: 0.00010218266596404227, l2: 0.00035415238660509483   Iteration 27 of 100, tot loss = 4.606080523243657, l1: 0.00010279020474020702, l2: 0.0003578178476783688   Iteration 28 of 100, tot loss = 4.626518888132913, l1: 0.00010393221821556966, l2: 0.00035871967078752017   Iteration 29 of 100, tot loss = 4.578464713589899, l1: 0.0001031340137254527, l2: 0.0003547124581708541   Iteration 30 of 100, tot loss = 4.577429095904033, l1: 0.00010325827982645326, l2: 0.00035448463022476063   Iteration 31 of 100, tot loss = 4.6939354788872505, l1: 0.00010519903826793927, l2: 0.00036419451005771875   Iteration 32 of 100, tot loss = 4.704273946583271, l1: 0.00010588688553525571, l2: 0.0003645405099632626   Iteration 33 of 100, tot loss = 4.715378407276038, l1: 0.00010470698962859443, l2: 0.0003668308521754983   Iteration 34 of 100, tot loss = 4.680383450844708, l1: 0.00010413967313926311, l2: 0.00036389867283801054   Iteration 35 of 100, tot loss = 4.730873060226441, l1: 0.00010557075178699702, l2: 0.00036751655570697037   Iteration 36 of 100, tot loss = 4.771967430909474, l1: 0.00010589085761441513, l2: 0.00037130588680156507   Iteration 37 of 100, tot loss = 4.756710020271507, l1: 0.00010628671264606276, l2: 0.00036938429056553523   Iteration 38 of 100, tot loss = 4.732726617863304, l1: 0.00010661413197292284, l2: 0.00036665853105277116   Iteration 39 of 100, tot loss = 4.715621734276796, l1: 0.00010615003096366611, l2: 0.0003654121444411337   Iteration 40 of 100, tot loss = 4.710692650079727, l1: 0.00010583466146272258, l2: 0.0003652346054877853   Iteration 41 of 100, tot loss = 4.681821247426475, l1: 0.00010518151913590661, l2: 0.0003630006078931662   Iteration 42 of 100, tot loss = 4.697754388763791, l1: 0.00010594164606508067, l2: 0.0003638337958753774   Iteration 43 of 100, tot loss = 4.69673582010491, l1: 0.00010627337863741898, l2: 0.0003634002063082271   Iteration 44 of 100, tot loss = 4.682574450969696, l1: 0.00010660414136509644, l2: 0.0003616533070850313   Iteration 45 of 100, tot loss = 4.677485100428263, l1: 0.0001065062097849376, l2: 0.00036124230343072364   Iteration 46 of 100, tot loss = 4.687213954718216, l1: 0.00010646153603612102, l2: 0.0003622598625319686   Iteration 47 of 100, tot loss = 4.701265837283844, l1: 0.00010660360497608286, l2: 0.0003635229823362835   Iteration 48 of 100, tot loss = 4.681282157699267, l1: 0.00010623025680918848, l2: 0.0003618979623449074   Iteration 49 of 100, tot loss = 4.737607882947338, l1: 0.00010739744464778912, l2: 0.0003663633480235668   Iteration 50 of 100, tot loss = 4.708570532798767, l1: 0.00010669939692888874, l2: 0.0003641576608060859   Iteration 51 of 100, tot loss = 4.7030684386982635, l1: 0.00010713579610512847, l2: 0.00036317105239540266   Iteration 52 of 100, tot loss = 4.661917324249561, l1: 0.0001064785390274035, l2: 0.0003597131981223356   Iteration 53 of 100, tot loss = 4.670836642103375, l1: 0.00010698679253900477, l2: 0.00036009687590983887   Iteration 54 of 100, tot loss = 4.611558205551571, l1: 0.00010571171225348264, l2: 0.00035544411231832647   Iteration 55 of 100, tot loss = 4.626349004832181, l1: 0.00010541981643076394, l2: 0.00035721508849581536   Iteration 56 of 100, tot loss = 4.631485255701201, l1: 0.00010558643452895922, l2: 0.0003575620955546453   Iteration 57 of 100, tot loss = 4.624816666569626, l1: 0.000105499081288591, l2: 0.0003569825902671079   Iteration 58 of 100, tot loss = 4.607867066202493, l1: 0.00010459158335740133, l2: 0.00035619512822575354   Iteration 59 of 100, tot loss = 4.616728182566368, l1: 0.00010466338327683319, l2: 0.0003570094398791128   Iteration 60 of 100, tot loss = 4.614943542083105, l1: 0.00010481032853325208, l2: 0.0003566840304605042   Iteration 61 of 100, tot loss = 4.585250528132329, l1: 0.0001043713289946623, l2: 0.0003541537284274723   Iteration 62 of 100, tot loss = 4.619434812376576, l1: 0.000104471478886309, l2: 0.00035747200660895737   Iteration 63 of 100, tot loss = 4.6386269633732145, l1: 0.00010508048731098248, l2: 0.0003587822131868008   Iteration 64 of 100, tot loss = 4.60176151804626, l1: 0.00010406971529164366, l2: 0.00035610644044936635   Iteration 65 of 100, tot loss = 4.609166275537931, l1: 0.00010382452024854361, l2: 0.0003570921110132566   Iteration 66 of 100, tot loss = 4.595571342742804, l1: 0.00010399470231663365, l2: 0.00035556243550893146   Iteration 67 of 100, tot loss = 4.606276923151158, l1: 0.00010441005382636464, l2: 0.00035621764211317717   Iteration 68 of 100, tot loss = 4.575468387673883, l1: 0.0001038580454833744, l2: 0.0003536887968092582   Iteration 69 of 100, tot loss = 4.598681665848995, l1: 0.00010427799550571775, l2: 0.0003555901742968843   Iteration 70 of 100, tot loss = 4.584240261146, l1: 0.00010431955311755052, l2: 0.00035410447640710376   Iteration 71 of 100, tot loss = 4.56145432465513, l1: 0.00010392173806420186, l2: 0.000352223697864503   Iteration 72 of 100, tot loss = 4.552548463145892, l1: 0.00010398377723201217, l2: 0.0003512710724559535   Iteration 73 of 100, tot loss = 4.5643747555066465, l1: 0.00010410322876900677, l2: 0.0003523342496769707   Iteration 74 of 100, tot loss = 4.5922448006836145, l1: 0.00010432475868878396, l2: 0.0003548997245505578   Iteration 75 of 100, tot loss = 4.574159773190816, l1: 0.00010372489715033831, l2: 0.0003536910833402847   Iteration 76 of 100, tot loss = 4.582237825581902, l1: 0.00010348141054955225, l2: 0.0003547423749344154   Iteration 77 of 100, tot loss = 4.5861253846775405, l1: 0.00010374512841098023, l2: 0.00035486741285535274   Iteration 78 of 100, tot loss = 4.574352568540817, l1: 0.00010380059995021348, l2: 0.00035363465949791507   Iteration 79 of 100, tot loss = 4.544800264925897, l1: 0.00010303949694631067, l2: 0.0003514405320799332   Iteration 80 of 100, tot loss = 4.542899145185947, l1: 0.00010286694891874504, l2: 0.00035142296801495834   Iteration 81 of 100, tot loss = 4.543571020350044, l1: 0.0001027646723270192, l2: 0.00035159243198893504   Iteration 82 of 100, tot loss = 4.53003726790591, l1: 0.00010283547378160775, l2: 0.0003501682552202355   Iteration 83 of 100, tot loss = 4.50766609519361, l1: 0.00010241485041803231, l2: 0.00034835176133545095   Iteration 84 of 100, tot loss = 4.51136525613921, l1: 0.00010251117943144414, l2: 0.0003486253482974245   Iteration 85 of 100, tot loss = 4.5091401282478785, l1: 0.00010245127611115629, l2: 0.00034846273877466206   Iteration 86 of 100, tot loss = 4.4768264044162835, l1: 0.00010187084089286441, l2: 0.0003458118014752345   Iteration 87 of 100, tot loss = 4.491374824238919, l1: 0.00010185119348555407, l2: 0.0003472862911715718   Iteration 88 of 100, tot loss = 4.48590441996401, l1: 0.00010189383535610183, l2: 0.0003466966085067146   Iteration 89 of 100, tot loss = 4.4705587772840865, l1: 0.00010164327196956068, l2: 0.00034541260759121206   Iteration 90 of 100, tot loss = 4.455059218406677, l1: 0.00010151277727143477, l2: 0.0003439931464122815   Iteration 91 of 100, tot loss = 4.486099308663672, l1: 0.00010201106716112418, l2: 0.0003465988652801121   Iteration 92 of 100, tot loss = 4.478116450102433, l1: 0.00010196382189188508, l2: 0.00034584782469724104   Iteration 93 of 100, tot loss = 4.486055953528291, l1: 0.00010193868768520363, l2: 0.00034666690951643854   Iteration 94 of 100, tot loss = 4.488506241047636, l1: 0.0001017787378665897, l2: 0.00034707188829185165   Iteration 95 of 100, tot loss = 4.480026834889462, l1: 0.00010130640600293286, l2: 0.00034669627937929413   Iteration 96 of 100, tot loss = 4.467996142804623, l1: 0.00010077118035193659, l2: 0.00034602843576673575   Iteration 97 of 100, tot loss = 4.4477145008205135, l1: 0.00010017576904519768, l2: 0.00034459568284087114   Iteration 98 of 100, tot loss = 4.479165179388864, l1: 0.00010076222022281418, l2: 0.00034715429931993084   Iteration 99 of 100, tot loss = 4.487468387141372, l1: 0.00010059641488513796, l2: 0.00034815042557176016   Iteration 100 of 100, tot loss = 4.522277011871338, l1: 0.00010117173169419403, l2: 0.0003510559711139649
   End of epoch 1213; saving model... 

Epoch 1214 of 2000
   Iteration 1 of 100, tot loss = 3.739539861679077, l1: 8.765977690927684e-05, l2: 0.0002862942055799067   Iteration 2 of 100, tot loss = 3.4672693014144897, l1: 9.014670649776235e-05, l2: 0.0002565802205936052   Iteration 3 of 100, tot loss = 3.5847319761912027, l1: 8.802542045790081e-05, l2: 0.0002704477762260164   Iteration 4 of 100, tot loss = 3.929502785205841, l1: 9.769026837602723e-05, l2: 0.00029526000798796304   Iteration 5 of 100, tot loss = 3.869174289703369, l1: 8.919047468225472e-05, l2: 0.0002977269497932866   Iteration 6 of 100, tot loss = 3.6445628801981607, l1: 8.299465359111007e-05, l2: 0.0002814616309478879   Iteration 7 of 100, tot loss = 3.8401534216744557, l1: 8.693859646362918e-05, l2: 0.0002970767423643598   Iteration 8 of 100, tot loss = 3.9307313561439514, l1: 8.980513484857511e-05, l2: 0.00030326799605973065   Iteration 9 of 100, tot loss = 4.047237873077393, l1: 9.367710057025154e-05, l2: 0.00031104668353994686   Iteration 10 of 100, tot loss = 3.9688527822494506, l1: 9.483068133704364e-05, l2: 0.00030205459333956244   Iteration 11 of 100, tot loss = 4.329103361476552, l1: 9.974292796951804e-05, l2: 0.00033316740618002683   Iteration 12 of 100, tot loss = 4.189149498939514, l1: 9.554674640336695e-05, l2: 0.0003233682024680699   Iteration 13 of 100, tot loss = 4.209414188678448, l1: 9.820830914227721e-05, l2: 0.00032273310800799384   Iteration 14 of 100, tot loss = 4.0386883190699985, l1: 9.508604099599844e-05, l2: 0.00030878278942379566   Iteration 15 of 100, tot loss = 4.081136862436931, l1: 9.601934846917478e-05, l2: 0.00031209433606515326   Iteration 16 of 100, tot loss = 4.050318509340286, l1: 9.551293646836712e-05, l2: 0.0003095189131272491   Iteration 17 of 100, tot loss = 4.226151690763586, l1: 9.794285805957556e-05, l2: 0.00032467231058570394   Iteration 18 of 100, tot loss = 4.25559041235182, l1: 9.917998593462269e-05, l2: 0.00032637905517023883   Iteration 19 of 100, tot loss = 4.405950345491108, l1: 0.00010136052783070084, l2: 0.00033923450667460104   Iteration 20 of 100, tot loss = 4.311614310741424, l1: 9.862229999271221e-05, l2: 0.00033253913061344066   Iteration 21 of 100, tot loss = 4.474287793749855, l1: 0.00010071690069578056, l2: 0.0003467118790251247   Iteration 22 of 100, tot loss = 4.423628568649292, l1: 0.000100056728776756, l2: 0.00034230612866601655   Iteration 23 of 100, tot loss = 4.352923217027084, l1: 9.951691279876168e-05, l2: 0.0003357754093999772   Iteration 24 of 100, tot loss = 4.419628828763962, l1: 0.00010071021218512517, l2: 0.0003412526712054387   Iteration 25 of 100, tot loss = 4.517168741226197, l1: 0.00010167281026951969, l2: 0.00035004406468942763   Iteration 26 of 100, tot loss = 4.632086524596581, l1: 0.00010250171292752314, l2: 0.0003607069397381005   Iteration 27 of 100, tot loss = 4.6355993571104825, l1: 0.00010301393275666569, l2: 0.00036054600301819545   Iteration 28 of 100, tot loss = 4.673318156174251, l1: 0.0001037229792148407, l2: 0.00036360883573901707   Iteration 29 of 100, tot loss = 4.65381879642092, l1: 0.00010222133177790213, l2: 0.00036316054696538326   Iteration 30 of 100, tot loss = 4.652172112464905, l1: 0.00010215512156719342, l2: 0.00036306208930909635   Iteration 31 of 100, tot loss = 4.614022570271646, l1: 0.00010158881430529178, l2: 0.0003598134423936567   Iteration 32 of 100, tot loss = 4.619698457419872, l1: 0.00010125130415872263, l2: 0.00036071854174224427   Iteration 33 of 100, tot loss = 4.664489724419334, l1: 0.00010235396511041361, l2: 0.0003640950078878439   Iteration 34 of 100, tot loss = 4.692175661816316, l1: 0.00010310231749933926, l2: 0.00036611524924142836   Iteration 35 of 100, tot loss = 4.67335524559021, l1: 0.00010177124609721692, l2: 0.00036556427949108183   Iteration 36 of 100, tot loss = 4.67884126636717, l1: 0.00010261724764859537, l2: 0.0003652668795742405   Iteration 37 of 100, tot loss = 4.629337169028617, l1: 0.00010214395582364444, l2: 0.0003607897614946941   Iteration 38 of 100, tot loss = 4.559324568823764, l1: 0.00010037006100710162, l2: 0.00035556239622804383   Iteration 39 of 100, tot loss = 4.567980500367971, l1: 0.00010095962473618177, l2: 0.00035583842523360194   Iteration 40 of 100, tot loss = 4.6377787500619885, l1: 0.00010254095532218344, l2: 0.0003612369200709509   Iteration 41 of 100, tot loss = 4.606677727001469, l1: 0.00010232869743315572, l2: 0.0003583390757628921   Iteration 42 of 100, tot loss = 4.584966350169409, l1: 0.00010209653460368004, l2: 0.00035640010104370524   Iteration 43 of 100, tot loss = 4.651784927345986, l1: 0.00010277593732056165, l2: 0.00036240255610400074   Iteration 44 of 100, tot loss = 4.645704071630131, l1: 0.00010286824586232383, l2: 0.0003617021618331571   Iteration 45 of 100, tot loss = 4.6031301154030695, l1: 0.00010237070051112419, l2: 0.0003579423119339885   Iteration 46 of 100, tot loss = 4.553353478079257, l1: 0.00010112490243678305, l2: 0.0003542104459484108   Iteration 47 of 100, tot loss = 4.519974670511611, l1: 0.00010044247786975169, l2: 0.0003515549897008199   Iteration 48 of 100, tot loss = 4.50685544560353, l1: 0.00010030945001441675, l2: 0.0003503760951086103   Iteration 49 of 100, tot loss = 4.561579215283296, l1: 0.00010155212479805079, l2: 0.0003546057969844919   Iteration 50 of 100, tot loss = 4.590540244579315, l1: 0.00010249632934574038, l2: 0.0003565576957771555   Iteration 51 of 100, tot loss = 4.565056793829974, l1: 0.00010184558763570499, l2: 0.00035466009228234634   Iteration 52 of 100, tot loss = 4.564679734981977, l1: 0.00010164671570796949, l2: 0.0003548212581126091   Iteration 53 of 100, tot loss = 4.60012382156444, l1: 0.00010214250931243921, l2: 0.00035786987249388023   Iteration 54 of 100, tot loss = 4.613174052150161, l1: 0.00010228795387471716, l2: 0.0003590294503805193   Iteration 55 of 100, tot loss = 4.616502027078108, l1: 0.00010212358941895548, l2: 0.00035952661207623105   Iteration 56 of 100, tot loss = 4.64700422329562, l1: 0.00010199970009645247, l2: 0.0003627007213903458   Iteration 57 of 100, tot loss = 4.645555205512465, l1: 0.00010245726796136678, l2: 0.0003620982517308572   Iteration 58 of 100, tot loss = 4.648569372193567, l1: 0.00010263927946778431, l2: 0.00036221765712353177   Iteration 59 of 100, tot loss = 4.685529460341243, l1: 0.00010348459046188016, l2: 0.00036506835465966646   Iteration 60 of 100, tot loss = 4.656791307528814, l1: 0.00010294262622968138, l2: 0.00036273650354511723   Iteration 61 of 100, tot loss = 4.686179084856002, l1: 0.00010351670348203787, l2: 0.000365101203435009   Iteration 62 of 100, tot loss = 4.67113946906982, l1: 0.00010356534872816184, l2: 0.00036354859677826867   Iteration 63 of 100, tot loss = 4.668746863092695, l1: 0.00010346305521485943, l2: 0.0003634116301471959   Iteration 64 of 100, tot loss = 4.6422803569585085, l1: 0.00010256693525434457, l2: 0.0003616610995322844   Iteration 65 of 100, tot loss = 4.650408159769499, l1: 0.00010279114598793407, l2: 0.0003622496695383094   Iteration 66 of 100, tot loss = 4.660963746634397, l1: 0.00010285595839597977, l2: 0.0003632404160188426   Iteration 67 of 100, tot loss = 4.664860058186659, l1: 0.00010287522510130223, l2: 0.00036361078021929726   Iteration 68 of 100, tot loss = 4.631940356072257, l1: 0.00010201129311206066, l2: 0.00036118274211356254   Iteration 69 of 100, tot loss = 4.582753637562627, l1: 0.00010103012396754913, l2: 0.0003572452393279451   Iteration 70 of 100, tot loss = 4.598905549730573, l1: 0.00010148048240807839, l2: 0.00035841007234661707   Iteration 71 of 100, tot loss = 4.587651494523169, l1: 0.00010148972858228839, l2: 0.0003572754208555996   Iteration 72 of 100, tot loss = 4.562413199080361, l1: 0.0001008998111097349, l2: 0.00035534150891989056   Iteration 73 of 100, tot loss = 4.597682544629868, l1: 0.00010149615792053662, l2: 0.0003582720966907601   Iteration 74 of 100, tot loss = 4.582026836034414, l1: 0.00010117008934832631, l2: 0.000357032594242687   Iteration 75 of 100, tot loss = 4.614944794972738, l1: 0.00010166505924037967, l2: 0.0003598294201462219   Iteration 76 of 100, tot loss = 4.601226734487634, l1: 0.00010150880380503, l2: 0.00035861386928253954   Iteration 77 of 100, tot loss = 4.602950396475854, l1: 0.00010181505115122184, l2: 0.0003584799879871599   Iteration 78 of 100, tot loss = 4.605182449022929, l1: 0.00010179727019604821, l2: 0.00035872097382357775   Iteration 79 of 100, tot loss = 4.620211199869083, l1: 0.00010189517186379908, l2: 0.00036012594731041244   Iteration 80 of 100, tot loss = 4.60679814517498, l1: 0.00010139203968719812, l2: 0.0003592877739720279   Iteration 81 of 100, tot loss = 4.63242538181352, l1: 0.00010165557093533147, l2: 0.00036158696654896587   Iteration 82 of 100, tot loss = 4.647506106190566, l1: 0.00010193232837326189, l2: 0.0003628182812684738   Iteration 83 of 100, tot loss = 4.654080146766571, l1: 0.00010216022402971019, l2: 0.0003632477895603571   Iteration 84 of 100, tot loss = 4.647593580541157, l1: 0.00010178556026186838, l2: 0.0003629737967414604   Iteration 85 of 100, tot loss = 4.637806207993451, l1: 0.00010134550256599837, l2: 0.0003624351174734971   Iteration 86 of 100, tot loss = 4.630738058755564, l1: 0.0001012603008644493, l2: 0.00036181350438963883   Iteration 87 of 100, tot loss = 4.612758940663831, l1: 0.0001010593979776002, l2: 0.00036021649563850984   Iteration 88 of 100, tot loss = 4.597803503274918, l1: 0.00010069886147076349, l2: 0.0003590814884029731   Iteration 89 of 100, tot loss = 4.6233987942170565, l1: 0.00010072657280258855, l2: 0.00036161330648242716   Iteration 90 of 100, tot loss = 4.622966472307841, l1: 0.00010042189710980488, l2: 0.00036187474987754183   Iteration 91 of 100, tot loss = 4.623152012353415, l1: 0.00010044607156337557, l2: 0.0003618691297271033   Iteration 92 of 100, tot loss = 4.6185477738795075, l1: 0.00010036719322670251, l2: 0.0003614875843239766   Iteration 93 of 100, tot loss = 4.619860600399715, l1: 0.00010040965548063057, l2: 0.00036157640457649027   Iteration 94 of 100, tot loss = 4.614970189459781, l1: 0.00010049948964188708, l2: 0.0003609975294935755   Iteration 95 of 100, tot loss = 4.612139403192621, l1: 0.00010041902852734845, l2: 0.0003607949117921587   Iteration 96 of 100, tot loss = 4.606373307605584, l1: 9.995847775220075e-05, l2: 0.00036067885306086583   Iteration 97 of 100, tot loss = 4.588537688107834, l1: 9.966111959788718e-05, l2: 0.00035919264929858754   Iteration 98 of 100, tot loss = 4.575055684362139, l1: 9.930093987424363e-05, l2: 0.00035820462876379644   Iteration 99 of 100, tot loss = 4.610849609278669, l1: 9.974296508425132e-05, l2: 0.00036134199542698985   Iteration 100 of 100, tot loss = 4.595917253494263, l1: 9.955907160474453e-05, l2: 0.0003600326532614417
   End of epoch 1214; saving model... 

Epoch 1215 of 2000
   Iteration 1 of 100, tot loss = 4.287975788116455, l1: 8.744755905354396e-05, l2: 0.0003413500089664012   Iteration 2 of 100, tot loss = 4.580621004104614, l1: 0.00010871130871237256, l2: 0.0003493507974781096   Iteration 3 of 100, tot loss = 6.241254965464274, l1: 0.00012789256046138084, l2: 0.0004962329403497279   Iteration 4 of 100, tot loss = 5.5914546251297, l1: 0.00011719319809344597, l2: 0.00044195226655574515   Iteration 5 of 100, tot loss = 5.119590139389038, l1: 0.00011082410055678338, l2: 0.0004011349141364917   Iteration 6 of 100, tot loss = 5.1632126569747925, l1: 0.00011164623113775936, l2: 0.0004046750327688642   Iteration 7 of 100, tot loss = 4.956737041473389, l1: 0.00010754485577178587, l2: 0.00038812884719975827   Iteration 8 of 100, tot loss = 4.858554184436798, l1: 0.0001037895526678767, l2: 0.0003820658657787135   Iteration 9 of 100, tot loss = 4.663069910473293, l1: 0.0001007111447203594, l2: 0.0003655958458289711   Iteration 10 of 100, tot loss = 4.630791211128235, l1: 0.00010225557998637669, l2: 0.0003608235405408777   Iteration 11 of 100, tot loss = 4.686671105298129, l1: 0.00010268736448117785, l2: 0.0003659797459311614   Iteration 12 of 100, tot loss = 4.885572214921315, l1: 0.00010530141723090007, l2: 0.0003832558080224165   Iteration 13 of 100, tot loss = 4.82760658630958, l1: 0.00010505011148500041, l2: 0.00037771055156973977   Iteration 14 of 100, tot loss = 4.7572290897369385, l1: 0.00010371196081645653, l2: 0.0003720109535996536   Iteration 15 of 100, tot loss = 4.775004609425863, l1: 0.0001038280810462311, l2: 0.0003736723844970887   Iteration 16 of 100, tot loss = 4.716195181012154, l1: 0.00010274647138430737, l2: 0.0003688730494104675   Iteration 17 of 100, tot loss = 4.8113648891448975, l1: 0.00010537078111049007, l2: 0.00037576571048703045   Iteration 18 of 100, tot loss = 4.85978180832333, l1: 0.00010708775840208141, l2: 0.0003788904246499038   Iteration 19 of 100, tot loss = 4.856745531684474, l1: 0.00010799304092008817, l2: 0.00037768151301670035   Iteration 20 of 100, tot loss = 4.9645375370979306, l1: 0.00011006801069015636, l2: 0.0003863857411488425   Iteration 21 of 100, tot loss = 4.995734158016386, l1: 0.00011039723730867817, l2: 0.0003891761776425743   Iteration 22 of 100, tot loss = 5.023398128422824, l1: 0.0001111962304672819, l2: 0.0003911435811790976   Iteration 23 of 100, tot loss = 4.962908350903055, l1: 0.00011110528177359019, l2: 0.0003851855524218358   Iteration 24 of 100, tot loss = 4.956247429052989, l1: 0.00010989632543593568, l2: 0.0003857284167073279   Iteration 25 of 100, tot loss = 4.925073432922363, l1: 0.00010847694939002395, l2: 0.0003840303927427158   Iteration 26 of 100, tot loss = 4.88254558123075, l1: 0.00010849497378400814, l2: 0.0003797595826864171   Iteration 27 of 100, tot loss = 4.934020201365153, l1: 0.00010945143838870097, l2: 0.000383950579648458   Iteration 28 of 100, tot loss = 4.917932629585266, l1: 0.00010775382751100031, l2: 0.0003840394331616283   Iteration 29 of 100, tot loss = 4.9522101632479965, l1: 0.00010825894674655564, l2: 0.00038696206746831666   Iteration 30 of 100, tot loss = 4.944904740651449, l1: 0.0001086623366669907, l2: 0.00038582813625301544   Iteration 31 of 100, tot loss = 4.8800265942850425, l1: 0.00010814391205184943, l2: 0.0003798587459905614   Iteration 32 of 100, tot loss = 4.908805035054684, l1: 0.00010809086370500154, l2: 0.0003827896380244056   Iteration 33 of 100, tot loss = 4.900931856848977, l1: 0.00010764443978780147, l2: 0.00038244874414169425   Iteration 34 of 100, tot loss = 4.837498643819024, l1: 0.00010671865910647765, l2: 0.00037703120324295014   Iteration 35 of 100, tot loss = 4.806782926831927, l1: 0.00010659923697988103, l2: 0.00037407905412172634   Iteration 36 of 100, tot loss = 4.817246238390605, l1: 0.00010648253232324755, l2: 0.0003752420905382476   Iteration 37 of 100, tot loss = 4.8398152170954525, l1: 0.00010760719246841413, l2: 0.00037637432860965664   Iteration 38 of 100, tot loss = 4.869845490706594, l1: 0.00010764619889270857, l2: 0.0003793383504288565   Iteration 39 of 100, tot loss = 4.821271878022414, l1: 0.00010603389856465257, l2: 0.0003760932893969883   Iteration 40 of 100, tot loss = 4.841760855913162, l1: 0.00010590385463729035, l2: 0.0003782722320465837   Iteration 41 of 100, tot loss = 4.829920786183055, l1: 0.00010568468239465047, l2: 0.00037730739681936075   Iteration 42 of 100, tot loss = 4.855568653061276, l1: 0.0001059800285604849, l2: 0.00037957683698983775   Iteration 43 of 100, tot loss = 4.846587042475855, l1: 0.00010585362178971991, l2: 0.00037880508272453796   Iteration 44 of 100, tot loss = 4.899555970322002, l1: 0.00010551795887956608, l2: 0.00038443763862067664   Iteration 45 of 100, tot loss = 4.89263260629442, l1: 0.0001053244443028234, l2: 0.00038393881728148295   Iteration 46 of 100, tot loss = 4.915475912716078, l1: 0.00010553966864751166, l2: 0.0003860079241744445   Iteration 47 of 100, tot loss = 4.957412024761768, l1: 0.00010651214317012419, l2: 0.0003892290610275132   Iteration 48 of 100, tot loss = 4.924241359035174, l1: 0.00010602361483809848, l2: 0.0003864005229843315   Iteration 49 of 100, tot loss = 4.942491906029837, l1: 0.00010662833650415878, l2: 0.00038762085617291835   Iteration 50 of 100, tot loss = 4.9084289312362674, l1: 0.00010656920014298521, l2: 0.00038427369465352966   Iteration 51 of 100, tot loss = 4.87953893811095, l1: 0.00010639098478599871, l2: 0.00038156291046499404   Iteration 52 of 100, tot loss = 4.903280936754667, l1: 0.00010649692698140055, l2: 0.0003838311679898582   Iteration 53 of 100, tot loss = 4.876915184956677, l1: 0.00010552994072500985, l2: 0.0003821615787016509   Iteration 54 of 100, tot loss = 4.843210701589231, l1: 0.0001049472139129648, l2: 0.0003793738573289442   Iteration 55 of 100, tot loss = 4.828945172916759, l1: 0.00010446511871784672, l2: 0.0003784293996762823   Iteration 56 of 100, tot loss = 4.803107223340443, l1: 0.0001037902826673027, l2: 0.0003765204408929484   Iteration 57 of 100, tot loss = 4.794874739228633, l1: 0.00010336182630884363, l2: 0.000376125649046643   Iteration 58 of 100, tot loss = 4.808490362660638, l1: 0.00010329960642938486, l2: 0.0003775494310659645   Iteration 59 of 100, tot loss = 4.8219233569452316, l1: 0.00010411816105785211, l2: 0.00037807417526383394   Iteration 60 of 100, tot loss = 4.8204708377520245, l1: 0.00010397841803448198, l2: 0.00037806866651711365   Iteration 61 of 100, tot loss = 4.843323836561109, l1: 0.00010436736222606564, l2: 0.0003799650222673768   Iteration 62 of 100, tot loss = 4.819423217927256, l1: 0.00010415874021418858, l2: 0.00037778358232425227   Iteration 63 of 100, tot loss = 4.806152044780671, l1: 0.00010388862845877792, l2: 0.00037672657691430124   Iteration 64 of 100, tot loss = 4.804350670427084, l1: 0.00010427675186974739, l2: 0.0003761583157029236   Iteration 65 of 100, tot loss = 4.807382037089421, l1: 0.00010384433973320903, l2: 0.00037689386517740785   Iteration 66 of 100, tot loss = 4.875850529381723, l1: 0.00010484625371728643, l2: 0.00038273880041246724   Iteration 67 of 100, tot loss = 4.861838589853315, l1: 0.0001046161440226298, l2: 0.0003815677161189491   Iteration 68 of 100, tot loss = 4.840909572208629, l1: 0.00010409266248162192, l2: 0.00037999829553026594   Iteration 69 of 100, tot loss = 4.824077699495398, l1: 0.00010369289831968659, l2: 0.0003787148722778142   Iteration 70 of 100, tot loss = 4.831200391905648, l1: 0.0001032672368767505, l2: 0.00037985280297497023   Iteration 71 of 100, tot loss = 4.8299871397690035, l1: 0.0001029897606534011, l2: 0.0003800089540727384   Iteration 72 of 100, tot loss = 4.82901276813613, l1: 0.0001030312222004189, l2: 0.00037987005554087873   Iteration 73 of 100, tot loss = 4.822001447416332, l1: 0.0001032781941805845, l2: 0.00037892195172863053   Iteration 74 of 100, tot loss = 4.787521684491956, l1: 0.00010283136219407645, l2: 0.0003759208075525994   Iteration 75 of 100, tot loss = 4.77549791653951, l1: 0.00010249627613423702, l2: 0.00037505351705476644   Iteration 76 of 100, tot loss = 4.7592147494617265, l1: 0.0001025869395514172, l2: 0.00037333453694468757   Iteration 77 of 100, tot loss = 4.755107814615423, l1: 0.00010239253781226391, l2: 0.0003731182452091681   Iteration 78 of 100, tot loss = 4.796791146963071, l1: 0.00010322016780078113, l2: 0.0003764589488034686   Iteration 79 of 100, tot loss = 4.803720024567616, l1: 0.00010317580473625478, l2: 0.00037719619967326333   Iteration 80 of 100, tot loss = 4.790702229738235, l1: 0.0001031995303947042, l2: 0.0003758706945518497   Iteration 81 of 100, tot loss = 4.80326497701951, l1: 0.00010327615552936842, l2: 0.0003770503440417671   Iteration 82 of 100, tot loss = 4.7781552425244955, l1: 0.00010303445391977353, l2: 0.00037478107235695395   Iteration 83 of 100, tot loss = 4.7682328798684726, l1: 0.00010297375836189422, l2: 0.00037384953147153854   Iteration 84 of 100, tot loss = 4.764192893391564, l1: 0.00010277044258968782, l2: 0.0003736488487491096   Iteration 85 of 100, tot loss = 4.792456464206471, l1: 0.00010322236788959023, l2: 0.0003760232807561646   Iteration 86 of 100, tot loss = 4.796179000721422, l1: 0.00010347372276449934, l2: 0.00037614417991324784   Iteration 87 of 100, tot loss = 4.779172631515854, l1: 0.00010319787045658417, l2: 0.00037471939499848577   Iteration 88 of 100, tot loss = 4.788300928744403, l1: 0.00010329191334015128, l2: 0.0003755381815360372   Iteration 89 of 100, tot loss = 4.80865246794197, l1: 0.00010367985755130596, l2: 0.00037718539173960727   Iteration 90 of 100, tot loss = 4.806654095649719, l1: 0.00010362337743572426, l2: 0.0003770420345568305   Iteration 91 of 100, tot loss = 4.789850164245773, l1: 0.00010339692560412184, l2: 0.00037558809332779845   Iteration 92 of 100, tot loss = 4.775372810985731, l1: 0.00010328413408506489, l2: 0.00037425314977830885   Iteration 93 of 100, tot loss = 4.763939549846034, l1: 0.00010316794718927403, l2: 0.00037322601041210796   Iteration 94 of 100, tot loss = 4.758346943145103, l1: 0.00010300628441364872, l2: 0.0003728284124356813   Iteration 95 of 100, tot loss = 4.752381560676977, l1: 0.00010256173034002515, l2: 0.0003726764283400323   Iteration 96 of 100, tot loss = 4.750545059641202, l1: 0.00010247999868321737, l2: 0.0003725745098866658   Iteration 97 of 100, tot loss = 4.7848382291105604, l1: 0.00010303786647333236, l2: 0.00037544595900342135   Iteration 98 of 100, tot loss = 4.771129384332774, l1: 0.0001025944890285728, l2: 0.0003745184517263587   Iteration 99 of 100, tot loss = 4.778521576313057, l1: 0.00010254795206216814, l2: 0.00037530420781371906   Iteration 100 of 100, tot loss = 4.808349299430847, l1: 0.00010292181657860055, l2: 0.0003779131152259652
   End of epoch 1215; saving model... 

Epoch 1216 of 2000
   Iteration 1 of 100, tot loss = 3.061904191970825, l1: 7.665444718440995e-05, l2: 0.00022953598818276078   Iteration 2 of 100, tot loss = 3.870468497276306, l1: 8.693878771737218e-05, l2: 0.00030010808404767886   Iteration 3 of 100, tot loss = 4.139570474624634, l1: 0.00010206327715422958, l2: 0.00031189377477858216   Iteration 4 of 100, tot loss = 4.268223583698273, l1: 9.956746544048656e-05, l2: 0.00032725489654694684   Iteration 5 of 100, tot loss = 3.955001163482666, l1: 9.300455712946132e-05, l2: 0.0003024955629371107   Iteration 6 of 100, tot loss = 4.176428397496541, l1: 9.367801006495331e-05, l2: 0.00032396483099243295   Iteration 7 of 100, tot loss = 4.96143456867763, l1: 0.0001034622754169894, l2: 0.0003926811831271542   Iteration 8 of 100, tot loss = 5.023690938949585, l1: 0.00010593367733235937, l2: 0.00039643542186240666   Iteration 9 of 100, tot loss = 4.676013006104363, l1: 9.893084481720709e-05, l2: 0.00036867046017303236   Iteration 10 of 100, tot loss = 4.588832724094391, l1: 9.890465516946279e-05, l2: 0.00035997862287331375   Iteration 11 of 100, tot loss = 4.5949191288514575, l1: 9.726281926057048e-05, l2: 0.00036222909958186477   Iteration 12 of 100, tot loss = 4.700102299451828, l1: 0.00010096284859173466, l2: 0.00036904738954035565   Iteration 13 of 100, tot loss = 4.67539370059967, l1: 0.00010089614750960699, l2: 0.0003666432323650672   Iteration 14 of 100, tot loss = 4.643757249627795, l1: 0.00010112822928931564, l2: 0.0003632475064867841   Iteration 15 of 100, tot loss = 4.566313147544861, l1: 9.84031551827987e-05, l2: 0.0003582281700801104   Iteration 16 of 100, tot loss = 4.596420831978321, l1: 9.985756742025842e-05, l2: 0.00035978452433482744   Iteration 17 of 100, tot loss = 4.521285723237431, l1: 9.921386192539049e-05, l2: 0.0003529147181646241   Iteration 18 of 100, tot loss = 4.508596175246769, l1: 9.897288287498264e-05, l2: 0.00035188674276772264   Iteration 19 of 100, tot loss = 4.547494957321568, l1: 9.89734877581022e-05, l2: 0.00035577601566298033   Iteration 20 of 100, tot loss = 4.556597632169724, l1: 9.857717777776998e-05, l2: 0.00035708259165403435   Iteration 21 of 100, tot loss = 4.556996203604198, l1: 9.86503121869949e-05, l2: 0.00035704931568553936   Iteration 22 of 100, tot loss = 4.573305536400188, l1: 9.90460952876178e-05, l2: 0.0003582844662808136   Iteration 23 of 100, tot loss = 4.635577922282011, l1: 9.946399159557389e-05, l2: 0.0003640938078245634   Iteration 24 of 100, tot loss = 4.545981074372928, l1: 9.823455335814894e-05, l2: 0.00035636356005852576   Iteration 25 of 100, tot loss = 4.578228421211243, l1: 9.898343385430053e-05, l2: 0.00035883941396605226   Iteration 26 of 100, tot loss = 4.519374063381782, l1: 9.823615003439884e-05, l2: 0.00035370126212141116   Iteration 27 of 100, tot loss = 4.549208733770582, l1: 9.95142098097993e-05, l2: 0.0003554066702404439   Iteration 28 of 100, tot loss = 4.578504839113781, l1: 0.00010081902109959628, l2: 0.00035703146932064556   Iteration 29 of 100, tot loss = 4.576145702394946, l1: 0.00010048917540878956, l2: 0.00035712540155710204   Iteration 30 of 100, tot loss = 4.594682077566783, l1: 9.986479174889003e-05, l2: 0.00035960342259689547   Iteration 31 of 100, tot loss = 4.5500904244761315, l1: 9.9290375440051e-05, l2: 0.0003557186733345471   Iteration 32 of 100, tot loss = 4.471292708069086, l1: 9.794150969355542e-05, l2: 0.00034918776736958534   Iteration 33 of 100, tot loss = 4.426912311351661, l1: 9.745786607681745e-05, l2: 0.00034523337142838335   Iteration 34 of 100, tot loss = 4.470722903223598, l1: 9.676484243006117e-05, l2: 0.00035030745279715015   Iteration 35 of 100, tot loss = 4.4679108245032175, l1: 9.735926120941128e-05, l2: 0.0003494318262840222   Iteration 36 of 100, tot loss = 4.4933095673720045, l1: 9.734284988856719e-05, l2: 0.00035198811181342334   Iteration 37 of 100, tot loss = 4.523665160746188, l1: 9.828731413722994e-05, l2: 0.0003540792067478587   Iteration 38 of 100, tot loss = 4.497583323403409, l1: 9.794590809370245e-05, l2: 0.00035181242894491573   Iteration 39 of 100, tot loss = 4.502381480657137, l1: 9.845605381997302e-05, l2: 0.0003517820990125004   Iteration 40 of 100, tot loss = 4.518563434481621, l1: 9.863950745057082e-05, l2: 0.00035321684154041575   Iteration 41 of 100, tot loss = 4.552635573759312, l1: 9.943344988775008e-05, l2: 0.0003558301126226646   Iteration 42 of 100, tot loss = 4.523563279992058, l1: 9.927924353784572e-05, l2: 0.00035307708977987725   Iteration 43 of 100, tot loss = 4.510696940643843, l1: 9.942626384991156e-05, l2: 0.0003516434354783404   Iteration 44 of 100, tot loss = 4.445000624114817, l1: 9.793201711132001e-05, l2: 0.0003465680506038056   Iteration 45 of 100, tot loss = 4.39844827387068, l1: 9.682912972898015e-05, l2: 0.00034301570299754126   Iteration 46 of 100, tot loss = 4.428422583186108, l1: 9.666068209575631e-05, l2: 0.0003461815810979992   Iteration 47 of 100, tot loss = 4.431230309161734, l1: 9.66763678235943e-05, l2: 0.0003464466677309866   Iteration 48 of 100, tot loss = 4.42106502999862, l1: 9.670528129390732e-05, l2: 0.00034540122608935536   Iteration 49 of 100, tot loss = 4.436360755745246, l1: 9.716699728136407e-05, l2: 0.0003464690831903254   Iteration 50 of 100, tot loss = 4.439945666790009, l1: 9.674344248196576e-05, l2: 0.00034725112927844746   Iteration 51 of 100, tot loss = 4.4281308300354905, l1: 9.704747300894073e-05, l2: 0.0003457656150167881   Iteration 52 of 100, tot loss = 4.381582661316945, l1: 9.631532611820148e-05, l2: 0.0003418429453002038   Iteration 53 of 100, tot loss = 4.451936359675425, l1: 9.779787117468175e-05, l2: 0.0003473957704560269   Iteration 54 of 100, tot loss = 4.4738332231839495, l1: 9.798188740980415e-05, l2: 0.00034940144056842354   Iteration 55 of 100, tot loss = 4.483233874494379, l1: 9.82286872220522e-05, l2: 0.0003500947064656595   Iteration 56 of 100, tot loss = 4.45394591987133, l1: 9.787366051179041e-05, l2: 0.00034752093753403254   Iteration 57 of 100, tot loss = 4.428751211417349, l1: 9.755326927437349e-05, l2: 0.00034532185772580926   Iteration 58 of 100, tot loss = 4.467304420882258, l1: 9.784694834550088e-05, l2: 0.00034888349884944355   Iteration 59 of 100, tot loss = 4.46648330405607, l1: 9.715534727902041e-05, l2: 0.0003494929879414454   Iteration 60 of 100, tot loss = 4.456521135568619, l1: 9.71233890595613e-05, l2: 0.00034852872922783715   Iteration 61 of 100, tot loss = 4.465359255915782, l1: 9.722764864773871e-05, l2: 0.00034930828107582006   Iteration 62 of 100, tot loss = 4.432621923185164, l1: 9.651743048086627e-05, l2: 0.00034674476609647934   Iteration 63 of 100, tot loss = 4.4285765659241445, l1: 9.623143203151856e-05, l2: 0.00034662622922388396   Iteration 64 of 100, tot loss = 4.387002477422357, l1: 9.543998567096423e-05, l2: 0.00034326026661801734   Iteration 65 of 100, tot loss = 4.404763722419739, l1: 9.599956056962792e-05, l2: 0.0003444768157071219   Iteration 66 of 100, tot loss = 4.396530062863321, l1: 9.566679275876871e-05, l2: 0.0003439862176133884   Iteration 67 of 100, tot loss = 4.403115804515668, l1: 9.585760161106878e-05, l2: 0.0003444539828733333   Iteration 68 of 100, tot loss = 4.379175727858263, l1: 9.571454435953295e-05, l2: 0.0003422030321791467   Iteration 69 of 100, tot loss = 4.41724946015123, l1: 9.633330668346362e-05, l2: 0.0003453916433500126   Iteration 70 of 100, tot loss = 4.4085467049053735, l1: 9.612903044658846e-05, l2: 0.0003447256439747954   Iteration 71 of 100, tot loss = 4.410925608285716, l1: 9.630038198018053e-05, l2: 0.00034479218232609385   Iteration 72 of 100, tot loss = 4.433623464571105, l1: 9.648549277901313e-05, l2: 0.00034687685688873497   Iteration 73 of 100, tot loss = 4.41877747725134, l1: 9.645057853298542e-05, l2: 0.0003454271723493959   Iteration 74 of 100, tot loss = 4.391578311855729, l1: 9.60188360320014e-05, l2: 0.0003431389984168188   Iteration 75 of 100, tot loss = 4.39314002195994, l1: 9.614601935027167e-05, l2: 0.0003431679861387238   Iteration 76 of 100, tot loss = 4.393453514889667, l1: 9.642405242821521e-05, l2: 0.00034292130237046975   Iteration 77 of 100, tot loss = 4.40188443041467, l1: 9.635296926528288e-05, l2: 0.00034383547703233585   Iteration 78 of 100, tot loss = 4.38300715195827, l1: 9.610958677979234e-05, l2: 0.0003421911318675997   Iteration 79 of 100, tot loss = 4.382783550250379, l1: 9.627319073681778e-05, l2: 0.0003420051676884317   Iteration 80 of 100, tot loss = 4.373255185782909, l1: 9.651801037762198e-05, l2: 0.00034080751156579937   Iteration 81 of 100, tot loss = 4.429290811220805, l1: 9.733368328234963e-05, l2: 0.00034559540178218605   Iteration 82 of 100, tot loss = 4.426387744705852, l1: 9.687744745971896e-05, l2: 0.0003457613306747545   Iteration 83 of 100, tot loss = 4.414401021348425, l1: 9.634162472730716e-05, l2: 0.000345098481167119   Iteration 84 of 100, tot loss = 4.420522503909611, l1: 9.652369949652071e-05, l2: 0.0003455285548712016   Iteration 85 of 100, tot loss = 4.4141736661686615, l1: 9.660374185734648e-05, l2: 0.00034481362860841567   Iteration 86 of 100, tot loss = 4.400295486283857, l1: 9.637720143283057e-05, l2: 0.00034365235123279785   Iteration 87 of 100, tot loss = 4.4072309102135145, l1: 9.652899541248481e-05, l2: 0.0003441940993405129   Iteration 88 of 100, tot loss = 4.401341220194643, l1: 9.648385776017676e-05, l2: 0.0003436502680746013   Iteration 89 of 100, tot loss = 4.421199947260739, l1: 9.652780450839978e-05, l2: 0.0003455921942485873   Iteration 90 of 100, tot loss = 4.421104678842757, l1: 9.625082461247479e-05, l2: 0.00034585964701060824   Iteration 91 of 100, tot loss = 4.45796028467325, l1: 9.652401590406393e-05, l2: 0.00034927201644737517   Iteration 92 of 100, tot loss = 4.452905334856199, l1: 9.65072458711802e-05, l2: 0.0003487832916067893   Iteration 93 of 100, tot loss = 4.443415984030692, l1: 9.66215435079404e-05, l2: 0.0003477200586912072   Iteration 94 of 100, tot loss = 4.456856183549191, l1: 9.681905806033644e-05, l2: 0.00034886656438639545   Iteration 95 of 100, tot loss = 4.456268575316981, l1: 9.654423211989189e-05, l2: 0.00034908262942304934   Iteration 96 of 100, tot loss = 4.47703763221701, l1: 9.696975359929638e-05, l2: 0.00035073401356081985   Iteration 97 of 100, tot loss = 4.466847067026748, l1: 9.683066140890323e-05, l2: 0.0003498540490628076   Iteration 98 of 100, tot loss = 4.4715030789375305, l1: 9.70109971492594e-05, l2: 0.0003501393148326315   Iteration 99 of 100, tot loss = 4.470542644009446, l1: 9.688585840084477e-05, l2: 0.00035016840977025114   Iteration 100 of 100, tot loss = 4.452996762990952, l1: 9.66111274829018e-05, l2: 0.000348688552621752
   End of epoch 1216; saving model... 

Epoch 1217 of 2000
   Iteration 1 of 100, tot loss = 5.682628631591797, l1: 0.0001305777404922992, l2: 0.00043768511386588216   Iteration 2 of 100, tot loss = 4.9763946533203125, l1: 0.00011965985686401837, l2: 0.0003779796097660437   Iteration 3 of 100, tot loss = 4.786535898844401, l1: 0.00010956614035724972, l2: 0.00036908744368702173   Iteration 4 of 100, tot loss = 4.289211213588715, l1: 0.00010251524327031802, l2: 0.0003264058723289054   Iteration 5 of 100, tot loss = 4.596045351028442, l1: 9.918496507452801e-05, l2: 0.0003604195633670315   Iteration 6 of 100, tot loss = 4.382144212722778, l1: 9.644179954193532e-05, l2: 0.00034177261356186744   Iteration 7 of 100, tot loss = 4.3185404028211325, l1: 9.200347683093111e-05, l2: 0.00033985055675397495   Iteration 8 of 100, tot loss = 4.336690992116928, l1: 9.16086519282544e-05, l2: 0.0003420604425627971   Iteration 9 of 100, tot loss = 4.59752615292867, l1: 9.339359611557383e-05, l2: 0.00036635901293872547   Iteration 10 of 100, tot loss = 4.799074578285217, l1: 9.853715964709408e-05, l2: 0.000381370291870553   Iteration 11 of 100, tot loss = 4.771521633321589, l1: 9.829734997104175e-05, l2: 0.00037885480957232755   Iteration 12 of 100, tot loss = 4.589544614156087, l1: 9.380338572858211e-05, l2: 0.00036515107058221474   Iteration 13 of 100, tot loss = 4.578795249645527, l1: 9.455111005361407e-05, l2: 0.0003633284112975861   Iteration 14 of 100, tot loss = 4.446226920400347, l1: 9.270260696731774e-05, l2: 0.00035192008155198503   Iteration 15 of 100, tot loss = 4.574935007095337, l1: 9.420667823481685e-05, l2: 0.0003632868193866064   Iteration 16 of 100, tot loss = 4.422359019517899, l1: 9.255648001271766e-05, l2: 0.0003496794188322383   Iteration 17 of 100, tot loss = 4.316801155314726, l1: 8.963169452994513e-05, l2: 0.00034204841786584654   Iteration 18 of 100, tot loss = 4.2407040860917835, l1: 8.958967211785623e-05, l2: 0.0003344807329186652   Iteration 19 of 100, tot loss = 4.224328066173353, l1: 8.974276677595059e-05, l2: 0.0003326900363103242   Iteration 20 of 100, tot loss = 4.223950791358948, l1: 8.976649660326075e-05, l2: 0.0003326285783259664   Iteration 21 of 100, tot loss = 4.174324398949032, l1: 8.911676386127337e-05, l2: 0.0003283156727861968   Iteration 22 of 100, tot loss = 4.1925648559223525, l1: 8.904671366591091e-05, l2: 0.00033020976909690285   Iteration 23 of 100, tot loss = 4.292430836221446, l1: 9.187718798942944e-05, l2: 0.00033736589324215186   Iteration 24 of 100, tot loss = 4.334143976370494, l1: 9.204600398030986e-05, l2: 0.00034136839046065387   Iteration 25 of 100, tot loss = 4.455036544799805, l1: 9.479505504714325e-05, l2: 0.00035070859594270586   Iteration 26 of 100, tot loss = 4.449317987148579, l1: 9.529151261981147e-05, l2: 0.0003496402832052599   Iteration 27 of 100, tot loss = 4.506301350063747, l1: 9.668590468613224e-05, l2: 0.0003539442259352654   Iteration 28 of 100, tot loss = 4.489369477544512, l1: 9.51905012698262e-05, l2: 0.00035374644254001657   Iteration 29 of 100, tot loss = 4.428670159701643, l1: 9.330226698537068e-05, l2: 0.00034956474475369883   Iteration 30 of 100, tot loss = 4.42158571879069, l1: 9.265927049758223e-05, l2: 0.00034949929686263205   Iteration 31 of 100, tot loss = 4.424787844381025, l1: 9.37746167877659e-05, l2: 0.00034870416294002245   Iteration 32 of 100, tot loss = 4.396848529577255, l1: 9.338963127447641e-05, l2: 0.000346295217241277   Iteration 33 of 100, tot loss = 4.421466942989465, l1: 9.434208385011351e-05, l2: 0.00034780460621484303   Iteration 34 of 100, tot loss = 4.479249491411097, l1: 9.584495359483887e-05, l2: 0.0003520799920592895   Iteration 35 of 100, tot loss = 4.533149119785854, l1: 9.755268609816475e-05, l2: 0.0003557622233139617   Iteration 36 of 100, tot loss = 4.535335606998867, l1: 9.732987907935037e-05, l2: 0.0003562036799849011   Iteration 37 of 100, tot loss = 4.551120435869372, l1: 9.824137157773146e-05, l2: 0.00035687066957581083   Iteration 38 of 100, tot loss = 4.597450582604659, l1: 9.886359858731004e-05, l2: 0.00036088145861867815   Iteration 39 of 100, tot loss = 4.649154296288123, l1: 9.944947184218715e-05, l2: 0.0003654659577967742   Iteration 40 of 100, tot loss = 4.680458581447601, l1: 0.00010047053838206921, l2: 0.0003675753192510456   Iteration 41 of 100, tot loss = 4.648605829331933, l1: 9.925827925610242e-05, l2: 0.0003656023032906488   Iteration 42 of 100, tot loss = 4.616632047153654, l1: 9.904363036011567e-05, l2: 0.0003626195740209715   Iteration 43 of 100, tot loss = 4.65105880138486, l1: 0.00010026044511442007, l2: 0.0003648454351837985   Iteration 44 of 100, tot loss = 4.658903853459791, l1: 0.00010084611097805795, l2: 0.00036504427473780447   Iteration 45 of 100, tot loss = 4.7157510068681505, l1: 0.0001021286123432219, l2: 0.0003694464892355932   Iteration 46 of 100, tot loss = 4.69872113932734, l1: 0.00010182233176158701, l2: 0.0003680497833082209   Iteration 47 of 100, tot loss = 4.702092606970605, l1: 0.00010234109504381195, l2: 0.00036786816657540645   Iteration 48 of 100, tot loss = 4.716737121343613, l1: 0.00010279795045183467, l2: 0.0003688757636458225   Iteration 49 of 100, tot loss = 4.669694564780411, l1: 0.00010185630986709338, l2: 0.00036511314843961853   Iteration 50 of 100, tot loss = 4.633313412666321, l1: 0.00010156857097172178, l2: 0.00036176277237245816   Iteration 51 of 100, tot loss = 4.611065224105237, l1: 0.0001012150007904982, l2: 0.0003598915237828395   Iteration 52 of 100, tot loss = 4.6096929265902595, l1: 0.00010164025255877417, l2: 0.000359329042671911   Iteration 53 of 100, tot loss = 4.576682540605653, l1: 0.000101080851959952, l2: 0.0003565874047915645   Iteration 54 of 100, tot loss = 4.59637224232709, l1: 0.00010136848666459425, l2: 0.00035826874065807916   Iteration 55 of 100, tot loss = 4.59751130884344, l1: 0.00010141502306479114, l2: 0.00035833611063108866   Iteration 56 of 100, tot loss = 4.591050556727818, l1: 0.00010165221907040436, l2: 0.0003574528395152551   Iteration 57 of 100, tot loss = 4.573583728388736, l1: 0.00010156106528531956, l2: 0.0003557973099655978   Iteration 58 of 100, tot loss = 4.550903608059061, l1: 0.00010132408891061866, l2: 0.0003537662743273641   Iteration 59 of 100, tot loss = 4.55856505895065, l1: 0.00010150374614799363, l2: 0.00035435276256945235   Iteration 60 of 100, tot loss = 4.529069217046102, l1: 0.00010108729814722513, l2: 0.0003518196266668383   Iteration 61 of 100, tot loss = 4.5450313833893325, l1: 0.0001016001927486972, l2: 0.00035290294837351643   Iteration 62 of 100, tot loss = 4.521340597060419, l1: 0.00010132216688831366, l2: 0.00035081189567027913   Iteration 63 of 100, tot loss = 4.504264615830921, l1: 0.00010124871337283699, l2: 0.00034917775118383507   Iteration 64 of 100, tot loss = 4.498759109526873, l1: 0.00010113049233950733, l2: 0.0003487454214337049   Iteration 65 of 100, tot loss = 4.482313581613394, l1: 0.00010098429484060035, l2: 0.0003472470660479023   Iteration 66 of 100, tot loss = 4.50412866563508, l1: 0.00010132875743720709, l2: 0.00034908411158319336   Iteration 67 of 100, tot loss = 4.501220169352062, l1: 0.00010144623468192155, l2: 0.00034867578495377256   Iteration 68 of 100, tot loss = 4.492055892944336, l1: 0.00010102455042429296, l2: 0.00034818104134447985   Iteration 69 of 100, tot loss = 4.514122852380725, l1: 0.00010119593407844017, l2: 0.0003502163534099911   Iteration 70 of 100, tot loss = 4.515187672206333, l1: 0.00010111695820731776, l2: 0.0003504018108027854   Iteration 71 of 100, tot loss = 4.5191590416599325, l1: 0.00010122651702389909, l2: 0.00035068938839839586   Iteration 72 of 100, tot loss = 4.505178272724152, l1: 0.00010116666130165363, l2: 0.00034935116733928834   Iteration 73 of 100, tot loss = 4.49123820866624, l1: 0.00010115580666373318, l2: 0.00034796801548808405   Iteration 74 of 100, tot loss = 4.483053858215745, l1: 0.00010099088870668532, l2: 0.00034731449842503344   Iteration 75 of 100, tot loss = 4.470936098098755, l1: 0.00010078368252531315, l2: 0.0003463099287667622   Iteration 76 of 100, tot loss = 4.454423800895088, l1: 0.00010055206458721506, l2: 0.00034489031704173324   Iteration 77 of 100, tot loss = 4.450259738154226, l1: 0.00010049484961956448, l2: 0.00034453112585923647   Iteration 78 of 100, tot loss = 4.475046503238189, l1: 0.00010099216011346866, l2: 0.0003465124917476295   Iteration 79 of 100, tot loss = 4.473263701306114, l1: 0.00010113870574727278, l2: 0.00034618766626926683   Iteration 80 of 100, tot loss = 4.4803660124540325, l1: 0.00010130204282177147, l2: 0.0003467345604803995   Iteration 81 of 100, tot loss = 4.451534135842029, l1: 0.00010081184388391306, l2: 0.00034434157175753543   Iteration 82 of 100, tot loss = 4.434089410595778, l1: 0.0001003093971010502, l2: 0.0003430995459827345   Iteration 83 of 100, tot loss = 4.422450447657022, l1: 0.00010014126729150023, l2: 0.00034210377959085694   Iteration 84 of 100, tot loss = 4.469211036250705, l1: 0.00010081600469545395, l2: 0.0003461051001934157   Iteration 85 of 100, tot loss = 4.484877533071181, l1: 0.0001010002861114438, l2: 0.0003474874682176639   Iteration 86 of 100, tot loss = 4.497083833051282, l1: 0.00010072844198412197, l2: 0.0003489799422569313   Iteration 87 of 100, tot loss = 4.471104627368094, l1: 0.00010021662405804442, l2: 0.000346893839733461   Iteration 88 of 100, tot loss = 4.4556199556047265, l1: 0.00010007202813126655, l2: 0.0003454899684914959   Iteration 89 of 100, tot loss = 4.454123446110929, l1: 9.991203482508702e-05, l2: 0.0003455003108397608   Iteration 90 of 100, tot loss = 4.4274804168277315, l1: 9.929038452456653e-05, l2: 0.0003434576582448143   Iteration 91 of 100, tot loss = 4.418971569983514, l1: 9.933582315660224e-05, l2: 0.00034256133501632855   Iteration 92 of 100, tot loss = 4.398619107578112, l1: 9.872265075935739e-05, l2: 0.0003411392610122794   Iteration 93 of 100, tot loss = 4.4357264682810795, l1: 9.943423833402615e-05, l2: 0.00034413840956560345   Iteration 94 of 100, tot loss = 4.438459228962026, l1: 9.935906703678019e-05, l2: 0.0003444868571216617   Iteration 95 of 100, tot loss = 4.426179803045172, l1: 9.914211481564531e-05, l2: 0.00034347586655425596   Iteration 96 of 100, tot loss = 4.414460323750973, l1: 9.886227295889209e-05, l2: 0.0003425837605088115   Iteration 97 of 100, tot loss = 4.420103557331046, l1: 9.911825709985857e-05, l2: 0.00034289209927629066   Iteration 98 of 100, tot loss = 4.422556806583794, l1: 9.930490556812123e-05, l2: 0.0003429507756218485   Iteration 99 of 100, tot loss = 4.4382526970872975, l1: 9.941754787995697e-05, l2: 0.0003444077222163535   Iteration 100 of 100, tot loss = 4.451669380664826, l1: 9.958023023500573e-05, l2: 0.00034558670842670835
   End of epoch 1217; saving model... 

Epoch 1218 of 2000
   Iteration 1 of 100, tot loss = 6.345542907714844, l1: 0.00014052278129383922, l2: 0.0004940315266139805   Iteration 2 of 100, tot loss = 5.8111231327056885, l1: 0.0001358915542368777, l2: 0.000445220764959231   Iteration 3 of 100, tot loss = 5.368146260579427, l1: 0.00012178536659727494, l2: 0.0004150292564493914   Iteration 4 of 100, tot loss = 5.44021475315094, l1: 0.00011318966062390245, l2: 0.00043083180935354903   Iteration 5 of 100, tot loss = 5.229507732391357, l1: 0.0001079068155377172, l2: 0.00041504395776428284   Iteration 6 of 100, tot loss = 5.000639637311299, l1: 0.000107851864110368, l2: 0.0003922121007538711   Iteration 7 of 100, tot loss = 4.990753207887922, l1: 0.00010960282504259209, l2: 0.0003894724982923695   Iteration 8 of 100, tot loss = 5.058791428804398, l1: 0.00010839987498911796, l2: 0.0003974792707595043   Iteration 9 of 100, tot loss = 4.94442179467943, l1: 0.00010678254410676245, l2: 0.0003876596359380831   Iteration 10 of 100, tot loss = 4.594394147396088, l1: 9.941819298546762e-05, l2: 0.00036002122214995325   Iteration 11 of 100, tot loss = 4.581965652379123, l1: 0.00010085006744537333, l2: 0.0003573464981110936   Iteration 12 of 100, tot loss = 4.448537935813268, l1: 0.00010117823452067871, l2: 0.0003436755590276637   Iteration 13 of 100, tot loss = 4.30574643611908, l1: 9.872094513132022e-05, l2: 0.0003318536999778679   Iteration 14 of 100, tot loss = 4.6605029702186584, l1: 0.00010501134758149939, l2: 0.0003610389498395047   Iteration 15 of 100, tot loss = 4.786921143531799, l1: 0.00010863502587502202, l2: 0.00037005709018558266   Iteration 16 of 100, tot loss = 4.801997773349285, l1: 0.00010840955201274483, l2: 0.0003717902254720684   Iteration 17 of 100, tot loss = 4.908198209369884, l1: 0.00010886088677901117, l2: 0.00038195893456063727   Iteration 18 of 100, tot loss = 5.040550410747528, l1: 0.00011045986805887272, l2: 0.00039359517460171547   Iteration 19 of 100, tot loss = 5.042521570858202, l1: 0.00011103953990019171, l2: 0.00039321261819599097   Iteration 20 of 100, tot loss = 5.047569578886032, l1: 0.0001113171372708166, l2: 0.0003934398206183687   Iteration 21 of 100, tot loss = 5.156081738926115, l1: 0.00011216898322648679, l2: 0.0004034391881543256   Iteration 22 of 100, tot loss = 5.25814445994117, l1: 0.00011479828092640012, l2: 0.0004110161627812142   Iteration 23 of 100, tot loss = 5.110356937284055, l1: 0.00011217498752597274, l2: 0.0003988607040019301   Iteration 24 of 100, tot loss = 5.12790289024512, l1: 0.00011275586939518689, l2: 0.0004000344197265804   Iteration 25 of 100, tot loss = 5.070923027992248, l1: 0.00011023859842680394, l2: 0.0003968537040054798   Iteration 26 of 100, tot loss = 5.065170916227194, l1: 0.00011067227723497826, l2: 0.0003958448147526584   Iteration 27 of 100, tot loss = 5.03971027886426, l1: 0.00010951322366492133, l2: 0.0003944578045910155   Iteration 28 of 100, tot loss = 5.011084373508181, l1: 0.00010903934630083054, l2: 0.000392069091114016   Iteration 29 of 100, tot loss = 4.950296702056096, l1: 0.0001076714673625498, l2: 0.0003873582032171945   Iteration 30 of 100, tot loss = 4.945696771144867, l1: 0.0001076942528015934, l2: 0.0003868754244952773   Iteration 31 of 100, tot loss = 4.928662673119576, l1: 0.00010823683377595679, l2: 0.000384629433674197   Iteration 32 of 100, tot loss = 4.964356992393732, l1: 0.00010879283399845008, l2: 0.00038764286546211224   Iteration 33 of 100, tot loss = 5.035176020680052, l1: 0.00011022932118397544, l2: 0.0003932882823529794   Iteration 34 of 100, tot loss = 5.003397832898533, l1: 0.00010942507780033767, l2: 0.00039091470620512744   Iteration 35 of 100, tot loss = 5.022943908827646, l1: 0.00010932907961042864, l2: 0.0003929653130139091   Iteration 36 of 100, tot loss = 4.991976860496733, l1: 0.00010864888847411041, l2: 0.0003905487990575946   Iteration 37 of 100, tot loss = 4.90651925190075, l1: 0.0001066676586806598, l2: 0.00038398426801683635   Iteration 38 of 100, tot loss = 4.867184889943976, l1: 0.00010607773564467942, l2: 0.00038064075488364324   Iteration 39 of 100, tot loss = 4.872679637028621, l1: 0.00010643296637005196, l2: 0.00038083499851218687   Iteration 40 of 100, tot loss = 4.8227891564369205, l1: 0.00010584136562101776, l2: 0.00037643755094904916   Iteration 41 of 100, tot loss = 4.817108921888398, l1: 0.00010586708958726376, l2: 0.0003758438031626411   Iteration 42 of 100, tot loss = 4.848896037964594, l1: 0.00010587264114292338, l2: 0.0003790169627921257   Iteration 43 of 100, tot loss = 4.893533651218858, l1: 0.00010687137952583387, l2: 0.0003824819851975325   Iteration 44 of 100, tot loss = 4.920822934670881, l1: 0.0001075449429415378, l2: 0.00038453735065889884   Iteration 45 of 100, tot loss = 4.941521422068278, l1: 0.00010832574884665923, l2: 0.00038582639309525906   Iteration 46 of 100, tot loss = 4.93473193956458, l1: 0.00010833209987891757, l2: 0.0003851410941063912   Iteration 47 of 100, tot loss = 4.884328416053285, l1: 0.00010765536686410493, l2: 0.0003807774746166344   Iteration 48 of 100, tot loss = 4.9078847368558245, l1: 0.00010804828631686784, l2: 0.0003827401875848106   Iteration 49 of 100, tot loss = 4.912857649277668, l1: 0.00010809795135080966, l2: 0.00038318781357980807   Iteration 50 of 100, tot loss = 4.954461784362793, l1: 0.00010878740140469745, l2: 0.00038665877655148507   Iteration 51 of 100, tot loss = 4.937868352029838, l1: 0.00010836495907735262, l2: 0.00038542187608340205   Iteration 52 of 100, tot loss = 4.973012053049528, l1: 0.00010896965851805781, l2: 0.00038833154674368695   Iteration 53 of 100, tot loss = 5.010126896624295, l1: 0.00010952241853305648, l2: 0.00039149027093717795   Iteration 54 of 100, tot loss = 5.034178142194395, l1: 0.000108978943495054, l2: 0.00039443887093242394   Iteration 55 of 100, tot loss = 5.022575855255127, l1: 0.00010874784916681661, l2: 0.0003935097367502749   Iteration 56 of 100, tot loss = 5.049588807991573, l1: 0.00010906879262100639, l2: 0.0003958900883195123   Iteration 57 of 100, tot loss = 5.057063964375279, l1: 0.00010951686591285754, l2: 0.0003961895300451209   Iteration 58 of 100, tot loss = 5.014460645872971, l1: 0.00010875053979769542, l2: 0.0003926955245772441   Iteration 59 of 100, tot loss = 4.981678558608233, l1: 0.00010828974768243161, l2: 0.0003898781075604039   Iteration 60 of 100, tot loss = 4.968408012390137, l1: 0.00010800527488754596, l2: 0.0003888355255185161   Iteration 61 of 100, tot loss = 4.987144259155774, l1: 0.00010834880458042178, l2: 0.00039036561984026835   Iteration 62 of 100, tot loss = 4.98074992241398, l1: 0.00010835844577447841, l2: 0.00038971654547851595   Iteration 63 of 100, tot loss = 4.946851995256212, l1: 0.00010808306493951629, l2: 0.0003866021334755397   Iteration 64 of 100, tot loss = 4.955178648233414, l1: 0.00010818503881182551, l2: 0.00038733282508474076   Iteration 65 of 100, tot loss = 4.913847468449519, l1: 0.00010760417577470295, l2: 0.00038378057025301344   Iteration 66 of 100, tot loss = 4.928445707667958, l1: 0.00010761432581189597, l2: 0.000385230244401927   Iteration 67 of 100, tot loss = 4.921721992207996, l1: 0.00010730132200831631, l2: 0.00038487087664152705   Iteration 68 of 100, tot loss = 4.910188219126533, l1: 0.00010702400611669017, l2: 0.00038399481512469183   Iteration 69 of 100, tot loss = 4.9289125083149345, l1: 0.0001075625995577286, l2: 0.0003853286506837585   Iteration 70 of 100, tot loss = 4.887885975837707, l1: 0.00010654074056739253, l2: 0.0003822478565520474   Iteration 71 of 100, tot loss = 4.871139949476215, l1: 0.00010609495351878061, l2: 0.00038101904104057124   Iteration 72 of 100, tot loss = 4.8568743367989855, l1: 0.00010561316988363008, l2: 0.00038007426337571815   Iteration 73 of 100, tot loss = 4.843297109211961, l1: 0.00010526606528812458, l2: 0.0003790636451225983   Iteration 74 of 100, tot loss = 4.842234978804717, l1: 0.00010542675748159102, l2: 0.00037879673952891216   Iteration 75 of 100, tot loss = 4.867512009938558, l1: 0.00010601313474277655, l2: 0.00038073806557804347   Iteration 76 of 100, tot loss = 4.886536340964468, l1: 0.00010638634902486381, l2: 0.000382267284102885   Iteration 77 of 100, tot loss = 4.880090595839859, l1: 0.00010649859820199259, l2: 0.00038151046020801286   Iteration 78 of 100, tot loss = 4.897207156205789, l1: 0.000106942812775602, l2: 0.00038277790157828864   Iteration 79 of 100, tot loss = 4.9034931931314585, l1: 0.00010727521220348846, l2: 0.00038307410579386955   Iteration 80 of 100, tot loss = 4.893807685375213, l1: 0.00010746728194135357, l2: 0.00038191348539839964   Iteration 81 of 100, tot loss = 4.892865928602808, l1: 0.00010738486719079155, l2: 0.00038190172441924614   Iteration 82 of 100, tot loss = 4.900654525291629, l1: 0.00010753960189659421, l2: 0.0003825258495564368   Iteration 83 of 100, tot loss = 4.889095648225531, l1: 0.0001073204616823862, l2: 0.0003815891020594681   Iteration 84 of 100, tot loss = 4.896065368538811, l1: 0.00010737965411473332, l2: 0.00038222688142143724   Iteration 85 of 100, tot loss = 4.883428018233356, l1: 0.00010709866747835323, l2: 0.0003812441328430877   Iteration 86 of 100, tot loss = 4.860711690991423, l1: 0.00010667890396583717, l2: 0.00037939226367175147   Iteration 87 of 100, tot loss = 4.838685507061838, l1: 0.00010618533629351349, l2: 0.0003776832128441411   Iteration 88 of 100, tot loss = 4.8506937568837944, l1: 0.00010646886899352963, l2: 0.00037860050493195143   Iteration 89 of 100, tot loss = 4.894055816564667, l1: 0.00010717432655582435, l2: 0.00038223125346611894   Iteration 90 of 100, tot loss = 4.894102091259427, l1: 0.00010747991182142868, l2: 0.00038193029597298136   Iteration 91 of 100, tot loss = 4.905193889534081, l1: 0.00010729434255224008, l2: 0.00038322504510038665   Iteration 92 of 100, tot loss = 4.898512249407561, l1: 0.00010734994548724701, l2: 0.00038250127830036473   Iteration 93 of 100, tot loss = 4.902965166235483, l1: 0.0001076018237928179, l2: 0.0003826946917650921   Iteration 94 of 100, tot loss = 4.893115829914175, l1: 0.00010735065517748924, l2: 0.00038196092677064876   Iteration 95 of 100, tot loss = 4.871962469502499, l1: 0.00010700414905410358, l2: 0.0003801920968965676   Iteration 96 of 100, tot loss = 4.881155389050643, l1: 0.00010738529522313911, l2: 0.0003807302427958348   Iteration 97 of 100, tot loss = 4.873290329864345, l1: 0.00010735505082303677, l2: 0.0003799739814022575   Iteration 98 of 100, tot loss = 4.846141000183261, l1: 0.00010693749572965316, l2: 0.00037767660340213465   Iteration 99 of 100, tot loss = 4.8489076368736495, l1: 0.00010704360201171684, l2: 0.0003778471605854598   Iteration 100 of 100, tot loss = 4.853219749927521, l1: 0.00010711721450206823, l2: 0.00037820475947228263
   End of epoch 1218; saving model... 

Epoch 1219 of 2000
   Iteration 1 of 100, tot loss = 2.7236881256103516, l1: 5.6096392654580995e-05, l2: 0.00021627241221722215   Iteration 2 of 100, tot loss = 2.604759693145752, l1: 6.332680823106784e-05, l2: 0.00019714915106305853   Iteration 3 of 100, tot loss = 2.9581173261006675, l1: 7.27476957157099e-05, l2: 0.00022306403358622143   Iteration 4 of 100, tot loss = 3.4259557723999023, l1: 8.673233514855383e-05, l2: 0.00025586324409232475   Iteration 5 of 100, tot loss = 3.7923118591308596, l1: 8.510250845574773e-05, l2: 0.0002941286802524701   Iteration 6 of 100, tot loss = 3.7052425146102905, l1: 8.309653458127286e-05, l2: 0.0002874277197406627   Iteration 7 of 100, tot loss = 3.745313916887556, l1: 8.468098998751625e-05, l2: 0.00028985040054457   Iteration 8 of 100, tot loss = 3.8006593585014343, l1: 8.253612259068177e-05, l2: 0.0002975298102683155   Iteration 9 of 100, tot loss = 3.7995914353264704, l1: 8.209654050814506e-05, l2: 0.0002978626014535419   Iteration 10 of 100, tot loss = 3.8022748470306396, l1: 8.222196229326073e-05, l2: 0.0002980055214720778   Iteration 11 of 100, tot loss = 4.038255474784157, l1: 8.784025670717132e-05, l2: 0.0003159852865130895   Iteration 12 of 100, tot loss = 3.988929589589437, l1: 8.790929465855395e-05, l2: 0.00031098366040775244   Iteration 13 of 100, tot loss = 4.158004173865685, l1: 8.904354511357199e-05, l2: 0.0003267568681621924   Iteration 14 of 100, tot loss = 4.452001776014056, l1: 9.523753163063833e-05, l2: 0.0003499626400298439   Iteration 15 of 100, tot loss = 4.437147649129232, l1: 9.480864803966446e-05, l2: 0.00034890610938115665   Iteration 16 of 100, tot loss = 4.337920919060707, l1: 9.310558357356058e-05, l2: 0.000340686501658638   Iteration 17 of 100, tot loss = 4.246520014370189, l1: 9.176735779071939e-05, l2: 0.000332884636865107   Iteration 18 of 100, tot loss = 4.278733703825209, l1: 9.272564198504874e-05, l2: 0.0003351477199531574   Iteration 19 of 100, tot loss = 4.535980048932527, l1: 9.755088505794687e-05, l2: 0.00035604711018478205   Iteration 20 of 100, tot loss = 4.564261531829834, l1: 9.843850220931926e-05, l2: 0.0003579876422008965   Iteration 21 of 100, tot loss = 4.524607181549072, l1: 9.839822294972172e-05, l2: 0.000354062486578533   Iteration 22 of 100, tot loss = 4.456223000179637, l1: 9.68109841117191e-05, l2: 0.00034881130771034145   Iteration 23 of 100, tot loss = 4.366945090501205, l1: 9.517060971120372e-05, l2: 0.00034152389106684893   Iteration 24 of 100, tot loss = 4.435165077447891, l1: 9.584808231011266e-05, l2: 0.00034766841842307866   Iteration 25 of 100, tot loss = 4.467244138717652, l1: 9.620850585633888e-05, l2: 0.00035051590122748167   Iteration 26 of 100, tot loss = 4.384189963340759, l1: 9.397884283456593e-05, l2: 0.0003444401473433782   Iteration 27 of 100, tot loss = 4.363348572342484, l1: 9.285952519685789e-05, l2: 0.00034347532588678104   Iteration 28 of 100, tot loss = 4.303582225527082, l1: 9.199855308647134e-05, l2: 0.00033835966366625925   Iteration 29 of 100, tot loss = 4.319311207738416, l1: 9.296788555040057e-05, l2: 0.0003389632294219437   Iteration 30 of 100, tot loss = 4.343878316879272, l1: 9.402932482771575e-05, l2: 0.0003403585001554651   Iteration 31 of 100, tot loss = 4.31738705019797, l1: 9.415098447202434e-05, l2: 0.00033758771408634684   Iteration 32 of 100, tot loss = 4.307046666741371, l1: 9.369256531499559e-05, l2: 0.00033701209531500353   Iteration 33 of 100, tot loss = 4.370211659055768, l1: 9.521211155295146e-05, l2: 0.00034180904913228005   Iteration 34 of 100, tot loss = 4.389336656121647, l1: 9.581530171357478e-05, l2: 0.00034311835813294984   Iteration 35 of 100, tot loss = 4.447648211887905, l1: 9.736313097943952e-05, l2: 0.0003474016844328227   Iteration 36 of 100, tot loss = 4.416169027487437, l1: 9.706740638648625e-05, l2: 0.000344549490465498   Iteration 37 of 100, tot loss = 4.455477205482689, l1: 9.77978199591382e-05, l2: 0.0003477498944817312   Iteration 38 of 100, tot loss = 4.412302876773634, l1: 9.651254549680743e-05, l2: 0.00034471773611393904   Iteration 39 of 100, tot loss = 4.417900800704956, l1: 9.674457443254188e-05, l2: 0.00034504550030060974   Iteration 40 of 100, tot loss = 4.422187155485153, l1: 9.712059227240388e-05, l2: 0.0003450981184869306   Iteration 41 of 100, tot loss = 4.40395240085881, l1: 9.664022954672052e-05, l2: 0.0003437550057708172   Iteration 42 of 100, tot loss = 4.4248106479644775, l1: 9.678233280802877e-05, l2: 0.0003456987279129126   Iteration 43 of 100, tot loss = 4.45652110077614, l1: 9.741837053551599e-05, l2: 0.00034823373527276914   Iteration 44 of 100, tot loss = 4.5013932098041884, l1: 9.813077560300536e-05, l2: 0.0003520085417469752   Iteration 45 of 100, tot loss = 4.517590607537164, l1: 9.821117281616252e-05, l2: 0.0003535478838279636   Iteration 46 of 100, tot loss = 4.524057906606923, l1: 9.881717138025282e-05, l2: 0.00035358861477225616   Iteration 47 of 100, tot loss = 4.52816782606409, l1: 9.871063872545146e-05, l2: 0.00035410613952293756   Iteration 48 of 100, tot loss = 4.528544267018636, l1: 9.866405957836832e-05, l2: 0.00035419036339590093   Iteration 49 of 100, tot loss = 4.57487373935933, l1: 9.953124071198174e-05, l2: 0.00035795612927117594   Iteration 50 of 100, tot loss = 4.548142428398132, l1: 9.845921100350097e-05, l2: 0.0003563550280523486   Iteration 51 of 100, tot loss = 4.5541129719977285, l1: 9.896991176384629e-05, l2: 0.00035644138109751556   Iteration 52 of 100, tot loss = 4.542596138440645, l1: 9.887442376146701e-05, l2: 0.00035538518596485327   Iteration 53 of 100, tot loss = 4.5229224393952565, l1: 9.850820445209601e-05, l2: 0.00035378403543222275   Iteration 54 of 100, tot loss = 4.538011228596723, l1: 9.897993435583043e-05, l2: 0.00035482118497990694   Iteration 55 of 100, tot loss = 4.560052139108831, l1: 9.930696065897461e-05, l2: 0.00035669825023929166   Iteration 56 of 100, tot loss = 4.5779249880995065, l1: 9.935137040884001e-05, l2: 0.0003584411260817433   Iteration 57 of 100, tot loss = 4.580860568766008, l1: 9.936411605163462e-05, l2: 0.000358721939080983   Iteration 58 of 100, tot loss = 4.571403696619231, l1: 9.894922211593627e-05, l2: 0.00035819114544147885   Iteration 59 of 100, tot loss = 4.609248779587826, l1: 9.951515972192037e-05, l2: 0.00036140971632587517   Iteration 60 of 100, tot loss = 4.579468858242035, l1: 9.918392194473806e-05, l2: 0.00035876296193843396   Iteration 61 of 100, tot loss = 4.578266991943609, l1: 9.94737353319012e-05, l2: 0.0003583529622000108   Iteration 62 of 100, tot loss = 4.559027710268574, l1: 9.907757069618862e-05, l2: 0.00035682519885613735   Iteration 63 of 100, tot loss = 4.525835794115824, l1: 9.857714781352866e-05, l2: 0.0003540064302912455   Iteration 64 of 100, tot loss = 4.511074610054493, l1: 9.78252065237939e-05, l2: 0.0003532822531724378   Iteration 65 of 100, tot loss = 4.538017412332388, l1: 9.837177332463817e-05, l2: 0.00035542996640567883   Iteration 66 of 100, tot loss = 4.572442712205829, l1: 9.862039339416448e-05, l2: 0.00035862387691546854   Iteration 67 of 100, tot loss = 4.537896903593149, l1: 9.813895037462031e-05, l2: 0.0003556507390249632   Iteration 68 of 100, tot loss = 4.522258471040165, l1: 9.787180624698522e-05, l2: 0.0003543540399679092   Iteration 69 of 100, tot loss = 4.5009960229846016, l1: 9.779775601877508e-05, l2: 0.00035230184532364535   Iteration 70 of 100, tot loss = 4.545325544902257, l1: 9.850112541503872e-05, l2: 0.0003560314283406894   Iteration 71 of 100, tot loss = 4.594771835165964, l1: 9.943435146288067e-05, l2: 0.0003600428319490239   Iteration 72 of 100, tot loss = 4.587161812517378, l1: 9.900918035378304e-05, l2: 0.0003597070006233278   Iteration 73 of 100, tot loss = 4.554366722498854, l1: 9.85727158764996e-05, l2: 0.00035686395629404803   Iteration 74 of 100, tot loss = 4.596071504257821, l1: 9.927923054272321e-05, l2: 0.00036032791929854383   Iteration 75 of 100, tot loss = 4.586920544306437, l1: 9.882732929933505e-05, l2: 0.00035986472445074467   Iteration 76 of 100, tot loss = 4.564225808570259, l1: 9.864971889328444e-05, l2: 0.0003577728614465358   Iteration 77 of 100, tot loss = 4.552613846667401, l1: 9.831736098057227e-05, l2: 0.00035694402331855243   Iteration 78 of 100, tot loss = 4.553678078529162, l1: 9.821085726658706e-05, l2: 0.0003571569498326892   Iteration 79 of 100, tot loss = 4.567141593257083, l1: 9.833690959958749e-05, l2: 0.0003583772493676648   Iteration 80 of 100, tot loss = 4.554792076349258, l1: 9.840720063039043e-05, l2: 0.00035707200677279614   Iteration 81 of 100, tot loss = 4.5768333423284835, l1: 9.881843392195375e-05, l2: 0.00035886490019756934   Iteration 82 of 100, tot loss = 4.580205103246177, l1: 9.868558042465493e-05, l2: 0.0003593349295154373   Iteration 83 of 100, tot loss = 4.586650687527944, l1: 9.910647082528246e-05, l2: 0.00035955859765449697   Iteration 84 of 100, tot loss = 4.574977871917543, l1: 9.898271773203131e-05, l2: 0.00035851506930027554   Iteration 85 of 100, tot loss = 4.5905250801759605, l1: 9.917673198608955e-05, l2: 0.0003598757757572457   Iteration 86 of 100, tot loss = 4.600020949230638, l1: 9.944979002432584e-05, l2: 0.0003605523046179198   Iteration 87 of 100, tot loss = 4.585699722684663, l1: 9.942903982417861e-05, l2: 0.0003591409319711999   Iteration 88 of 100, tot loss = 4.584797176447782, l1: 9.954884225490704e-05, l2: 0.0003589308749228208   Iteration 89 of 100, tot loss = 4.580756444609567, l1: 9.956489328727953e-05, l2: 0.000358510750353734   Iteration 90 of 100, tot loss = 4.587115944756402, l1: 9.987904805812933e-05, l2: 0.00035883254572076514   Iteration 91 of 100, tot loss = 4.579925159831624, l1: 9.980730648929059e-05, l2: 0.00035818520892452407   Iteration 92 of 100, tot loss = 4.577436980993851, l1: 0.00010002411027412366, l2: 0.00035771958747595227   Iteration 93 of 100, tot loss = 4.578300819602064, l1: 9.996336298808205e-05, l2: 0.00035786671878179155   Iteration 94 of 100, tot loss = 4.580949925361796, l1: 0.00010012444479488817, l2: 0.0003579705473694435   Iteration 95 of 100, tot loss = 4.564441698475888, l1: 9.968114199613123e-05, l2: 0.0003567630276512845   Iteration 96 of 100, tot loss = 4.573254165550073, l1: 9.975919219111044e-05, l2: 0.00035756622446569963   Iteration 97 of 100, tot loss = 4.572675584517803, l1: 9.952594516960875e-05, l2: 0.0003577416131726572   Iteration 98 of 100, tot loss = 4.578871617511827, l1: 9.971214266053201e-05, l2: 0.00035817501932852995   Iteration 99 of 100, tot loss = 4.560651475732977, l1: 9.946638193937498e-05, l2: 0.00035659876573275814   Iteration 100 of 100, tot loss = 4.584925727844238, l1: 9.981015700759599e-05, l2: 0.0003586824153899215
   End of epoch 1219; saving model... 

Epoch 1220 of 2000
   Iteration 1 of 100, tot loss = 3.9779696464538574, l1: 7.542270759586245e-05, l2: 0.00032237425330094993   Iteration 2 of 100, tot loss = 3.901901602745056, l1: 8.852174505591393e-05, l2: 0.0003016684058820829   Iteration 3 of 100, tot loss = 3.6431676546732583, l1: 7.509210627176799e-05, l2: 0.0002892246605673184   Iteration 4 of 100, tot loss = 4.824348330497742, l1: 9.923758898366941e-05, l2: 0.00038319724990287796   Iteration 5 of 100, tot loss = 4.653626394271851, l1: 9.9905193928862e-05, l2: 0.00036545745097100736   Iteration 6 of 100, tot loss = 4.894649863243103, l1: 0.00010801021865821288, l2: 0.00038145477204428363   Iteration 7 of 100, tot loss = 4.868835278919765, l1: 0.00010794314429014256, l2: 0.0003789403916536165   Iteration 8 of 100, tot loss = 4.619130939245224, l1: 0.00010434419573357445, l2: 0.0003575689061108278   Iteration 9 of 100, tot loss = 4.561182578404744, l1: 0.00010428867123361367, l2: 0.0003518295949713017   Iteration 10 of 100, tot loss = 4.4893066644668576, l1: 0.00010319580214854796, l2: 0.00034573486918816344   Iteration 11 of 100, tot loss = 4.5674896456978535, l1: 0.00010341744498212144, l2: 0.00035333152283618057   Iteration 12 of 100, tot loss = 4.6655916174252825, l1: 0.00010582870133172644, l2: 0.0003607304618829706   Iteration 13 of 100, tot loss = 4.808526057463426, l1: 0.0001072266646728027, l2: 0.00037362594314170285   Iteration 14 of 100, tot loss = 4.887908714158194, l1: 0.00010957163898897957, l2: 0.0003792192344138   Iteration 15 of 100, tot loss = 4.832116270065308, l1: 0.00010957551688382713, l2: 0.0003736361114230628   Iteration 16 of 100, tot loss = 4.960797742009163, l1: 0.00011068625804000476, l2: 0.00038539351862709736   Iteration 17 of 100, tot loss = 4.933965556761798, l1: 0.0001111092597591992, l2: 0.00038228729852776537   Iteration 18 of 100, tot loss = 4.982472856839498, l1: 0.00011225899510868153, l2: 0.00038598829430864297   Iteration 19 of 100, tot loss = 5.0976120170794035, l1: 0.00011403381148212295, l2: 0.00039572739364015624   Iteration 20 of 100, tot loss = 4.972030746936798, l1: 0.00011230909130972577, l2: 0.00038489398648380304   Iteration 21 of 100, tot loss = 5.099882228033883, l1: 0.00011308569654439842, l2: 0.00039690252812024917   Iteration 22 of 100, tot loss = 5.190851460803639, l1: 0.00011518735498777295, l2: 0.000403897793810095   Iteration 23 of 100, tot loss = 5.111925135488096, l1: 0.00011398103300092296, l2: 0.00039721148420105   Iteration 24 of 100, tot loss = 5.154252539078395, l1: 0.00011414811994351719, l2: 0.00040127713812883786   Iteration 25 of 100, tot loss = 5.137720384597778, l1: 0.00011252612588577904, l2: 0.0004012459161458537   Iteration 26 of 100, tot loss = 5.077903866767883, l1: 0.00011184374572901736, l2: 0.0003959466440182251   Iteration 27 of 100, tot loss = 5.110438249729298, l1: 0.0001123379871488497, l2: 0.00039870584118438675   Iteration 28 of 100, tot loss = 5.132417959826333, l1: 0.00011200338654556877, l2: 0.0004012384134901887   Iteration 29 of 100, tot loss = 5.089640099426796, l1: 0.00011214605672586034, l2: 0.0003968179572156855   Iteration 30 of 100, tot loss = 4.982320686181386, l1: 0.00010962714441120625, l2: 0.0003886049283513178   Iteration 31 of 100, tot loss = 4.890156288300791, l1: 0.00010736821469993541, l2: 0.0003816474176522705   Iteration 32 of 100, tot loss = 4.960701454430819, l1: 0.00010829773077603022, l2: 0.0003877724175254116   Iteration 33 of 100, tot loss = 4.93751754543998, l1: 0.0001087350350589435, l2: 0.00038501672238591266   Iteration 34 of 100, tot loss = 4.954541953171001, l1: 0.00010901746474487213, l2: 0.0003864367334993885   Iteration 35 of 100, tot loss = 4.924914022854396, l1: 0.00010868861448086266, l2: 0.0003838027910595494   Iteration 36 of 100, tot loss = 4.928493277894126, l1: 0.00010827058536556756, l2: 0.00038457874486792006   Iteration 37 of 100, tot loss = 4.8499820457922445, l1: 0.00010634886623577315, l2: 0.000378649340819794   Iteration 38 of 100, tot loss = 4.786572108143254, l1: 0.00010525608099430285, l2: 0.00037340113263589476   Iteration 39 of 100, tot loss = 4.764451610736358, l1: 0.00010554837251383548, l2: 0.0003708967911407877   Iteration 40 of 100, tot loss = 4.772811207175255, l1: 0.0001053182305440714, l2: 0.0003719628937687958   Iteration 41 of 100, tot loss = 4.805817973322984, l1: 0.00010561474296193366, l2: 0.00037496705807847674   Iteration 42 of 100, tot loss = 4.758030916963305, l1: 0.00010460744852025528, l2: 0.0003711956472086188   Iteration 43 of 100, tot loss = 4.768762502559396, l1: 0.00010360828656662624, l2: 0.0003732679683179053   Iteration 44 of 100, tot loss = 4.742144739085978, l1: 0.00010298985439889789, l2: 0.00037122462427677504   Iteration 45 of 100, tot loss = 4.710220323668586, l1: 0.00010261779381026928, l2: 0.000368404243231958   Iteration 46 of 100, tot loss = 4.692722556383713, l1: 0.00010263400271588066, l2: 0.00036663825733779487   Iteration 47 of 100, tot loss = 4.755789673074763, l1: 0.00010363502711852755, l2: 0.00037194394470598706   Iteration 48 of 100, tot loss = 4.754617986579736, l1: 0.00010409044633282367, l2: 0.0003713713567776722   Iteration 49 of 100, tot loss = 4.728609754114735, l1: 0.00010366374363930306, l2: 0.00036919723610555257   Iteration 50 of 100, tot loss = 4.8195765948295595, l1: 0.00010499743970285636, l2: 0.00037696022394811734   Iteration 51 of 100, tot loss = 4.814267483412051, l1: 0.00010485620092082422, l2: 0.000376570551238461   Iteration 52 of 100, tot loss = 4.776830395826926, l1: 0.00010378194768306387, l2: 0.0003739010957920422   Iteration 53 of 100, tot loss = 4.7363217349322335, l1: 0.00010347640506191758, l2: 0.00037015577226894784   Iteration 54 of 100, tot loss = 4.725504184210742, l1: 0.00010352732331537279, l2: 0.00036902309942956046   Iteration 55 of 100, tot loss = 4.751667488705028, l1: 0.00010437172823003493, l2: 0.00037079502415673975   Iteration 56 of 100, tot loss = 4.725052320531437, l1: 0.00010360131656333189, l2: 0.00036890391908985166   Iteration 57 of 100, tot loss = 4.726246505452876, l1: 0.00010406013200081999, l2: 0.00036856452188977417   Iteration 58 of 100, tot loss = 4.723637767906847, l1: 0.0001036687191462361, l2: 0.00036869506082690226   Iteration 59 of 100, tot loss = 4.731709599494934, l1: 0.0001040162583117539, l2: 0.00036915470510563356   Iteration 60 of 100, tot loss = 4.738203261295954, l1: 0.00010423644143884304, l2: 0.00036958388809580354   Iteration 61 of 100, tot loss = 4.771138220536904, l1: 0.0001043310980183706, l2: 0.0003727827274591708   Iteration 62 of 100, tot loss = 4.780697482247507, l1: 0.00010467511715579613, l2: 0.0003733946350328023   Iteration 63 of 100, tot loss = 4.809379053494287, l1: 0.00010485519738895597, l2: 0.0003760827119491758   Iteration 64 of 100, tot loss = 4.802917359396815, l1: 0.00010506189943271238, l2: 0.000375229840301472   Iteration 65 of 100, tot loss = 4.790480828285217, l1: 0.00010433088363452743, l2: 0.0003747172031300859   Iteration 66 of 100, tot loss = 4.81631965528835, l1: 0.00010491884412741373, l2: 0.00037671312571602   Iteration 67 of 100, tot loss = 4.792093851673069, l1: 0.00010479018563463756, l2: 0.00037441920396114297   Iteration 68 of 100, tot loss = 4.789341581218383, l1: 0.00010511966887433493, l2: 0.00037381449360777074   Iteration 69 of 100, tot loss = 4.760194990945899, l1: 0.00010428145527628763, l2: 0.0003717380480510115   Iteration 70 of 100, tot loss = 4.745063129493168, l1: 0.00010396688932295157, l2: 0.0003705394279677421   Iteration 71 of 100, tot loss = 4.740519673051969, l1: 0.0001039114632847896, l2: 0.0003701405079943508   Iteration 72 of 100, tot loss = 4.757470172312525, l1: 0.0001042654933169413, l2: 0.0003714815276099317   Iteration 73 of 100, tot loss = 4.762966660604085, l1: 0.00010420167875477777, l2: 0.00037209499126606403   Iteration 74 of 100, tot loss = 4.7487439287675395, l1: 0.00010422281269023408, l2: 0.0003706515842192882   Iteration 75 of 100, tot loss = 4.740824516614278, l1: 0.00010430598781870988, l2: 0.00036977646794791023   Iteration 76 of 100, tot loss = 4.754857140152078, l1: 0.00010482190492965808, l2: 0.0003706638128374164   Iteration 77 of 100, tot loss = 4.735936881659867, l1: 0.00010449589488463923, l2: 0.000369097796713256   Iteration 78 of 100, tot loss = 4.720490383796203, l1: 0.00010435861226235433, l2: 0.00036769042960189   Iteration 79 of 100, tot loss = 4.7299869528299645, l1: 0.00010436935383639346, l2: 0.00036862934460639484   Iteration 80 of 100, tot loss = 4.717742164433003, l1: 0.00010414399316687195, l2: 0.00036763022653758527   Iteration 81 of 100, tot loss = 4.691780480337732, l1: 0.0001034512684715015, l2: 0.0003657267830639296   Iteration 82 of 100, tot loss = 4.705606991198005, l1: 0.00010380185231929872, l2: 0.0003667588504712756   Iteration 83 of 100, tot loss = 4.725748195705644, l1: 0.00010420474138234304, l2: 0.00036837008125317023   Iteration 84 of 100, tot loss = 4.733768106926055, l1: 0.00010433981913679635, l2: 0.00036903699427577   Iteration 85 of 100, tot loss = 4.718639464939342, l1: 0.00010418057380079785, l2: 0.0003676833752679693   Iteration 86 of 100, tot loss = 4.719766296619593, l1: 0.00010423745351185654, l2: 0.0003677391786220332   Iteration 87 of 100, tot loss = 4.715853385541631, l1: 0.00010419971893016977, l2: 0.00036738562195604647   Iteration 88 of 100, tot loss = 4.727426622401584, l1: 0.00010435347460522529, l2: 0.00036838918954758395   Iteration 89 of 100, tot loss = 4.732249229141836, l1: 0.00010450718465936109, l2: 0.0003687177403448045   Iteration 90 of 100, tot loss = 4.7383375181092156, l1: 0.00010455634441718252, l2: 0.0003692774092390512   Iteration 91 of 100, tot loss = 4.722278504581242, l1: 0.0001043260425641355, l2: 0.0003679018097630303   Iteration 92 of 100, tot loss = 4.731520568547041, l1: 0.00010462524590801205, l2: 0.0003685268126466089   Iteration 93 of 100, tot loss = 4.704149555134517, l1: 0.00010422650337850135, l2: 0.0003661884538521389   Iteration 94 of 100, tot loss = 4.709700778443763, l1: 0.00010440719918907679, l2: 0.00036656288036883395   Iteration 95 of 100, tot loss = 4.744749522209167, l1: 0.00010504774262172807, l2: 0.000369427211147635   Iteration 96 of 100, tot loss = 4.766756585488717, l1: 0.00010538532721208564, l2: 0.0003712903327747578   Iteration 97 of 100, tot loss = 4.744789741702916, l1: 0.00010495654534735746, l2: 0.00036952243013710703   Iteration 98 of 100, tot loss = 4.736122563177226, l1: 0.00010467307940268014, l2: 0.0003689391780834721   Iteration 99 of 100, tot loss = 4.738725376851631, l1: 0.00010429898865175235, l2: 0.0003695735502682363   Iteration 100 of 100, tot loss = 4.758106111288071, l1: 0.00010450118461449164, l2: 0.00037130942771909756
   End of epoch 1220; saving model... 

Epoch 1221 of 2000
   Iteration 1 of 100, tot loss = 3.712277889251709, l1: 9.379929542774335e-05, l2: 0.0002774285094346851   Iteration 2 of 100, tot loss = 4.092803001403809, l1: 9.484251131652854e-05, l2: 0.00031443778425455093   Iteration 3 of 100, tot loss = 3.589641809463501, l1: 8.548869664082304e-05, l2: 0.00027347547681226086   Iteration 4 of 100, tot loss = 3.564192295074463, l1: 7.896460374468006e-05, l2: 0.0002774546192085836   Iteration 5 of 100, tot loss = 3.9341876029968263, l1: 8.590400684624911e-05, l2: 0.000307514748419635   Iteration 6 of 100, tot loss = 4.019068479537964, l1: 8.75931740059362e-05, l2: 0.00031431366854424897   Iteration 7 of 100, tot loss = 3.911522592817034, l1: 8.835982901343544e-05, l2: 0.0003027924269969974   Iteration 8 of 100, tot loss = 3.631504148244858, l1: 8.289916240755701e-05, l2: 0.00028025124993291683   Iteration 9 of 100, tot loss = 3.7509150769975452, l1: 8.44041690773641e-05, l2: 0.0002906873350083414   Iteration 10 of 100, tot loss = 3.781597447395325, l1: 8.389830909436569e-05, l2: 0.00029426143446471543   Iteration 11 of 100, tot loss = 3.8857330192219126, l1: 8.65273667097261e-05, l2: 0.0003020459341562607   Iteration 12 of 100, tot loss = 3.8877285917599997, l1: 8.870477055703911e-05, l2: 0.0003000680881086737   Iteration 13 of 100, tot loss = 4.160000159190251, l1: 9.254173066717788e-05, l2: 0.0003234582881514843   Iteration 14 of 100, tot loss = 4.318136402538845, l1: 9.549159273904349e-05, l2: 0.00033632205200514624   Iteration 15 of 100, tot loss = 4.343449131647746, l1: 9.633047205473607e-05, l2: 0.00033801444612133007   Iteration 16 of 100, tot loss = 4.363156363368034, l1: 9.76182300291839e-05, l2: 0.00033869741128000896   Iteration 17 of 100, tot loss = 4.334513411802404, l1: 9.775283979251981e-05, l2: 0.0003356985073290108   Iteration 18 of 100, tot loss = 4.366533279418945, l1: 9.96989644287775e-05, l2: 0.0003369543685241499   Iteration 19 of 100, tot loss = 4.284839830900493, l1: 9.834454555713914e-05, l2: 0.0003301394432415499   Iteration 20 of 100, tot loss = 4.466046833992005, l1: 0.00010090444011439104, l2: 0.0003457002472714521   Iteration 21 of 100, tot loss = 4.515973181951614, l1: 0.00010106153023939224, l2: 0.00035053579181077934   Iteration 22 of 100, tot loss = 4.598280191421509, l1: 0.00010254796440924771, l2: 0.0003572800593577664   Iteration 23 of 100, tot loss = 4.563956271047178, l1: 0.00010233741024579933, l2: 0.00035405822057762873   Iteration 24 of 100, tot loss = 4.635606437921524, l1: 0.0001031990471650109, l2: 0.00036036159872310236   Iteration 25 of 100, tot loss = 4.6595172023773195, l1: 0.00010343208879930898, l2: 0.0003625196346547455   Iteration 26 of 100, tot loss = 4.715925005766062, l1: 0.00010375351680872533, l2: 0.00036783898674632207   Iteration 27 of 100, tot loss = 4.706451371864036, l1: 0.00010443316056841502, l2: 0.0003662119798482982   Iteration 28 of 100, tot loss = 4.668993702956608, l1: 0.00010377951392521416, l2: 0.00036311986020466847   Iteration 29 of 100, tot loss = 4.61598854229368, l1: 0.00010314288274144176, l2: 0.0003584559744922444   Iteration 30 of 100, tot loss = 4.631373953819275, l1: 0.00010333862689246112, l2: 0.0003597987704173041   Iteration 31 of 100, tot loss = 4.618127645984773, l1: 0.00010247418826874045, l2: 0.0003593385781935086   Iteration 32 of 100, tot loss = 4.658520363271236, l1: 0.00010335349747947475, l2: 0.00036249854065317777   Iteration 33 of 100, tot loss = 4.674875151027333, l1: 0.0001036165335928231, l2: 0.0003638709829167717   Iteration 34 of 100, tot loss = 4.6812626544167015, l1: 0.00010427430242998526, l2: 0.00036385196461410755   Iteration 35 of 100, tot loss = 4.774166754313877, l1: 0.00010640171281660774, l2: 0.0003710149641847238   Iteration 36 of 100, tot loss = 4.738837805059221, l1: 0.00010581107886132991, l2: 0.0003680727034710192   Iteration 37 of 100, tot loss = 4.725262377713178, l1: 0.00010593232464334751, l2: 0.00036659391441168517   Iteration 38 of 100, tot loss = 4.799860722140262, l1: 0.00010678721293133046, l2: 0.0003731988594824399   Iteration 39 of 100, tot loss = 4.753266102228409, l1: 0.00010570003919733259, l2: 0.00036962657162430097   Iteration 40 of 100, tot loss = 4.803107810020447, l1: 0.00010651117045199499, l2: 0.0003737996110430686   Iteration 41 of 100, tot loss = 4.7936809237410385, l1: 0.00010675389004485092, l2: 0.00037261420291587253   Iteration 42 of 100, tot loss = 4.776927789052327, l1: 0.0001063445351541131, l2: 0.00037134824482823854   Iteration 43 of 100, tot loss = 4.781985227451768, l1: 0.00010671633022274215, l2: 0.0003714821938300765   Iteration 44 of 100, tot loss = 4.777179652994329, l1: 0.00010677880064494357, l2: 0.00037093916530879636   Iteration 45 of 100, tot loss = 4.8235986179775665, l1: 0.00010720309203154304, l2: 0.0003751567703956324   Iteration 46 of 100, tot loss = 4.813271512155947, l1: 0.00010751703777477023, l2: 0.0003738101143465863   Iteration 47 of 100, tot loss = 4.771406341106333, l1: 0.00010684914989124469, l2: 0.0003702914846337777   Iteration 48 of 100, tot loss = 4.750404929121335, l1: 0.00010664138820478304, l2: 0.0003683991053549107   Iteration 49 of 100, tot loss = 4.789620112399666, l1: 0.00010741021773986974, l2: 0.0003715517944941411   Iteration 50 of 100, tot loss = 4.806948161125183, l1: 0.00010801084892591461, l2: 0.0003726839687442407   Iteration 51 of 100, tot loss = 4.751759145774093, l1: 0.0001065547349914114, l2: 0.0003686211812058829   Iteration 52 of 100, tot loss = 4.726462868543772, l1: 0.00010610171858178756, l2: 0.00036654457009997434   Iteration 53 of 100, tot loss = 4.741218602882241, l1: 0.00010611265322538356, l2: 0.00036800920909472724   Iteration 54 of 100, tot loss = 4.7197752087204545, l1: 0.00010576780207177055, l2: 0.00036620972079819895   Iteration 55 of 100, tot loss = 4.74560526934537, l1: 0.00010532395357255486, l2: 0.00036923657515382566   Iteration 56 of 100, tot loss = 4.774315178394318, l1: 0.00010568493202559434, l2: 0.00037174658817613296   Iteration 57 of 100, tot loss = 4.736144425576193, l1: 0.00010518748687198305, l2: 0.0003684269582950755   Iteration 58 of 100, tot loss = 4.724055388878131, l1: 0.00010485174406594837, l2: 0.00036755379726949696   Iteration 59 of 100, tot loss = 4.743541515479653, l1: 0.00010510131915768813, l2: 0.00036925283434279893   Iteration 60 of 100, tot loss = 4.765520707766215, l1: 0.00010556376601016382, l2: 0.00037098830701628077   Iteration 61 of 100, tot loss = 4.74844690229072, l1: 0.00010535823254985354, l2: 0.0003694864595881434   Iteration 62 of 100, tot loss = 4.779060463751516, l1: 0.00010564495137406348, l2: 0.00037226109669709036   Iteration 63 of 100, tot loss = 4.7711770799424915, l1: 0.0001055875999020553, l2: 0.00037153011016059843   Iteration 64 of 100, tot loss = 4.781361915171146, l1: 0.00010595807810886981, l2: 0.00037217811495793285   Iteration 65 of 100, tot loss = 4.743104186424842, l1: 0.0001054710360991661, l2: 0.00036883938412826797   Iteration 66 of 100, tot loss = 4.749230493198741, l1: 0.00010585251975774581, l2: 0.00036907053083378935   Iteration 67 of 100, tot loss = 4.739633005056808, l1: 0.00010601115295074449, l2: 0.00036795214843699957   Iteration 68 of 100, tot loss = 4.71060656449374, l1: 0.00010521867914255435, l2: 0.0003658419783973573   Iteration 69 of 100, tot loss = 4.694120918495067, l1: 0.00010463303582240032, l2: 0.00036477905730176985   Iteration 70 of 100, tot loss = 4.7148617676326205, l1: 0.00010520372753879721, l2: 0.00036628245060066025   Iteration 71 of 100, tot loss = 4.692154017972275, l1: 0.00010471373511983318, l2: 0.0003645016678976236   Iteration 72 of 100, tot loss = 4.6604445113076105, l1: 0.00010394262345572416, l2: 0.00036210182871501375   Iteration 73 of 100, tot loss = 4.659605503082275, l1: 0.00010419712814164335, l2: 0.00036176342289091074   Iteration 74 of 100, tot loss = 4.634130107389914, l1: 0.00010368361285686292, l2: 0.00035972939863190606   Iteration 75 of 100, tot loss = 4.621062758763631, l1: 0.00010355575155699625, l2: 0.00035855052507637687   Iteration 76 of 100, tot loss = 4.583816672626295, l1: 0.00010270474563424757, l2: 0.00035567692248150706   Iteration 77 of 100, tot loss = 4.6040666382034106, l1: 0.00010322723226801662, l2: 0.0003571794326749763   Iteration 78 of 100, tot loss = 4.602008244930169, l1: 0.00010310626147167149, l2: 0.00035709456409303803   Iteration 79 of 100, tot loss = 4.5871495415892785, l1: 0.00010298324430757791, l2: 0.0003557317109949462   Iteration 80 of 100, tot loss = 4.590916776657105, l1: 0.00010339300265513885, l2: 0.00035569867650337984   Iteration 81 of 100, tot loss = 4.558836242299021, l1: 0.00010266529500734544, l2: 0.00035321833077174276   Iteration 82 of 100, tot loss = 4.585786226319104, l1: 0.0001030422188306125, l2: 0.0003555364049289648   Iteration 83 of 100, tot loss = 4.6164266919515216, l1: 0.00010336735308610734, l2: 0.00035827531737376406   Iteration 84 of 100, tot loss = 4.624797270411537, l1: 0.00010326218588965102, l2: 0.00035921754239271174   Iteration 85 of 100, tot loss = 4.59707072762882, l1: 0.0001027310495641466, l2: 0.0003569760245120372   Iteration 86 of 100, tot loss = 4.573069181553153, l1: 0.00010248890908388963, l2: 0.0003548180104062763   Iteration 87 of 100, tot loss = 4.552993245508479, l1: 0.00010221794017807355, l2: 0.0003530813858445584   Iteration 88 of 100, tot loss = 4.548318123275584, l1: 0.00010190353482771157, l2: 0.00035292827884735414   Iteration 89 of 100, tot loss = 4.5177934290318005, l1: 0.00010118882456798305, l2: 0.0003505905196096832   Iteration 90 of 100, tot loss = 4.524930820200178, l1: 0.00010138993420696351, l2: 0.00035110314881118637   Iteration 91 of 100, tot loss = 4.537206218792842, l1: 0.00010135044099265392, l2: 0.0003523701820445789   Iteration 92 of 100, tot loss = 4.535239119892535, l1: 0.0001012533465378194, l2: 0.00035227056650910527   Iteration 93 of 100, tot loss = 4.522287426456328, l1: 0.0001009837701971147, l2: 0.0003512449736546685   Iteration 94 of 100, tot loss = 4.5370444452508965, l1: 0.00010128235926652239, l2: 0.00035242208631392174   Iteration 95 of 100, tot loss = 4.540265450979534, l1: 0.00010124473954324218, l2: 0.0003527818066925791   Iteration 96 of 100, tot loss = 4.519178093721469, l1: 0.00010102834581478722, l2: 0.00035088946469841176   Iteration 97 of 100, tot loss = 4.510173145028734, l1: 0.00010104356418374551, l2: 0.0003499737513172392   Iteration 98 of 100, tot loss = 4.501432008889257, l1: 0.00010111262299131117, l2: 0.0003490305788712861   Iteration 99 of 100, tot loss = 4.504182525355406, l1: 0.0001014379080460816, l2: 0.00034898034525644787   Iteration 100 of 100, tot loss = 4.484988988637924, l1: 0.00010104625394887989, l2: 0.00034745264551020225
   End of epoch 1221; saving model... 

Epoch 1222 of 2000
   Iteration 1 of 100, tot loss = 3.46431565284729, l1: 5.5186061217682436e-05, l2: 0.0002912454947363585   Iteration 2 of 100, tot loss = 5.879068493843079, l1: 9.689044418337289e-05, l2: 0.0004910163843305781   Iteration 3 of 100, tot loss = 5.938338041305542, l1: 0.00011064995608952206, l2: 0.00048318384991337854   Iteration 4 of 100, tot loss = 6.327619850635529, l1: 0.00011582713432289893, l2: 0.000516934844199568   Iteration 5 of 100, tot loss = 5.801973342895508, l1: 0.0001150258227426093, l2: 0.00046517150476574896   Iteration 6 of 100, tot loss = 5.291275143623352, l1: 0.00010658758219506126, l2: 0.0004225399291802508   Iteration 7 of 100, tot loss = 4.851200887135097, l1: 0.0001000728620316035, l2: 0.00038504722345221253   Iteration 8 of 100, tot loss = 4.767371624708176, l1: 0.00010027682947111316, l2: 0.0003764603279705625   Iteration 9 of 100, tot loss = 4.680237293243408, l1: 9.769774947522415e-05, l2: 0.00037032597659466165   Iteration 10 of 100, tot loss = 4.73966064453125, l1: 9.947785511030816e-05, l2: 0.0003744882094906643   Iteration 11 of 100, tot loss = 4.663848660208962, l1: 9.744521627329628e-05, l2: 0.0003689396491443569   Iteration 12 of 100, tot loss = 4.414527714252472, l1: 9.21641664414589e-05, l2: 0.0003492886050177428   Iteration 13 of 100, tot loss = 4.338926443686852, l1: 8.872565152929522e-05, l2: 0.0003451669942408514   Iteration 14 of 100, tot loss = 4.420674920082092, l1: 8.842954360131574e-05, l2: 0.00035363794969660897   Iteration 15 of 100, tot loss = 4.386643282572428, l1: 8.897968703725685e-05, l2: 0.0003496846416965127   Iteration 16 of 100, tot loss = 4.496004104614258, l1: 9.162254718830809e-05, l2: 0.0003579778622224694   Iteration 17 of 100, tot loss = 4.639295662150664, l1: 9.353534722536365e-05, l2: 0.00037039421765845925   Iteration 18 of 100, tot loss = 4.728171904881795, l1: 9.558515820471157e-05, l2: 0.00037723202937437844   Iteration 19 of 100, tot loss = 4.641043135994359, l1: 9.490491012050035e-05, l2: 0.0003691994007615569   Iteration 20 of 100, tot loss = 4.5613584280014035, l1: 9.381875497638248e-05, l2: 0.0003623170850914903   Iteration 21 of 100, tot loss = 4.503073067892165, l1: 9.346608463342168e-05, l2: 0.000356841219833032   Iteration 22 of 100, tot loss = 4.474760012193159, l1: 9.314535881540823e-05, l2: 0.00035433064013804227   Iteration 23 of 100, tot loss = 4.4615484942560615, l1: 9.318387310486287e-05, l2: 0.0003529709735986493   Iteration 24 of 100, tot loss = 4.498755991458893, l1: 9.319549978196544e-05, l2: 0.0003566800975628818   Iteration 25 of 100, tot loss = 4.545827312469482, l1: 9.413329942617566e-05, l2: 0.0003604494302999228   Iteration 26 of 100, tot loss = 4.461804600862356, l1: 9.337689978285477e-05, l2: 0.0003528035589484856   Iteration 27 of 100, tot loss = 4.405502495942293, l1: 9.371067803672136e-05, l2: 0.0003468395707723512   Iteration 28 of 100, tot loss = 4.308661886623928, l1: 9.161073791931682e-05, l2: 0.0003392554499441758   Iteration 29 of 100, tot loss = 4.300612252334068, l1: 9.232039344075522e-05, l2: 0.00033774083090849734   Iteration 30 of 100, tot loss = 4.294198258717855, l1: 9.184036741013793e-05, l2: 0.00033757945833106837   Iteration 31 of 100, tot loss = 4.265285961089596, l1: 9.077487500748717e-05, l2: 0.00033575372071936727   Iteration 32 of 100, tot loss = 4.278209067881107, l1: 9.146880404387048e-05, l2: 0.0003363521027495153   Iteration 33 of 100, tot loss = 4.416637370080659, l1: 9.316618558262813e-05, l2: 0.0003484975525432012   Iteration 34 of 100, tot loss = 4.419413629700156, l1: 9.317371276505154e-05, l2: 0.00034876765087520816   Iteration 35 of 100, tot loss = 4.44701075553894, l1: 9.372472403421333e-05, l2: 0.0003509763520144458   Iteration 36 of 100, tot loss = 4.489075084527333, l1: 9.462718975959838e-05, l2: 0.000354280319394699   Iteration 37 of 100, tot loss = 4.426835788262857, l1: 9.376508515325022e-05, l2: 0.00034891849429334034   Iteration 38 of 100, tot loss = 4.412932075952229, l1: 9.363334548221534e-05, l2: 0.00034765986301384767   Iteration 39 of 100, tot loss = 4.428626347810794, l1: 9.352685531037144e-05, l2: 0.0003493357795284679   Iteration 40 of 100, tot loss = 4.428817123174667, l1: 9.354146432087873e-05, l2: 0.00034934024843096266   Iteration 41 of 100, tot loss = 4.523882860090675, l1: 9.437750078377132e-05, l2: 0.00035801078526518965   Iteration 42 of 100, tot loss = 4.528283794720967, l1: 9.463966030341995e-05, l2: 0.00035818871921427283   Iteration 43 of 100, tot loss = 4.489105108172395, l1: 9.396916814080742e-05, l2: 0.0003549413427948778   Iteration 44 of 100, tot loss = 4.494928083636544, l1: 9.379073567701694e-05, l2: 0.0003557020728211765   Iteration 45 of 100, tot loss = 4.504110913806492, l1: 9.420566356210556e-05, l2: 0.00035620542864004773   Iteration 46 of 100, tot loss = 4.538393678872482, l1: 9.51471709255281e-05, l2: 0.00035869219699753046   Iteration 47 of 100, tot loss = 4.526014571494245, l1: 9.473938855003229e-05, l2: 0.00035786206841448997   Iteration 48 of 100, tot loss = 4.526334236065547, l1: 9.50800911141414e-05, l2: 0.00035755333252988447   Iteration 49 of 100, tot loss = 4.571506889498964, l1: 9.609359930975039e-05, l2: 0.0003610570886533479   Iteration 50 of 100, tot loss = 4.596207342147827, l1: 9.67750253767008e-05, l2: 0.0003628457087324932   Iteration 51 of 100, tot loss = 4.635461601556516, l1: 9.734639089027256e-05, l2: 0.00036619976904772806   Iteration 52 of 100, tot loss = 4.651428809532752, l1: 9.681732871495814e-05, l2: 0.00036832555241954443   Iteration 53 of 100, tot loss = 4.663972926589678, l1: 9.762022235454938e-05, l2: 0.0003687770700055825   Iteration 54 of 100, tot loss = 4.631271026752613, l1: 9.662187154322897e-05, l2: 0.00036650523112189993   Iteration 55 of 100, tot loss = 4.694099460948597, l1: 9.778980946231802e-05, l2: 0.00037162013732912867   Iteration 56 of 100, tot loss = 4.69898304768971, l1: 9.759653468271219e-05, l2: 0.0003723017710269362   Iteration 57 of 100, tot loss = 4.67838470141093, l1: 9.741505077966537e-05, l2: 0.00037042342030377896   Iteration 58 of 100, tot loss = 4.684789151980959, l1: 9.741935355123132e-05, l2: 0.0003710595623927255   Iteration 59 of 100, tot loss = 4.643538717496193, l1: 9.675870745213924e-05, l2: 0.00036759516500504844   Iteration 60 of 100, tot loss = 4.631800591945648, l1: 9.668205469400466e-05, l2: 0.00036649800507196535   Iteration 61 of 100, tot loss = 4.633431837206981, l1: 9.683785174274817e-05, l2: 0.00036650533228730936   Iteration 62 of 100, tot loss = 4.656909515780788, l1: 9.72673568016504e-05, l2: 0.00036842359517205264   Iteration 63 of 100, tot loss = 4.669326452981858, l1: 9.767610303935639e-05, l2: 0.00036925654202359657   Iteration 64 of 100, tot loss = 4.6449785605072975, l1: 9.761963713117439e-05, l2: 0.00036687821852865454   Iteration 65 of 100, tot loss = 4.616138667326707, l1: 9.74458812449414e-05, l2: 0.0003641679851660648   Iteration 66 of 100, tot loss = 4.6463678497256655, l1: 9.824146334060721e-05, l2: 0.0003663953212901216   Iteration 67 of 100, tot loss = 4.6564419411901214, l1: 9.831867440092836e-05, l2: 0.00036732551994483306   Iteration 68 of 100, tot loss = 4.667935332831214, l1: 9.886979162675904e-05, l2: 0.00036792374200612194   Iteration 69 of 100, tot loss = 4.641573080118151, l1: 9.85521922078645e-05, l2: 0.00036560511617226177   Iteration 70 of 100, tot loss = 4.646177915164403, l1: 9.854072694517006e-05, l2: 0.0003660770650770116   Iteration 71 of 100, tot loss = 4.633968235741199, l1: 9.818886528463877e-05, l2: 0.00036520795863036486   Iteration 72 of 100, tot loss = 4.641538788874944, l1: 9.816556727552476e-05, l2: 0.0003659883123974497   Iteration 73 of 100, tot loss = 4.624951656550577, l1: 9.774229603290099e-05, l2: 0.00036475287008016333   Iteration 74 of 100, tot loss = 4.612952103485933, l1: 9.792250548835491e-05, l2: 0.0003633727054064464   Iteration 75 of 100, tot loss = 4.60916805267334, l1: 9.774220311859002e-05, l2: 0.00036317460297141224   Iteration 76 of 100, tot loss = 4.575638356961702, l1: 9.699924425819375e-05, l2: 0.000360564592196361   Iteration 77 of 100, tot loss = 4.599634139568774, l1: 9.733005780227748e-05, l2: 0.00036263335707851434   Iteration 78 of 100, tot loss = 4.592950111780411, l1: 9.717971429031772e-05, l2: 0.0003621152979739679   Iteration 79 of 100, tot loss = 4.595892236202578, l1: 9.728364054779698e-05, l2: 0.0003623055840129198   Iteration 80 of 100, tot loss = 4.641926652193069, l1: 9.802769654925214e-05, l2: 0.000366164969818783   Iteration 81 of 100, tot loss = 4.644305482322787, l1: 9.841125197622373e-05, l2: 0.0003660192973997996   Iteration 82 of 100, tot loss = 4.651139247708205, l1: 9.861268628766283e-05, l2: 0.0003665012393356869   Iteration 83 of 100, tot loss = 4.659259198659874, l1: 9.90174511056225e-05, l2: 0.00036690846970585363   Iteration 84 of 100, tot loss = 4.652934835070655, l1: 9.857542370030257e-05, l2: 0.00036671806100500924   Iteration 85 of 100, tot loss = 4.6340116444756, l1: 9.828899740515386e-05, l2: 0.0003651121680912397   Iteration 86 of 100, tot loss = 4.618033198423164, l1: 9.809673539102943e-05, l2: 0.0003637065855464032   Iteration 87 of 100, tot loss = 4.624777147139626, l1: 9.816378353688137e-05, l2: 0.000364313931933647   Iteration 88 of 100, tot loss = 4.613533212379976, l1: 9.823765164831208e-05, l2: 0.00036311567022163547   Iteration 89 of 100, tot loss = 4.600592431057705, l1: 9.79473892523478e-05, l2: 0.00036211185444413234   Iteration 90 of 100, tot loss = 4.595407798555162, l1: 9.811671803213862e-05, l2: 0.0003614240621876282   Iteration 91 of 100, tot loss = 4.581066956886878, l1: 9.786663097227095e-05, l2: 0.0003602400648850272   Iteration 92 of 100, tot loss = 4.60032403209935, l1: 9.830182313200329e-05, l2: 0.00036173058011310945   Iteration 93 of 100, tot loss = 4.591425521399385, l1: 9.840363031065451e-05, l2: 0.00036073892182665525   Iteration 94 of 100, tot loss = 4.579144931854086, l1: 9.822430063727648e-05, l2: 0.000359690192550609   Iteration 95 of 100, tot loss = 4.5894377733531755, l1: 9.811863909769607e-05, l2: 0.0003608251384177588   Iteration 96 of 100, tot loss = 4.5641514013210935, l1: 9.779603252961049e-05, l2: 0.00035861910782841733   Iteration 97 of 100, tot loss = 4.567781192740214, l1: 9.775741494467155e-05, l2: 0.00035902070446832817   Iteration 98 of 100, tot loss = 4.560886903684967, l1: 9.757966509656695e-05, l2: 0.00035850902542956554   Iteration 99 of 100, tot loss = 4.587514458280621, l1: 9.7901363474331e-05, l2: 0.0003608500821906321   Iteration 100 of 100, tot loss = 4.568390989303589, l1: 9.767654315510298e-05, l2: 0.00035916255568736234
   End of epoch 1222; saving model... 

Epoch 1223 of 2000
   Iteration 1 of 100, tot loss = 4.03366756439209, l1: 9.164957009488717e-05, l2: 0.0003117172163911164   Iteration 2 of 100, tot loss = 4.3468852043151855, l1: 9.956277790479362e-05, l2: 0.00033512576192151755   Iteration 3 of 100, tot loss = 4.711833159128825, l1: 0.00010028478815608348, l2: 0.000370898536251237   Iteration 4 of 100, tot loss = 4.630221009254456, l1: 0.00010284674317517783, l2: 0.0003601753560360521   Iteration 5 of 100, tot loss = 4.408556890487671, l1: 0.00010014828585553914, l2: 0.0003407074022106826   Iteration 6 of 100, tot loss = 4.460832079251607, l1: 0.00010235834755197477, l2: 0.00034372486212911707   Iteration 7 of 100, tot loss = 4.35741012437003, l1: 0.0001035467950194808, l2: 0.0003321942183122571   Iteration 8 of 100, tot loss = 4.491304725408554, l1: 0.00010664688124961685, l2: 0.0003424835922487546   Iteration 9 of 100, tot loss = 4.81219024128384, l1: 0.00011365017821339684, l2: 0.00036756884785265557   Iteration 10 of 100, tot loss = 4.835205435752869, l1: 0.00011283556668786331, l2: 0.00037068497913423927   Iteration 11 of 100, tot loss = 5.146983645179055, l1: 0.00011761051295748489, l2: 0.00039708785532804376   Iteration 12 of 100, tot loss = 5.4185066024462385, l1: 0.00011879178418894298, l2: 0.00042305888443176326   Iteration 13 of 100, tot loss = 5.3107126492720385, l1: 0.00011742035992658482, l2: 0.00041365091321774974   Iteration 14 of 100, tot loss = 5.316793765340533, l1: 0.00011912141157413966, l2: 0.00041255797467394065   Iteration 15 of 100, tot loss = 5.206725851694743, l1: 0.00011730195207443709, l2: 0.0004033706422584752   Iteration 16 of 100, tot loss = 5.176957339048386, l1: 0.00011639372178251506, l2: 0.0004013020206912188   Iteration 17 of 100, tot loss = 5.142475633060231, l1: 0.00011557902283076306, l2: 0.0003986685478594154   Iteration 18 of 100, tot loss = 4.98727888531155, l1: 0.0001123846996051725, l2: 0.00038634319611850917   Iteration 19 of 100, tot loss = 4.903314853969373, l1: 0.00010961452694334041, l2: 0.0003807169659443102   Iteration 20 of 100, tot loss = 4.81212946176529, l1: 0.0001083245793779497, l2: 0.00037288837338564915   Iteration 21 of 100, tot loss = 4.801952804837908, l1: 0.00010879925286558101, l2: 0.000371396034357271   Iteration 22 of 100, tot loss = 4.795484878800132, l1: 0.00010885864338144364, l2: 0.0003706898503217169   Iteration 23 of 100, tot loss = 4.771071485851122, l1: 0.00010909922575988078, l2: 0.0003680079294692563   Iteration 24 of 100, tot loss = 4.702678610881169, l1: 0.00010811506293369651, l2: 0.0003621528054888283   Iteration 25 of 100, tot loss = 4.664536485671997, l1: 0.00010681934611056932, l2: 0.0003596343094250187   Iteration 26 of 100, tot loss = 4.6045919840152445, l1: 0.00010639087919956252, l2: 0.0003540683263474001   Iteration 27 of 100, tot loss = 4.685360934999254, l1: 0.00010769399860119274, l2: 0.00036084210161654347   Iteration 28 of 100, tot loss = 4.69331956761224, l1: 0.00010702525326321068, l2: 0.00036230671009564375   Iteration 29 of 100, tot loss = 4.6299343849050585, l1: 0.00010593372820133889, l2: 0.0003570597167207121   Iteration 30 of 100, tot loss = 4.682402841250101, l1: 0.00010507844017411116, l2: 0.00036316185044900823   Iteration 31 of 100, tot loss = 4.6321576179996615, l1: 0.00010361959791607074, l2: 0.00035959617009446505   Iteration 32 of 100, tot loss = 4.61534571647644, l1: 0.00010339797904634906, l2: 0.00035813659906125395   Iteration 33 of 100, tot loss = 4.5986645438454365, l1: 0.00010307690227460681, l2: 0.00035678955839213097   Iteration 34 of 100, tot loss = 4.5938277384814095, l1: 0.00010365045925616013, l2: 0.00035573232076678644   Iteration 35 of 100, tot loss = 4.626229422433036, l1: 0.0001039854550201978, l2: 0.00035863749300395805   Iteration 36 of 100, tot loss = 4.628127257029216, l1: 0.00010404208155200144, l2: 0.000358770649957781   Iteration 37 of 100, tot loss = 4.61207239047901, l1: 0.00010419228803240569, l2: 0.00035701495692813516   Iteration 38 of 100, tot loss = 4.615411708229466, l1: 0.00010420028794262754, l2: 0.00035734088880034457   Iteration 39 of 100, tot loss = 4.616025350032708, l1: 0.00010424749519025239, l2: 0.000357355045614382   Iteration 40 of 100, tot loss = 4.670407354831696, l1: 0.00010507940951356431, l2: 0.00036196133223711515   Iteration 41 of 100, tot loss = 4.673933540902486, l1: 0.00010558592511210363, l2: 0.00036180743496160863   Iteration 42 of 100, tot loss = 4.691396406718662, l1: 0.00010614030578012934, l2: 0.0003629993415892213   Iteration 43 of 100, tot loss = 4.637684267620708, l1: 0.00010550092972766305, l2: 0.00035826750383928936   Iteration 44 of 100, tot loss = 4.646544228900563, l1: 0.00010561979979684111, l2: 0.00035903462941694835   Iteration 45 of 100, tot loss = 4.660050307379828, l1: 0.0001060985369905312, l2: 0.0003599064993775553   Iteration 46 of 100, tot loss = 4.668734913286955, l1: 0.0001059487256735219, l2: 0.00036092477081262547   Iteration 47 of 100, tot loss = 4.676701271787603, l1: 0.00010586271983288981, l2: 0.00036180741153657436   Iteration 48 of 100, tot loss = 4.69251944621404, l1: 0.00010599959144504585, l2: 0.00036325235790476046   Iteration 49 of 100, tot loss = 4.689646458139225, l1: 0.00010605191182920102, l2: 0.00036291273878602195   Iteration 50 of 100, tot loss = 4.712427043914795, l1: 0.0001062671076215338, l2: 0.00036497560155112295   Iteration 51 of 100, tot loss = 4.711769636939554, l1: 0.00010618792807423127, l2: 0.00036498904057011446   Iteration 52 of 100, tot loss = 4.67739534836549, l1: 0.00010562363548580414, l2: 0.00036211590430376906   Iteration 53 of 100, tot loss = 4.661022636125672, l1: 0.00010533873906099888, l2: 0.000360763529623103   Iteration 54 of 100, tot loss = 4.657049744217484, l1: 0.00010566359546997032, l2: 0.00036004138409798206   Iteration 55 of 100, tot loss = 4.628452101620761, l1: 0.00010532344246960499, l2: 0.0003575217730196362   Iteration 56 of 100, tot loss = 4.651218414306641, l1: 0.0001058338532727378, l2: 0.00035928799323820774   Iteration 57 of 100, tot loss = 4.618033166517291, l1: 0.00010509669740904907, l2: 0.0003567066241725626   Iteration 58 of 100, tot loss = 4.593637265008072, l1: 0.00010459598292825455, l2: 0.00035476774842365934   Iteration 59 of 100, tot loss = 4.648920378442538, l1: 0.00010494309760160523, l2: 0.0003599489450067976   Iteration 60 of 100, tot loss = 4.6466508666674295, l1: 0.00010507634991275457, l2: 0.00035958874116962154   Iteration 61 of 100, tot loss = 4.621861059157575, l1: 0.00010455006015006087, l2: 0.0003576360501684859   Iteration 62 of 100, tot loss = 4.6470844976363646, l1: 0.00010516885254952697, l2: 0.00035953960116697295   Iteration 63 of 100, tot loss = 4.76335278768388, l1: 0.00010684552802962976, l2: 0.0003694897542508053   Iteration 64 of 100, tot loss = 4.730378694832325, l1: 0.00010611823358885886, l2: 0.00036691963919111004   Iteration 65 of 100, tot loss = 4.71428291614239, l1: 0.00010605157854465338, l2: 0.0003653767167214448   Iteration 66 of 100, tot loss = 4.689723459157077, l1: 0.00010570157290644464, l2: 0.0003632707768260981   Iteration 67 of 100, tot loss = 4.689475653776482, l1: 0.00010566027624768886, l2: 0.00036328729309850554   Iteration 68 of 100, tot loss = 4.676645629546222, l1: 0.00010540997080919195, l2: 0.00036225459617151716   Iteration 69 of 100, tot loss = 4.72499728548354, l1: 0.00010585772564071163, l2: 0.0003666420074085287   Iteration 70 of 100, tot loss = 4.6906775406428745, l1: 0.00010499805399116927, l2: 0.0003640697046648711   Iteration 71 of 100, tot loss = 4.665727793330878, l1: 0.0001047363097518748, l2: 0.0003618364739658075   Iteration 72 of 100, tot loss = 4.653843330012427, l1: 0.00010468835908896936, l2: 0.0003606959782902979   Iteration 73 of 100, tot loss = 4.70740624649884, l1: 0.00010557915133899333, l2: 0.0003651614772946867   Iteration 74 of 100, tot loss = 4.717731430723861, l1: 0.00010554600983694497, l2: 0.0003662271377145992   Iteration 75 of 100, tot loss = 4.6928692722320555, l1: 0.00010478073255702232, l2: 0.00036450619886939725   Iteration 76 of 100, tot loss = 4.6628638035372685, l1: 0.00010416353870823514, l2: 0.0003621228457440769   Iteration 77 of 100, tot loss = 4.679662794261784, l1: 0.00010424700530484778, l2: 0.0003637192780666005   Iteration 78 of 100, tot loss = 4.661649477787507, l1: 0.00010385587376778205, l2: 0.0003623090779934174   Iteration 79 of 100, tot loss = 4.640399178372154, l1: 0.00010351506509461523, l2: 0.00036052485656794866   Iteration 80 of 100, tot loss = 4.65522153377533, l1: 0.00010357925420976244, l2: 0.00036194290296407414   Iteration 81 of 100, tot loss = 4.69210778342353, l1: 0.00010390135651471576, l2: 0.0003653094251525163   Iteration 82 of 100, tot loss = 4.686229694180373, l1: 0.00010375126311839444, l2: 0.00036487170912484387   Iteration 83 of 100, tot loss = 4.712443840072815, l1: 0.00010406474827603628, l2: 0.00036717963850978446   Iteration 84 of 100, tot loss = 4.738402729942685, l1: 0.00010470283952946331, l2: 0.00036913743644531464   Iteration 85 of 100, tot loss = 4.738150899550494, l1: 0.00010466826583176632, l2: 0.00036914682714268566   Iteration 86 of 100, tot loss = 4.7387671747872995, l1: 0.00010471069693922736, l2: 0.00036916602335242165   Iteration 87 of 100, tot loss = 4.741760407371083, l1: 0.00010488553681202105, l2: 0.0003692905067990737   Iteration 88 of 100, tot loss = 4.747336002913388, l1: 0.00010501017550268443, l2: 0.0003697234278661199   Iteration 89 of 100, tot loss = 4.765644705697392, l1: 0.0001053247850792472, l2: 0.00037123968829976374   Iteration 90 of 100, tot loss = 4.791794215308295, l1: 0.00010575967777792054, l2: 0.00037341974675655367   Iteration 91 of 100, tot loss = 4.77703488790072, l1: 0.0001057659560151828, l2: 0.0003719375357492366   Iteration 92 of 100, tot loss = 4.799657533998075, l1: 0.00010636576047071013, l2: 0.00037359999576279813   Iteration 93 of 100, tot loss = 4.802866917784496, l1: 0.00010650886448978957, l2: 0.0003737778301470943   Iteration 94 of 100, tot loss = 4.80862613688124, l1: 0.00010666309188967174, l2: 0.0003741995246952122   Iteration 95 of 100, tot loss = 4.7922623659435075, l1: 0.00010645360453054309, l2: 0.00037277263503423646   Iteration 96 of 100, tot loss = 4.818120253582795, l1: 0.00010679120092997134, l2: 0.0003750208270503208   Iteration 97 of 100, tot loss = 4.798081771614625, l1: 0.00010634050824554618, l2: 0.0003734676714340878   Iteration 98 of 100, tot loss = 4.7803606865357375, l1: 0.00010596586605747781, l2: 0.0003720702049892624   Iteration 99 of 100, tot loss = 4.790415563968697, l1: 0.00010598327457726077, l2: 0.00037305828417895945   Iteration 100 of 100, tot loss = 4.788782594203949, l1: 0.00010589424513455015, l2: 0.00037298401672160254
   End of epoch 1223; saving model... 

Epoch 1224 of 2000
   Iteration 1 of 100, tot loss = 3.797612190246582, l1: 0.00010191337787546217, l2: 0.00027784783742390573   Iteration 2 of 100, tot loss = 4.438444137573242, l1: 0.00011668869410641491, l2: 0.00032715573615860194   Iteration 3 of 100, tot loss = 5.2939527829488116, l1: 0.00012112948267410199, l2: 0.0004082658075882743   Iteration 4 of 100, tot loss = 4.668489098548889, l1: 0.00011376894144632388, l2: 0.0003530799804138951   Iteration 5 of 100, tot loss = 4.521551656723022, l1: 0.0001084895950043574, l2: 0.00034366557956673206   Iteration 6 of 100, tot loss = 4.4885135889053345, l1: 0.00010412348274257965, l2: 0.0003447278819900627   Iteration 7 of 100, tot loss = 5.312143427985055, l1: 0.00011492123199527018, l2: 0.00041629311246132214   Iteration 8 of 100, tot loss = 4.954803436994553, l1: 0.00010946098791464465, l2: 0.00038601935739279725   Iteration 9 of 100, tot loss = 4.874060763253106, l1: 0.00011042556111028211, l2: 0.0003769805156884508   Iteration 10 of 100, tot loss = 5.171499037742615, l1: 0.00011554545781109483, l2: 0.0004016044462332502   Iteration 11 of 100, tot loss = 5.154036110097712, l1: 0.00011674252164084464, l2: 0.00039866108934141016   Iteration 12 of 100, tot loss = 5.328172584374745, l1: 0.00011919399427521664, l2: 0.00041362326495194185   Iteration 13 of 100, tot loss = 5.297428186123188, l1: 0.00011824339890154079, l2: 0.0004114994237450166   Iteration 14 of 100, tot loss = 5.079793742724827, l1: 0.00011330442482306222, l2: 0.0003946749524662404   Iteration 15 of 100, tot loss = 5.114936113357544, l1: 0.00011379489442333579, l2: 0.000397698719946978   Iteration 16 of 100, tot loss = 4.949168220162392, l1: 0.00011063928695875802, l2: 0.0003842775367957074   Iteration 17 of 100, tot loss = 4.988506583606496, l1: 0.00011258775339229032, l2: 0.0003862629083040006   Iteration 18 of 100, tot loss = 4.866285694970025, l1: 0.00011058380267867405, l2: 0.0003760447701400456   Iteration 19 of 100, tot loss = 4.967720483478747, l1: 0.0001131656876838717, l2: 0.0003836063620646631   Iteration 20 of 100, tot loss = 5.009252738952637, l1: 0.00011354706439306028, l2: 0.0003873782094160561   Iteration 21 of 100, tot loss = 4.9514517557053335, l1: 0.0001107912193263127, l2: 0.0003843539556588179   Iteration 22 of 100, tot loss = 4.975372314453125, l1: 0.00011140588057142767, l2: 0.00038613134992457077   Iteration 23 of 100, tot loss = 4.987918895223866, l1: 0.00011172311402017088, l2: 0.00038706877397919965   Iteration 24 of 100, tot loss = 4.963381667931874, l1: 0.00011045391420339001, l2: 0.0003858842507421893   Iteration 25 of 100, tot loss = 4.939647750854492, l1: 0.0001102356193587184, l2: 0.00038372915296349677   Iteration 26 of 100, tot loss = 5.074932428506704, l1: 0.00011217451649897087, l2: 0.00039531872439860867   Iteration 27 of 100, tot loss = 4.981539152286671, l1: 0.000111436826517564, l2: 0.00038671708629138904   Iteration 28 of 100, tot loss = 4.9735197595187595, l1: 0.00011153077632895605, l2: 0.00038582119772659747   Iteration 29 of 100, tot loss = 5.000775772949745, l1: 0.00011124246872174714, l2: 0.00038883510770292245   Iteration 30 of 100, tot loss = 4.930364751815796, l1: 0.00011025280400644988, l2: 0.0003827836708903002   Iteration 31 of 100, tot loss = 4.993890039382443, l1: 0.00011098796441670387, l2: 0.00038840103802466466   Iteration 32 of 100, tot loss = 4.9792011976242065, l1: 0.00011020719557564007, l2: 0.00038771292247474776   Iteration 33 of 100, tot loss = 4.925493189782808, l1: 0.00010947776238688014, l2: 0.00038307155508929014   Iteration 34 of 100, tot loss = 4.902410836780772, l1: 0.00010872573051635888, l2: 0.0003815153523807085   Iteration 35 of 100, tot loss = 4.866944265365601, l1: 0.00010793605428521654, l2: 0.00037875837172448104   Iteration 36 of 100, tot loss = 4.861569596661462, l1: 0.00010763627263562132, l2: 0.00037852068756668206   Iteration 37 of 100, tot loss = 4.89564754511859, l1: 0.00010864753346902796, l2: 0.0003809172206249944   Iteration 38 of 100, tot loss = 4.821305770623057, l1: 0.00010760052081375187, l2: 0.0003745300560503414   Iteration 39 of 100, tot loss = 4.78977526151217, l1: 0.00010774840764217795, l2: 0.0003712291184526223   Iteration 40 of 100, tot loss = 4.744941854476929, l1: 0.00010702345371100818, l2: 0.00036747073172591624   Iteration 41 of 100, tot loss = 4.742857886523735, l1: 0.00010696466016248096, l2: 0.00036732112937720447   Iteration 42 of 100, tot loss = 4.782247111910865, l1: 0.00010810227468027733, l2: 0.00037012243687751747   Iteration 43 of 100, tot loss = 4.766893553179364, l1: 0.00010820992441907482, l2: 0.00036847943082711726   Iteration 44 of 100, tot loss = 4.7422860427336255, l1: 0.0001070060593106477, l2: 0.00036722254497528246   Iteration 45 of 100, tot loss = 4.708096398247613, l1: 0.00010637299330685184, l2: 0.00036443664640602137   Iteration 46 of 100, tot loss = 4.720162920329882, l1: 0.0001064652061240454, l2: 0.00036555108533281344   Iteration 47 of 100, tot loss = 4.713775005746395, l1: 0.00010614890406747903, l2: 0.00036522859591059387   Iteration 48 of 100, tot loss = 4.6828595250844955, l1: 0.00010592280242841905, l2: 0.0003623631497854755   Iteration 49 of 100, tot loss = 4.683510726811934, l1: 0.00010655575563444527, l2: 0.0003617953168161746   Iteration 50 of 100, tot loss = 4.7685641145706175, l1: 0.00010803379809658509, l2: 0.0003688226133817807   Iteration 51 of 100, tot loss = 4.75376737351511, l1: 0.00010757920637272535, l2: 0.0003677975311053588   Iteration 52 of 100, tot loss = 4.726960301399231, l1: 0.0001075615865864585, l2: 0.00036513444417953276   Iteration 53 of 100, tot loss = 4.692231515668473, l1: 0.00010709203171537188, l2: 0.00036213111998908715   Iteration 54 of 100, tot loss = 4.722731568195202, l1: 0.00010771275002399407, l2: 0.00036456040672621585   Iteration 55 of 100, tot loss = 4.718140519749035, l1: 0.00010772832895533859, l2: 0.00036408572261941363   Iteration 56 of 100, tot loss = 4.723189111266818, l1: 0.00010805047525406865, l2: 0.00036426843585754147   Iteration 57 of 100, tot loss = 4.743895576711287, l1: 0.00010829525541467184, l2: 0.00036609430185707056   Iteration 58 of 100, tot loss = 4.724309547194119, l1: 0.00010795138279780955, l2: 0.0003644795714504214   Iteration 59 of 100, tot loss = 4.738527924327527, l1: 0.0001076778273766079, l2: 0.00036617496491891287   Iteration 60 of 100, tot loss = 4.73457422653834, l1: 0.0001074144060718633, l2: 0.00036604301640181803   Iteration 61 of 100, tot loss = 4.704036908071549, l1: 0.00010690659783889357, l2: 0.0003634970928599569   Iteration 62 of 100, tot loss = 4.722779297059582, l1: 0.00010735651947822677, l2: 0.0003649214103784142   Iteration 63 of 100, tot loss = 4.731664347270178, l1: 0.0001076119430460191, l2: 0.00036555449113375436   Iteration 64 of 100, tot loss = 4.761510409414768, l1: 0.00010817071716928695, l2: 0.00036798032328988484   Iteration 65 of 100, tot loss = 4.760028002812312, l1: 0.00010816766326360476, l2: 0.0003678351365334283   Iteration 66 of 100, tot loss = 4.744279630256422, l1: 0.00010815145392508853, l2: 0.0003662765086505026   Iteration 67 of 100, tot loss = 4.71665482022869, l1: 0.00010763295630119921, l2: 0.0003640325249124096   Iteration 68 of 100, tot loss = 4.718056840055129, l1: 0.00010786231450765627, l2: 0.0003639433690456345   Iteration 69 of 100, tot loss = 4.714567744213602, l1: 0.0001076713412223161, l2: 0.00036378543312742335   Iteration 70 of 100, tot loss = 4.71915533883231, l1: 0.00010779324142536747, l2: 0.0003641222927918924   Iteration 71 of 100, tot loss = 4.699428810200221, l1: 0.00010730099447627395, l2: 0.0003626418868112575   Iteration 72 of 100, tot loss = 4.7055973841084375, l1: 0.00010769470054583508, l2: 0.0003628650378636343   Iteration 73 of 100, tot loss = 4.676422030958411, l1: 0.00010719258502270861, l2: 0.00036044961817511547   Iteration 74 of 100, tot loss = 4.63148996314487, l1: 0.00010614833206853143, l2: 0.00035700066439096336   Iteration 75 of 100, tot loss = 4.604441833496094, l1: 0.00010543469987169374, l2: 0.0003550094835615406   Iteration 76 of 100, tot loss = 4.60882454169424, l1: 0.00010572040711945432, l2: 0.00035516204738278727   Iteration 77 of 100, tot loss = 4.59067330112705, l1: 0.00010534120555313202, l2: 0.00035372612485359725   Iteration 78 of 100, tot loss = 4.5824279846289215, l1: 0.00010538911744963395, l2: 0.00035285368115485954   Iteration 79 of 100, tot loss = 4.56205903729306, l1: 0.00010494798719068794, l2: 0.00035125791641170325   Iteration 80 of 100, tot loss = 4.601968851685524, l1: 0.0001054530938517928, l2: 0.0003547437912857276   Iteration 81 of 100, tot loss = 4.627317825953166, l1: 0.00010560201277527355, l2: 0.0003571297700680723   Iteration 82 of 100, tot loss = 4.607329365683765, l1: 0.00010540951133429544, l2: 0.0003553234254046934   Iteration 83 of 100, tot loss = 4.614571683378105, l1: 0.00010555001069614187, l2: 0.0003559071577998462   Iteration 84 of 100, tot loss = 4.609808328605833, l1: 0.00010534086866408914, l2: 0.00035563996460127997   Iteration 85 of 100, tot loss = 4.636205631143906, l1: 0.00010580837917907903, l2: 0.00035781218406662126   Iteration 86 of 100, tot loss = 4.647274042284766, l1: 0.00010587262208583673, l2: 0.0003588547827581182   Iteration 87 of 100, tot loss = 4.665109801566464, l1: 0.00010592563911364547, l2: 0.00036058534119390594   Iteration 88 of 100, tot loss = 4.665639766237953, l1: 0.00010586365622409424, l2: 0.0003607003205119823   Iteration 89 of 100, tot loss = 4.666700837317477, l1: 0.00010590352200339889, l2: 0.00036076656160117496   Iteration 90 of 100, tot loss = 4.70093031194475, l1: 0.00010650613045678861, l2: 0.0003635869004129846   Iteration 91 of 100, tot loss = 4.669031918703855, l1: 0.00010597706606639115, l2: 0.0003609261254826049   Iteration 92 of 100, tot loss = 4.678216975668202, l1: 0.00010587956941653037, l2: 0.00036194212810622287   Iteration 93 of 100, tot loss = 4.674883970650294, l1: 0.00010595639044671164, l2: 0.0003615320063701841   Iteration 94 of 100, tot loss = 4.6463011553946965, l1: 0.0001054603711615547, l2: 0.0003591697442313121   Iteration 95 of 100, tot loss = 4.616823262917368, l1: 0.00010479835440683497, l2: 0.0003568839718354866   Iteration 96 of 100, tot loss = 4.631531388809283, l1: 0.0001051291589912277, l2: 0.00035802397981872974   Iteration 97 of 100, tot loss = 4.624801668924155, l1: 0.00010502342180545158, l2: 0.00035745674518018614   Iteration 98 of 100, tot loss = 4.597845800068914, l1: 0.0001045676535795792, l2: 0.00035521692649238   Iteration 99 of 100, tot loss = 4.608888893416434, l1: 0.00010497848201008641, l2: 0.0003559104076673681   Iteration 100 of 100, tot loss = 4.601918432712555, l1: 0.00010501894137632917, l2: 0.0003551729024911765
   End of epoch 1224; saving model... 

Epoch 1225 of 2000
   Iteration 1 of 100, tot loss = 5.022527694702148, l1: 0.00011225837079109624, l2: 0.0003899943840224296   Iteration 2 of 100, tot loss = 6.600951194763184, l1: 0.00013434066568152048, l2: 0.0005257544253254309   Iteration 3 of 100, tot loss = 5.838759422302246, l1: 0.00011752611317206174, l2: 0.00046634980632613104   Iteration 4 of 100, tot loss = 5.642364859580994, l1: 0.0001104282091546338, l2: 0.0004538082575891167   Iteration 5 of 100, tot loss = 6.425952816009522, l1: 0.00011848379508592189, l2: 0.0005241114646196366   Iteration 6 of 100, tot loss = 6.301182190577189, l1: 0.00012030068561822797, l2: 0.0005098175121626506   Iteration 7 of 100, tot loss = 5.961305107389178, l1: 0.00011269253133962463, l2: 0.00048343796100068303   Iteration 8 of 100, tot loss = 5.539015471935272, l1: 0.00010714977361203637, l2: 0.00044675175558950286   Iteration 9 of 100, tot loss = 5.116226381725735, l1: 9.927453356795013e-05, l2: 0.0004123480887048774   Iteration 10 of 100, tot loss = 4.921698331832886, l1: 9.751481993589551e-05, l2: 0.0003946549986721948   Iteration 11 of 100, tot loss = 4.849915027618408, l1: 9.653297456679866e-05, l2: 0.00038845851668156683   Iteration 12 of 100, tot loss = 4.859177788098653, l1: 9.492789649812039e-05, l2: 0.00039098986841660616   Iteration 13 of 100, tot loss = 4.774372944465051, l1: 9.548051233510845e-05, l2: 0.0003819567702101687   Iteration 14 of 100, tot loss = 4.762273958751133, l1: 9.325507458665275e-05, l2: 0.0003829723095155454   Iteration 15 of 100, tot loss = 4.7688800175984705, l1: 9.428177727386356e-05, l2: 0.00038260621173928183   Iteration 16 of 100, tot loss = 4.707273975014687, l1: 9.186785064230207e-05, l2: 0.0003788595349760726   Iteration 17 of 100, tot loss = 4.576055652954999, l1: 9.077153748983298e-05, l2: 0.00036683401712627313   Iteration 18 of 100, tot loss = 4.507830209202236, l1: 8.872897281738308e-05, l2: 0.000362054036587425   Iteration 19 of 100, tot loss = 4.511136142831099, l1: 8.931635391873945e-05, l2: 0.0003617972504702936   Iteration 20 of 100, tot loss = 4.519530737400055, l1: 9.051941851794254e-05, l2: 0.00036143364632152953   Iteration 21 of 100, tot loss = 4.6872398853302, l1: 9.385289359627114e-05, l2: 0.00037487108507082753   Iteration 22 of 100, tot loss = 4.794682686979121, l1: 9.60804182713301e-05, l2: 0.0003833878416282294   Iteration 23 of 100, tot loss = 4.810764447502468, l1: 9.648763491144484e-05, l2: 0.00038458880115523124   Iteration 24 of 100, tot loss = 4.868900289138158, l1: 9.802548447623849e-05, l2: 0.0003888645363379813   Iteration 25 of 100, tot loss = 4.790831661224365, l1: 9.757766412803903e-05, l2: 0.00038150549458805473   Iteration 26 of 100, tot loss = 4.780617567209097, l1: 9.71690936309572e-05, l2: 0.00038089265637189295   Iteration 27 of 100, tot loss = 4.745270534797951, l1: 9.763525573631611e-05, l2: 0.0003768917915833838   Iteration 28 of 100, tot loss = 4.783451744488308, l1: 9.794051298480813e-05, l2: 0.0003804046568153093   Iteration 29 of 100, tot loss = 4.746412836272141, l1: 9.812932819778356e-05, l2: 0.0003765119507643876   Iteration 30 of 100, tot loss = 4.831393305460612, l1: 9.934682869546425e-05, l2: 0.000383792496965422   Iteration 31 of 100, tot loss = 4.856786081867833, l1: 0.00010016737548041067, l2: 0.00038551122857062445   Iteration 32 of 100, tot loss = 4.936661168932915, l1: 0.0001011159604331624, l2: 0.00039255015281014494   Iteration 33 of 100, tot loss = 4.949234023238674, l1: 0.00010183281119680032, l2: 0.000393090587529128   Iteration 34 of 100, tot loss = 4.936738140442792, l1: 0.00010160762180663262, l2: 0.0003920661885572104   Iteration 35 of 100, tot loss = 4.912323747362409, l1: 0.00010159357646313896, l2: 0.0003896387946692162   Iteration 36 of 100, tot loss = 4.963646941714817, l1: 0.00010280699178919248, l2: 0.0003935576989735839   Iteration 37 of 100, tot loss = 4.947404951662631, l1: 0.00010261871785907126, l2: 0.00039212177373859023   Iteration 38 of 100, tot loss = 4.884079506522731, l1: 0.00010108196463414443, l2: 0.0003873259824774179   Iteration 39 of 100, tot loss = 4.853486415667412, l1: 0.00010129105803804495, l2: 0.0003840575802127998   Iteration 40 of 100, tot loss = 4.802831262350082, l1: 0.00010004825107898797, l2: 0.00038023487213649786   Iteration 41 of 100, tot loss = 4.797096816504874, l1: 0.00010005598454792961, l2: 0.00037965369439206835   Iteration 42 of 100, tot loss = 4.75419232958839, l1: 9.97842899301932e-05, l2: 0.00037563494074025324   Iteration 43 of 100, tot loss = 4.700708117595939, l1: 9.938937948264052e-05, l2: 0.00037068143002364005   Iteration 44 of 100, tot loss = 4.682368034666235, l1: 9.878340130274988e-05, l2: 0.0003694533998813395   Iteration 45 of 100, tot loss = 4.6493394056955974, l1: 9.884756485310694e-05, l2: 0.0003660863731056452   Iteration 46 of 100, tot loss = 4.663328880849092, l1: 9.920688234674542e-05, l2: 0.00036712600305453753   Iteration 47 of 100, tot loss = 4.633900540940305, l1: 9.853926538663144e-05, l2: 0.00036485078608359584   Iteration 48 of 100, tot loss = 4.615859736998876, l1: 9.880894822344999e-05, l2: 0.00036277702323180466   Iteration 49 of 100, tot loss = 4.551553010940552, l1: 9.763140138713833e-05, l2: 0.00035752389759385997   Iteration 50 of 100, tot loss = 4.574394793510437, l1: 9.810164556256495e-05, l2: 0.00035933783248765395   Iteration 51 of 100, tot loss = 4.577480825723386, l1: 9.848655035431661e-05, l2: 0.00035926153085550624   Iteration 52 of 100, tot loss = 4.565404667304112, l1: 9.855249118118081e-05, l2: 0.0003579879739845637   Iteration 53 of 100, tot loss = 4.561690011114444, l1: 9.874227763711528e-05, l2: 0.00035742672247030194   Iteration 54 of 100, tot loss = 4.552465125366494, l1: 9.80059277800912e-05, l2: 0.0003572405842499359   Iteration 55 of 100, tot loss = 4.564782866564664, l1: 9.819099981210787e-05, l2: 0.000358287286574275   Iteration 56 of 100, tot loss = 4.5432343653270175, l1: 9.777080283259108e-05, l2: 0.0003565526336127992   Iteration 57 of 100, tot loss = 4.517654025763796, l1: 9.744493722233496e-05, l2: 0.0003543204654481981   Iteration 58 of 100, tot loss = 4.511948314206354, l1: 9.725911576393189e-05, l2: 0.0003539357160755979   Iteration 59 of 100, tot loss = 4.52125032877518, l1: 9.752439211173994e-05, l2: 0.0003546006416199508   Iteration 60 of 100, tot loss = 4.531446758906046, l1: 9.769077369128353e-05, l2: 0.000355453903224164   Iteration 61 of 100, tot loss = 4.5100025778911155, l1: 9.706373271373955e-05, l2: 0.00035393652582510574   Iteration 62 of 100, tot loss = 4.499666479326064, l1: 9.716953031930907e-05, l2: 0.0003527971181733113   Iteration 63 of 100, tot loss = 4.502294710704258, l1: 9.741217397118268e-05, l2: 0.00035281729790204695   Iteration 64 of 100, tot loss = 4.481335703283548, l1: 9.715552886291334e-05, l2: 0.0003509780422064068   Iteration 65 of 100, tot loss = 4.446584261380709, l1: 9.630170635108908e-05, l2: 0.00034835672055263646   Iteration 66 of 100, tot loss = 4.498976794156161, l1: 9.681947432744147e-05, l2: 0.0003530782059621715   Iteration 67 of 100, tot loss = 4.48319731541534, l1: 9.67586227543235e-05, l2: 0.0003515611098186155   Iteration 68 of 100, tot loss = 4.51800131447175, l1: 9.720486666671658e-05, l2: 0.0003545952655686586   Iteration 69 of 100, tot loss = 4.515355134355849, l1: 9.730435978280434e-05, l2: 0.000354231154483716   Iteration 70 of 100, tot loss = 4.488308892931257, l1: 9.686456313439911e-05, l2: 0.00035196632712281175   Iteration 71 of 100, tot loss = 4.506870840636777, l1: 9.718456085850025e-05, l2: 0.0003535025244788595   Iteration 72 of 100, tot loss = 4.482388923565547, l1: 9.686737404182268e-05, l2: 0.0003513715194761365   Iteration 73 of 100, tot loss = 4.5042577214436985, l1: 9.741643350013239e-05, l2: 0.0003530093393374668   Iteration 74 of 100, tot loss = 4.55993707437773, l1: 9.813512912621042e-05, l2: 0.00035785857923926686   Iteration 75 of 100, tot loss = 4.5584419790903725, l1: 9.815781144425274e-05, l2: 0.00035768638733619205   Iteration 76 of 100, tot loss = 4.563932447057021, l1: 9.84102156699488e-05, l2: 0.00035798302977989234   Iteration 77 of 100, tot loss = 4.554904804601298, l1: 9.788621282023559e-05, l2: 0.000357604268280632   Iteration 78 of 100, tot loss = 4.567739581450438, l1: 9.789645012530785e-05, l2: 0.0003588775082160002   Iteration 79 of 100, tot loss = 4.5907633123518545, l1: 9.821103622361758e-05, l2: 0.000360865295677493   Iteration 80 of 100, tot loss = 4.584038797020912, l1: 9.787826556930667e-05, l2: 0.00036052561499673176   Iteration 81 of 100, tot loss = 4.583305102807504, l1: 9.779294837412804e-05, l2: 0.0003605375627277097   Iteration 82 of 100, tot loss = 4.567321160944497, l1: 9.76393213665976e-05, l2: 0.0003590927954064682   Iteration 83 of 100, tot loss = 4.555581075599394, l1: 9.713165175537186e-05, l2: 0.00035842645637051436   Iteration 84 of 100, tot loss = 4.557130041576567, l1: 9.725015427518104e-05, l2: 0.0003584628502721898   Iteration 85 of 100, tot loss = 4.569958428775563, l1: 9.767091747455518e-05, l2: 0.0003593249260174001   Iteration 86 of 100, tot loss = 4.569184763486995, l1: 9.803699250305937e-05, l2: 0.0003588814846765207   Iteration 87 of 100, tot loss = 4.552434310145761, l1: 9.796020408273386e-05, l2: 0.0003572832277826108   Iteration 88 of 100, tot loss = 4.557149223305962, l1: 9.811880855440077e-05, l2: 0.0003575961150504141   Iteration 89 of 100, tot loss = 4.536084689451068, l1: 9.793539577637266e-05, l2: 0.0003556730743992048   Iteration 90 of 100, tot loss = 4.541768015755547, l1: 9.802474103505826e-05, l2: 0.0003561520617545789   Iteration 91 of 100, tot loss = 4.567786677853092, l1: 9.841671703475435e-05, l2: 0.0003583619521692866   Iteration 92 of 100, tot loss = 4.574321347734203, l1: 9.84796312442758e-05, l2: 0.0003589525052951143   Iteration 93 of 100, tot loss = 4.5785460882289435, l1: 9.879238824660499e-05, l2: 0.00035906222230199   Iteration 94 of 100, tot loss = 4.587326085313838, l1: 9.908033407942213e-05, l2: 0.000359652276142213   Iteration 95 of 100, tot loss = 4.56807205551549, l1: 9.886945703520293e-05, l2: 0.00035793775030852935   Iteration 96 of 100, tot loss = 4.555053420364857, l1: 9.886686067754151e-05, l2: 0.00035663848332963727   Iteration 97 of 100, tot loss = 4.539342194488368, l1: 9.869194261612102e-05, l2: 0.0003552422785264987   Iteration 98 of 100, tot loss = 4.527626979107759, l1: 9.846649118117057e-05, l2: 0.00035429620840325384   Iteration 99 of 100, tot loss = 4.525042049812548, l1: 9.846258064793103e-05, l2: 0.000354041625725604   Iteration 100 of 100, tot loss = 4.543313648700714, l1: 9.885855615721084e-05, l2: 0.00035547281018807555
   End of epoch 1225; saving model... 

Epoch 1226 of 2000
   Iteration 1 of 100, tot loss = 5.3711137771606445, l1: 9.785746806301177e-05, l2: 0.0004392538976389915   Iteration 2 of 100, tot loss = 4.573744773864746, l1: 9.178928303299472e-05, l2: 0.00036558517604134977   Iteration 3 of 100, tot loss = 3.7959850629170737, l1: 8.163876918843016e-05, l2: 0.00029795971931889653   Iteration 4 of 100, tot loss = 3.576138436794281, l1: 8.090482333500404e-05, l2: 0.00027670900817611255   Iteration 5 of 100, tot loss = 3.922491216659546, l1: 8.628506766399369e-05, l2: 0.0003059640497667715   Iteration 6 of 100, tot loss = 3.9473903576533, l1: 8.959142238988231e-05, l2: 0.0003051476111674371   Iteration 7 of 100, tot loss = 4.003239188875471, l1: 9.10177976558251e-05, l2: 0.0003093061199511534   Iteration 8 of 100, tot loss = 3.992931306362152, l1: 8.700005446371506e-05, l2: 0.00031229307387548033   Iteration 9 of 100, tot loss = 4.189572705162896, l1: 9.251925277769462e-05, l2: 0.0003264380195307442   Iteration 10 of 100, tot loss = 4.428850793838501, l1: 9.568314962962176e-05, l2: 0.00034720192925306035   Iteration 11 of 100, tot loss = 4.423154007304799, l1: 9.639748399918476e-05, l2: 0.00034591791468714786   Iteration 12 of 100, tot loss = 4.327633460362752, l1: 9.427948771190131e-05, l2: 0.0003384838552544049   Iteration 13 of 100, tot loss = 4.512339445260855, l1: 9.796745003908515e-05, l2: 0.0003532664951098223   Iteration 14 of 100, tot loss = 4.4918524878365655, l1: 9.689402031654026e-05, l2: 0.00035229123126815205   Iteration 15 of 100, tot loss = 4.515048027038574, l1: 9.78172589384485e-05, l2: 0.00035368754470255226   Iteration 16 of 100, tot loss = 4.494296759366989, l1: 9.799053646020184e-05, l2: 0.0003514391401040484   Iteration 17 of 100, tot loss = 4.620290054994471, l1: 0.00010007405966034104, l2: 0.00036195494484512466   Iteration 18 of 100, tot loss = 4.56518030166626, l1: 9.886286049246944e-05, l2: 0.0003576551688537519   Iteration 19 of 100, tot loss = 4.502206325531006, l1: 9.772713019629009e-05, l2: 0.00035249350185040385   Iteration 20 of 100, tot loss = 4.485308408737183, l1: 9.78584526819759e-05, l2: 0.0003506723871396389   Iteration 21 of 100, tot loss = 4.426022211710612, l1: 9.669068423959071e-05, l2: 0.00034591153616063473   Iteration 22 of 100, tot loss = 4.420865817503496, l1: 9.645704779937991e-05, l2: 0.0003456295334300111   Iteration 23 of 100, tot loss = 4.45886348641437, l1: 9.699465827788632e-05, l2: 0.00034889168898149836   Iteration 24 of 100, tot loss = 4.497332413991292, l1: 9.814834008163113e-05, l2: 0.00035158490027242806   Iteration 25 of 100, tot loss = 4.600027618408203, l1: 0.00010019720517448149, l2: 0.00035980555519927294   Iteration 26 of 100, tot loss = 4.547885683866648, l1: 9.898362553100854e-05, l2: 0.00035580494183635054   Iteration 27 of 100, tot loss = 4.495023639113815, l1: 9.796968421411563e-05, l2: 0.0003515326793098615   Iteration 28 of 100, tot loss = 4.46612993308476, l1: 9.774933628900076e-05, l2: 0.0003488636565245023   Iteration 29 of 100, tot loss = 4.554248563174544, l1: 9.881663836181517e-05, l2: 0.0003566082179594528   Iteration 30 of 100, tot loss = 4.516294169425964, l1: 9.804843733339416e-05, l2: 0.00035358098005720725   Iteration 31 of 100, tot loss = 4.489281662048832, l1: 9.731753485656583e-05, l2: 0.000351610631989916   Iteration 32 of 100, tot loss = 4.556265331804752, l1: 9.778766741419531e-05, l2: 0.00035783886505669216   Iteration 33 of 100, tot loss = 4.561076893950954, l1: 9.788531149917452e-05, l2: 0.000358222377888689   Iteration 34 of 100, tot loss = 4.615759744363673, l1: 9.912101246728151e-05, l2: 0.00036245496234829156   Iteration 35 of 100, tot loss = 4.74389352117266, l1: 0.00010164242942534786, l2: 0.0003727469235725169   Iteration 36 of 100, tot loss = 4.770569860935211, l1: 0.00010246830778568337, l2: 0.00037458867963222373   Iteration 37 of 100, tot loss = 4.787742711402275, l1: 0.0001029387594088975, l2: 0.0003758355136646109   Iteration 38 of 100, tot loss = 4.753927839429755, l1: 0.00010219917394830478, l2: 0.00037319361198521957   Iteration 39 of 100, tot loss = 4.712902460342798, l1: 0.00010166150707682858, l2: 0.00036962874097904813   Iteration 40 of 100, tot loss = 4.748345923423767, l1: 0.0001025283178933023, l2: 0.00037230627676763104   Iteration 41 of 100, tot loss = 4.786143489000274, l1: 0.00010329162144377578, l2: 0.0003753227296018428   Iteration 42 of 100, tot loss = 4.738500912984212, l1: 0.00010196819303451949, l2: 0.0003718819006323992   Iteration 43 of 100, tot loss = 4.804567037626755, l1: 0.00010317839295630409, l2: 0.0003772783133764426   Iteration 44 of 100, tot loss = 4.738587043502114, l1: 0.00010174106352100021, l2: 0.0003721176432339813   Iteration 45 of 100, tot loss = 4.741398260328505, l1: 0.00010185559884929615, l2: 0.0003722842291204466   Iteration 46 of 100, tot loss = 4.768518738124682, l1: 0.00010204094961290653, l2: 0.0003748109257962231   Iteration 47 of 100, tot loss = 4.793570660530253, l1: 0.00010315187543536477, l2: 0.00037620519225961194   Iteration 48 of 100, tot loss = 4.788544595241547, l1: 0.00010326317366586106, l2: 0.0003755912869867946   Iteration 49 of 100, tot loss = 4.77469912353827, l1: 0.00010341597404402244, l2: 0.0003740539387039536   Iteration 50 of 100, tot loss = 4.781124029159546, l1: 0.00010383554545114748, l2: 0.0003742768586380407   Iteration 51 of 100, tot loss = 4.769810283885283, l1: 0.00010379781080843589, l2: 0.00037318321889923773   Iteration 52 of 100, tot loss = 4.803700575461755, l1: 0.00010443782188163963, l2: 0.0003759322370080134   Iteration 53 of 100, tot loss = 4.831521915939619, l1: 0.00010471671236512023, l2: 0.000378435480909176   Iteration 54 of 100, tot loss = 4.840443381556758, l1: 0.00010454169575866588, l2: 0.0003795026433955947   Iteration 55 of 100, tot loss = 4.852758615667169, l1: 0.00010496504969937219, l2: 0.0003803108125628734   Iteration 56 of 100, tot loss = 4.8497053895677835, l1: 0.0001049987070343507, l2: 0.00037997183244442567   Iteration 57 of 100, tot loss = 4.8185125526629, l1: 0.00010467638588906674, l2: 0.0003771748702172517   Iteration 58 of 100, tot loss = 4.825324169520674, l1: 0.0001050010874239198, l2: 0.00037753132998879097   Iteration 59 of 100, tot loss = 4.84635764865552, l1: 0.00010548282542435642, l2: 0.0003791529398166843   Iteration 60 of 100, tot loss = 4.907013889153799, l1: 0.00010628982454363722, l2: 0.00038441156454306716   Iteration 61 of 100, tot loss = 4.896684228396807, l1: 0.00010589299617110003, l2: 0.0003837754264794534   Iteration 62 of 100, tot loss = 4.869923457022636, l1: 0.00010521051573901317, l2: 0.00038178182993790197   Iteration 63 of 100, tot loss = 4.869386328591241, l1: 0.00010526102888102215, l2: 0.00038167760367431336   Iteration 64 of 100, tot loss = 4.8839643858373165, l1: 0.00010517065197745978, l2: 0.0003832257862086408   Iteration 65 of 100, tot loss = 4.862386707159189, l1: 0.00010484112426638603, l2: 0.00038139754637645987   Iteration 66 of 100, tot loss = 4.898613405950142, l1: 0.00010548873273585218, l2: 0.00038437260699345535   Iteration 67 of 100, tot loss = 4.907807033453414, l1: 0.00010605276073155397, l2: 0.0003847279416487566   Iteration 68 of 100, tot loss = 4.919221229412976, l1: 0.00010646427129840424, l2: 0.0003854578509858316   Iteration 69 of 100, tot loss = 4.891587153725002, l1: 0.00010563266129094595, l2: 0.00038352605348547405   Iteration 70 of 100, tot loss = 4.9229287283761165, l1: 0.00010619559834594839, l2: 0.0003860972742716383   Iteration 71 of 100, tot loss = 4.911911218938693, l1: 0.00010629905837433884, l2: 0.00038489206320225773   Iteration 72 of 100, tot loss = 4.9328252408239575, l1: 0.00010694475824695676, l2: 0.0003863377650203701   Iteration 73 of 100, tot loss = 4.947833420479134, l1: 0.00010702920274662287, l2: 0.0003877541389992486   Iteration 74 of 100, tot loss = 4.973098355370599, l1: 0.00010730388906243181, l2: 0.000390005946109021   Iteration 75 of 100, tot loss = 4.957404667536418, l1: 0.00010740206082118675, l2: 0.000388338405561323   Iteration 76 of 100, tot loss = 4.966744426049684, l1: 0.00010755584748709992, l2: 0.0003891185950028318   Iteration 77 of 100, tot loss = 4.962357920485657, l1: 0.0001075848034248905, l2: 0.00038865098862775734   Iteration 78 of 100, tot loss = 4.976973243248769, l1: 0.000107697829382512, l2: 0.00038999949510280904   Iteration 79 of 100, tot loss = 4.9694371434706675, l1: 0.00010777179824477179, l2: 0.0003891719164049748   Iteration 80 of 100, tot loss = 4.99326379597187, l1: 0.00010831543477252126, l2: 0.0003910109448042931   Iteration 81 of 100, tot loss = 4.989678409364489, l1: 0.00010841151487073619, l2: 0.0003905563261704864   Iteration 82 of 100, tot loss = 5.026586826254682, l1: 0.00010903617383120013, l2: 0.00039362250798858885   Iteration 83 of 100, tot loss = 5.029678100562958, l1: 0.00010875436997725572, l2: 0.0003942134390383151   Iteration 84 of 100, tot loss = 5.013229421206883, l1: 0.0001083813985176329, l2: 0.0003929415425296784   Iteration 85 of 100, tot loss = 5.022051642922794, l1: 0.00010882261515636107, l2: 0.0003933825482176069   Iteration 86 of 100, tot loss = 5.010740590649982, l1: 0.00010870651546378381, l2: 0.00039236754284586843   Iteration 87 of 100, tot loss = 5.008962357181242, l1: 0.00010889424556389922, l2: 0.0003920019893296834   Iteration 88 of 100, tot loss = 4.977392174980857, l1: 0.00010814997052544972, l2: 0.0003895892461349087   Iteration 89 of 100, tot loss = 4.9419518173410655, l1: 0.00010742287850007415, l2: 0.00038677230238616257   Iteration 90 of 100, tot loss = 4.91587796608607, l1: 0.00010684855454504335, l2: 0.00038473924117473263   Iteration 91 of 100, tot loss = 4.932877272039979, l1: 0.00010686669482530995, l2: 0.00038642103139024513   Iteration 92 of 100, tot loss = 4.93124026707981, l1: 0.00010639309549214491, l2: 0.00038673093007199225   Iteration 93 of 100, tot loss = 4.91589504544453, l1: 0.0001063990889487022, l2: 0.00038519041438747235   Iteration 94 of 100, tot loss = 4.902706566009115, l1: 0.00010625887881225083, l2: 0.00038401177648375644   Iteration 95 of 100, tot loss = 4.89758662048139, l1: 0.000106023474769532, l2: 0.0003837351861875504   Iteration 96 of 100, tot loss = 4.862658320615689, l1: 0.00010537326988924178, l2: 0.0003808925611489637   Iteration 97 of 100, tot loss = 4.859860222364209, l1: 0.00010548392235728691, l2: 0.00038050209883463656   Iteration 98 of 100, tot loss = 4.853566479926207, l1: 0.00010548303016919966, l2: 0.0003798736168911481   Iteration 99 of 100, tot loss = 4.842533806357721, l1: 0.00010512987956653979, l2: 0.00037912350032429185   Iteration 100 of 100, tot loss = 4.81968630194664, l1: 0.00010478998061444144, l2: 0.00037717864899605046
   End of epoch 1226; saving model... 

Epoch 1227 of 2000
   Iteration 1 of 100, tot loss = 4.640987396240234, l1: 7.417121378239244e-05, l2: 0.0003899275616277009   Iteration 2 of 100, tot loss = 4.2749940156936646, l1: 8.768092811806127e-05, l2: 0.00033981849264819175   Iteration 3 of 100, tot loss = 4.236732721328735, l1: 9.796036708091076e-05, l2: 0.00032571291861434776   Iteration 4 of 100, tot loss = 4.505539238452911, l1: 0.00010645726979419123, l2: 0.00034409667568979785   Iteration 5 of 100, tot loss = 4.403448295593262, l1: 0.00010756240226328373, l2: 0.00033278244663961234   Iteration 6 of 100, tot loss = 4.40476640065511, l1: 0.00010485337891926368, l2: 0.0003356232773512602   Iteration 7 of 100, tot loss = 4.269237995147705, l1: 9.903726562957413e-05, l2: 0.00032788654789328575   Iteration 8 of 100, tot loss = 4.386306047439575, l1: 0.00010224825655313907, l2: 0.0003363823561812751   Iteration 9 of 100, tot loss = 4.3536720805697975, l1: 0.00010196474613621831, l2: 0.0003334024675293929   Iteration 10 of 100, tot loss = 4.439122438430786, l1: 0.00010477280884515494, l2: 0.0003391394449863583   Iteration 11 of 100, tot loss = 4.340811425989324, l1: 0.00010218882115705955, l2: 0.0003318923303264786   Iteration 12 of 100, tot loss = 4.268971999486287, l1: 0.00010018771354225464, l2: 0.00032670949440216646   Iteration 13 of 100, tot loss = 4.225710813815777, l1: 9.94311233695883e-05, l2: 0.00032313996388648567   Iteration 14 of 100, tot loss = 4.13544191632952, l1: 9.753833001013845e-05, l2: 0.00031600586745688427   Iteration 15 of 100, tot loss = 4.036823431650798, l1: 9.406328754266725e-05, l2: 0.0003096190611055742   Iteration 16 of 100, tot loss = 4.0432175397872925, l1: 9.359229125038837e-05, l2: 0.0003107294687652029   Iteration 17 of 100, tot loss = 4.055536550634048, l1: 9.447032832266653e-05, l2: 0.0003110833332517787   Iteration 18 of 100, tot loss = 4.014264769024319, l1: 9.417157990456972e-05, l2: 0.0003072549039239271   Iteration 19 of 100, tot loss = 4.221839427947998, l1: 9.549237553361117e-05, l2: 0.0003266915752503433   Iteration 20 of 100, tot loss = 4.30907633304596, l1: 9.800109801290091e-05, l2: 0.00033290654246229676   Iteration 21 of 100, tot loss = 4.253144843237741, l1: 9.697730690407167e-05, l2: 0.00032833718467459435   Iteration 22 of 100, tot loss = 4.26210116256367, l1: 9.632837761521593e-05, l2: 0.0003298817464383319   Iteration 23 of 100, tot loss = 4.27643084526062, l1: 9.660928069775844e-05, l2: 0.0003310338120765822   Iteration 24 of 100, tot loss = 4.315531998872757, l1: 9.722910363052506e-05, l2: 0.00033432410418754444   Iteration 25 of 100, tot loss = 4.322840356826783, l1: 9.82399785425514e-05, l2: 0.0003340440650936216   Iteration 26 of 100, tot loss = 4.410968606288616, l1: 9.982344972041364e-05, l2: 0.00034127341775904194   Iteration 27 of 100, tot loss = 4.60569707552592, l1: 0.00010244149113229166, l2: 0.0003581282249393149   Iteration 28 of 100, tot loss = 4.625925413199833, l1: 0.00010177092086191155, l2: 0.00036082162945863923   Iteration 29 of 100, tot loss = 4.633038446821016, l1: 0.00010217226472691279, l2: 0.00036113158948237784   Iteration 30 of 100, tot loss = 4.6977192958196, l1: 0.00010278943518642336, l2: 0.0003669825042986001   Iteration 31 of 100, tot loss = 4.66496003827741, l1: 0.00010253351713125144, l2: 0.00036396249572956754   Iteration 32 of 100, tot loss = 4.670334406197071, l1: 0.00010278178751832456, l2: 0.0003642516612671898   Iteration 33 of 100, tot loss = 4.6919262842698535, l1: 0.00010299147283093947, l2: 0.00036620116390457207   Iteration 34 of 100, tot loss = 4.61941049379461, l1: 0.00010185895863447941, l2: 0.0003600820992313161   Iteration 35 of 100, tot loss = 4.582785374777657, l1: 0.0001016863201844639, l2: 0.0003565922254763011   Iteration 36 of 100, tot loss = 4.640699677997166, l1: 0.00010255345801447725, l2: 0.00036151651794271957   Iteration 37 of 100, tot loss = 4.598837974909189, l1: 0.00010212799758379776, l2: 0.00035775580791114653   Iteration 38 of 100, tot loss = 4.626378015467995, l1: 0.00010316416611251617, l2: 0.00035947364391942854   Iteration 39 of 100, tot loss = 4.647200297086667, l1: 0.00010301803316300114, l2: 0.00036170200529830676   Iteration 40 of 100, tot loss = 4.617508512735367, l1: 0.0001024785551635432, l2: 0.0003592723045585444   Iteration 41 of 100, tot loss = 4.628229647147946, l1: 0.00010259013261685784, l2: 0.00036023284071522605   Iteration 42 of 100, tot loss = 4.621033549308777, l1: 0.00010224388928950898, l2: 0.00035985947380097407   Iteration 43 of 100, tot loss = 4.58927592565847, l1: 0.0001017140985336588, l2: 0.00035721350218938274   Iteration 44 of 100, tot loss = 4.608666506680575, l1: 0.00010224313584992409, l2: 0.0003586235235905571   Iteration 45 of 100, tot loss = 4.613168419731988, l1: 0.00010265024369194483, l2: 0.0003586666066742813   Iteration 46 of 100, tot loss = 4.618194507515949, l1: 0.00010256315078510417, l2: 0.00035925630863462373   Iteration 47 of 100, tot loss = 4.649484066253013, l1: 0.00010287588365633595, l2: 0.0003620725310224287   Iteration 48 of 100, tot loss = 4.647247691949208, l1: 0.00010280918074082972, l2: 0.00036191559684084496   Iteration 49 of 100, tot loss = 4.653200733418367, l1: 0.00010286765550239468, l2: 0.00036245242636698315   Iteration 50 of 100, tot loss = 4.684426097869873, l1: 0.00010341588684241287, l2: 0.00036502673086943107   Iteration 51 of 100, tot loss = 4.669492155897851, l1: 0.00010293542308157638, l2: 0.0003640138000300081   Iteration 52 of 100, tot loss = 4.654607364764581, l1: 0.00010309510435035918, l2: 0.0003623656395155614   Iteration 53 of 100, tot loss = 4.638812654423264, l1: 0.00010278084307671788, l2: 0.000361100429798617   Iteration 54 of 100, tot loss = 4.62602843620159, l1: 0.0001028307892233392, l2: 0.0003597720618320939   Iteration 55 of 100, tot loss = 4.609147106517445, l1: 0.00010187205698457547, l2: 0.00035904266061896286   Iteration 56 of 100, tot loss = 4.605595162936619, l1: 0.00010217347458560003, l2: 0.0003583860486417377   Iteration 57 of 100, tot loss = 4.613069124389113, l1: 0.00010239000294588327, l2: 0.00035891691687427006   Iteration 58 of 100, tot loss = 4.598809201141884, l1: 0.00010234379235420232, l2: 0.00035753713520283905   Iteration 59 of 100, tot loss = 4.58472752571106, l1: 0.00010213169193544457, l2: 0.000356341067699605   Iteration 60 of 100, tot loss = 4.602492042382559, l1: 0.00010244984750897856, l2: 0.00035779936321584196   Iteration 61 of 100, tot loss = 4.578473939270269, l1: 0.00010230135107956842, l2: 0.0003555460489969548   Iteration 62 of 100, tot loss = 4.571058307924578, l1: 0.00010243431396731702, l2: 0.000354671522913005   Iteration 63 of 100, tot loss = 4.575769965610807, l1: 0.0001025315333598493, l2: 0.0003550454689141747   Iteration 64 of 100, tot loss = 4.539629407227039, l1: 0.0001020512399350082, l2: 0.0003519117065025057   Iteration 65 of 100, tot loss = 4.567087012070876, l1: 0.00010250062819418067, l2: 0.00035420807835180313   Iteration 66 of 100, tot loss = 4.556731010928298, l1: 0.00010233757684270104, l2: 0.00035333552968399033   Iteration 67 of 100, tot loss = 4.564823104374445, l1: 0.00010235964292582166, l2: 0.00035412267303101217   Iteration 68 of 100, tot loss = 4.577682680943433, l1: 0.00010275307704432635, l2: 0.0003550151966063662   Iteration 69 of 100, tot loss = 4.59293414198834, l1: 0.00010323278922073044, l2: 0.0003560606301239138   Iteration 70 of 100, tot loss = 4.58322342463902, l1: 0.00010317358948148986, l2: 0.00035514875801579494   Iteration 71 of 100, tot loss = 4.566655078404386, l1: 0.0001030300595181909, l2: 0.0003536354533550789   Iteration 72 of 100, tot loss = 4.547053678168191, l1: 0.00010271411110807094, l2: 0.0003519912618988504   Iteration 73 of 100, tot loss = 4.559499286625483, l1: 0.00010268473647868146, l2: 0.0003532651968919098   Iteration 74 of 100, tot loss = 4.557392233126873, l1: 0.00010246452099660953, l2: 0.0003532747072608185   Iteration 75 of 100, tot loss = 4.559769487380981, l1: 0.00010252938232345816, l2: 0.0003534475713968277   Iteration 76 of 100, tot loss = 4.52874745820698, l1: 0.0001020202001822518, l2: 0.00035085455046531645   Iteration 77 of 100, tot loss = 4.546019560330874, l1: 0.00010252228098756682, l2: 0.0003520796797767451   Iteration 78 of 100, tot loss = 4.574024879015409, l1: 0.00010273560170996159, l2: 0.00035466689102274057   Iteration 79 of 100, tot loss = 4.567926974236211, l1: 0.00010279518898607444, l2: 0.0003539975133910186   Iteration 80 of 100, tot loss = 4.556014409661293, l1: 0.00010262701393912722, l2: 0.0003529744319166639   Iteration 81 of 100, tot loss = 4.552499974215472, l1: 0.00010239022055800349, l2: 0.00035285978182075247   Iteration 82 of 100, tot loss = 4.5449009465008245, l1: 0.00010233231007920794, l2: 0.00035215778969847254   Iteration 83 of 100, tot loss = 4.583454137825104, l1: 0.00010289976625606497, l2: 0.0003554456523992682   Iteration 84 of 100, tot loss = 4.562648310547783, l1: 0.00010265958835156024, l2: 0.0003536052475586378   Iteration 85 of 100, tot loss = 4.559602790720323, l1: 0.00010269898206495461, l2: 0.0003532613019856131   Iteration 86 of 100, tot loss = 4.562970818475235, l1: 0.00010276684901497035, l2: 0.000353530237652892   Iteration 87 of 100, tot loss = 4.615046481976564, l1: 0.000103580227130743, l2: 0.0003579244254780505   Iteration 88 of 100, tot loss = 4.627000112425197, l1: 0.00010408205439075986, l2: 0.0003586179609555984   Iteration 89 of 100, tot loss = 4.635710649276048, l1: 0.00010450498531722945, l2: 0.0003590660839762769   Iteration 90 of 100, tot loss = 4.635257156689962, l1: 0.00010442410053251984, l2: 0.00035910161920279886   Iteration 91 of 100, tot loss = 4.651079086156992, l1: 0.00010471226882405713, l2: 0.0003603956437283349   Iteration 92 of 100, tot loss = 4.664563228254733, l1: 0.00010512827560413153, l2: 0.00036132805104463603   Iteration 93 of 100, tot loss = 4.66688524523089, l1: 0.00010499860300408346, l2: 0.0003616899251624922   Iteration 94 of 100, tot loss = 4.640025813528832, l1: 0.00010443301126794355, l2: 0.00035956957373778   Iteration 95 of 100, tot loss = 4.60658340579585, l1: 0.00010369172443078193, l2: 0.00035696661966779316   Iteration 96 of 100, tot loss = 4.608830999583006, l1: 0.00010366622840744337, l2: 0.0003572168748178228   Iteration 97 of 100, tot loss = 4.611624743520599, l1: 0.00010369596390033228, l2: 0.0003574665135658375   Iteration 98 of 100, tot loss = 4.625156627625835, l1: 0.00010392223498920615, l2: 0.00035859343084406907   Iteration 99 of 100, tot loss = 4.651185846087908, l1: 0.00010428167690700532, l2: 0.00036083691035552573   Iteration 100 of 100, tot loss = 4.624228391647339, l1: 0.0001036229582678061, l2: 0.0003587998835428152
   End of epoch 1227; saving model... 

Epoch 1228 of 2000
   Iteration 1 of 100, tot loss = 3.567596197128296, l1: 0.00010655888036126271, l2: 0.0002502007409930229   Iteration 2 of 100, tot loss = 3.6063531637191772, l1: 8.404246909776703e-05, l2: 0.0002765928511507809   Iteration 3 of 100, tot loss = 3.5268574555714927, l1: 9.040766599355265e-05, l2: 0.00026227808848489076   Iteration 4 of 100, tot loss = 3.3071460127830505, l1: 9.184502414427698e-05, l2: 0.00023886958297225647   Iteration 5 of 100, tot loss = 4.078363561630249, l1: 0.00010382725740782916, l2: 0.00030400909890886395   Iteration 6 of 100, tot loss = 4.51237157980601, l1: 0.00010941128615134706, l2: 0.00034182587357160327   Iteration 7 of 100, tot loss = 4.518408400671823, l1: 0.00010360560762429876, l2: 0.00034823523310478777   Iteration 8 of 100, tot loss = 4.748308926820755, l1: 0.00010858637506316882, l2: 0.00036624451786337886   Iteration 9 of 100, tot loss = 4.741281112035115, l1: 0.00010633980612813805, l2: 0.00036778830528621457   Iteration 10 of 100, tot loss = 4.628469681739807, l1: 0.00010355437771067955, l2: 0.00035929259174736214   Iteration 11 of 100, tot loss = 4.562610712918368, l1: 0.00010310410090658644, l2: 0.00035315697129011494   Iteration 12 of 100, tot loss = 4.8871785799662275, l1: 0.00010708569304066866, l2: 0.00038163216595421545   Iteration 13 of 100, tot loss = 4.823616834787222, l1: 0.00010677553207362787, l2: 0.00037558615328564955   Iteration 14 of 100, tot loss = 4.812787260328021, l1: 0.00010703573518964862, l2: 0.00037424299288042154   Iteration 15 of 100, tot loss = 4.694629065195719, l1: 0.00010546190799990049, l2: 0.00036400100022243955   Iteration 16 of 100, tot loss = 4.7571923434734344, l1: 0.00010570167387413676, l2: 0.00037001756209065206   Iteration 17 of 100, tot loss = 4.686834138982436, l1: 0.0001037506791523329, l2: 0.0003649327349301208   Iteration 18 of 100, tot loss = 4.665680355495876, l1: 0.00010366551618466878, l2: 0.0003629025191508441   Iteration 19 of 100, tot loss = 4.6312563544825505, l1: 0.00010414023410320576, l2: 0.000358985399974412   Iteration 20 of 100, tot loss = 4.584981215000153, l1: 0.00010402676671219523, l2: 0.00035447135305730624   Iteration 21 of 100, tot loss = 4.85539124125526, l1: 0.00010802724552507113, l2: 0.0003775118730984451   Iteration 22 of 100, tot loss = 4.838927431540056, l1: 0.00010743443305149081, l2: 0.0003764583049236204   Iteration 23 of 100, tot loss = 4.901484790055648, l1: 0.00010804525980189362, l2: 0.0003821032144554445   Iteration 24 of 100, tot loss = 5.040116439263026, l1: 0.00010906202169280732, l2: 0.0003949496203858871   Iteration 25 of 100, tot loss = 4.977563095092774, l1: 0.00010782965808175504, l2: 0.000389926650095731   Iteration 26 of 100, tot loss = 5.02598236157344, l1: 0.0001091325975721702, l2: 0.00039346563929799374   Iteration 27 of 100, tot loss = 5.051897949642605, l1: 0.00010912528739060724, l2: 0.00039606450742145106   Iteration 28 of 100, tot loss = 5.044033425194876, l1: 0.00010816300735833855, l2: 0.00039624033524887636   Iteration 29 of 100, tot loss = 5.013148899736075, l1: 0.00010775115264675997, l2: 0.0003935637370008847   Iteration 30 of 100, tot loss = 4.92690761089325, l1: 0.00010631387170481807, l2: 0.00038637688897627716   Iteration 31 of 100, tot loss = 4.909356770976897, l1: 0.00010644058925062117, l2: 0.00038449508710689244   Iteration 32 of 100, tot loss = 4.847458302974701, l1: 0.00010584111896605464, l2: 0.0003789047109421517   Iteration 33 of 100, tot loss = 4.807212656194514, l1: 0.00010568028400567445, l2: 0.00037504098101194495   Iteration 34 of 100, tot loss = 4.8191877813900215, l1: 0.0001061954689623021, l2: 0.0003757233092020375   Iteration 35 of 100, tot loss = 4.836479159763881, l1: 0.00010686633218678512, l2: 0.0003767815830152748   Iteration 36 of 100, tot loss = 4.917238566610548, l1: 0.00010817962781099293, l2: 0.00038354422940756194   Iteration 37 of 100, tot loss = 4.872986181362255, l1: 0.00010721248592646491, l2: 0.0003800861332156519   Iteration 38 of 100, tot loss = 4.83182366270768, l1: 0.00010661130995446768, l2: 0.0003765710567485688   Iteration 39 of 100, tot loss = 4.836371837518154, l1: 0.00010709308303492621, l2: 0.0003765441006413685   Iteration 40 of 100, tot loss = 4.787702435255051, l1: 0.00010665250774763991, l2: 0.0003721177352417726   Iteration 41 of 100, tot loss = 4.754673498432811, l1: 0.00010645781074389361, l2: 0.00036900953841168525   Iteration 42 of 100, tot loss = 4.800958945637658, l1: 0.00010702615968295417, l2: 0.0003730697339744351   Iteration 43 of 100, tot loss = 4.744034367938374, l1: 0.00010612337148834973, l2: 0.0003682800645122391   Iteration 44 of 100, tot loss = 4.760866923765703, l1: 0.00010579041868781628, l2: 0.0003702962729386689   Iteration 45 of 100, tot loss = 4.795654826694065, l1: 0.00010625742676590259, l2: 0.00037330805433965803   Iteration 46 of 100, tot loss = 4.759163467780404, l1: 0.00010580052121835963, l2: 0.0003701158237400586   Iteration 47 of 100, tot loss = 4.727546002002472, l1: 0.00010559408048643394, l2: 0.0003671605180839988   Iteration 48 of 100, tot loss = 4.781221201022466, l1: 0.00010671139777211162, l2: 0.0003714107206178596   Iteration 49 of 100, tot loss = 4.813365595681327, l1: 0.00010738236478016692, l2: 0.0003739541927495097   Iteration 50 of 100, tot loss = 4.804349336624146, l1: 0.00010712346193031408, l2: 0.0003733114700298756   Iteration 51 of 100, tot loss = 4.87363935919369, l1: 0.00010802546004175812, l2: 0.00037933847360203374   Iteration 52 of 100, tot loss = 4.882872132154612, l1: 0.00010783805989306599, l2: 0.0003804491509352094   Iteration 53 of 100, tot loss = 4.864762360194944, l1: 0.00010750301219061685, l2: 0.00037897322195107645   Iteration 54 of 100, tot loss = 4.834121699686404, l1: 0.00010700630297867099, l2: 0.000376405865406721   Iteration 55 of 100, tot loss = 4.803800517862493, l1: 0.00010648255887314339, l2: 0.0003738974911605262   Iteration 56 of 100, tot loss = 4.773341689790998, l1: 0.00010573394865787122, l2: 0.00037160021877623094   Iteration 57 of 100, tot loss = 4.753639731490821, l1: 0.00010478924135289465, l2: 0.00037057473035243324   Iteration 58 of 100, tot loss = 4.747492494254277, l1: 0.0001045755553009101, l2: 0.00037017369222353566   Iteration 59 of 100, tot loss = 4.753363059738935, l1: 0.00010463036572110792, l2: 0.0003707059390223348   Iteration 60 of 100, tot loss = 4.730295153458913, l1: 0.00010436457632749807, l2: 0.00036866493780204717   Iteration 61 of 100, tot loss = 4.75144763461879, l1: 0.00010501478450872828, l2: 0.00037012997796171207   Iteration 62 of 100, tot loss = 4.746509109773943, l1: 0.00010494432412557334, l2: 0.00036970658613661784   Iteration 63 of 100, tot loss = 4.732525322172377, l1: 0.00010469921512089463, l2: 0.00036855331656249565   Iteration 64 of 100, tot loss = 4.750168222934008, l1: 0.0001049811831990155, l2: 0.0003700356380704761   Iteration 65 of 100, tot loss = 4.724671745300293, l1: 0.00010451131193594148, l2: 0.000367955861460919   Iteration 66 of 100, tot loss = 4.708658796368224, l1: 0.00010422107383915731, l2: 0.00036664480483688584   Iteration 67 of 100, tot loss = 4.7102841619235365, l1: 0.00010452206876848254, l2: 0.000366506346808955   Iteration 68 of 100, tot loss = 4.700767348794376, l1: 0.00010433487434912582, l2: 0.0003657418601505924   Iteration 69 of 100, tot loss = 4.665629241777503, l1: 0.00010378186962570643, l2: 0.00036278105424413377   Iteration 70 of 100, tot loss = 4.65374414239611, l1: 0.00010324057158348816, l2: 0.00036213384238570665   Iteration 71 of 100, tot loss = 4.62640646813621, l1: 0.000102528305843138, l2: 0.00036011234070928636   Iteration 72 of 100, tot loss = 4.604095505343543, l1: 0.00010233800134705639, l2: 0.0003580715488674792   Iteration 73 of 100, tot loss = 4.579384232220584, l1: 0.00010186266983310654, l2: 0.00035607575320109266   Iteration 74 of 100, tot loss = 4.599587727237392, l1: 0.0001022861004064514, l2: 0.00035767267183620935   Iteration 75 of 100, tot loss = 4.619185241063436, l1: 0.00010246498519942786, l2: 0.0003594535385491326   Iteration 76 of 100, tot loss = 4.626276791095734, l1: 0.00010256833210357734, l2: 0.0003600593460616533   Iteration 77 of 100, tot loss = 4.62129926371884, l1: 0.00010228590841719566, l2: 0.0003598440166747246   Iteration 78 of 100, tot loss = 4.591435374357761, l1: 0.0001015953378858588, l2: 0.00035754819822240953   Iteration 79 of 100, tot loss = 4.601180405556401, l1: 0.00010144895787573349, l2: 0.00035866908158051864   Iteration 80 of 100, tot loss = 4.616688391566276, l1: 0.00010171102180720482, l2: 0.0003599578159992234   Iteration 81 of 100, tot loss = 4.603278392626915, l1: 0.0001012246529295046, l2: 0.0003591031851146347   Iteration 82 of 100, tot loss = 4.591140444685773, l1: 0.0001011573439893432, l2: 0.0003579566991680711   Iteration 83 of 100, tot loss = 4.567913549492158, l1: 0.0001004240505400125, l2: 0.00035636730303466096   Iteration 84 of 100, tot loss = 4.570877279554095, l1: 0.00010054745412378993, l2: 0.00035654027254037956   Iteration 85 of 100, tot loss = 4.5876150019028605, l1: 0.00010068251080263186, l2: 0.0003580789881251643   Iteration 86 of 100, tot loss = 4.597267660983773, l1: 0.00010065814311992442, l2: 0.00035906862213714913   Iteration 87 of 100, tot loss = 4.594672098927115, l1: 0.00010056993979278246, l2: 0.0003588972688483974   Iteration 88 of 100, tot loss = 4.576956071636894, l1: 0.00010017832119046943, l2: 0.00035751728467981923   Iteration 89 of 100, tot loss = 4.551065404763382, l1: 9.960658006890667e-05, l2: 0.0003554999590929849   Iteration 90 of 100, tot loss = 4.535861746470133, l1: 9.931828163745296e-05, l2: 0.0003542678915740301   Iteration 91 of 100, tot loss = 4.537997968904265, l1: 9.953646508370434e-05, l2: 0.00035426333024688475   Iteration 92 of 100, tot loss = 4.541309698768284, l1: 9.952450265067772e-05, l2: 0.00035460646588193333   Iteration 93 of 100, tot loss = 4.534157655572378, l1: 9.94660983441688e-05, l2: 0.0003539496658480055   Iteration 94 of 100, tot loss = 4.525175688114572, l1: 9.938274772448882e-05, l2: 0.00035313481961604844   Iteration 95 of 100, tot loss = 4.52129198877435, l1: 9.915635623895612e-05, l2: 0.00035297284086577985   Iteration 96 of 100, tot loss = 4.532396838068962, l1: 9.948394957367175e-05, l2: 0.00035375573255199316   Iteration 97 of 100, tot loss = 4.540984935367231, l1: 9.941776769449359e-05, l2: 0.0003546807244684086   Iteration 98 of 100, tot loss = 4.550754118938835, l1: 9.963157698417044e-05, l2: 0.0003554438334671134   Iteration 99 of 100, tot loss = 4.5550614511123815, l1: 9.975751369222622e-05, l2: 0.00035574862991008115   Iteration 100 of 100, tot loss = 4.559071884155274, l1: 9.978325600968673e-05, l2: 0.00035612393083283677
   End of epoch 1228; saving model... 

Epoch 1229 of 2000
   Iteration 1 of 100, tot loss = 4.530619144439697, l1: 0.00011463202099548653, l2: 0.0003384299052413553   Iteration 2 of 100, tot loss = 6.1644251346588135, l1: 0.00012660966967814602, l2: 0.0004898328479612246   Iteration 3 of 100, tot loss = 5.210741360982259, l1: 0.00011850810309018318, l2: 0.000402566035821413   Iteration 4 of 100, tot loss = 4.7172465324401855, l1: 0.00010992985698976554, l2: 0.0003617947986640502   Iteration 5 of 100, tot loss = 5.065071105957031, l1: 0.00011944092984776944, l2: 0.00038706618652213366   Iteration 6 of 100, tot loss = 5.281234423319499, l1: 0.00011687680550191241, l2: 0.00041124663403024897   Iteration 7 of 100, tot loss = 5.408956663949149, l1: 0.00012064365416465859, l2: 0.00042025201297032515   Iteration 8 of 100, tot loss = 5.078102916479111, l1: 0.00011638225350907305, l2: 0.0003914280387107283   Iteration 9 of 100, tot loss = 5.173516088061863, l1: 0.00011670203255360118, l2: 0.0004006495776896675   Iteration 10 of 100, tot loss = 5.028670334815979, l1: 0.0001165245244919788, l2: 0.0003863425110466778   Iteration 11 of 100, tot loss = 4.846456332640215, l1: 0.00011256898208309643, l2: 0.00037207665445748717   Iteration 12 of 100, tot loss = 4.766813377539317, l1: 0.00010999189726135228, l2: 0.0003666894444904756   Iteration 13 of 100, tot loss = 4.608231782913208, l1: 0.00010800780952334977, l2: 0.0003528153711858277   Iteration 14 of 100, tot loss = 4.977718642779759, l1: 0.0001108588174767127, l2: 0.00038691304923434345   Iteration 15 of 100, tot loss = 4.864174222946167, l1: 0.00010907187388511374, l2: 0.0003773455498352026   Iteration 16 of 100, tot loss = 4.987635508179665, l1: 0.00011164658781126491, l2: 0.00038711696379323257   Iteration 17 of 100, tot loss = 5.035595851785996, l1: 0.00011180282394086723, l2: 0.00039175676390775205   Iteration 18 of 100, tot loss = 4.983357946077983, l1: 0.00010890375577016837, l2: 0.0003894320422356638   Iteration 19 of 100, tot loss = 4.931468499334235, l1: 0.0001083584226618864, l2: 0.00038478843098824943   Iteration 20 of 100, tot loss = 4.893168079853058, l1: 0.00010567380450083874, l2: 0.0003836430078081321   Iteration 21 of 100, tot loss = 4.803888650167556, l1: 0.00010453221641780277, l2: 0.000375856652334776   Iteration 22 of 100, tot loss = 4.786536986177618, l1: 0.0001041428606020583, l2: 0.00037451084195212883   Iteration 23 of 100, tot loss = 4.818020312682442, l1: 0.00010456995685513982, l2: 0.0003772320797788384   Iteration 24 of 100, tot loss = 4.747078686952591, l1: 0.00010288493255454038, l2: 0.000371822940602821   Iteration 25 of 100, tot loss = 4.851177854537964, l1: 0.00010510678490391001, l2: 0.0003800110041629523   Iteration 26 of 100, tot loss = 4.818929149554326, l1: 0.0001041346485390722, l2: 0.0003777582695385298   Iteration 27 of 100, tot loss = 4.727447262516728, l1: 0.00010245873959825581, l2: 0.0003702859895484936   Iteration 28 of 100, tot loss = 4.660565674304962, l1: 0.00010107223546843411, l2: 0.00036498433421781683   Iteration 29 of 100, tot loss = 4.685844594034656, l1: 0.00010200811021636112, l2: 0.00036657635265046023   Iteration 30 of 100, tot loss = 4.6281891425450645, l1: 0.00010090925679833163, l2: 0.00036190966105399035   Iteration 31 of 100, tot loss = 4.56897004958122, l1: 0.00010009162090887736, l2: 0.00035680538736238714   Iteration 32 of 100, tot loss = 4.583128049969673, l1: 9.926848235863872e-05, l2: 0.000359044326160074   Iteration 33 of 100, tot loss = 4.529801571007931, l1: 9.800622788272008e-05, l2: 0.0003549739322448714   Iteration 34 of 100, tot loss = 4.525571907267851, l1: 9.807491861626178e-05, l2: 0.00035448227486888166   Iteration 35 of 100, tot loss = 4.469017362594604, l1: 9.700820612904084e-05, l2: 0.0003498935331923089   Iteration 36 of 100, tot loss = 4.555608451366425, l1: 9.921876891086059e-05, l2: 0.0003563420783999997   Iteration 37 of 100, tot loss = 4.563604039114875, l1: 9.893396634504677e-05, l2: 0.00035742643868198266   Iteration 38 of 100, tot loss = 4.589138024731686, l1: 9.882415456798442e-05, l2: 0.00036008964831891815   Iteration 39 of 100, tot loss = 4.535252632238926, l1: 9.744076268338502e-05, l2: 0.00035608450110512186   Iteration 40 of 100, tot loss = 4.523419260978699, l1: 9.760990024005878e-05, l2: 0.0003547320269717602   Iteration 41 of 100, tot loss = 4.504303920559767, l1: 9.781041045221727e-05, l2: 0.00035261998283847167   Iteration 42 of 100, tot loss = 4.561831167766026, l1: 9.860630381722113e-05, l2: 0.00035757681533661004   Iteration 43 of 100, tot loss = 4.584440242412478, l1: 9.89962560239757e-05, l2: 0.0003594477694118829   Iteration 44 of 100, tot loss = 4.558358197862452, l1: 9.885741722180022e-05, l2: 0.00035697840377741323   Iteration 45 of 100, tot loss = 4.581724320517646, l1: 9.89713975286577e-05, l2: 0.00035920103578569577   Iteration 46 of 100, tot loss = 4.550551419672758, l1: 9.860847920050298e-05, l2: 0.0003564466637528866   Iteration 47 of 100, tot loss = 4.591240238636098, l1: 9.945960707053483e-05, l2: 0.0003596644178577798   Iteration 48 of 100, tot loss = 4.5908073137203855, l1: 9.969572730976022e-05, l2: 0.00035938500514021143   Iteration 49 of 100, tot loss = 4.600817023491373, l1: 0.00010021021288204273, l2: 0.00035987149006972205   Iteration 50 of 100, tot loss = 4.586023626327514, l1: 0.00010020052919571753, l2: 0.0003584018343826756   Iteration 51 of 100, tot loss = 4.602488545810475, l1: 0.00010066428277401857, l2: 0.00035958457281630413   Iteration 52 of 100, tot loss = 4.685193731234624, l1: 0.00010138234903514295, l2: 0.00036713702474326757   Iteration 53 of 100, tot loss = 4.6908916977216615, l1: 0.00010198039678968753, l2: 0.0003671087730974662   Iteration 54 of 100, tot loss = 4.685954659073441, l1: 0.00010225041673779574, l2: 0.0003663450490286643   Iteration 55 of 100, tot loss = 4.686703803322532, l1: 0.00010264054164343344, l2: 0.00036602983812124214   Iteration 56 of 100, tot loss = 4.674193833555494, l1: 0.00010222730887627611, l2: 0.0003651920742413495   Iteration 57 of 100, tot loss = 4.689029384077641, l1: 0.00010211533771597028, l2: 0.0003667876002860762   Iteration 58 of 100, tot loss = 4.700112704573007, l1: 0.00010217400110910791, l2: 0.0003678372690799357   Iteration 59 of 100, tot loss = 4.672935251462257, l1: 0.0001018001525677638, l2: 0.0003654933719544561   Iteration 60 of 100, tot loss = 4.651180899143219, l1: 0.00010164914183405927, l2: 0.00036346894751962584   Iteration 61 of 100, tot loss = 4.673012948427044, l1: 0.00010163066571160433, l2: 0.0003656706294201345   Iteration 62 of 100, tot loss = 4.638133514312006, l1: 0.00010070028750511307, l2: 0.000363113064292608   Iteration 63 of 100, tot loss = 4.664148228509085, l1: 0.00010098633240900993, l2: 0.0003654284911814131   Iteration 64 of 100, tot loss = 4.637356992810965, l1: 0.00010060203032935533, l2: 0.0003631336694525089   Iteration 65 of 100, tot loss = 4.619385077403142, l1: 0.00010037789548872612, l2: 0.00036156061239755495   Iteration 66 of 100, tot loss = 4.640378868941105, l1: 0.0001007686692129496, l2: 0.00036326921800730014   Iteration 67 of 100, tot loss = 4.6019074667745565, l1: 0.00010009122467921255, l2: 0.0003600995223135201   Iteration 68 of 100, tot loss = 4.576680989826427, l1: 9.980233008879141e-05, l2: 0.0003578657693544622   Iteration 69 of 100, tot loss = 4.561457948408265, l1: 9.973210404311523e-05, l2: 0.000356413691011055   Iteration 70 of 100, tot loss = 4.549302894728524, l1: 9.967006673312945e-05, l2: 0.0003552602229839457   Iteration 71 of 100, tot loss = 4.583451650512051, l1: 0.00010004648013396228, l2: 0.00035829868489010655   Iteration 72 of 100, tot loss = 4.631286611159642, l1: 0.00010086771286902756, l2: 0.00036226094865317765   Iteration 73 of 100, tot loss = 4.608029489647852, l1: 0.00010080078124564639, l2: 0.0003600021681113628   Iteration 74 of 100, tot loss = 4.6256206100051465, l1: 0.00010106002539671592, l2: 0.0003615020362524995   Iteration 75 of 100, tot loss = 4.612694234848022, l1: 0.00010101519403785156, l2: 0.0003602542303269729   Iteration 76 of 100, tot loss = 4.637257591674202, l1: 0.00010138510692501625, l2: 0.00036234065287556546   Iteration 77 of 100, tot loss = 4.649991589707214, l1: 0.00010149569241004972, l2: 0.00036350346768317235   Iteration 78 of 100, tot loss = 4.668549711887653, l1: 0.0001016611905260316, l2: 0.0003651937815727881   Iteration 79 of 100, tot loss = 4.6571543518500995, l1: 0.00010171524171468425, l2: 0.00036400019451297963   Iteration 80 of 100, tot loss = 4.652069720625877, l1: 0.0001017587601836567, l2: 0.00036344821255624994   Iteration 81 of 100, tot loss = 4.632443213168486, l1: 0.00010136587549407714, l2: 0.0003618784464412817   Iteration 82 of 100, tot loss = 4.633528497160935, l1: 0.00010150478791307782, l2: 0.0003618480624892272   Iteration 83 of 100, tot loss = 4.606316468801843, l1: 0.00010096508579529228, l2: 0.0003596665616447375   Iteration 84 of 100, tot loss = 4.59559934196018, l1: 0.00010053441686371419, l2: 0.0003590255179179145   Iteration 85 of 100, tot loss = 4.593127130059635, l1: 0.00010047281098463025, l2: 0.0003588399026364855   Iteration 86 of 100, tot loss = 4.581476979477461, l1: 0.00010044868746305393, l2: 0.00035769901110460373   Iteration 87 of 100, tot loss = 4.559938321168396, l1: 0.00010009328056101408, l2: 0.0003559005523004152   Iteration 88 of 100, tot loss = 4.543781299482692, l1: 9.978397615255248e-05, l2: 0.0003545941548178565   Iteration 89 of 100, tot loss = 4.544527667292049, l1: 0.00010003862459260938, l2: 0.0003544141432490158   Iteration 90 of 100, tot loss = 4.544961603482564, l1: 0.00010000245929404627, l2: 0.00035449370237377784   Iteration 91 of 100, tot loss = 4.565566825342702, l1: 0.00010033396540745674, l2: 0.00035622271844748776   Iteration 92 of 100, tot loss = 4.542787341967873, l1: 9.999817978931115e-05, l2: 0.0003542805558105971   Iteration 93 of 100, tot loss = 4.559430581267162, l1: 0.00010031860629853953, l2: 0.0003556244535952486   Iteration 94 of 100, tot loss = 4.556524335069859, l1: 0.0001003227520259122, l2: 0.0003553296834418669   Iteration 95 of 100, tot loss = 4.551318321729961, l1: 0.00010019061894701353, l2: 0.00035494121532936236   Iteration 96 of 100, tot loss = 4.542734990517299, l1: 0.00010009242468337713, l2: 0.00035418107654550113   Iteration 97 of 100, tot loss = 4.541245381856702, l1: 0.00010026486614657825, l2: 0.0003538596742071173   Iteration 98 of 100, tot loss = 4.5743386988737145, l1: 0.0001008828440921652, l2: 0.0003565510278163782   Iteration 99 of 100, tot loss = 4.561465639056581, l1: 0.00010072295473873229, l2: 0.00035542361126657615   Iteration 100 of 100, tot loss = 4.549649970531464, l1: 0.00010059531614388106, l2: 0.0003543696831911802
   End of epoch 1229; saving model... 

Epoch 1230 of 2000
   Iteration 1 of 100, tot loss = 2.827770948410034, l1: 5.492237687576562e-05, l2: 0.00022785471810493618   Iteration 2 of 100, tot loss = 3.584464192390442, l1: 8.48567760840524e-05, l2: 0.0002735896341619082   Iteration 3 of 100, tot loss = 3.437314748764038, l1: 8.332514941381912e-05, l2: 0.0002604063192848116   Iteration 4 of 100, tot loss = 3.9244571328163147, l1: 8.941713895183057e-05, l2: 0.00030302857339847833   Iteration 5 of 100, tot loss = 4.698078870773315, l1: 0.00010223230929113924, l2: 0.0003675755811855197   Iteration 6 of 100, tot loss = 4.583419521649678, l1: 9.97005590761546e-05, l2: 0.00035864139499608427   Iteration 7 of 100, tot loss = 4.587669951575143, l1: 0.00010076068429043517, l2: 0.0003580063140751528   Iteration 8 of 100, tot loss = 4.745553523302078, l1: 0.00010440207643114263, l2: 0.0003701532768900506   Iteration 9 of 100, tot loss = 4.689215951495701, l1: 0.00010437960170545719, l2: 0.0003645419945112533   Iteration 10 of 100, tot loss = 4.578250479698181, l1: 0.0001025480371026788, l2: 0.00035527701256796715   Iteration 11 of 100, tot loss = 4.9771013043143535, l1: 0.00010891837948954411, l2: 0.00038879175289449364   Iteration 12 of 100, tot loss = 4.9286161462465925, l1: 0.00010650191567644167, l2: 0.0003863597012241371   Iteration 13 of 100, tot loss = 5.072036137947669, l1: 0.00010942859136356183, l2: 0.0003977750244442947   Iteration 14 of 100, tot loss = 5.041077937398638, l1: 0.00010946816317820256, l2: 0.0003946396318497136   Iteration 15 of 100, tot loss = 4.915098873774211, l1: 0.00010757269046735018, l2: 0.0003839371968448783   Iteration 16 of 100, tot loss = 5.021160736680031, l1: 0.00010865335116250208, l2: 0.0003934627247872413   Iteration 17 of 100, tot loss = 4.981455508400412, l1: 0.00010945822235525531, l2: 0.0003886873284286326   Iteration 18 of 100, tot loss = 4.908037106196086, l1: 0.0001085539940201367, l2: 0.0003822497164946981   Iteration 19 of 100, tot loss = 4.876472924884997, l1: 0.00010895245175465549, l2: 0.0003786948409948596   Iteration 20 of 100, tot loss = 4.915331840515137, l1: 0.00010769764594442677, l2: 0.00038383553983294404   Iteration 21 of 100, tot loss = 4.770484169324239, l1: 0.00010435878790477069, l2: 0.0003726896303262384   Iteration 22 of 100, tot loss = 4.729379691860893, l1: 0.00010478414448946502, l2: 0.00036815382587850433   Iteration 23 of 100, tot loss = 4.79050946753958, l1: 0.00010503380374006076, l2: 0.00037401714539357823   Iteration 24 of 100, tot loss = 4.676603024204572, l1: 0.00010326378272414634, l2: 0.0003643965216421445   Iteration 25 of 100, tot loss = 4.673500332832337, l1: 0.00010450197951286099, l2: 0.00036284805566538125   Iteration 26 of 100, tot loss = 4.719246667165023, l1: 0.00010600970717160426, l2: 0.0003659149599395907   Iteration 27 of 100, tot loss = 4.711492030708878, l1: 0.00010624579172818576, l2: 0.0003649034107285034   Iteration 28 of 100, tot loss = 4.618285736867359, l1: 0.0001038439813783043, l2: 0.00035798459170369564   Iteration 29 of 100, tot loss = 4.678677867198813, l1: 0.00010398974763088988, l2: 0.0003638780381779411   Iteration 30 of 100, tot loss = 4.596475478013357, l1: 0.00010297915614501107, l2: 0.0003566683910321444   Iteration 31 of 100, tot loss = 4.545567908594685, l1: 0.00010253190081247368, l2: 0.0003520248893408045   Iteration 32 of 100, tot loss = 4.5071696899831295, l1: 0.00010239325786187692, l2: 0.00034832371056836564   Iteration 33 of 100, tot loss = 4.485113378727075, l1: 0.0001014420853276864, l2: 0.0003470692520777723   Iteration 34 of 100, tot loss = 4.5612732277197, l1: 0.00010231304526421488, l2: 0.0003538142764762811   Iteration 35 of 100, tot loss = 4.567869312422616, l1: 0.00010252366659447684, l2: 0.00035426326378780817   Iteration 36 of 100, tot loss = 4.637756059567134, l1: 0.0001038143480198212, l2: 0.00035996125613261637   Iteration 37 of 100, tot loss = 4.6323752435478, l1: 0.00010353932245650228, l2: 0.00035969820030889397   Iteration 38 of 100, tot loss = 4.631574596229353, l1: 0.00010391013682620771, l2: 0.00035924732142838796   Iteration 39 of 100, tot loss = 4.599331675431667, l1: 0.00010322101736211409, l2: 0.0003567121494621134   Iteration 40 of 100, tot loss = 4.556843361258506, l1: 0.00010254508024445385, l2: 0.0003531392550939927   Iteration 41 of 100, tot loss = 4.621948844049035, l1: 0.00010306504705889582, l2: 0.0003591298367742949   Iteration 42 of 100, tot loss = 4.6297164389065335, l1: 0.00010326842649582042, l2: 0.0003597032174140969   Iteration 43 of 100, tot loss = 4.597633436668751, l1: 0.00010236168981104721, l2: 0.0003574016537154527   Iteration 44 of 100, tot loss = 4.588413181630048, l1: 0.00010197055507887853, l2: 0.0003568707625990183   Iteration 45 of 100, tot loss = 4.599118611547682, l1: 0.00010252944331215177, l2: 0.00035738241697092437   Iteration 46 of 100, tot loss = 4.550994051539379, l1: 0.00010154285353011169, l2: 0.00035355655089243436   Iteration 47 of 100, tot loss = 4.520610629244054, l1: 0.00010090901304601118, l2: 0.0003511520487162225   Iteration 48 of 100, tot loss = 4.523489626745383, l1: 0.00010149028397184641, l2: 0.0003508586775448445   Iteration 49 of 100, tot loss = 4.526424050331116, l1: 0.00010163477863712541, l2: 0.00035100762538697416   Iteration 50 of 100, tot loss = 4.508084275722504, l1: 0.00010107704380061477, l2: 0.00034973138273926453   Iteration 51 of 100, tot loss = 4.496350066334593, l1: 0.00010143422325342602, l2: 0.0003482007821588138   Iteration 52 of 100, tot loss = 4.511912054740465, l1: 0.00010092994992961534, l2: 0.0003502612551248883   Iteration 53 of 100, tot loss = 4.5620978215955335, l1: 0.00010176702679293055, l2: 0.000354442755606043   Iteration 54 of 100, tot loss = 4.576574054029253, l1: 0.00010220222180510905, l2: 0.0003554551839004961   Iteration 55 of 100, tot loss = 4.60332961732691, l1: 0.00010210285505639727, l2: 0.0003582301065431569   Iteration 56 of 100, tot loss = 4.62858758015292, l1: 0.000102488474632472, l2: 0.0003603702833580818   Iteration 57 of 100, tot loss = 4.646470448427033, l1: 0.00010278451883462829, l2: 0.0003618625257666664   Iteration 58 of 100, tot loss = 4.660099015153688, l1: 0.00010273496107025682, l2: 0.0003632749403236787   Iteration 59 of 100, tot loss = 4.653411309597856, l1: 0.00010266751768679153, l2: 0.0003626736133039724   Iteration 60 of 100, tot loss = 4.665054490168889, l1: 0.00010267842674996549, l2: 0.0003638270221320757   Iteration 61 of 100, tot loss = 4.668163747083946, l1: 0.00010313170703918078, l2: 0.0003636846675243626   Iteration 62 of 100, tot loss = 4.719435839883743, l1: 0.00010352641601780159, l2: 0.00036841716815439623   Iteration 63 of 100, tot loss = 4.722471958114987, l1: 0.0001034593356991305, l2: 0.00036878786063642197   Iteration 64 of 100, tot loss = 4.721865909174085, l1: 0.00010349617753036, l2: 0.0003686904135520308   Iteration 65 of 100, tot loss = 4.709488991590646, l1: 0.00010321857788277646, l2: 0.00036773032155067013   Iteration 66 of 100, tot loss = 4.746350662274794, l1: 0.00010348470283188914, l2: 0.0003711503639792544   Iteration 67 of 100, tot loss = 4.7512172211462, l1: 0.0001039857216615494, l2: 0.0003711360012817044   Iteration 68 of 100, tot loss = 4.767782476018457, l1: 0.0001043821979264441, l2: 0.0003723960508068558   Iteration 69 of 100, tot loss = 4.735000107599341, l1: 0.00010389905503910521, l2: 0.0003696009569440334   Iteration 70 of 100, tot loss = 4.727702564852578, l1: 0.00010367537127292183, l2: 0.00036909488657589206   Iteration 71 of 100, tot loss = 4.734190741055448, l1: 0.00010364565231368392, l2: 0.00036977342363532213   Iteration 72 of 100, tot loss = 4.73129045466582, l1: 0.00010370981716631731, l2: 0.00036941922998165764   Iteration 73 of 100, tot loss = 4.741489705974108, l1: 0.00010381982060171638, l2: 0.0003703291516365126   Iteration 74 of 100, tot loss = 4.729413955598264, l1: 0.00010368006248451845, l2: 0.00036926133455110813   Iteration 75 of 100, tot loss = 4.710239230791728, l1: 0.00010350457014283166, l2: 0.0003675193547193582   Iteration 76 of 100, tot loss = 4.736454899373808, l1: 0.00010391038918705338, l2: 0.000369735102523057   Iteration 77 of 100, tot loss = 4.73740262954266, l1: 0.00010403633496025577, l2: 0.0003697039299060941   Iteration 78 of 100, tot loss = 4.731996112909073, l1: 0.00010406585435983009, l2: 0.0003691337585153703   Iteration 79 of 100, tot loss = 4.719809591015683, l1: 0.00010389861093739731, l2: 0.0003680823497176524   Iteration 80 of 100, tot loss = 4.71689999550581, l1: 0.00010419246573292184, l2: 0.00036749753508047434   Iteration 81 of 100, tot loss = 4.7379972066408325, l1: 0.00010465186255995506, l2: 0.0003691478591070246   Iteration 82 of 100, tot loss = 4.74974557684689, l1: 0.00010485070237248182, l2: 0.0003701238561117817   Iteration 83 of 100, tot loss = 4.751377638564052, l1: 0.00010485226645748725, l2: 0.0003702854977833691   Iteration 84 of 100, tot loss = 4.754350598369326, l1: 0.00010482511944636437, l2: 0.0003706099404163459   Iteration 85 of 100, tot loss = 4.75268086124869, l1: 0.0001047774392645806, l2: 0.00037049064669511555   Iteration 86 of 100, tot loss = 4.744426027286885, l1: 0.00010474685074959145, l2: 0.00036969575215385533   Iteration 87 of 100, tot loss = 4.76420273040903, l1: 0.00010535149357011328, l2: 0.0003710687794420204   Iteration 88 of 100, tot loss = 4.7704541019418025, l1: 0.00010573754670159278, l2: 0.00037130786371232136   Iteration 89 of 100, tot loss = 4.774290147792088, l1: 0.00010573274450732415, l2: 0.00037169627000949233   Iteration 90 of 100, tot loss = 4.8106057577663, l1: 0.00010621813853504137, l2: 0.00037484243722347955   Iteration 91 of 100, tot loss = 4.852082492231013, l1: 0.00010660241097646797, l2: 0.0003786058381652705   Iteration 92 of 100, tot loss = 4.831262994071712, l1: 0.00010613742628977533, l2: 0.0003769888729869348   Iteration 93 of 100, tot loss = 4.834821240876311, l1: 0.00010621554028275373, l2: 0.00037726658391415755   Iteration 94 of 100, tot loss = 4.8064302117266555, l1: 0.00010561042975670251, l2: 0.0003750325914997449   Iteration 95 of 100, tot loss = 4.793384081438968, l1: 0.00010536865206392432, l2: 0.00037396975615257885   Iteration 96 of 100, tot loss = 4.793271006395419, l1: 0.00010546251576215582, l2: 0.00037386458492013236   Iteration 97 of 100, tot loss = 4.787869784020886, l1: 0.00010549708647736272, l2: 0.00037328989188590076   Iteration 98 of 100, tot loss = 4.756453291494019, l1: 0.00010480638782758675, l2: 0.0003708389413077384   Iteration 99 of 100, tot loss = 4.744597123126791, l1: 0.00010464783601969659, l2: 0.00036981187624424096   Iteration 100 of 100, tot loss = 4.737651554346084, l1: 0.00010456287916895235, l2: 0.0003692022763425484
   End of epoch 1230; saving model... 

Epoch 1231 of 2000
   Iteration 1 of 100, tot loss = 2.3183445930480957, l1: 8.51159420562908e-05, l2: 0.00014671852113679051   Iteration 2 of 100, tot loss = 4.027144432067871, l1: 0.00010746294719865546, l2: 0.00029525149147957563   Iteration 3 of 100, tot loss = 4.323073228200276, l1: 0.00010186866469060381, l2: 0.0003304386530847599   Iteration 4 of 100, tot loss = 4.428165555000305, l1: 0.00010236833441013005, l2: 0.0003404482122277841   Iteration 5 of 100, tot loss = 4.806220531463623, l1: 0.00011101082345703617, l2: 0.0003696112195029855   Iteration 6 of 100, tot loss = 4.883632183074951, l1: 0.00011249859138236691, l2: 0.00037586461985483766   Iteration 7 of 100, tot loss = 4.978762626647949, l1: 0.00011092592363378831, l2: 0.0003869503312411585   Iteration 8 of 100, tot loss = 4.997154712677002, l1: 0.00011364037709427066, l2: 0.0003860750875901431   Iteration 9 of 100, tot loss = 4.9383871290418835, l1: 0.00011378366131490718, l2: 0.0003800550443379001   Iteration 10 of 100, tot loss = 4.9481974124908445, l1: 0.00011391768130124547, l2: 0.0003809020563494414   Iteration 11 of 100, tot loss = 4.946347279982134, l1: 0.00011150527544404295, l2: 0.0003831294518683783   Iteration 12 of 100, tot loss = 4.943210919698079, l1: 0.00011192048320178098, l2: 0.00038240060772902024   Iteration 13 of 100, tot loss = 4.965085066281832, l1: 0.00011266743520256848, l2: 0.00038384107084801566   Iteration 14 of 100, tot loss = 4.756991522652762, l1: 0.00010760363797349523, l2: 0.0003680955144643251   Iteration 15 of 100, tot loss = 4.733941586812337, l1: 0.00010572138877857166, l2: 0.00036767276857669153   Iteration 16 of 100, tot loss = 4.661829873919487, l1: 0.00010429272992951155, l2: 0.00036189025558996946   Iteration 17 of 100, tot loss = 4.70726606425117, l1: 0.00010555035709437695, l2: 0.0003651762466795523   Iteration 18 of 100, tot loss = 4.7892939117219715, l1: 0.00010696612965451398, l2: 0.0003719632578496304   Iteration 19 of 100, tot loss = 4.717446364854512, l1: 0.0001051700564858038, l2: 0.00036657457607553194   Iteration 20 of 100, tot loss = 4.660779452323913, l1: 0.00010390284969616915, l2: 0.0003621750918682665   Iteration 21 of 100, tot loss = 4.662961301349458, l1: 0.00010308411219739355, l2: 0.0003632120136725938   Iteration 22 of 100, tot loss = 4.611764073371887, l1: 0.00010316139394655528, l2: 0.0003580150091279806   Iteration 23 of 100, tot loss = 4.5716346450473955, l1: 0.0001010435436277792, l2: 0.0003561199169464247   Iteration 24 of 100, tot loss = 4.522239804267883, l1: 0.00010017984626150185, l2: 0.0003520441302195347   Iteration 25 of 100, tot loss = 4.655894432067871, l1: 0.00010203228550381027, l2: 0.0003635571536142379   Iteration 26 of 100, tot loss = 4.633649569291335, l1: 0.00010211915720831782, l2: 0.0003612457949202508   Iteration 27 of 100, tot loss = 4.538234030758893, l1: 0.0001005252599026318, l2: 0.0003532981386848001   Iteration 28 of 100, tot loss = 4.643111935683659, l1: 0.00010156458146021967, l2: 0.0003627466082564622   Iteration 29 of 100, tot loss = 4.647767519128734, l1: 0.00010140834713418936, l2: 0.0003633683999992329   Iteration 30 of 100, tot loss = 4.636092209815979, l1: 0.00010108985564632652, l2: 0.0003625193605936753   Iteration 31 of 100, tot loss = 4.633547052260368, l1: 0.00010100129739223649, l2: 0.00036235340362069225   Iteration 32 of 100, tot loss = 4.710308559238911, l1: 0.0001013802559555188, l2: 0.00036965059598514927   Iteration 33 of 100, tot loss = 4.720951289841623, l1: 0.00010182444671183035, l2: 0.0003702706788758268   Iteration 34 of 100, tot loss = 4.682966667063096, l1: 0.00010170635334147578, l2: 0.00036659030978198107   Iteration 35 of 100, tot loss = 4.713650158473423, l1: 0.00010254818438884936, l2: 0.0003688168292033619   Iteration 36 of 100, tot loss = 4.642957839700911, l1: 0.00010155793238987422, l2: 0.0003627378496781199   Iteration 37 of 100, tot loss = 4.6329387136407805, l1: 0.00010099787519917495, l2: 0.0003622959948434318   Iteration 38 of 100, tot loss = 4.657423063328392, l1: 0.00010150858093095992, l2: 0.00036423372435628583   Iteration 39 of 100, tot loss = 4.6566611131032305, l1: 0.00010153915000279457, l2: 0.00036412695995890175   Iteration 40 of 100, tot loss = 4.602580028772354, l1: 0.00010008954250224633, l2: 0.00036016845915582963   Iteration 41 of 100, tot loss = 4.66572244574384, l1: 0.00010069495719352101, l2: 0.00036587728621869736   Iteration 42 of 100, tot loss = 4.691736499468486, l1: 0.00010104167259192937, l2: 0.00036813197602030066   Iteration 43 of 100, tot loss = 4.720690588618433, l1: 0.00010149837349714754, l2: 0.00037057068385725275   Iteration 44 of 100, tot loss = 4.674168234521693, l1: 0.00010035894129859199, l2: 0.00036705788046343844   Iteration 45 of 100, tot loss = 4.69306476910909, l1: 0.00010096468387119886, l2: 0.00036834179158581215   Iteration 46 of 100, tot loss = 4.655372806217359, l1: 9.991824868927284e-05, l2: 0.0003656190301230906   Iteration 47 of 100, tot loss = 4.626060140893815, l1: 9.879626962542039e-05, l2: 0.00036380974268738895   Iteration 48 of 100, tot loss = 4.646441111962001, l1: 9.921530871300395e-05, l2: 0.0003654288005539759   Iteration 49 of 100, tot loss = 4.66913484067333, l1: 9.9410009151681e-05, l2: 0.0003675034736539712   Iteration 50 of 100, tot loss = 4.698856887817382, l1: 9.966473844542633e-05, l2: 0.0003702209488255903   Iteration 51 of 100, tot loss = 4.661961709751802, l1: 9.927286929964526e-05, l2: 0.00036692330025949095   Iteration 52 of 100, tot loss = 4.6430878180723925, l1: 9.934439696315586e-05, l2: 0.00036496438322221645   Iteration 53 of 100, tot loss = 4.717404572468884, l1: 0.00010048726567820611, l2: 0.0003712531890311577   Iteration 54 of 100, tot loss = 4.712383605815746, l1: 0.00010061167382378631, l2: 0.0003706266841079384   Iteration 55 of 100, tot loss = 4.708922542225231, l1: 0.00010078369847509417, l2: 0.0003701085534306582   Iteration 56 of 100, tot loss = 4.690552532672882, l1: 0.00010040274478991964, l2: 0.000368652506430018   Iteration 57 of 100, tot loss = 4.698734542779755, l1: 0.00010100761146372461, l2: 0.0003688658407395052   Iteration 58 of 100, tot loss = 4.682104677989565, l1: 0.0001005476889735417, l2: 0.00036766277681353726   Iteration 59 of 100, tot loss = 4.661969669794632, l1: 0.00010034959743310445, l2: 0.0003658473677637245   Iteration 60 of 100, tot loss = 4.648982938130697, l1: 0.00010039105321387372, l2: 0.0003645072392828297   Iteration 61 of 100, tot loss = 4.664690822851463, l1: 0.00010054280424870352, l2: 0.0003659262765126425   Iteration 62 of 100, tot loss = 4.631223109460646, l1: 9.99253607592583e-05, l2: 0.0003631969485732336   Iteration 63 of 100, tot loss = 4.636029977647085, l1: 0.00010006704825274128, l2: 0.0003635359476209574   Iteration 64 of 100, tot loss = 4.656476892530918, l1: 0.00010057121022555293, l2: 0.0003650764770100068   Iteration 65 of 100, tot loss = 4.6659678752605735, l1: 0.00010046439033780748, l2: 0.00036613239521662204   Iteration 66 of 100, tot loss = 4.6696452372001875, l1: 0.00010064498482361986, l2: 0.0003663195366794808   Iteration 67 of 100, tot loss = 4.6626715233076865, l1: 0.00010035451226424663, l2: 0.00036591263757254096   Iteration 68 of 100, tot loss = 4.684061618412242, l1: 0.0001006966340890572, l2: 0.0003677095249740352   Iteration 69 of 100, tot loss = 4.662982149400573, l1: 0.00010029535216112298, l2: 0.0003660028598750469   Iteration 70 of 100, tot loss = 4.682407988820757, l1: 0.0001003592705923698, l2: 0.00036788152522473997   Iteration 71 of 100, tot loss = 4.637519960672083, l1: 9.942130587583499e-05, l2: 0.0003643306870952996   Iteration 72 of 100, tot loss = 4.607182025909424, l1: 9.878674225344892e-05, l2: 0.0003619314572157843   Iteration 73 of 100, tot loss = 4.641259931538203, l1: 9.976162105537185e-05, l2: 0.0003643643691273062   Iteration 74 of 100, tot loss = 4.665123333802095, l1: 9.998150720069109e-05, l2: 0.0003665308235016513   Iteration 75 of 100, tot loss = 4.65533395131429, l1: 9.964572032913565e-05, l2: 0.0003658876722329296   Iteration 76 of 100, tot loss = 4.644330473322618, l1: 9.975932296488042e-05, l2: 0.00036467372171357154   Iteration 77 of 100, tot loss = 4.655041988793906, l1: 9.993004585032702e-05, l2: 0.00036557415088671926   Iteration 78 of 100, tot loss = 4.680045473269927, l1: 0.00010013470362024143, l2: 0.0003678698415639631   Iteration 79 of 100, tot loss = 4.7182450385033325, l1: 0.00010097606212219011, l2: 0.0003708484397480512   Iteration 80 of 100, tot loss = 4.723669418692589, l1: 0.00010118585414602421, l2: 0.0003711810858476383   Iteration 81 of 100, tot loss = 4.712111570216991, l1: 0.00010122115692169761, l2: 0.000369989998292977   Iteration 82 of 100, tot loss = 4.682594540642529, l1: 0.00010071238679435138, l2: 0.0003675470655186318   Iteration 83 of 100, tot loss = 4.670648324920471, l1: 0.00010045637547196988, l2: 0.00036660845541179246   Iteration 84 of 100, tot loss = 4.654105308510008, l1: 0.00010020003983732923, l2: 0.00036521048943021536   Iteration 85 of 100, tot loss = 4.660647737278658, l1: 0.0001003389983345508, l2: 0.0003657257741639007   Iteration 86 of 100, tot loss = 4.664766763531884, l1: 0.00010056481462311768, l2: 0.00036591186036212303   Iteration 87 of 100, tot loss = 4.660903851191203, l1: 0.00010080444026870878, l2: 0.0003652859437540334   Iteration 88 of 100, tot loss = 4.65584734082222, l1: 0.00010071262522615143, l2: 0.0003648721079695282   Iteration 89 of 100, tot loss = 4.665852051102713, l1: 0.00010083549499172937, l2: 0.00036574970874242126   Iteration 90 of 100, tot loss = 4.651257321569655, l1: 0.0001006494721271641, l2: 0.00036447625857868437   Iteration 91 of 100, tot loss = 4.630858953182514, l1: 0.00010040579134231218, l2: 0.00036268010260575646   Iteration 92 of 100, tot loss = 4.621469793112381, l1: 0.00010002673214963262, l2: 0.00036212024600029946   Iteration 93 of 100, tot loss = 4.60279009931831, l1: 9.990519460169248e-05, l2: 0.0003603738140388672   Iteration 94 of 100, tot loss = 4.592317583713125, l1: 9.995039080751396e-05, l2: 0.00035928136616774356   Iteration 95 of 100, tot loss = 4.604919451161435, l1: 0.00010039215087186006, l2: 0.0003600997932249141   Iteration 96 of 100, tot loss = 4.638457449773948, l1: 0.00010102281722386881, l2: 0.00036282292686943646   Iteration 97 of 100, tot loss = 4.612656797330404, l1: 0.00010047945967748204, l2: 0.0003607862192588935   Iteration 98 of 100, tot loss = 4.612559384229232, l1: 0.00010040158222367683, l2: 0.00036085435515685404   Iteration 99 of 100, tot loss = 4.62480920011347, l1: 0.0001005402581990847, l2: 0.0003619406605580342   Iteration 100 of 100, tot loss = 4.6482573103904725, l1: 0.00010097693600982893, l2: 0.0003638487937132595
   End of epoch 1231; saving model... 

Epoch 1232 of 2000
   Iteration 1 of 100, tot loss = 4.852561950683594, l1: 0.0001185689470730722, l2: 0.0003666872507892549   Iteration 2 of 100, tot loss = 4.950827360153198, l1: 9.134762876783498e-05, l2: 0.0004037350881844759   Iteration 3 of 100, tot loss = 4.606283187866211, l1: 8.790876988011102e-05, l2: 0.00037271953381908435   Iteration 4 of 100, tot loss = 4.527202486991882, l1: 8.938119208323769e-05, l2: 0.0003633390515460633   Iteration 5 of 100, tot loss = 4.253084135055542, l1: 8.857317880028859e-05, l2: 0.0003367352328496054   Iteration 6 of 100, tot loss = 4.590523521105449, l1: 9.524686053434077e-05, l2: 0.0003638054889355165   Iteration 7 of 100, tot loss = 4.8114845752716064, l1: 9.93044367143219e-05, l2: 0.0003818440205317789   Iteration 8 of 100, tot loss = 4.77339032292366, l1: 9.65030922088772e-05, l2: 0.00038083594154159073   Iteration 9 of 100, tot loss = 4.832241561677721, l1: 9.773429014280232e-05, l2: 0.0003854898661504396   Iteration 10 of 100, tot loss = 4.920844388008118, l1: 9.716017593746074e-05, l2: 0.00039492426585638893   Iteration 11 of 100, tot loss = 4.843024665659124, l1: 9.630807207114148e-05, l2: 0.00038799439392856914   Iteration 12 of 100, tot loss = 4.89984518289566, l1: 9.799587921103618e-05, l2: 0.0003919886403309647   Iteration 13 of 100, tot loss = 5.00770196547875, l1: 0.00010183243956327296, l2: 0.0003989377571717621   Iteration 14 of 100, tot loss = 4.924769742148263, l1: 0.00010006641397402356, l2: 0.00039241056012022976   Iteration 15 of 100, tot loss = 4.81725827852885, l1: 9.932632819982245e-05, l2: 0.0003823994988730798   Iteration 16 of 100, tot loss = 4.895888224244118, l1: 0.00010120525121237733, l2: 0.00038838356886117253   Iteration 17 of 100, tot loss = 4.819829365786384, l1: 0.00010158892623354298, l2: 0.0003803940079010585   Iteration 18 of 100, tot loss = 4.821459333101909, l1: 0.00010232942966265707, l2: 0.00037981650125907943   Iteration 19 of 100, tot loss = 4.70018714352658, l1: 9.992625123949868e-05, l2: 0.0003700924617556953   Iteration 20 of 100, tot loss = 4.747096264362336, l1: 0.00010025395586126251, l2: 0.00037445566995302214   Iteration 21 of 100, tot loss = 4.634770518257504, l1: 9.873550828030732e-05, l2: 0.00036474154330790043   Iteration 22 of 100, tot loss = 4.61629931493239, l1: 9.845254135143477e-05, l2: 0.0003631773896896365   Iteration 23 of 100, tot loss = 4.583702595337577, l1: 9.880736283578344e-05, l2: 0.00035956289629330456   Iteration 24 of 100, tot loss = 4.543153524398804, l1: 9.930391646169785e-05, l2: 0.000355011435506943   Iteration 25 of 100, tot loss = 4.555407733917236, l1: 9.975824053981341e-05, l2: 0.00035578253329731523   Iteration 26 of 100, tot loss = 4.54570720745967, l1: 9.951870868882487e-05, l2: 0.0003550520117502086   Iteration 27 of 100, tot loss = 4.50777664007964, l1: 9.956638960930932e-05, l2: 0.0003512112738530117   Iteration 28 of 100, tot loss = 4.517356864043644, l1: 9.937190757877293e-05, l2: 0.00035236377880210057   Iteration 29 of 100, tot loss = 4.488047172283304, l1: 9.93846031257096e-05, l2: 0.00034942011377003427   Iteration 30 of 100, tot loss = 4.556646951039633, l1: 9.988364533152587e-05, l2: 0.0003557810491959875   Iteration 31 of 100, tot loss = 4.49948093968053, l1: 9.900801559260506e-05, l2: 0.00035094007738910977   Iteration 32 of 100, tot loss = 4.4811346754431725, l1: 9.974511510790762e-05, l2: 0.0003483683517515601   Iteration 33 of 100, tot loss = 4.525540460239757, l1: 0.00010041197795847741, l2: 0.00035214206780603325   Iteration 34 of 100, tot loss = 4.616460021804361, l1: 0.0001015036504053141, l2: 0.00036014235015232666   Iteration 35 of 100, tot loss = 4.634188100269863, l1: 0.00010222718462630708, l2: 0.0003611916231290836   Iteration 36 of 100, tot loss = 4.6649319132169085, l1: 0.00010323470036787007, l2: 0.00036325848850538023   Iteration 37 of 100, tot loss = 4.720006395030666, l1: 0.00010384229687121194, l2: 0.0003681583396895361   Iteration 38 of 100, tot loss = 4.723911944188569, l1: 0.00010277989071925588, l2: 0.0003696113007639437   Iteration 39 of 100, tot loss = 4.731278413381332, l1: 0.00010328926920266941, l2: 0.0003698385684028602   Iteration 40 of 100, tot loss = 4.711802744865418, l1: 0.00010255697443426471, l2: 0.0003686232961626956   Iteration 41 of 100, tot loss = 4.701078089272103, l1: 0.00010290683260281207, l2: 0.0003672009727934648   Iteration 42 of 100, tot loss = 4.752421878633045, l1: 0.00010364502607747757, l2: 0.00037159715811001313   Iteration 43 of 100, tot loss = 4.6960651209188065, l1: 0.00010265589687975968, l2: 0.0003669506113510579   Iteration 44 of 100, tot loss = 4.647413969039917, l1: 0.00010219266443362523, l2: 0.0003625487284311517   Iteration 45 of 100, tot loss = 4.657605658637153, l1: 0.0001027572963923578, l2: 0.00036300326594048077   Iteration 46 of 100, tot loss = 4.649605232736339, l1: 0.00010217837576498783, l2: 0.0003627821437397   Iteration 47 of 100, tot loss = 4.6816820895418205, l1: 0.00010230040146431766, l2: 0.0003658678039581455   Iteration 48 of 100, tot loss = 4.635802373290062, l1: 0.00010152539387793998, l2: 0.00036205483987335657   Iteration 49 of 100, tot loss = 4.616679211052096, l1: 0.00010116775073194211, l2: 0.0003605001669600416   Iteration 50 of 100, tot loss = 4.588786196708679, l1: 0.00010087317823490594, l2: 0.000358005438465625   Iteration 51 of 100, tot loss = 4.609565346848731, l1: 0.00010118526716171033, l2: 0.0003597712647580706   Iteration 52 of 100, tot loss = 4.6081647001779995, l1: 0.00010113872884595857, l2: 0.0003596777387429029   Iteration 53 of 100, tot loss = 4.6023596682638495, l1: 0.00010076566468342207, l2: 0.00035947029926937146   Iteration 54 of 100, tot loss = 4.693469793708236, l1: 0.00010175340375334835, l2: 0.00036759357317350805   Iteration 55 of 100, tot loss = 4.6816295146942135, l1: 0.00010176644368171268, l2: 0.00036639650563963434   Iteration 56 of 100, tot loss = 4.733141860791615, l1: 0.00010240952639963195, l2: 0.00037090465775690973   Iteration 57 of 100, tot loss = 4.727551238578663, l1: 0.0001023726063865245, l2: 0.0003703825161465558   Iteration 58 of 100, tot loss = 4.727960385125259, l1: 0.0001023860305426281, l2: 0.0003704100065455162   Iteration 59 of 100, tot loss = 4.750326007099475, l1: 0.00010274252251246392, l2: 0.0003722900770391511   Iteration 60 of 100, tot loss = 4.772735138734181, l1: 0.00010354556719297156, l2: 0.0003737279461347498   Iteration 61 of 100, tot loss = 4.74603277347127, l1: 0.00010287835920891786, l2: 0.00037172491772894246   Iteration 62 of 100, tot loss = 4.758126716459951, l1: 0.00010339785483431056, l2: 0.0003724148165577302   Iteration 63 of 100, tot loss = 4.743135755024259, l1: 0.00010325879223288096, l2: 0.00037105478270585456   Iteration 64 of 100, tot loss = 4.7534366101026535, l1: 0.00010348564643436475, l2: 0.0003718580142049177   Iteration 65 of 100, tot loss = 4.749414656712458, l1: 0.00010363064832815255, l2: 0.00037131081693447555   Iteration 66 of 100, tot loss = 4.770993825161096, l1: 0.00010398664794179653, l2: 0.00037311273362402886   Iteration 67 of 100, tot loss = 4.7580336634792495, l1: 0.00010359282011327806, l2: 0.0003722105453263468   Iteration 68 of 100, tot loss = 4.754865825176239, l1: 0.00010354121080608285, l2: 0.0003719453705022769   Iteration 69 of 100, tot loss = 4.740853012471959, l1: 0.00010330790399271739, l2: 0.0003707773959858046   Iteration 70 of 100, tot loss = 4.745682573318481, l1: 0.00010370512469048013, l2: 0.000370863131580076   Iteration 71 of 100, tot loss = 4.753939024159606, l1: 0.00010385583131287156, l2: 0.00037153807024366525   Iteration 72 of 100, tot loss = 4.733738839626312, l1: 0.00010367780552971656, l2: 0.0003696960777435581   Iteration 73 of 100, tot loss = 4.734420390978252, l1: 0.00010383809905312637, l2: 0.0003696039395984135   Iteration 74 of 100, tot loss = 4.755200398934854, l1: 0.00010385543022051491, l2: 0.00037166460930854336   Iteration 75 of 100, tot loss = 4.718465445836385, l1: 0.00010298967793157013, l2: 0.00036885686645594733   Iteration 76 of 100, tot loss = 4.682724924463975, l1: 0.00010242818370898021, l2: 0.00036584430869736123   Iteration 77 of 100, tot loss = 4.69585158917811, l1: 0.00010265333572509941, l2: 0.0003669318229386389   Iteration 78 of 100, tot loss = 4.6936790606914425, l1: 0.0001028679967137764, l2: 0.00036649990887291584   Iteration 79 of 100, tot loss = 4.678284237656412, l1: 0.00010266670128485318, l2: 0.00036516172212365826   Iteration 80 of 100, tot loss = 4.664063248038292, l1: 0.00010198885265708669, l2: 0.00036441747161006786   Iteration 81 of 100, tot loss = 4.665779458151923, l1: 0.00010223126239761319, l2: 0.00036434668283708346   Iteration 82 of 100, tot loss = 4.6729276907153245, l1: 0.00010252476603477035, l2: 0.00036476800277142037   Iteration 83 of 100, tot loss = 4.73044709986951, l1: 0.00010324387324774773, l2: 0.0003698008366038529   Iteration 84 of 100, tot loss = 4.739914306572506, l1: 0.00010343119806964838, l2: 0.0003705602321133483   Iteration 85 of 100, tot loss = 4.705873384195216, l1: 0.00010282669750535313, l2: 0.00036776064045946385   Iteration 86 of 100, tot loss = 4.691901338654895, l1: 0.0001027990575219223, l2: 0.0003663910759970286   Iteration 87 of 100, tot loss = 4.692215128876697, l1: 0.00010284596563038556, l2: 0.00036637554709242936   Iteration 88 of 100, tot loss = 4.732907846570015, l1: 0.00010364274467891929, l2: 0.0003696480406771033   Iteration 89 of 100, tot loss = 4.734825791937582, l1: 0.00010395201944662363, l2: 0.0003695305602970334   Iteration 90 of 100, tot loss = 4.7267024768723385, l1: 0.00010388544437268542, l2: 0.0003687848036255067   Iteration 91 of 100, tot loss = 4.699974939063355, l1: 0.00010349327123629256, l2: 0.00036650422287613646   Iteration 92 of 100, tot loss = 4.682841514763624, l1: 0.00010302752019450291, l2: 0.00036525663144066527   Iteration 93 of 100, tot loss = 4.675302606756969, l1: 0.00010299153522067573, l2: 0.00036453872559822454   Iteration 94 of 100, tot loss = 4.697156940368896, l1: 0.00010336595717470776, l2: 0.00036634973707321553   Iteration 95 of 100, tot loss = 4.690393652414021, l1: 0.0001034686553267468, l2: 0.00036557071021218833   Iteration 96 of 100, tot loss = 4.717261576404174, l1: 0.00010395299780914986, l2: 0.0003677731601783307   Iteration 97 of 100, tot loss = 4.715662689553094, l1: 0.000104177650634305, l2: 0.00036738861889876043   Iteration 98 of 100, tot loss = 4.714132186101407, l1: 0.0001040306517691589, l2: 0.0003673825674329181   Iteration 99 of 100, tot loss = 4.720295630320154, l1: 0.0001041924527574641, l2: 0.00036783711079301103   Iteration 100 of 100, tot loss = 4.7445085084438325, l1: 0.00010418770842079538, l2: 0.0003702631429769099
   End of epoch 1232; saving model... 

Epoch 1233 of 2000
   Iteration 1 of 100, tot loss = 3.230501651763916, l1: 9.407111065229401e-05, l2: 0.0002289790427312255   Iteration 2 of 100, tot loss = 3.2066333293914795, l1: 8.639768930152059e-05, l2: 0.00023426563711836934   Iteration 3 of 100, tot loss = 5.09493366877238, l1: 0.0001044243205493937, l2: 0.00040506905255218345   Iteration 4 of 100, tot loss = 5.0834163427352905, l1: 0.00010536749505263288, l2: 0.00040297414670931175   Iteration 5 of 100, tot loss = 5.0789844512939455, l1: 0.0001054347027093172, l2: 0.00040246375137940047   Iteration 6 of 100, tot loss = 4.999040126800537, l1: 0.00010375099979379836, l2: 0.00039615302133218694   Iteration 7 of 100, tot loss = 5.249236788068499, l1: 0.00011014529965385529, l2: 0.0004147783911321312   Iteration 8 of 100, tot loss = 4.930169105529785, l1: 0.00010602096244838322, l2: 0.0003869959600706352   Iteration 9 of 100, tot loss = 4.611303594377306, l1: 0.00010133080003369186, l2: 0.00035979956899407424   Iteration 10 of 100, tot loss = 4.703940486907959, l1: 0.00010182700061704964, l2: 0.0003685670570121147   Iteration 11 of 100, tot loss = 4.759554472836581, l1: 0.0001042314225659621, l2: 0.0003717240324476734   Iteration 12 of 100, tot loss = 4.838114420572917, l1: 0.0001067345244033883, l2: 0.0003770769241479381   Iteration 13 of 100, tot loss = 4.869762310614953, l1: 0.00010841590343401409, l2: 0.0003785603319276841   Iteration 14 of 100, tot loss = 4.9882644925798685, l1: 0.00010942503286059946, l2: 0.0003894014215412816   Iteration 15 of 100, tot loss = 4.857592884699503, l1: 0.00010655287236052876, l2: 0.00037920642159103106   Iteration 16 of 100, tot loss = 4.848435088992119, l1: 0.00010620058174026781, l2: 0.0003786429324463825   Iteration 17 of 100, tot loss = 4.901880783193252, l1: 0.0001063178908011383, l2: 0.00038387019081068607   Iteration 18 of 100, tot loss = 4.881204591857062, l1: 0.00010703388195704772, l2: 0.00038108658029361523   Iteration 19 of 100, tot loss = 4.8279851863258765, l1: 0.00010661443306353728, l2: 0.00037618408877230987   Iteration 20 of 100, tot loss = 4.718981158733368, l1: 0.0001035458893966279, l2: 0.00036835222927038557   Iteration 21 of 100, tot loss = 4.876270055770874, l1: 0.0001055679214394851, l2: 0.00038205908536578396   Iteration 22 of 100, tot loss = 4.813629768111489, l1: 0.00010457552724057513, l2: 0.0003767874513869174   Iteration 23 of 100, tot loss = 4.873129792835401, l1: 0.00010551596025597182, l2: 0.00038179702001482087   Iteration 24 of 100, tot loss = 4.846418688694636, l1: 0.00010470656449494224, l2: 0.0003799353041055535   Iteration 25 of 100, tot loss = 4.809399633407593, l1: 0.000104824456066126, l2: 0.0003761155070969835   Iteration 26 of 100, tot loss = 4.876320701379043, l1: 0.00010355946184203136, l2: 0.0003840726080502813   Iteration 27 of 100, tot loss = 4.831124429349546, l1: 0.00010327959858437275, l2: 0.0003798328446767603   Iteration 28 of 100, tot loss = 4.833602939333234, l1: 0.00010414437630450786, l2: 0.0003792159191756842   Iteration 29 of 100, tot loss = 4.767288018917215, l1: 0.00010295666878187933, l2: 0.00037377213523321755   Iteration 30 of 100, tot loss = 4.731244516372681, l1: 0.00010301393037176846, l2: 0.0003701105233631097   Iteration 31 of 100, tot loss = 4.756046002910983, l1: 0.00010389520356247592, l2: 0.00037170939979278635   Iteration 32 of 100, tot loss = 4.731973230838776, l1: 0.0001034918312825539, l2: 0.000369705493994843   Iteration 33 of 100, tot loss = 4.739146666093306, l1: 0.00010420761596337384, l2: 0.0003697070532132674   Iteration 34 of 100, tot loss = 4.7913687649895165, l1: 0.00010490120653497904, l2: 0.0003742356729002067   Iteration 35 of 100, tot loss = 4.798850931440081, l1: 0.00010493838209575707, l2: 0.00037494671373029375   Iteration 36 of 100, tot loss = 4.803612775272793, l1: 0.00010513682122513678, l2: 0.00037522445806340937   Iteration 37 of 100, tot loss = 4.796265705211742, l1: 0.00010490412810090552, l2: 0.0003747224451413982   Iteration 38 of 100, tot loss = 4.798476482692518, l1: 0.00010520175735444737, l2: 0.00037464589350795567   Iteration 39 of 100, tot loss = 4.723821484125578, l1: 0.00010336511369155816, l2: 0.0003690170374167605   Iteration 40 of 100, tot loss = 4.7098953276872635, l1: 0.00010292089682479854, l2: 0.00036806863827223426   Iteration 41 of 100, tot loss = 4.675207129338893, l1: 0.00010272489506438964, l2: 0.0003647958205320032   Iteration 42 of 100, tot loss = 4.669695800258999, l1: 0.00010181401428001533, l2: 0.0003651555681634428   Iteration 43 of 100, tot loss = 4.681014019389485, l1: 0.0001016455632431411, l2: 0.00036645584149546057   Iteration 44 of 100, tot loss = 4.643841307271611, l1: 0.0001009053046965908, l2: 0.000363478828644888   Iteration 45 of 100, tot loss = 4.6270790815353395, l1: 0.00010036426505798267, l2: 0.00036234364573222897   Iteration 46 of 100, tot loss = 4.626793247202168, l1: 0.00010016479253586705, l2: 0.0003625145346513423   Iteration 47 of 100, tot loss = 4.631137104744607, l1: 0.00010042355558091894, l2: 0.00036269015734123584   Iteration 48 of 100, tot loss = 4.620507396757603, l1: 0.00010001010665898018, l2: 0.0003620406350819394   Iteration 49 of 100, tot loss = 4.644080363974279, l1: 0.00010055570630950625, l2: 0.0003638523327880444   Iteration 50 of 100, tot loss = 4.643959414958954, l1: 9.982238232623786e-05, l2: 0.00036457356123719364   Iteration 51 of 100, tot loss = 4.600767979434893, l1: 9.879353827130341e-05, l2: 0.0003612832615366571   Iteration 52 of 100, tot loss = 4.584206477953837, l1: 9.879689913269921e-05, l2: 0.00035962375029787206   Iteration 53 of 100, tot loss = 4.628833417622548, l1: 9.993389484973379e-05, l2: 0.0003629494483147765   Iteration 54 of 100, tot loss = 4.642305018725218, l1: 0.0001003221361854769, l2: 0.0003639083666645025   Iteration 55 of 100, tot loss = 4.616329234296625, l1: 9.956077668572438e-05, l2: 0.00036207214765123684   Iteration 56 of 100, tot loss = 4.623687903795924, l1: 9.989757817103444e-05, l2: 0.00036247121352062095   Iteration 57 of 100, tot loss = 4.612594765529298, l1: 9.974929548262847e-05, l2: 0.0003615101825409128   Iteration 58 of 100, tot loss = 4.606387658365842, l1: 0.00010012307826275055, l2: 0.00036051568895345554   Iteration 59 of 100, tot loss = 4.600465978606272, l1: 0.000100131691628337, l2: 0.0003599149075837932   Iteration 60 of 100, tot loss = 4.608874108393987, l1: 0.00010026785439549712, l2: 0.0003606195578565045   Iteration 61 of 100, tot loss = 4.617673101972361, l1: 0.00010089073899119985, l2: 0.0003608765725149452   Iteration 62 of 100, tot loss = 4.650774450071396, l1: 0.00010150943577877291, l2: 0.0003635680099720165   Iteration 63 of 100, tot loss = 4.6773772788426236, l1: 0.00010160459345252815, l2: 0.0003661331348767918   Iteration 64 of 100, tot loss = 4.688180970028043, l1: 0.00010170593844804898, l2: 0.0003671121587558446   Iteration 65 of 100, tot loss = 4.666106214890113, l1: 0.00010146456843358464, l2: 0.0003651460531937818   Iteration 66 of 100, tot loss = 4.636862659093105, l1: 0.00010068189989066344, l2: 0.0003630043660946698   Iteration 67 of 100, tot loss = 4.59313223611063, l1: 9.983895169944614e-05, l2: 0.0003594742721266377   Iteration 68 of 100, tot loss = 4.624693193856408, l1: 0.00010034585963748053, l2: 0.00036212345944744916   Iteration 69 of 100, tot loss = 4.631165307501088, l1: 0.00010053758323656571, l2: 0.0003625789477550627   Iteration 70 of 100, tot loss = 4.608949719156538, l1: 9.987172021023331e-05, l2: 0.00036102325164912534   Iteration 71 of 100, tot loss = 4.603435714479903, l1: 9.97495519929454e-05, l2: 0.0003605940191462164   Iteration 72 of 100, tot loss = 4.5967407921950025, l1: 9.957238474574599e-05, l2: 0.0003601016942411661   Iteration 73 of 100, tot loss = 4.5826977442388666, l1: 9.934722332480007e-05, l2: 0.0003589225505567985   Iteration 74 of 100, tot loss = 4.577422064703864, l1: 9.947364619497298e-05, l2: 0.0003582685599827233   Iteration 75 of 100, tot loss = 4.581038519541423, l1: 9.962550854349198e-05, l2: 0.0003584783427262058   Iteration 76 of 100, tot loss = 4.603367836851823, l1: 9.996893511829273e-05, l2: 0.0003603678476218575   Iteration 77 of 100, tot loss = 4.609916247330703, l1: 0.00010036338993246884, l2: 0.0003606282339382965   Iteration 78 of 100, tot loss = 4.619255255430173, l1: 0.00010064895510507855, l2: 0.0003612765699822026   Iteration 79 of 100, tot loss = 4.658478767057009, l1: 0.00010152693799751387, l2: 0.00036432093806576597   Iteration 80 of 100, tot loss = 4.6520258486270905, l1: 0.00010172000415877846, l2: 0.0003634825803601416   Iteration 81 of 100, tot loss = 4.6742220219270685, l1: 0.00010225086800436444, l2: 0.00036517133394451697   Iteration 82 of 100, tot loss = 4.695448195062032, l1: 0.0001025009779568347, l2: 0.0003670438412128289   Iteration 83 of 100, tot loss = 4.704592204955687, l1: 0.00010259720099373182, l2: 0.00036786201919137266   Iteration 84 of 100, tot loss = 4.736858390626454, l1: 0.0001027951771209787, l2: 0.00037089066123977926   Iteration 85 of 100, tot loss = 4.783530224070829, l1: 0.00010359868289978134, l2: 0.00037475433851153975   Iteration 86 of 100, tot loss = 4.7824936356655385, l1: 0.00010338504077036876, l2: 0.0003748643222172896   Iteration 87 of 100, tot loss = 4.794673437359689, l1: 0.00010347303797406594, l2: 0.0003759943051852457   Iteration 88 of 100, tot loss = 4.794017965143377, l1: 0.00010356088854347367, l2: 0.0003758409071782477   Iteration 89 of 100, tot loss = 4.78720839639728, l1: 0.00010366903844759066, l2: 0.0003750518005566274   Iteration 90 of 100, tot loss = 4.804589096705119, l1: 0.00010389687599349094, l2: 0.0003765620333271929   Iteration 91 of 100, tot loss = 4.840193208757338, l1: 0.00010473819242139183, l2: 0.0003792811277870007   Iteration 92 of 100, tot loss = 4.8213442538095554, l1: 0.00010442979656426874, l2: 0.000377704628076374   Iteration 93 of 100, tot loss = 4.803628647199241, l1: 0.00010403028586209672, l2: 0.0003763325779976684   Iteration 94 of 100, tot loss = 4.813281513274984, l1: 0.00010406462810196706, l2: 0.0003772635222474569   Iteration 95 of 100, tot loss = 4.8065526686216655, l1: 0.00010389754380938892, l2: 0.00037675772220705097   Iteration 96 of 100, tot loss = 4.809782840311527, l1: 0.00010411318032765848, l2: 0.0003768651026803127   Iteration 97 of 100, tot loss = 4.818987701357026, l1: 0.00010427726303214044, l2: 0.00037762150663484044   Iteration 98 of 100, tot loss = 4.820373741947875, l1: 0.00010414916156447844, l2: 0.00037788821200778405   Iteration 99 of 100, tot loss = 4.8045750145960335, l1: 0.00010402569196138748, l2: 0.00037643180897275943   Iteration 100 of 100, tot loss = 4.783551766872406, l1: 0.00010349637697800063, l2: 0.00037485879904124885
   End of epoch 1233; saving model... 

Epoch 1234 of 2000
   Iteration 1 of 100, tot loss = 3.4878814220428467, l1: 5.110655183671042e-05, l2: 0.00029768157401122153   Iteration 2 of 100, tot loss = 3.7132378816604614, l1: 7.607999577885494e-05, l2: 0.0002952437789645046   Iteration 3 of 100, tot loss = 3.5208166440327964, l1: 7.636999119616424e-05, l2: 0.00027571165992412716   Iteration 4 of 100, tot loss = 3.8069506883621216, l1: 8.317804713442456e-05, l2: 0.00029751701731584035   Iteration 5 of 100, tot loss = 4.824923992156982, l1: 0.00010241099953418597, l2: 0.00038008140109013766   Iteration 6 of 100, tot loss = 5.166477282842, l1: 0.00010542956442805007, l2: 0.0004112181656334239   Iteration 7 of 100, tot loss = 5.1767867633274625, l1: 0.00010979605057010693, l2: 0.0004078826230917392   Iteration 8 of 100, tot loss = 5.338962554931641, l1: 0.00011325558261887636, l2: 0.0004206406720186351   Iteration 9 of 100, tot loss = 5.072195212046306, l1: 0.00010848823714897865, l2: 0.0003987312827828444   Iteration 10 of 100, tot loss = 5.05905818939209, l1: 0.00010750919464044272, l2: 0.0003983966249506921   Iteration 11 of 100, tot loss = 4.961699854243886, l1: 0.00010529523106842217, l2: 0.0003908747563731264   Iteration 12 of 100, tot loss = 5.051253179709117, l1: 0.00010796486458275467, l2: 0.0003971604552740852   Iteration 13 of 100, tot loss = 5.061307668685913, l1: 0.00010988736735621038, l2: 0.00039624339954641   Iteration 14 of 100, tot loss = 5.209252408572605, l1: 0.0001121020011071648, l2: 0.00040882324018249553   Iteration 15 of 100, tot loss = 5.172103452682495, l1: 0.00011316358577460051, l2: 0.00040404676110483706   Iteration 16 of 100, tot loss = 5.197872951626778, l1: 0.00011412771254981635, l2: 0.0004056595844303956   Iteration 17 of 100, tot loss = 5.1361909052904915, l1: 0.00011419810385525445, l2: 0.00039942098802010366   Iteration 18 of 100, tot loss = 5.029444005754259, l1: 0.00011195337922092424, l2: 0.00039099102367698733   Iteration 19 of 100, tot loss = 4.899933037004973, l1: 0.00010819390138893046, l2: 0.0003817994047973403   Iteration 20 of 100, tot loss = 4.996012496948242, l1: 0.00011047920743294526, l2: 0.00038912204545340503   Iteration 21 of 100, tot loss = 4.876893509001959, l1: 0.00010744462772327963, l2: 0.00038024472693602246   Iteration 22 of 100, tot loss = 4.795115134932778, l1: 0.00010610019142570144, l2: 0.00037341132569550115   Iteration 23 of 100, tot loss = 4.876252060351164, l1: 0.00010758484764348553, l2: 0.00038004036152573383   Iteration 24 of 100, tot loss = 4.826045622428258, l1: 0.00010772853086867447, l2: 0.00037487603428113897   Iteration 25 of 100, tot loss = 4.797341432571411, l1: 0.00010787048260681331, l2: 0.00037186366273090244   Iteration 26 of 100, tot loss = 4.83101631127871, l1: 0.00010842239246882785, l2: 0.0003746792398697625   Iteration 27 of 100, tot loss = 4.809279256396824, l1: 0.0001091197816465326, l2: 0.0003718081452124925   Iteration 28 of 100, tot loss = 4.7263029388019016, l1: 0.00010744728518017967, l2: 0.000365183009957296   Iteration 29 of 100, tot loss = 4.724841767343982, l1: 0.00010765419875528535, l2: 0.0003648299798128548   Iteration 30 of 100, tot loss = 4.751581486066183, l1: 0.00010751691976717363, l2: 0.00036764123069588095   Iteration 31 of 100, tot loss = 4.763622337772, l1: 0.00010733277726572968, l2: 0.0003690294594684195   Iteration 32 of 100, tot loss = 4.762244068086147, l1: 0.0001070608655027172, l2: 0.0003691635438372032   Iteration 33 of 100, tot loss = 4.773261366468487, l1: 0.00010666666926364556, l2: 0.0003706594694504571   Iteration 34 of 100, tot loss = 4.683421475045821, l1: 0.00010477633291242561, l2: 0.00036356581660905196   Iteration 35 of 100, tot loss = 4.725549551418849, l1: 0.00010549567991152539, l2: 0.0003670592771543722   Iteration 36 of 100, tot loss = 4.6763748824596405, l1: 0.0001045637045535841, l2: 0.0003630737854918051   Iteration 37 of 100, tot loss = 4.639142374734621, l1: 0.00010406288089831612, l2: 0.00035985135843008255   Iteration 38 of 100, tot loss = 4.641107449406071, l1: 0.00010330219533659004, l2: 0.00036080855160865833   Iteration 39 of 100, tot loss = 4.674562738491939, l1: 0.00010426125579844945, l2: 0.0003631950206069562   Iteration 40 of 100, tot loss = 4.65073079764843, l1: 0.0001039902482261823, l2: 0.0003610828342061723   Iteration 41 of 100, tot loss = 4.655307034166848, l1: 0.00010459929363583442, l2: 0.0003609314121493343   Iteration 42 of 100, tot loss = 4.650882808935075, l1: 0.00010470608400022943, l2: 0.00036038219938442734   Iteration 43 of 100, tot loss = 4.7623801979907725, l1: 0.00010622276436892165, l2: 0.0003700152585732339   Iteration 44 of 100, tot loss = 4.6980649124492295, l1: 0.00010501410981140162, l2: 0.0003647923843924549   Iteration 45 of 100, tot loss = 4.713943237728543, l1: 0.00010423915866542504, l2: 0.00036715516730004717   Iteration 46 of 100, tot loss = 4.803399345149165, l1: 0.00010565928151545054, l2: 0.0003746806564184068   Iteration 47 of 100, tot loss = 4.751180527058054, l1: 0.00010492174121107847, l2: 0.00037019631479856893   Iteration 48 of 100, tot loss = 4.791516462961833, l1: 0.00010515155577195401, l2: 0.00037400009296106873   Iteration 49 of 100, tot loss = 4.792747215348847, l1: 0.00010563780923970329, l2: 0.00037363691496596275   Iteration 50 of 100, tot loss = 4.851840400695801, l1: 0.00010641339838912244, l2: 0.00037877064401982353   Iteration 51 of 100, tot loss = 4.850591257506726, l1: 0.00010624289172804257, l2: 0.00037881623589497643   Iteration 52 of 100, tot loss = 4.821802033827855, l1: 0.00010539277179934568, l2: 0.0003767874337567124   Iteration 53 of 100, tot loss = 4.80303926287957, l1: 0.00010519469370197874, l2: 0.0003751092346029286   Iteration 54 of 100, tot loss = 4.836147114082619, l1: 0.00010579693190703876, l2: 0.0003778177819589877   Iteration 55 of 100, tot loss = 4.818758431347933, l1: 0.0001052190283107021, l2: 0.0003766568173887208   Iteration 56 of 100, tot loss = 4.76978696669851, l1: 0.00010437237239914663, l2: 0.00037260632650161697   Iteration 57 of 100, tot loss = 4.770115672496328, l1: 0.00010471158831231215, l2: 0.00037229998116871635   Iteration 58 of 100, tot loss = 4.766608694504047, l1: 0.000104941574167681, l2: 0.0003717192980573224   Iteration 59 of 100, tot loss = 4.748634883912943, l1: 0.00010463297575508971, l2: 0.0003702305153730499   Iteration 60 of 100, tot loss = 4.792410401503245, l1: 0.00010555426421584949, l2: 0.00037368677876656873   Iteration 61 of 100, tot loss = 4.752192723946493, l1: 0.00010484189529126495, l2: 0.00037037738008218526   Iteration 62 of 100, tot loss = 4.754398876620877, l1: 0.00010504340406764493, l2: 0.0003703964868045953   Iteration 63 of 100, tot loss = 4.727382709109594, l1: 0.00010417906112649241, l2: 0.0003685592130909393   Iteration 64 of 100, tot loss = 4.690456233918667, l1: 0.0001031534568483039, l2: 0.000365892169838844   Iteration 65 of 100, tot loss = 4.666581993836623, l1: 0.0001031154337957, l2: 0.00036354276906842224   Iteration 66 of 100, tot loss = 4.658826600421559, l1: 0.00010305957877073635, l2: 0.00036282308450829464   Iteration 67 of 100, tot loss = 4.6745178414814506, l1: 0.00010327789619731815, l2: 0.000364173890470263   Iteration 68 of 100, tot loss = 4.703745186328888, l1: 0.00010368116251329946, l2: 0.00036669335874167327   Iteration 69 of 100, tot loss = 4.701783391012662, l1: 0.00010371022253293896, l2: 0.0003664681191920586   Iteration 70 of 100, tot loss = 4.727528439249311, l1: 0.00010397821700150547, l2: 0.00036877462906496867   Iteration 71 of 100, tot loss = 4.701604974101967, l1: 0.00010322368105104469, l2: 0.00036693681818498694   Iteration 72 of 100, tot loss = 4.716336915890376, l1: 0.00010379570282263255, l2: 0.00036783798997122276   Iteration 73 of 100, tot loss = 4.722888146361259, l1: 0.0001039471199089336, l2: 0.0003683416963050984   Iteration 74 of 100, tot loss = 4.7647256883415015, l1: 0.00010431045218963943, l2: 0.00037216211824891835   Iteration 75 of 100, tot loss = 4.780285704930623, l1: 0.00010438534503919073, l2: 0.00037364322692155837   Iteration 76 of 100, tot loss = 4.771785343948164, l1: 0.0001039309530058649, l2: 0.00037324758274113074   Iteration 77 of 100, tot loss = 4.77439199484788, l1: 0.00010374896548644567, l2: 0.00037369023601814815   Iteration 78 of 100, tot loss = 4.769325644542009, l1: 0.00010374500874231438, l2: 0.00037318755774042354   Iteration 79 of 100, tot loss = 4.794222834744031, l1: 0.00010405547003802561, l2: 0.0003753668154130065   Iteration 80 of 100, tot loss = 4.7844655245542524, l1: 0.00010397597930023039, l2: 0.0003744705751159927   Iteration 81 of 100, tot loss = 4.772243664588458, l1: 0.00010384017842164964, l2: 0.0003733841898523409   Iteration 82 of 100, tot loss = 4.775945570410752, l1: 0.00010335414272714576, l2: 0.00037424041599989303   Iteration 83 of 100, tot loss = 4.78715083685266, l1: 0.00010354258185806572, l2: 0.00037517250341598886   Iteration 84 of 100, tot loss = 4.767283768880935, l1: 0.00010346754628526036, l2: 0.0003732608319772962   Iteration 85 of 100, tot loss = 4.742998440125409, l1: 0.00010289058837395928, l2: 0.00037140925705898554   Iteration 86 of 100, tot loss = 4.737301646276962, l1: 0.00010290528709531848, l2: 0.00037082487891217   Iteration 87 of 100, tot loss = 4.73828359307914, l1: 0.00010283829058231465, l2: 0.0003709900699724475   Iteration 88 of 100, tot loss = 4.724114992401817, l1: 0.00010229992420301477, l2: 0.00037011157655797433   Iteration 89 of 100, tot loss = 4.725248234995296, l1: 0.0001019236323158152, l2: 0.00037060119260725135   Iteration 90 of 100, tot loss = 4.730669090482924, l1: 0.00010182411691251521, l2: 0.00037124279365848955   Iteration 91 of 100, tot loss = 4.719471391740736, l1: 0.00010183310327452741, l2: 0.00037011403733081503   Iteration 92 of 100, tot loss = 4.729651642882305, l1: 0.00010224313186684057, l2: 0.00037072203422608055   Iteration 93 of 100, tot loss = 4.722115306444065, l1: 0.00010183975770218318, l2: 0.0003703717749142739   Iteration 94 of 100, tot loss = 4.747585712595189, l1: 0.00010220045898793305, l2: 0.000372558114325697   Iteration 95 of 100, tot loss = 4.72277842069927, l1: 0.00010155306736123749, l2: 0.00037072477678425217   Iteration 96 of 100, tot loss = 4.735470409194629, l1: 0.00010189766131437257, l2: 0.00037164938142571674   Iteration 97 of 100, tot loss = 4.728557699734402, l1: 0.00010170268065736571, l2: 0.00037115309135412277   Iteration 98 of 100, tot loss = 4.709632121786779, l1: 0.00010148151433932814, l2: 0.0003694816997657264   Iteration 99 of 100, tot loss = 4.715513260677607, l1: 0.00010140403581758481, l2: 0.0003701472924516602   Iteration 100 of 100, tot loss = 4.726528174877167, l1: 0.00010162471222429304, l2: 0.0003710281073290389
   End of epoch 1234; saving model... 

Epoch 1235 of 2000
   Iteration 1 of 100, tot loss = 2.0753417015075684, l1: 5.008566949982196e-05, l2: 0.00015744849224574864   Iteration 2 of 100, tot loss = 2.555174946784973, l1: 5.450041135190986e-05, l2: 0.0002010170865105465   Iteration 3 of 100, tot loss = 4.273369868596395, l1: 8.427085170599942e-05, l2: 0.00034306612603055936   Iteration 4 of 100, tot loss = 4.494679391384125, l1: 9.434643834538292e-05, l2: 0.00035512149042915553   Iteration 5 of 100, tot loss = 4.585920667648315, l1: 9.672740561654792e-05, l2: 0.0003618646529503167   Iteration 6 of 100, tot loss = 4.799716591835022, l1: 9.879003967701767e-05, l2: 0.0003811816083422552   Iteration 7 of 100, tot loss = 4.764620133808681, l1: 0.00010027694224845618, l2: 0.0003761850613435464   Iteration 8 of 100, tot loss = 4.865324705839157, l1: 0.00010359968473494519, l2: 0.0003829327724815812   Iteration 9 of 100, tot loss = 4.716194947560628, l1: 0.00010199604407211559, l2: 0.00036962343923126656   Iteration 10 of 100, tot loss = 4.542514634132385, l1: 0.00010125584958586842, l2: 0.00035299560404382645   Iteration 11 of 100, tot loss = 4.48384672945196, l1: 0.00010131984361743724, l2: 0.00034706481859426606   Iteration 12 of 100, tot loss = 4.63420691092809, l1: 0.00010516634089678216, l2: 0.00035825434315484017   Iteration 13 of 100, tot loss = 4.558116380984966, l1: 0.00010545608865168806, l2: 0.00035035554230069887   Iteration 14 of 100, tot loss = 4.508981278964451, l1: 0.00010415623650520242, l2: 0.0003467418843813773   Iteration 15 of 100, tot loss = 4.5266003449757894, l1: 0.00010420520169039568, l2: 0.0003484548263562222   Iteration 16 of 100, tot loss = 4.429248571395874, l1: 0.00010265405035170261, l2: 0.00034027079982479336   Iteration 17 of 100, tot loss = 4.537991355447208, l1: 0.00010397105036056874, l2: 0.0003498280806097147   Iteration 18 of 100, tot loss = 4.5232690705193415, l1: 0.00010485030886937036, l2: 0.0003474765924491092   Iteration 19 of 100, tot loss = 4.590442833147551, l1: 0.00010341067651384755, l2: 0.0003556336014298722   Iteration 20 of 100, tot loss = 4.537477171421051, l1: 0.00010119781454704934, l2: 0.0003525498970702756   Iteration 21 of 100, tot loss = 4.598877804619925, l1: 0.00010134534022654407, l2: 0.00035854243642894463   Iteration 22 of 100, tot loss = 4.622890028086576, l1: 0.00010234697252930015, l2: 0.00035994202599712565   Iteration 23 of 100, tot loss = 4.509077455686486, l1: 9.957932722856007e-05, l2: 0.000351328414778792   Iteration 24 of 100, tot loss = 4.494654148817062, l1: 9.946667008383277e-05, l2: 0.00034999874151253607   Iteration 25 of 100, tot loss = 4.5198904895782475, l1: 0.0001003696650150232, l2: 0.0003516193822724745   Iteration 26 of 100, tot loss = 4.5756281064106865, l1: 0.00010182572469392863, l2: 0.00035573708625564066   Iteration 27 of 100, tot loss = 4.578551672123097, l1: 0.00010082719462742616, l2: 0.0003570279724145722   Iteration 28 of 100, tot loss = 4.605315285069602, l1: 0.00010100759858947381, l2: 0.00035952393071576286   Iteration 29 of 100, tot loss = 4.629966563191907, l1: 0.00010197961217209953, l2: 0.00036101704533613317   Iteration 30 of 100, tot loss = 4.669302439689636, l1: 0.00010290109542741751, l2: 0.00036402914993232115   Iteration 31 of 100, tot loss = 4.697690063907254, l1: 0.00010420534301609282, l2: 0.0003655636640858927   Iteration 32 of 100, tot loss = 4.714829631149769, l1: 0.0001042809824411961, l2: 0.00036720198249895475   Iteration 33 of 100, tot loss = 4.699672503904863, l1: 0.00010427133031154165, l2: 0.0003656959218488103   Iteration 34 of 100, tot loss = 4.6608677471385285, l1: 0.00010395508776555824, l2: 0.0003621316885555108   Iteration 35 of 100, tot loss = 4.664906351906913, l1: 0.0001043089124972799, l2: 0.00036218172379968956   Iteration 36 of 100, tot loss = 4.590140008264118, l1: 0.00010301677230017958, l2: 0.00035599722953823704   Iteration 37 of 100, tot loss = 4.599259141329172, l1: 0.0001031057367367179, l2: 0.00035682017836291844   Iteration 38 of 100, tot loss = 4.58246449106618, l1: 0.00010263373051681188, l2: 0.0003556127194315195   Iteration 39 of 100, tot loss = 4.662349673417898, l1: 0.00010438750896122283, l2: 0.00036184745840728283   Iteration 40 of 100, tot loss = 4.599018648266792, l1: 0.00010278139816364274, l2: 0.00035712046701519284   Iteration 41 of 100, tot loss = 4.593374348268276, l1: 0.00010288499701035595, l2: 0.0003564524384096219   Iteration 42 of 100, tot loss = 4.5594521562258405, l1: 0.0001019987837588858, l2: 0.00035394643262926754   Iteration 43 of 100, tot loss = 4.588692662327788, l1: 0.00010274664321535288, l2: 0.0003561226242942042   Iteration 44 of 100, tot loss = 4.639013070951808, l1: 0.00010363564400573854, l2: 0.00036026566422978334   Iteration 45 of 100, tot loss = 4.602399071057637, l1: 0.00010334614974757035, l2: 0.0003568937584835415   Iteration 46 of 100, tot loss = 4.568900706975357, l1: 0.00010274003674883558, l2: 0.00035415003493772417   Iteration 47 of 100, tot loss = 4.555995066115197, l1: 0.0001023055914668069, l2: 0.00035329391570524017   Iteration 48 of 100, tot loss = 4.5382853870590525, l1: 0.00010163140192768576, l2: 0.0003521971375448629   Iteration 49 of 100, tot loss = 4.517984674901379, l1: 0.00010131440025621227, l2: 0.00035048406796853003   Iteration 50 of 100, tot loss = 4.523544914722443, l1: 0.00010124822569196112, l2: 0.00035110626660753044   Iteration 51 of 100, tot loss = 4.525225856724908, l1: 0.00010162987204168138, l2: 0.000350892714421064   Iteration 52 of 100, tot loss = 4.526253977647195, l1: 0.00010184392508213372, l2: 0.0003507814729416098   Iteration 53 of 100, tot loss = 4.528226728709239, l1: 0.00010168506669974447, l2: 0.00035113760628210347   Iteration 54 of 100, tot loss = 4.4696066070486, l1: 0.00010039543236349278, l2: 0.00034656522849992917   Iteration 55 of 100, tot loss = 4.4449121518568555, l1: 0.0001000612063349267, l2: 0.00034443000926826655   Iteration 56 of 100, tot loss = 4.447994279009955, l1: 0.00010018060096886725, l2: 0.0003446188271222387   Iteration 57 of 100, tot loss = 4.4615084539379986, l1: 0.00010042102197379184, l2: 0.0003457298236051493   Iteration 58 of 100, tot loss = 4.4408514170811095, l1: 0.00010011449777611515, l2: 0.0003439706441864034   Iteration 59 of 100, tot loss = 4.456020710831981, l1: 0.00010049036513842343, l2: 0.0003451117060603021   Iteration 60 of 100, tot loss = 4.476428635915121, l1: 0.00010106343355194743, l2: 0.00034657943021253835   Iteration 61 of 100, tot loss = 4.476872584858879, l1: 0.00010097113939633853, l2: 0.00034671611919308266   Iteration 62 of 100, tot loss = 4.4809319280808975, l1: 0.00010094262635844174, l2: 0.00034715056655489117   Iteration 63 of 100, tot loss = 4.451425734020415, l1: 0.00010007427957293296, l2: 0.00034506829413456016   Iteration 64 of 100, tot loss = 4.414718825370073, l1: 9.938137134213321e-05, l2: 0.00034209051148081926   Iteration 65 of 100, tot loss = 4.477439928054809, l1: 0.00010033953037166681, l2: 0.00034740446325696   Iteration 66 of 100, tot loss = 4.446811777172667, l1: 9.965309221694373e-05, l2: 0.0003450280863475972   Iteration 67 of 100, tot loss = 4.461321538953639, l1: 9.95859554677736e-05, l2: 0.0003465461987114178   Iteration 68 of 100, tot loss = 4.448009869631599, l1: 9.932561929685214e-05, l2: 0.0003454753679012784   Iteration 69 of 100, tot loss = 4.403012602225594, l1: 9.833646407165308e-05, l2: 0.0003419647962774447   Iteration 70 of 100, tot loss = 4.392651007856641, l1: 9.787136597359287e-05, l2: 0.00034139373466522167   Iteration 71 of 100, tot loss = 4.422690601416037, l1: 9.820649031841551e-05, l2: 0.0003440625700996336   Iteration 72 of 100, tot loss = 4.437684544258648, l1: 9.833521734334581e-05, l2: 0.00034543323686698894   Iteration 73 of 100, tot loss = 4.450821009400773, l1: 9.837408786502425e-05, l2: 0.00034670801269136086   Iteration 74 of 100, tot loss = 4.438263675651035, l1: 9.796614019275719e-05, l2: 0.00034586022691753996   Iteration 75 of 100, tot loss = 4.444227010409037, l1: 9.812151130366449e-05, l2: 0.0003463011895655654   Iteration 76 of 100, tot loss = 4.421255812833183, l1: 9.743967946537611e-05, l2: 0.00034468590178891504   Iteration 77 of 100, tot loss = 4.446467769610417, l1: 9.78726696645593e-05, l2: 0.00034677410714083364   Iteration 78 of 100, tot loss = 4.429130228666159, l1: 9.73959895185171e-05, l2: 0.0003455170333789936   Iteration 79 of 100, tot loss = 4.426295864431164, l1: 9.734838855779978e-05, l2: 0.0003452811980895592   Iteration 80 of 100, tot loss = 4.414703576266765, l1: 9.708171382953879e-05, l2: 0.0003443886441345967   Iteration 81 of 100, tot loss = 4.399098642078447, l1: 9.692830986121991e-05, l2: 0.000342981554521104   Iteration 82 of 100, tot loss = 4.385670360995502, l1: 9.640218505831777e-05, l2: 0.0003421648511792272   Iteration 83 of 100, tot loss = 4.381660635212818, l1: 9.613378791851048e-05, l2: 0.0003420322759501939   Iteration 84 of 100, tot loss = 4.378392760242734, l1: 9.587519517004867e-05, l2: 0.00034196408099281445   Iteration 85 of 100, tot loss = 4.369197521490209, l1: 9.577359827851658e-05, l2: 0.00034114615401075057   Iteration 86 of 100, tot loss = 4.38737031609513, l1: 9.605445158613716e-05, l2: 0.0003426825805522534   Iteration 87 of 100, tot loss = 4.4127399907715015, l1: 9.665440101626104e-05, l2: 0.0003446195984290024   Iteration 88 of 100, tot loss = 4.412623759020459, l1: 9.680406566820403e-05, l2: 0.0003444583105440622   Iteration 89 of 100, tot loss = 4.445134191030867, l1: 9.73710944951941e-05, l2: 0.00034714232483719153   Iteration 90 of 100, tot loss = 4.447363261381785, l1: 9.73078771494329e-05, l2: 0.0003474284496203634   Iteration 91 of 100, tot loss = 4.456448697781825, l1: 9.7612846370244e-05, l2: 0.0003480320238440482   Iteration 92 of 100, tot loss = 4.474719722633776, l1: 9.810167830437422e-05, l2: 0.00034937029475100724   Iteration 93 of 100, tot loss = 4.4705728830829745, l1: 9.81630567963215e-05, l2: 0.00034889423215488894   Iteration 94 of 100, tot loss = 4.448520164540473, l1: 9.782538705002615e-05, l2: 0.0003470266300552039   Iteration 95 of 100, tot loss = 4.473606793504012, l1: 9.838616970228031e-05, l2: 0.00034897451014809407   Iteration 96 of 100, tot loss = 4.4671470783650875, l1: 9.842506710810994e-05, l2: 0.00034828964097262843   Iteration 97 of 100, tot loss = 4.470613210471635, l1: 9.870296366663995e-05, l2: 0.0003483583575346238   Iteration 98 of 100, tot loss = 4.466756150430562, l1: 9.864378741013399e-05, l2: 0.00034803182799285945   Iteration 99 of 100, tot loss = 4.473802042729927, l1: 9.886777306838676e-05, l2: 0.0003485124317295332   Iteration 100 of 100, tot loss = 4.473760010004043, l1: 9.910087050229776e-05, l2: 0.0003482751310366439
   End of epoch 1235; saving model... 

Epoch 1236 of 2000
   Iteration 1 of 100, tot loss = 5.646786689758301, l1: 0.00013756089902017266, l2: 0.0004271178040653467   Iteration 2 of 100, tot loss = 6.09728741645813, l1: 0.0001341953538940288, l2: 0.0004755333939101547   Iteration 3 of 100, tot loss = 5.058436791102092, l1: 0.00010683093933039345, l2: 0.0003990127394596736   Iteration 4 of 100, tot loss = 4.842466652393341, l1: 0.00010573844610917149, l2: 0.0003785082226386294   Iteration 5 of 100, tot loss = 4.23907368183136, l1: 9.32292125071399e-05, l2: 0.0003306781582068652   Iteration 6 of 100, tot loss = 4.205151577790578, l1: 8.690556630123562e-05, l2: 0.0003336095930232356   Iteration 7 of 100, tot loss = 4.454300352505276, l1: 9.560717288487857e-05, l2: 0.0003498228657658079   Iteration 8 of 100, tot loss = 4.279785588383675, l1: 9.040159011419746e-05, l2: 0.0003375769701960962   Iteration 9 of 100, tot loss = 4.209595375590855, l1: 9.243630524401346e-05, l2: 0.000328523236223393   Iteration 10 of 100, tot loss = 4.339159643650055, l1: 9.493160432612058e-05, l2: 0.00033898436522576957   Iteration 11 of 100, tot loss = 4.203110402280634, l1: 9.271511391323821e-05, l2: 0.00032759593159426004   Iteration 12 of 100, tot loss = 4.2093871136506396, l1: 9.231165586243151e-05, l2: 0.00032862705969212885   Iteration 13 of 100, tot loss = 4.2546902528175945, l1: 9.297304523687881e-05, l2: 0.0003324959868153271   Iteration 14 of 100, tot loss = 4.182881789548056, l1: 9.292953239180082e-05, l2: 0.00032535865154516486   Iteration 15 of 100, tot loss = 4.157788141568502, l1: 9.210197871046451e-05, l2: 0.0003236768398589144   Iteration 16 of 100, tot loss = 4.148076467216015, l1: 9.389782985635975e-05, l2: 0.00032090982131194323   Iteration 17 of 100, tot loss = 4.169625135029063, l1: 9.455487816012464e-05, l2: 0.00032240763881846387   Iteration 18 of 100, tot loss = 4.016158050960964, l1: 9.10851946779682e-05, l2: 0.0003105306136098483   Iteration 19 of 100, tot loss = 4.108962259794536, l1: 9.185370282993015e-05, l2: 0.00031904252720874195   Iteration 20 of 100, tot loss = 4.192029094696045, l1: 9.422825642104726e-05, l2: 0.000324974656541599   Iteration 21 of 100, tot loss = 4.25361290432158, l1: 9.490628525825395e-05, l2: 0.0003304550087445283   Iteration 22 of 100, tot loss = 4.2734033411199395, l1: 9.626210687036456e-05, l2: 0.00033107823070498523   Iteration 23 of 100, tot loss = 4.346586766450302, l1: 9.732876819528073e-05, l2: 0.0003373299114173278   Iteration 24 of 100, tot loss = 4.274594267209371, l1: 9.646154254975652e-05, l2: 0.0003309978862186351   Iteration 25 of 100, tot loss = 4.309107475280761, l1: 9.669280058005825e-05, l2: 0.00033421794942114504   Iteration 26 of 100, tot loss = 4.357517132392297, l1: 9.823190983017691e-05, l2: 0.00033751980439634423   Iteration 27 of 100, tot loss = 4.391984709986934, l1: 9.849271844831053e-05, l2: 0.00034070575202349573   Iteration 28 of 100, tot loss = 4.4395643302372525, l1: 9.857720397121739e-05, l2: 0.000345379229007189   Iteration 29 of 100, tot loss = 4.401091583843889, l1: 9.799201456942307e-05, l2: 0.00034211714428865575   Iteration 30 of 100, tot loss = 4.4335845867792765, l1: 9.887780324788765e-05, l2: 0.00034448065416654573   Iteration 31 of 100, tot loss = 4.3479057165884205, l1: 9.681937480044942e-05, l2: 0.00033797119554477716   Iteration 32 of 100, tot loss = 4.3154849819839, l1: 9.577467085364333e-05, l2: 0.000335773826463992   Iteration 33 of 100, tot loss = 4.322372375112591, l1: 9.588481771061197e-05, l2: 0.0003363524195843969   Iteration 34 of 100, tot loss = 4.328084184842951, l1: 9.668304782655254e-05, l2: 0.0003361253710703322   Iteration 35 of 100, tot loss = 4.286475014686585, l1: 9.625537578748273e-05, l2: 0.0003323921256066699   Iteration 36 of 100, tot loss = 4.268287483188841, l1: 9.530115575115714e-05, l2: 0.00033152759189963236   Iteration 37 of 100, tot loss = 4.286387878495294, l1: 9.573385496214787e-05, l2: 0.0003329049319979055   Iteration 38 of 100, tot loss = 4.311452335432956, l1: 9.635326683852135e-05, l2: 0.0003347919658776128   Iteration 39 of 100, tot loss = 4.27705590847211, l1: 9.601420942299928e-05, l2: 0.00033169138045289006   Iteration 40 of 100, tot loss = 4.2806971162557605, l1: 9.594165749149397e-05, l2: 0.0003321280535601545   Iteration 41 of 100, tot loss = 4.279017864204034, l1: 9.626448268025386e-05, l2: 0.0003316373033335478   Iteration 42 of 100, tot loss = 4.288829210258665, l1: 9.614268024701491e-05, l2: 0.00033274024081904265   Iteration 43 of 100, tot loss = 4.2844802141189575, l1: 9.668529606261832e-05, l2: 0.00033176272530866744   Iteration 44 of 100, tot loss = 4.323305387388576, l1: 9.745339114240117e-05, l2: 0.0003348771470799957   Iteration 45 of 100, tot loss = 4.363258751233419, l1: 9.821841861897458e-05, l2: 0.00033810745614270367   Iteration 46 of 100, tot loss = 4.44347278708997, l1: 9.971786146153413e-05, l2: 0.0003446294171938106   Iteration 47 of 100, tot loss = 4.502143106562026, l1: 0.00010054010961036337, l2: 0.00034967420171906654   Iteration 48 of 100, tot loss = 4.479409667352836, l1: 0.000100532212551722, l2: 0.00034740875526040327   Iteration 49 of 100, tot loss = 4.53903506969919, l1: 0.00010173873083751497, l2: 0.00035216477735215153   Iteration 50 of 100, tot loss = 4.533259489536285, l1: 0.00010219234449323267, l2: 0.00035113360558170826   Iteration 51 of 100, tot loss = 4.574459022166682, l1: 0.00010275469970770692, l2: 0.0003546912037064413   Iteration 52 of 100, tot loss = 4.560379246106515, l1: 0.00010246991685282689, l2: 0.0003535680091142869   Iteration 53 of 100, tot loss = 4.572031666647713, l1: 0.00010295773986485383, l2: 0.00035424542806620867   Iteration 54 of 100, tot loss = 4.571312796186517, l1: 0.00010289203432707668, l2: 0.00035423924661396694   Iteration 55 of 100, tot loss = 4.5445433725010265, l1: 0.00010230958035787229, l2: 0.00035214475805828854   Iteration 56 of 100, tot loss = 4.532266133597919, l1: 0.00010219377288324592, l2: 0.0003510328416658532   Iteration 57 of 100, tot loss = 4.500173683752093, l1: 0.00010194190907528984, l2: 0.00034807546022388953   Iteration 58 of 100, tot loss = 4.480547134218545, l1: 0.00010107171999999113, l2: 0.0003469829943837951   Iteration 59 of 100, tot loss = 4.476653731475442, l1: 0.00010089571194346378, l2: 0.00034676966187346855   Iteration 60 of 100, tot loss = 4.474187550942103, l1: 0.00010078052030924786, l2: 0.0003466382358359018   Iteration 61 of 100, tot loss = 4.449836111459576, l1: 0.00010050906253201872, l2: 0.0003444745496858186   Iteration 62 of 100, tot loss = 4.4341842378339456, l1: 0.0001004601509198584, l2: 0.0003429582741978999   Iteration 63 of 100, tot loss = 4.445965428200979, l1: 0.00010084720992063734, l2: 0.0003437493340618583   Iteration 64 of 100, tot loss = 4.455159613862634, l1: 0.00010069342505403256, l2: 0.00034482253772694094   Iteration 65 of 100, tot loss = 4.44442686851208, l1: 0.00010069256548796637, l2: 0.00034375012283607457   Iteration 66 of 100, tot loss = 4.446507881988179, l1: 0.00010060595416165818, l2: 0.0003440448351532475   Iteration 67 of 100, tot loss = 4.4998885998085365, l1: 0.00010165081457399985, l2: 0.0003483380469469242   Iteration 68 of 100, tot loss = 4.495847198893042, l1: 0.00010175315888131187, l2: 0.0003478315623343566   Iteration 69 of 100, tot loss = 4.523682043172311, l1: 0.00010249059788833804, l2: 0.00034987760823307747   Iteration 70 of 100, tot loss = 4.539397650105613, l1: 0.0001029019137474409, l2: 0.0003510378527737755   Iteration 71 of 100, tot loss = 4.5735575195769185, l1: 0.00010333747456865189, l2: 0.00035401827899221014   Iteration 72 of 100, tot loss = 4.568552767237027, l1: 0.0001036738313410347, l2: 0.0003531814470786938   Iteration 73 of 100, tot loss = 4.55286111407084, l1: 0.00010300012527046445, l2: 0.0003522859879511039   Iteration 74 of 100, tot loss = 4.554255110186499, l1: 0.00010305647978187867, l2: 0.00035236903322338303   Iteration 75 of 100, tot loss = 4.583278673489889, l1: 0.00010376724530942738, l2: 0.0003545606239155556   Iteration 76 of 100, tot loss = 4.578354620619824, l1: 0.00010374447274922491, l2: 0.00035409099138249936   Iteration 77 of 100, tot loss = 4.580511277372187, l1: 0.00010376445264570632, l2: 0.0003542866772902501   Iteration 78 of 100, tot loss = 4.576965452768864, l1: 0.00010361143890454863, l2: 0.0003540851084369187   Iteration 79 of 100, tot loss = 4.574557821961898, l1: 0.00010365462754013111, l2: 0.0003538011564781795   Iteration 80 of 100, tot loss = 4.560970975458622, l1: 0.00010340289427404059, l2: 0.00035269420495751545   Iteration 81 of 100, tot loss = 4.569185181900307, l1: 0.00010351924837227726, l2: 0.00035339927090257176   Iteration 82 of 100, tot loss = 4.565664025341592, l1: 0.00010343625780944598, l2: 0.0003531301461895587   Iteration 83 of 100, tot loss = 4.574295189007219, l1: 0.00010362242378998571, l2: 0.00035380709657574586   Iteration 84 of 100, tot loss = 4.5967955773785, l1: 0.00010402483764794721, l2: 0.00035565472162921825   Iteration 85 of 100, tot loss = 4.604098831906038, l1: 0.00010410130429697935, l2: 0.0003563085805474068   Iteration 86 of 100, tot loss = 4.598596862582273, l1: 0.00010403754902992744, l2: 0.0003558221385816957   Iteration 87 of 100, tot loss = 4.583839406912354, l1: 0.00010399285057591724, l2: 0.0003543910915005267   Iteration 88 of 100, tot loss = 4.616421354087916, l1: 0.00010407491589086766, l2: 0.0003575672212305521   Iteration 89 of 100, tot loss = 4.6109406345346, l1: 0.00010422974111371035, l2: 0.0003568643237741434   Iteration 90 of 100, tot loss = 4.594262023766835, l1: 0.00010391742285315154, l2: 0.00035550878092180935   Iteration 91 of 100, tot loss = 4.568793206424504, l1: 0.00010347048557557902, l2: 0.0003534088365076069   Iteration 92 of 100, tot loss = 4.541395992040634, l1: 0.00010294505049703572, l2: 0.00035119455012887636   Iteration 93 of 100, tot loss = 4.529659410958649, l1: 0.00010276641126350319, l2: 0.0003501995312054992   Iteration 94 of 100, tot loss = 4.526162798100329, l1: 0.0001028520021444568, l2: 0.0003497642790031758   Iteration 95 of 100, tot loss = 4.540618912797225, l1: 0.0001031159779047716, l2: 0.00035094591470337226   Iteration 96 of 100, tot loss = 4.549757794787486, l1: 0.00010317641677678087, l2: 0.00035179936427690944   Iteration 97 of 100, tot loss = 4.530090839592452, l1: 0.0001028180119000044, l2: 0.0003501910736697928   Iteration 98 of 100, tot loss = 4.523633345049255, l1: 0.00010271282672522143, l2: 0.00034965050931810875   Iteration 99 of 100, tot loss = 4.523804521319842, l1: 0.00010263792607631073, l2: 0.000349742527392627   Iteration 100 of 100, tot loss = 4.524395409822464, l1: 0.00010265877743222518, l2: 0.000349780764663592
   End of epoch 1236; saving model... 

Epoch 1237 of 2000
   Iteration 1 of 100, tot loss = 2.9468460083007812, l1: 5.57072416995652e-05, l2: 0.00023897735809441656   Iteration 2 of 100, tot loss = 4.0709240436553955, l1: 8.243817501352169e-05, l2: 0.00032465425465488806   Iteration 3 of 100, tot loss = 4.405591328938802, l1: 9.847452990167464e-05, l2: 0.00034208461390032124   Iteration 4 of 100, tot loss = 4.461052417755127, l1: 9.496170059719589e-05, l2: 0.0003511435461405199   Iteration 5 of 100, tot loss = 4.594051837921143, l1: 9.95754569885321e-05, l2: 0.00035982973349746317   Iteration 6 of 100, tot loss = 4.215278307596843, l1: 9.182174896219901e-05, l2: 0.00032970608784429106   Iteration 7 of 100, tot loss = 4.136384725570679, l1: 8.968272023983965e-05, l2: 0.00032395575752681386   Iteration 8 of 100, tot loss = 4.126763254404068, l1: 9.136101698459242e-05, l2: 0.00032131530861079227   Iteration 9 of 100, tot loss = 4.156093147065905, l1: 9.320628830917283e-05, l2: 0.0003224030289048743   Iteration 10 of 100, tot loss = 4.093661689758301, l1: 8.915536382119172e-05, l2: 0.00032021080696722495   Iteration 11 of 100, tot loss = 4.025899800387296, l1: 8.905254799174145e-05, l2: 0.0003135374347700483   Iteration 12 of 100, tot loss = 4.063162485758464, l1: 8.862813835245713e-05, l2: 0.0003176881133792146   Iteration 13 of 100, tot loss = 3.944261385844304, l1: 8.556541769604127e-05, l2: 0.00030886072244566795   Iteration 14 of 100, tot loss = 4.030801177024841, l1: 8.592738283498744e-05, l2: 0.00031715273715755235   Iteration 15 of 100, tot loss = 4.0712343692779545, l1: 8.739526092540472e-05, l2: 0.0003197281796019524   Iteration 16 of 100, tot loss = 4.134700909256935, l1: 8.773605759415659e-05, l2: 0.00032573403586866334   Iteration 17 of 100, tot loss = 4.196300324271707, l1: 8.895463414613486e-05, l2: 0.0003306754019029219   Iteration 18 of 100, tot loss = 4.189568082491557, l1: 8.913036920906355e-05, l2: 0.00032982644269294623   Iteration 19 of 100, tot loss = 4.183859737295854, l1: 9.016774052533468e-05, l2: 0.0003282182365900984   Iteration 20 of 100, tot loss = 4.240835726261139, l1: 9.073791698028799e-05, l2: 0.0003333456581458449   Iteration 21 of 100, tot loss = 4.247989506948562, l1: 9.190601018157654e-05, l2: 0.0003328929430738624   Iteration 22 of 100, tot loss = 4.177805911410939, l1: 9.110536385295828e-05, l2: 0.0003266752299334092   Iteration 23 of 100, tot loss = 4.195857099864794, l1: 9.146367159226666e-05, l2: 0.0003281220415374264   Iteration 24 of 100, tot loss = 4.297913819551468, l1: 9.425216497523554e-05, l2: 0.00033553921881927334   Iteration 25 of 100, tot loss = 4.343060598373413, l1: 9.496278333244846e-05, l2: 0.0003393432783195749   Iteration 26 of 100, tot loss = 4.2766074033883905, l1: 9.386858045427989e-05, l2: 0.0003337921617248167   Iteration 27 of 100, tot loss = 4.3497602851302535, l1: 9.532661190152996e-05, l2: 0.0003396494182568319   Iteration 28 of 100, tot loss = 4.380374823297773, l1: 9.597553538956813e-05, l2: 0.00034206194998530136   Iteration 29 of 100, tot loss = 4.411706151633427, l1: 9.752042613650576e-05, l2: 0.0003436501910669151   Iteration 30 of 100, tot loss = 4.356722005208334, l1: 9.640463079752711e-05, l2: 0.000339267571689561   Iteration 31 of 100, tot loss = 4.36966972966348, l1: 9.585137154170943e-05, l2: 0.00034111560296056973   Iteration 32 of 100, tot loss = 4.393010899424553, l1: 9.623146274861938e-05, l2: 0.00034306962879782077   Iteration 33 of 100, tot loss = 4.3248718146121865, l1: 9.529177138271432e-05, l2: 0.0003371954120335762   Iteration 34 of 100, tot loss = 4.270170569419861, l1: 9.459551181541482e-05, l2: 0.0003324215469868196   Iteration 35 of 100, tot loss = 4.28476847921099, l1: 9.486313397896342e-05, l2: 0.00033361371606588364   Iteration 36 of 100, tot loss = 4.31770325369305, l1: 9.560854293037362e-05, l2: 0.0003361617847177614   Iteration 37 of 100, tot loss = 4.352542484128797, l1: 9.577505064756937e-05, l2: 0.00033947919901284213   Iteration 38 of 100, tot loss = 4.278221475450616, l1: 9.466788038767653e-05, l2: 0.0003331542684463784   Iteration 39 of 100, tot loss = 4.240967817795583, l1: 9.416794739827777e-05, l2: 0.0003299288356897588   Iteration 40 of 100, tot loss = 4.241602486371994, l1: 9.444366251045721e-05, l2: 0.0003297165869298624   Iteration 41 of 100, tot loss = 4.371617753331254, l1: 9.650332779018777e-05, l2: 0.00034065844907095976   Iteration 42 of 100, tot loss = 4.366218549864633, l1: 9.660958303789986e-05, l2: 0.00034001227392027865   Iteration 43 of 100, tot loss = 4.389842582303424, l1: 9.69461141848601e-05, l2: 0.000342038145686803   Iteration 44 of 100, tot loss = 4.3769732171838935, l1: 9.685116070944456e-05, l2: 0.00034084616246135823   Iteration 45 of 100, tot loss = 4.352265400356717, l1: 9.637499501372481e-05, l2: 0.0003388515463383454   Iteration 46 of 100, tot loss = 4.366613087446793, l1: 9.720621109527596e-05, l2: 0.0003394550987413032   Iteration 47 of 100, tot loss = 4.359196216502088, l1: 9.696492812276739e-05, l2: 0.0003389546946756185   Iteration 48 of 100, tot loss = 4.341871867577235, l1: 9.707970965185571e-05, l2: 0.0003371074784202695   Iteration 49 of 100, tot loss = 4.366831078821299, l1: 9.734428890717064e-05, l2: 0.0003393388199453641   Iteration 50 of 100, tot loss = 4.395464782714844, l1: 9.737636130012106e-05, l2: 0.0003421701173647307   Iteration 51 of 100, tot loss = 4.399775907105091, l1: 9.729552039061673e-05, l2: 0.0003426820710278573   Iteration 52 of 100, tot loss = 4.362332426584684, l1: 9.665184657373735e-05, l2: 0.0003395813966143088   Iteration 53 of 100, tot loss = 4.414042895694949, l1: 9.728873851170922e-05, l2: 0.0003441155515830823   Iteration 54 of 100, tot loss = 4.44165877059654, l1: 9.786616761562914e-05, l2: 0.000346299709269294   Iteration 55 of 100, tot loss = 4.455622430281205, l1: 9.814466155314056e-05, l2: 0.0003474175818899477   Iteration 56 of 100, tot loss = 4.509144178458622, l1: 9.908212509149703e-05, l2: 0.0003518322934853911   Iteration 57 of 100, tot loss = 4.540841763479667, l1: 9.909669939875847e-05, l2: 0.0003549874777570694   Iteration 58 of 100, tot loss = 4.521483367887036, l1: 9.888840759231243e-05, l2: 0.0003532599301502944   Iteration 59 of 100, tot loss = 4.549270965285221, l1: 9.923988317634705e-05, l2: 0.0003556872138919277   Iteration 60 of 100, tot loss = 4.525704308350881, l1: 9.863005683049171e-05, l2: 0.0003539403745283683   Iteration 61 of 100, tot loss = 4.5080979613007095, l1: 9.87324042920382e-05, l2: 0.00035207739222000854   Iteration 62 of 100, tot loss = 4.520476502756918, l1: 9.892658348572666e-05, l2: 0.00035312106680770913   Iteration 63 of 100, tot loss = 4.522357274615575, l1: 9.905006883191019e-05, l2: 0.0003531856586166199   Iteration 64 of 100, tot loss = 4.50211001560092, l1: 9.88160352903833e-05, l2: 0.000351394966173757   Iteration 65 of 100, tot loss = 4.484489910419171, l1: 9.868791133228045e-05, l2: 0.00034976107977294864   Iteration 66 of 100, tot loss = 4.497547669844194, l1: 9.912840648735944e-05, l2: 0.0003506263603798509   Iteration 67 of 100, tot loss = 4.488398804593442, l1: 9.883280524005766e-05, l2: 0.00035000707517176475   Iteration 68 of 100, tot loss = 4.494809708174537, l1: 9.902351436775301e-05, l2: 0.00035045745672130794   Iteration 69 of 100, tot loss = 4.481628863707833, l1: 9.87542537230816e-05, l2: 0.0003494086329052495   Iteration 70 of 100, tot loss = 4.46425655569349, l1: 9.846116263361182e-05, l2: 0.00034796449326677246   Iteration 71 of 100, tot loss = 4.448779579619287, l1: 9.814007368642168e-05, l2: 0.00034673788447857795   Iteration 72 of 100, tot loss = 4.425908439689213, l1: 9.79529830450095e-05, l2: 0.000344637860810811   Iteration 73 of 100, tot loss = 4.435670401952038, l1: 9.825028221350967e-05, l2: 0.000345316757599517   Iteration 74 of 100, tot loss = 4.433695251877244, l1: 9.8101433429071e-05, l2: 0.0003452680911003562   Iteration 75 of 100, tot loss = 4.425288190841675, l1: 9.777856665702226e-05, l2: 0.0003447502519702539   Iteration 76 of 100, tot loss = 4.421529816953759, l1: 9.777030242587084e-05, l2: 0.00034438267884022363   Iteration 77 of 100, tot loss = 4.410133049085543, l1: 9.757425618477395e-05, l2: 0.0003434390480881073   Iteration 78 of 100, tot loss = 4.404688312457158, l1: 9.755217806974915e-05, l2: 0.0003429166526751569   Iteration 79 of 100, tot loss = 4.402296123625357, l1: 9.743984693055677e-05, l2: 0.0003427897649596127   Iteration 80 of 100, tot loss = 4.399148878455162, l1: 9.726864650474453e-05, l2: 0.00034264624100615037   Iteration 81 of 100, tot loss = 4.403377011970237, l1: 9.752117406561063e-05, l2: 0.00034281652672992397   Iteration 82 of 100, tot loss = 4.41396325099759, l1: 9.77271244731283e-05, l2: 0.0003436692007575979   Iteration 83 of 100, tot loss = 4.397464091519275, l1: 9.737770610134094e-05, l2: 0.00034236870334393915   Iteration 84 of 100, tot loss = 4.388557879697709, l1: 9.719215099071985e-05, l2: 0.00034166363738698973   Iteration 85 of 100, tot loss = 4.43927928980659, l1: 9.793104353080065e-05, l2: 0.0003459968857521958   Iteration 86 of 100, tot loss = 4.425347946410955, l1: 9.780778592082856e-05, l2: 0.00034472700915716284   Iteration 87 of 100, tot loss = 4.4279618235840195, l1: 9.80431321441683e-05, l2: 0.0003447530504473988   Iteration 88 of 100, tot loss = 4.4384105124256825, l1: 9.804855566967903e-05, l2: 0.00034579249553313605   Iteration 89 of 100, tot loss = 4.429023099749275, l1: 9.804534534707906e-05, l2: 0.00034485696445433735   Iteration 90 of 100, tot loss = 4.422420660654704, l1: 9.82606163031758e-05, l2: 0.0003439814497445089   Iteration 91 of 100, tot loss = 4.425262314932687, l1: 9.842595666139079e-05, l2: 0.00034410027473023836   Iteration 92 of 100, tot loss = 4.443194316781086, l1: 9.874976520855536e-05, l2: 0.0003455696662612077   Iteration 93 of 100, tot loss = 4.440300895321753, l1: 9.884567333101195e-05, l2: 0.0003451844162535503   Iteration 94 of 100, tot loss = 4.444264366271648, l1: 9.886374759167046e-05, l2: 0.00034556268888372095   Iteration 95 of 100, tot loss = 4.446371058413857, l1: 9.909301424111044e-05, l2: 0.00034554409166479385   Iteration 96 of 100, tot loss = 4.43071395655473, l1: 9.879884354783523e-05, l2: 0.00034427255210781976   Iteration 97 of 100, tot loss = 4.441271088786961, l1: 9.877908117223061e-05, l2: 0.00034534802750957956   Iteration 98 of 100, tot loss = 4.491127427743406, l1: 9.958904080790211e-05, l2: 0.00034952370199845266   Iteration 99 of 100, tot loss = 4.503073297365748, l1: 9.971625147936536e-05, l2: 0.00035059107793847835   Iteration 100 of 100, tot loss = 4.50311845779419, l1: 9.971871822926915e-05, l2: 0.00035059312722296454
   End of epoch 1237; saving model... 

Epoch 1238 of 2000
   Iteration 1 of 100, tot loss = 3.6994516849517822, l1: 0.00011156246910104528, l2: 0.00025838270084932446   Iteration 2 of 100, tot loss = 3.896114468574524, l1: 0.00011482559420983307, l2: 0.00027478586707729846   Iteration 3 of 100, tot loss = 4.119271834691365, l1: 0.00010586674519193669, l2: 0.00030606045038439333   Iteration 4 of 100, tot loss = 3.5678974986076355, l1: 9.417135697731283e-05, l2: 0.00026261840321240015   Iteration 5 of 100, tot loss = 3.5901588916778566, l1: 9.367975726490841e-05, l2: 0.0002653361385455355   Iteration 6 of 100, tot loss = 3.777046879132589, l1: 9.043514971078064e-05, l2: 0.0002872695404221304   Iteration 7 of 100, tot loss = 4.0567455632346014, l1: 9.745136984357876e-05, l2: 0.0003082231913659988   Iteration 8 of 100, tot loss = 4.078414708375931, l1: 9.616871375328628e-05, l2: 0.0003116727584711043   Iteration 9 of 100, tot loss = 4.25012485186259, l1: 9.833552919897354e-05, l2: 0.0003266769555112761   Iteration 10 of 100, tot loss = 4.556823039054871, l1: 0.00010384633496869355, l2: 0.0003518359691952355   Iteration 11 of 100, tot loss = 4.509648431431163, l1: 9.950787121355957e-05, l2: 0.00035145697157449007   Iteration 12 of 100, tot loss = 4.470262865225474, l1: 9.771287144152059e-05, l2: 0.0003493134148205475   Iteration 13 of 100, tot loss = 4.338326435822707, l1: 9.433774987253576e-05, l2: 0.0003394948938736119   Iteration 14 of 100, tot loss = 4.220125845500401, l1: 9.29197584420243e-05, l2: 0.00032909282474845113   Iteration 15 of 100, tot loss = 4.131582005818685, l1: 9.197750963115443e-05, l2: 0.0003211806896918764   Iteration 16 of 100, tot loss = 4.059916570782661, l1: 9.252794734493364e-05, l2: 0.0003134637090624892   Iteration 17 of 100, tot loss = 4.187129287158742, l1: 9.430840145796537e-05, l2: 0.0003244045253856765   Iteration 18 of 100, tot loss = 4.248677213986714, l1: 9.510271209162763e-05, l2: 0.0003297650069321713   Iteration 19 of 100, tot loss = 4.276914759686119, l1: 9.579139364895557e-05, l2: 0.0003319000786991398   Iteration 20 of 100, tot loss = 4.4134874701499935, l1: 9.919051699398551e-05, l2: 0.00034215822743135507   Iteration 21 of 100, tot loss = 4.536794696535383, l1: 0.00010142059770822969, l2: 0.00035225887092695175   Iteration 22 of 100, tot loss = 4.572912768884138, l1: 0.00010215078774225813, l2: 0.00035514048795448616   Iteration 23 of 100, tot loss = 4.486052533854609, l1: 0.00010013239491103298, l2: 0.00034847285702277946   Iteration 24 of 100, tot loss = 4.394717156887054, l1: 9.784077610675013e-05, l2: 0.00034163093854052323   Iteration 25 of 100, tot loss = 4.453065090179443, l1: 9.888961765682325e-05, l2: 0.00034641688922420143   Iteration 26 of 100, tot loss = 4.436428528565627, l1: 9.976333296016001e-05, l2: 0.00034387951689915586   Iteration 27 of 100, tot loss = 4.354179709045975, l1: 9.766981424440423e-05, l2: 0.00033774815394146436   Iteration 28 of 100, tot loss = 4.393472986561911, l1: 9.819711976888357e-05, l2: 0.00034115017634966146   Iteration 29 of 100, tot loss = 4.397174251490626, l1: 9.807717601531828e-05, l2: 0.000341640246915631   Iteration 30 of 100, tot loss = 4.5192439158757525, l1: 0.00010020307199738454, l2: 0.0003517213156252789   Iteration 31 of 100, tot loss = 4.535051861116963, l1: 0.00010050664951127293, l2: 0.00035299853217082036   Iteration 32 of 100, tot loss = 4.572692506015301, l1: 0.00010140021561255708, l2: 0.0003558690309546364   Iteration 33 of 100, tot loss = 4.580745487502127, l1: 0.00010138711348355918, l2: 0.00035668743084622264   Iteration 34 of 100, tot loss = 4.620670395738938, l1: 0.00010269315798188258, l2: 0.00035937387806103183   Iteration 35 of 100, tot loss = 4.553584003448487, l1: 0.00010177185332785094, l2: 0.0003535865436008732   Iteration 36 of 100, tot loss = 4.57886188560062, l1: 0.0001027867885770522, l2: 0.0003550993953669806   Iteration 37 of 100, tot loss = 4.577229345167005, l1: 0.00010217606580448085, l2: 0.00035554686414921105   Iteration 38 of 100, tot loss = 4.610085525010762, l1: 0.00010262530763851061, l2: 0.0003583832405734268   Iteration 39 of 100, tot loss = 4.584502091774573, l1: 0.0001027178563489519, l2: 0.00035573234848785575   Iteration 40 of 100, tot loss = 4.542938160896301, l1: 0.00010200797742072609, l2: 0.00035228583474236077   Iteration 41 of 100, tot loss = 4.574716602883687, l1: 0.0001025938496551556, l2: 0.00035487780618628984   Iteration 42 of 100, tot loss = 4.549375534057617, l1: 0.00010269095141016546, l2: 0.0003522465977723533   Iteration 43 of 100, tot loss = 4.520304779673731, l1: 0.00010228909284409764, l2: 0.00034974138069692135   Iteration 44 of 100, tot loss = 4.493623955683275, l1: 0.00010156581786511445, l2: 0.0003477965732269116   Iteration 45 of 100, tot loss = 4.491305918163723, l1: 0.00010151606458950685, l2: 0.00034761452277760126   Iteration 46 of 100, tot loss = 4.563004343406014, l1: 0.00010237998147836744, l2: 0.00035392044842212823   Iteration 47 of 100, tot loss = 4.576767632301817, l1: 0.00010247429499360534, l2: 0.00035520246409702095   Iteration 48 of 100, tot loss = 4.645014926791191, l1: 0.0001029257424155124, l2: 0.00036157574595563347   Iteration 49 of 100, tot loss = 4.650721048822208, l1: 0.00010308143835721481, l2: 0.000361990662023178   Iteration 50 of 100, tot loss = 4.61402060508728, l1: 0.0001025012484024046, l2: 0.0003589008079143241   Iteration 51 of 100, tot loss = 4.621155757530063, l1: 0.0001026240339948951, l2: 0.00035949153700094743   Iteration 52 of 100, tot loss = 4.592307356687693, l1: 0.00010253501589501796, l2: 0.0003566957152193376   Iteration 53 of 100, tot loss = 4.580040927203196, l1: 0.00010253761035339678, l2: 0.00035546647790019877   Iteration 54 of 100, tot loss = 4.591434986503036, l1: 0.00010255869825346895, l2: 0.0003565847962616115   Iteration 55 of 100, tot loss = 4.660511168566617, l1: 0.00010353460761094043, l2: 0.00036251650493465025   Iteration 56 of 100, tot loss = 4.664135281528745, l1: 0.00010352796425650013, l2: 0.00036288555955772087   Iteration 57 of 100, tot loss = 4.688460596820764, l1: 0.00010345976695310113, l2: 0.0003653862888888015   Iteration 58 of 100, tot loss = 4.704518511377532, l1: 0.00010388202588666021, l2: 0.0003665698216003687   Iteration 59 of 100, tot loss = 4.713944713948137, l1: 0.00010422142749105707, l2: 0.00036717304033503505   Iteration 60 of 100, tot loss = 4.698059499263763, l1: 0.00010412610214795374, l2: 0.0003656798442534637   Iteration 61 of 100, tot loss = 4.710458063688434, l1: 0.00010447092379346017, l2: 0.00036657487970708153   Iteration 62 of 100, tot loss = 4.745449677590401, l1: 0.00010466395794919077, l2: 0.0003698810068007949   Iteration 63 of 100, tot loss = 4.757847524824596, l1: 0.00010486659431037315, l2: 0.000370918155235741   Iteration 64 of 100, tot loss = 4.745184000581503, l1: 0.00010460209551865773, l2: 0.0003699163014516671   Iteration 65 of 100, tot loss = 4.708633419183585, l1: 0.00010400391793854606, l2: 0.00036685942099071465   Iteration 66 of 100, tot loss = 4.660546922322475, l1: 0.00010310719770848553, l2: 0.0003629474915876265   Iteration 67 of 100, tot loss = 4.654467559572476, l1: 0.00010319300846860452, l2: 0.00036225374451397097   Iteration 68 of 100, tot loss = 4.644522016539293, l1: 0.0001028003865512266, l2: 0.0003616518122940119   Iteration 69 of 100, tot loss = 4.719327437704888, l1: 0.00010311903115243827, l2: 0.00036881370834780154   Iteration 70 of 100, tot loss = 4.722609596593039, l1: 0.00010309105990537708, l2: 0.0003691698949426479   Iteration 71 of 100, tot loss = 4.790184298031766, l1: 0.00010415110427600948, l2: 0.00037486732104862055   Iteration 72 of 100, tot loss = 4.769923579361704, l1: 0.00010400183014604004, l2: 0.00037299052312139084   Iteration 73 of 100, tot loss = 4.775096088239591, l1: 0.00010407241882616016, l2: 0.00037343718487472746   Iteration 74 of 100, tot loss = 4.782341439981718, l1: 0.0001045117639256893, l2: 0.0003737223756812651   Iteration 75 of 100, tot loss = 4.77467405796051, l1: 0.00010432773822685704, l2: 0.0003731396630367575   Iteration 76 of 100, tot loss = 4.776009871771461, l1: 0.00010412280540836754, l2: 0.0003734781772723899   Iteration 77 of 100, tot loss = 4.773126575853918, l1: 0.00010394644418915782, l2: 0.0003733662089942229   Iteration 78 of 100, tot loss = 4.778601149717967, l1: 0.0001040225292970904, l2: 0.00037383758125286427   Iteration 79 of 100, tot loss = 4.7371659565575515, l1: 0.00010309160818365001, l2: 0.0003706249830245241   Iteration 80 of 100, tot loss = 4.746399895846844, l1: 0.00010308098012501432, l2: 0.0003715590048159356   Iteration 81 of 100, tot loss = 4.7466768850514915, l1: 0.00010316080375492607, l2: 0.0003715068801452785   Iteration 82 of 100, tot loss = 4.775046666947807, l1: 0.00010380848635111584, l2: 0.00037369617561258873   Iteration 83 of 100, tot loss = 4.7795431111232345, l1: 0.00010382106720718152, l2: 0.00037413323911731635   Iteration 84 of 100, tot loss = 4.785312986090069, l1: 0.00010386731476804319, l2: 0.00037466397892179276   Iteration 85 of 100, tot loss = 4.778916587549097, l1: 0.00010376469859586317, l2: 0.00037412695562713506   Iteration 86 of 100, tot loss = 4.7742673283399535, l1: 0.00010373483387094132, l2: 0.0003736918945431298   Iteration 87 of 100, tot loss = 4.780312262732407, l1: 0.00010404796373860471, l2: 0.00037398325843367213   Iteration 88 of 100, tot loss = 4.79606576805765, l1: 0.00010422612074927946, l2: 0.0003753804523347009   Iteration 89 of 100, tot loss = 4.7949013107278375, l1: 0.00010433506145857515, l2: 0.0003751550660157325   Iteration 90 of 100, tot loss = 4.781480297777388, l1: 0.00010416188860189222, l2: 0.0003739861375328878   Iteration 91 of 100, tot loss = 4.793583661645323, l1: 0.00010424507103040354, l2: 0.0003751132918386157   Iteration 92 of 100, tot loss = 4.772623465113018, l1: 0.00010395791124882753, l2: 0.0003733044319707678   Iteration 93 of 100, tot loss = 4.765974079408953, l1: 0.0001038620541431135, l2: 0.00037273535049915993   Iteration 94 of 100, tot loss = 4.771262375598258, l1: 0.00010374130141962211, l2: 0.0003733849324759076   Iteration 95 of 100, tot loss = 4.752749439289695, l1: 0.00010344742366118896, l2: 0.0003718275166312723   Iteration 96 of 100, tot loss = 4.7814708314836025, l1: 0.00010377611795320263, l2: 0.00037437096155675437   Iteration 97 of 100, tot loss = 4.789191311167688, l1: 0.00010405003217549409, l2: 0.00037486909573043205   Iteration 98 of 100, tot loss = 4.789114890049915, l1: 0.0001042758363577637, l2: 0.00037463564943636255   Iteration 99 of 100, tot loss = 4.770409782727559, l1: 0.00010389878728098914, l2: 0.0003731421878763634   Iteration 100 of 100, tot loss = 4.7567874109745025, l1: 0.00010360289936215849, l2: 0.0003720758386771195
   End of epoch 1238; saving model... 

Epoch 1239 of 2000
   Iteration 1 of 100, tot loss = 3.7742464542388916, l1: 9.860984573606402e-05, l2: 0.00027881478308700025   Iteration 2 of 100, tot loss = 3.7525341510772705, l1: 0.00010171595204155892, l2: 0.00027353745826985687   Iteration 3 of 100, tot loss = 3.736626307169596, l1: 0.00010475475573912263, l2: 0.00026890786830335855   Iteration 4 of 100, tot loss = 4.659510612487793, l1: 0.00011462331895017996, l2: 0.0003513277479214594   Iteration 5 of 100, tot loss = 4.57091875076294, l1: 0.00010897602915065363, l2: 0.0003481158521026373   Iteration 6 of 100, tot loss = 4.511382659276326, l1: 0.00010674699539473902, l2: 0.0003443912767882769   Iteration 7 of 100, tot loss = 4.222974436623709, l1: 0.00010043994864515428, l2: 0.0003218575002392754   Iteration 8 of 100, tot loss = 4.344954073429108, l1: 0.00010149170884687919, l2: 0.0003330037015984999   Iteration 9 of 100, tot loss = 4.619554254743788, l1: 0.00010761087954354782, l2: 0.0003543445507400773   Iteration 10 of 100, tot loss = 4.619826364517212, l1: 0.00010634556747390889, l2: 0.0003556370720616542   Iteration 11 of 100, tot loss = 4.530100323937156, l1: 0.00010401299699548294, l2: 0.00034899703909600663   Iteration 12 of 100, tot loss = 4.531148175398509, l1: 0.00010187586970763125, l2: 0.0003512389509220763   Iteration 13 of 100, tot loss = 5.003411641487708, l1: 0.00010907345802899307, l2: 0.00039126770906687644   Iteration 14 of 100, tot loss = 4.839132189750671, l1: 0.00010699098441234258, l2: 0.0003769222374622976   Iteration 15 of 100, tot loss = 4.895157798131307, l1: 0.00010554474380721027, l2: 0.00038397104071918875   Iteration 16 of 100, tot loss = 4.730729818344116, l1: 0.00010382101845607394, l2: 0.00036925196764059365   Iteration 17 of 100, tot loss = 4.731564914478975, l1: 0.00010319453234414515, l2: 0.0003699619624325458   Iteration 18 of 100, tot loss = 4.698599868350559, l1: 0.00010382275124559076, l2: 0.00036603723937231634   Iteration 19 of 100, tot loss = 4.735626346186588, l1: 0.00010467467808140147, l2: 0.0003688879583741685   Iteration 20 of 100, tot loss = 4.759828567504883, l1: 0.0001040985451254528, l2: 0.0003718843145179562   Iteration 21 of 100, tot loss = 4.669311966214861, l1: 0.00010237546147046877, l2: 0.0003645557375247812   Iteration 22 of 100, tot loss = 4.585580305619673, l1: 0.0001015188671854875, l2: 0.0003570391649687239   Iteration 23 of 100, tot loss = 4.568024801171345, l1: 0.00010170535793092668, l2: 0.00035509712293075967   Iteration 24 of 100, tot loss = 4.670138259728749, l1: 0.00010289388319506543, l2: 0.0003641199430906757   Iteration 25 of 100, tot loss = 4.614521446228028, l1: 0.00010199227661360055, l2: 0.00035945986805018036   Iteration 26 of 100, tot loss = 4.621511441010695, l1: 0.00010135443853169608, l2: 0.000360796706585321   Iteration 27 of 100, tot loss = 4.622170730873391, l1: 0.00010142130459253504, l2: 0.00036079577047429566   Iteration 28 of 100, tot loss = 4.597481540271214, l1: 0.00010062773357536312, l2: 0.00035912042273724055   Iteration 29 of 100, tot loss = 4.653393564553096, l1: 0.00010137034179682939, l2: 0.00036396901675194885   Iteration 30 of 100, tot loss = 4.661349026362101, l1: 0.00010170379343132178, l2: 0.00036443111263603594   Iteration 31 of 100, tot loss = 4.603528568821568, l1: 0.00010122032225837992, l2: 0.00035913253762579015   Iteration 32 of 100, tot loss = 4.565366394817829, l1: 0.00010002034150602412, l2: 0.0003565163005987415   Iteration 33 of 100, tot loss = 4.605000532034672, l1: 0.00010041427242068687, l2: 0.0003600857823833146   Iteration 34 of 100, tot loss = 4.578392744064331, l1: 0.000100769850285039, l2: 0.00035706942606553   Iteration 35 of 100, tot loss = 4.5317565441131595, l1: 9.95862735310636e-05, l2: 0.00035358938247164975   Iteration 36 of 100, tot loss = 4.542093296845754, l1: 0.00010002759103776448, l2: 0.00035418173946608376   Iteration 37 of 100, tot loss = 4.527743977469367, l1: 0.00010058168698231236, l2: 0.00035219271140286346   Iteration 38 of 100, tot loss = 4.471642180493004, l1: 9.967341468023063e-05, l2: 0.00034749080422395647   Iteration 39 of 100, tot loss = 4.538206638433994, l1: 0.000100428901649325, l2: 0.00035339176321987255   Iteration 40 of 100, tot loss = 4.516655272245407, l1: 0.00010016568203354836, l2: 0.0003514998457831098   Iteration 41 of 100, tot loss = 4.500443574858875, l1: 9.972653726169771e-05, l2: 0.0003503178208697269   Iteration 42 of 100, tot loss = 4.463264896756127, l1: 9.945572007078833e-05, l2: 0.0003468707703480807   Iteration 43 of 100, tot loss = 4.470830673395201, l1: 9.958719254977633e-05, l2: 0.0003474958753348653   Iteration 44 of 100, tot loss = 4.4565300074490635, l1: 9.929985927473436e-05, l2: 0.0003463531422973822   Iteration 45 of 100, tot loss = 4.465924697452121, l1: 0.0001000250929791946, l2: 0.00034656737795254837   Iteration 46 of 100, tot loss = 4.494802474975586, l1: 0.00010042218467709102, l2: 0.00034905806387541816   Iteration 47 of 100, tot loss = 4.483982405763991, l1: 0.00010028996876691111, l2: 0.0003481082722886485   Iteration 48 of 100, tot loss = 4.500032837192218, l1: 0.00010048916882017996, l2: 0.0003495141151385421   Iteration 49 of 100, tot loss = 4.475432429994855, l1: 9.998294864294158e-05, l2: 0.0003475602950820015   Iteration 50 of 100, tot loss = 4.419030385017395, l1: 9.908874235406984e-05, l2: 0.000342814296745928   Iteration 51 of 100, tot loss = 4.388933261235555, l1: 9.818958264362413e-05, l2: 0.00034070374400186005   Iteration 52 of 100, tot loss = 4.363994667163262, l1: 9.834567355518139e-05, l2: 0.00033805379350199544   Iteration 53 of 100, tot loss = 4.3303253605680645, l1: 9.73320038267631e-05, l2: 0.0003357005327827965   Iteration 54 of 100, tot loss = 4.310426058592619, l1: 9.708782946054944e-05, l2: 0.00033395477725274824   Iteration 55 of 100, tot loss = 4.298558217828924, l1: 9.64210207298906e-05, l2: 0.00033343480198792265   Iteration 56 of 100, tot loss = 4.2757365831307, l1: 9.5693195167509e-05, l2: 0.0003318804638183792   Iteration 57 of 100, tot loss = 4.287188768386841, l1: 9.618205979023243e-05, l2: 0.0003325368180460493   Iteration 58 of 100, tot loss = 4.2909279568441985, l1: 9.61698629468848e-05, l2: 0.00033292293351380443   Iteration 59 of 100, tot loss = 4.269079273029909, l1: 9.592037081424447e-05, l2: 0.0003309875569588473   Iteration 60 of 100, tot loss = 4.257209960619608, l1: 9.585395546309882e-05, l2: 0.0003298670411216638   Iteration 61 of 100, tot loss = 4.244379465697242, l1: 9.56148676956874e-05, l2: 0.000328823079457163   Iteration 62 of 100, tot loss = 4.251241976214994, l1: 9.56110449600777e-05, l2: 0.00032951315334949253   Iteration 63 of 100, tot loss = 4.2522755274696955, l1: 9.600695164639518e-05, l2: 0.00032922060222878666   Iteration 64 of 100, tot loss = 4.237156808376312, l1: 9.586480820189536e-05, l2: 0.00032785087398679025   Iteration 65 of 100, tot loss = 4.255021542769212, l1: 9.617683306434908e-05, l2: 0.0003293253231766777   Iteration 66 of 100, tot loss = 4.278038653460416, l1: 9.649083448025384e-05, l2: 0.0003313130328024272   Iteration 67 of 100, tot loss = 4.294369832793278, l1: 9.637426991326693e-05, l2: 0.0003330627155061047   Iteration 68 of 100, tot loss = 4.271190401385812, l1: 9.576505298729684e-05, l2: 0.00033135398948883554   Iteration 69 of 100, tot loss = 4.31311143308446, l1: 9.662616503192112e-05, l2: 0.0003346849800858552   Iteration 70 of 100, tot loss = 4.303564354351589, l1: 9.67518490922105e-05, l2: 0.00033360458796128766   Iteration 71 of 100, tot loss = 4.338261641247172, l1: 9.700723634344118e-05, l2: 0.0003368189296247372   Iteration 72 of 100, tot loss = 4.2963257316086025, l1: 9.612786299031641e-05, l2: 0.00033350471201427153   Iteration 73 of 100, tot loss = 4.297709871644843, l1: 9.5796497552241e-05, l2: 0.000333974491832746   Iteration 74 of 100, tot loss = 4.279715143345498, l1: 9.560845491422869e-05, l2: 0.00033236306164228054   Iteration 75 of 100, tot loss = 4.294971922238668, l1: 9.570846198281894e-05, l2: 0.00033378873301747567   Iteration 76 of 100, tot loss = 4.260891912799132, l1: 9.51423093475138e-05, l2: 0.00033094688469167234   Iteration 77 of 100, tot loss = 4.272724503046506, l1: 9.552935273847664e-05, l2: 0.00033174310071929056   Iteration 78 of 100, tot loss = 4.2410064354921, l1: 9.486381951809072e-05, l2: 0.000329236827103439   Iteration 79 of 100, tot loss = 4.258355617523193, l1: 9.538026864027392e-05, l2: 0.0003304552957172435   Iteration 80 of 100, tot loss = 4.242564553022385, l1: 9.499572079221252e-05, l2: 0.00032926073736234686   Iteration 81 of 100, tot loss = 4.219110306398369, l1: 9.461279401920712e-05, l2: 0.00032729823938154124   Iteration 82 of 100, tot loss = 4.210458601393351, l1: 9.465986721163684e-05, l2: 0.0003263859958324025   Iteration 83 of 100, tot loss = 4.226924514196005, l1: 9.510807066395632e-05, l2: 0.00032758438342225636   Iteration 84 of 100, tot loss = 4.224988253343673, l1: 9.513017314367018e-05, l2: 0.0003273686548449665   Iteration 85 of 100, tot loss = 4.22477567616631, l1: 9.541707977940164e-05, l2: 0.00032706049070625076   Iteration 86 of 100, tot loss = 4.246961014215336, l1: 9.56970853135423e-05, l2: 0.00032899901939239244   Iteration 87 of 100, tot loss = 4.248965304473351, l1: 9.56805467059643e-05, l2: 0.0003292159870335158   Iteration 88 of 100, tot loss = 4.256408992138776, l1: 9.576476837868209e-05, l2: 0.0003298761342772526   Iteration 89 of 100, tot loss = 4.249453622303652, l1: 9.566235890366095e-05, l2: 0.0003292830064964711   Iteration 90 of 100, tot loss = 4.250846515761482, l1: 9.58968714662155e-05, l2: 0.0003291877829825454   Iteration 91 of 100, tot loss = 4.27586532163096, l1: 9.646619678728262e-05, l2: 0.0003311203380600181   Iteration 92 of 100, tot loss = 4.269673414852308, l1: 9.637272919264987e-05, l2: 0.0003305946149497607   Iteration 93 of 100, tot loss = 4.27416116960587, l1: 9.626166545160074e-05, l2: 0.0003311544538929968   Iteration 94 of 100, tot loss = 4.2722648458277925, l1: 9.62675715508088e-05, l2: 0.0003309589156607275   Iteration 95 of 100, tot loss = 4.273104145652369, l1: 9.60693987221212e-05, l2: 0.00033124101855240665   Iteration 96 of 100, tot loss = 4.2706160098314285, l1: 9.588383492579548e-05, l2: 0.0003311777684302797   Iteration 97 of 100, tot loss = 4.2722534346826295, l1: 9.554798430379134e-05, l2: 0.00033167736134047805   Iteration 98 of 100, tot loss = 4.281827303828026, l1: 9.549214667700022e-05, l2: 0.00033269058555134153   Iteration 99 of 100, tot loss = 4.286854493497598, l1: 9.578159509633546e-05, l2: 0.00033290385622284236   Iteration 100 of 100, tot loss = 4.295074801445008, l1: 9.58718732726993e-05, l2: 0.00033363560905854686
   End of epoch 1239; saving model... 

Epoch 1240 of 2000
   Iteration 1 of 100, tot loss = 7.021697044372559, l1: 0.00016109630814753473, l2: 0.0005410734447650611   Iteration 2 of 100, tot loss = 6.072529077529907, l1: 0.00011567079855012707, l2: 0.0004915821191389114   Iteration 3 of 100, tot loss = 5.734336694081624, l1: 0.00012094957977145289, l2: 0.0004524840915109962   Iteration 4 of 100, tot loss = 4.939808011054993, l1: 0.00010067962011817144, l2: 0.0003933011830667965   Iteration 5 of 100, tot loss = 4.719518995285034, l1: 0.00010378494844189844, l2: 0.00036816695355810223   Iteration 6 of 100, tot loss = 4.185370147228241, l1: 9.378447733373226e-05, l2: 0.00032475253829034045   Iteration 7 of 100, tot loss = 4.6723053966249735, l1: 0.00010072923708191541, l2: 0.00036650130849531185   Iteration 8 of 100, tot loss = 4.446957156062126, l1: 9.663703485784936e-05, l2: 0.0003480586874502478   Iteration 9 of 100, tot loss = 4.200448155403137, l1: 9.18181342260343e-05, l2: 0.00032822668759359256   Iteration 10 of 100, tot loss = 4.052056157588959, l1: 8.73692177265184e-05, l2: 0.0003178364029736258   Iteration 11 of 100, tot loss = 4.23846802928231, l1: 9.016088766987774e-05, l2: 0.0003336859160547399   Iteration 12 of 100, tot loss = 4.249490648508072, l1: 8.930618211403878e-05, l2: 0.0003356428848443708   Iteration 13 of 100, tot loss = 4.2003346131398125, l1: 8.853478720993735e-05, l2: 0.0003314986769020414   Iteration 14 of 100, tot loss = 4.357889711856842, l1: 9.149874495051336e-05, l2: 0.00034429022655656027   Iteration 15 of 100, tot loss = 4.293182984987895, l1: 8.924590608027453e-05, l2: 0.00034007239252484094   Iteration 16 of 100, tot loss = 4.269522599875927, l1: 8.96031124284491e-05, l2: 0.000337349148139765   Iteration 17 of 100, tot loss = 4.30036333729239, l1: 8.861565412885016e-05, l2: 0.0003414206790587153   Iteration 18 of 100, tot loss = 4.366690893967946, l1: 8.872211078091318e-05, l2: 0.0003479469798751072   Iteration 19 of 100, tot loss = 4.4145931632895214, l1: 8.890503714179718e-05, l2: 0.0003525542798390808   Iteration 20 of 100, tot loss = 4.365142339468003, l1: 8.806956902844832e-05, l2: 0.0003484446649963502   Iteration 21 of 100, tot loss = 4.321871513412113, l1: 8.831718427938453e-05, l2: 0.00034386996759123924   Iteration 22 of 100, tot loss = 4.285967658866536, l1: 8.74554610742383e-05, l2: 0.0003411413060596467   Iteration 23 of 100, tot loss = 4.377093610556229, l1: 8.830185327931996e-05, l2: 0.00034940751008790636   Iteration 24 of 100, tot loss = 4.375944996873538, l1: 8.836555783394336e-05, l2: 0.0003492289439842959   Iteration 25 of 100, tot loss = 4.395056519508362, l1: 8.879692730261013e-05, l2: 0.0003507087257457897   Iteration 26 of 100, tot loss = 4.354756589119251, l1: 8.800939646719668e-05, l2: 0.00034746626344992995   Iteration 27 of 100, tot loss = 4.315937117294029, l1: 8.845588093175104e-05, l2: 0.00034313783194024665   Iteration 28 of 100, tot loss = 4.343414123569216, l1: 8.91852591199235e-05, l2: 0.00034515615438327326   Iteration 29 of 100, tot loss = 4.413694402267193, l1: 9.125057464377184e-05, l2: 0.00035011886613009946   Iteration 30 of 100, tot loss = 4.397917075951894, l1: 9.187289275966274e-05, l2: 0.0003479188148048706   Iteration 31 of 100, tot loss = 4.470138976650853, l1: 9.297428820695308e-05, l2: 0.0003540396099842544   Iteration 32 of 100, tot loss = 4.427541423588991, l1: 9.196895609875355e-05, l2: 0.00035078518658338   Iteration 33 of 100, tot loss = 4.429936094717546, l1: 9.195366047523831e-05, l2: 0.0003510399489614153   Iteration 34 of 100, tot loss = 4.364772000733544, l1: 9.046430044263081e-05, l2: 0.00034601289970690715   Iteration 35 of 100, tot loss = 4.357313254901341, l1: 9.098355876631103e-05, l2: 0.0003447477672515171   Iteration 36 of 100, tot loss = 4.308063159386317, l1: 9.019667221663339e-05, l2: 0.00034060964430358983   Iteration 37 of 100, tot loss = 4.281241639240368, l1: 8.920863232609967e-05, l2: 0.00033891553214965135   Iteration 38 of 100, tot loss = 4.321962987121783, l1: 9.006734236256881e-05, l2: 0.00034212895652193477   Iteration 39 of 100, tot loss = 4.348711554820721, l1: 9.053153366832517e-05, l2: 0.0003443396219667286   Iteration 40 of 100, tot loss = 4.3408150464296344, l1: 9.093418784686947e-05, l2: 0.0003431473178352462   Iteration 41 of 100, tot loss = 4.325767636299133, l1: 9.083676568258031e-05, l2: 0.00034173999851989765   Iteration 42 of 100, tot loss = 4.302932753449395, l1: 9.077706350321282e-05, l2: 0.00033951621229616214   Iteration 43 of 100, tot loss = 4.347207648809566, l1: 9.192588329951408e-05, l2: 0.00034279488250450773   Iteration 44 of 100, tot loss = 4.395397459918803, l1: 9.281383137766335e-05, l2: 0.0003467259154025338   Iteration 45 of 100, tot loss = 4.433469965722826, l1: 9.382503364273968e-05, l2: 0.00034952196260241584   Iteration 46 of 100, tot loss = 4.439479115216629, l1: 9.437618562356954e-05, l2: 0.0003495717249714526   Iteration 47 of 100, tot loss = 4.440797493812886, l1: 9.44204107230341e-05, l2: 0.0003496593374020836   Iteration 48 of 100, tot loss = 4.407953607539336, l1: 9.417530168320809e-05, l2: 0.0003466200581290953   Iteration 49 of 100, tot loss = 4.400804264204843, l1: 9.378496379584398e-05, l2: 0.00034629546161693504   Iteration 50 of 100, tot loss = 4.409448831081391, l1: 9.393267253472005e-05, l2: 0.00034701221011346206   Iteration 51 of 100, tot loss = 4.387984984061298, l1: 9.323676032877033e-05, l2: 0.0003455617375484686   Iteration 52 of 100, tot loss = 4.386317645127956, l1: 9.345147582979944e-05, l2: 0.00034518028825494606   Iteration 53 of 100, tot loss = 4.425650072547625, l1: 9.396798026809504e-05, l2: 0.0003485970261159687   Iteration 54 of 100, tot loss = 4.443139427238041, l1: 9.425027046408677e-05, l2: 0.0003500636712824753   Iteration 55 of 100, tot loss = 4.453038690306924, l1: 9.483983674065463e-05, l2: 0.0003504640314283527   Iteration 56 of 100, tot loss = 4.474733026964324, l1: 9.493133932535005e-05, l2: 0.0003525419625865262   Iteration 57 of 100, tot loss = 4.454224902286864, l1: 9.423896756887093e-05, l2: 0.0003511835221025537   Iteration 58 of 100, tot loss = 4.469427567103813, l1: 9.417585163740521e-05, l2: 0.00035276690498998005   Iteration 59 of 100, tot loss = 4.495416235115568, l1: 9.472588684175053e-05, l2: 0.0003548157359453676   Iteration 60 of 100, tot loss = 4.474185667435328, l1: 9.493401794316014e-05, l2: 0.0003524845480569638   Iteration 61 of 100, tot loss = 4.488522570641314, l1: 9.552051713279845e-05, l2: 0.00035333173964019926   Iteration 62 of 100, tot loss = 4.482783173361132, l1: 9.536748415660939e-05, l2: 0.0003529108329413218   Iteration 63 of 100, tot loss = 4.465854279578678, l1: 9.525563834199963e-05, l2: 0.0003513297895782642   Iteration 64 of 100, tot loss = 4.447225162759423, l1: 9.466898166010651e-05, l2: 0.00035005353447559173   Iteration 65 of 100, tot loss = 4.4472936355150665, l1: 9.477516029549476e-05, l2: 0.00034995420316520794   Iteration 66 of 100, tot loss = 4.405068323467717, l1: 9.396424998571588e-05, l2: 0.0003465425823268368   Iteration 67 of 100, tot loss = 4.361218391959347, l1: 9.308537254813684e-05, l2: 0.000343036466584812   Iteration 68 of 100, tot loss = 4.342015031506033, l1: 9.260078995259177e-05, l2: 0.00034160071332093244   Iteration 69 of 100, tot loss = 4.3498315154642295, l1: 9.275699161715211e-05, l2: 0.00034222616006340155   Iteration 70 of 100, tot loss = 4.335411936896188, l1: 9.251391269832051e-05, l2: 0.0003410272812678678   Iteration 71 of 100, tot loss = 4.352258507634552, l1: 9.271355786911724e-05, l2: 0.0003425122926734768   Iteration 72 of 100, tot loss = 4.338864482111401, l1: 9.261775560137014e-05, l2: 0.0003412686925609402   Iteration 73 of 100, tot loss = 4.3290084714758885, l1: 9.219274544779033e-05, l2: 0.0003407081013604399   Iteration 74 of 100, tot loss = 4.3264335973842725, l1: 9.209954058295241e-05, l2: 0.0003405438188024532   Iteration 75 of 100, tot loss = 4.3223471864064535, l1: 9.160207647558613e-05, l2: 0.00034063264194022244   Iteration 76 of 100, tot loss = 4.29900020988364, l1: 9.146240101568095e-05, l2: 0.0003384376197668883   Iteration 77 of 100, tot loss = 4.331540302796797, l1: 9.194436680953405e-05, l2: 0.0003412096628309953   Iteration 78 of 100, tot loss = 4.362465989895356, l1: 9.269258006474373e-05, l2: 0.00034355401863323117   Iteration 79 of 100, tot loss = 4.367239436016807, l1: 9.30950279060981e-05, l2: 0.00034362891561107097   Iteration 80 of 100, tot loss = 4.360300862789154, l1: 9.332637159786827e-05, l2: 0.0003427037144319911   Iteration 81 of 100, tot loss = 4.351579516022293, l1: 9.30864734982606e-05, l2: 0.00034207147792237523   Iteration 82 of 100, tot loss = 4.32275829809468, l1: 9.2626314609533e-05, l2: 0.00033964951500997924   Iteration 83 of 100, tot loss = 4.341699406325099, l1: 9.292699892777973e-05, l2: 0.0003412429411462172   Iteration 84 of 100, tot loss = 4.345967390707561, l1: 9.258802518680958e-05, l2: 0.0003420087134491907   Iteration 85 of 100, tot loss = 4.331514317849103, l1: 9.242210696410278e-05, l2: 0.00034072932432857616   Iteration 86 of 100, tot loss = 4.359737350497135, l1: 9.284130739911762e-05, l2: 0.00034313242746108254   Iteration 87 of 100, tot loss = 4.334385725273483, l1: 9.251504960194251e-05, l2: 0.0003409235228566271   Iteration 88 of 100, tot loss = 4.35180315239863, l1: 9.292277544855924e-05, l2: 0.00034225753924229963   Iteration 89 of 100, tot loss = 4.3698309954632535, l1: 9.31429888450101e-05, l2: 0.00034384010989137854   Iteration 90 of 100, tot loss = 4.389196349514855, l1: 9.332133542759241e-05, l2: 0.00034559829873614945   Iteration 91 of 100, tot loss = 4.387799253830543, l1: 9.35294275212992e-05, l2: 0.00034525049704825506   Iteration 92 of 100, tot loss = 4.365627207185911, l1: 9.316262588507253e-05, l2: 0.00034340009409314246   Iteration 93 of 100, tot loss = 4.35365451664053, l1: 9.290329402848398e-05, l2: 0.0003424621570037968   Iteration 94 of 100, tot loss = 4.343489506143205, l1: 9.295764485091724e-05, l2: 0.0003413913052119166   Iteration 95 of 100, tot loss = 4.370611382785596, l1: 9.347909899109877e-05, l2: 0.0003435820387829536   Iteration 96 of 100, tot loss = 4.367439415305853, l1: 9.35086766276072e-05, l2: 0.0003432352645328744   Iteration 97 of 100, tot loss = 4.360489939905934, l1: 9.3625668126048e-05, l2: 0.0003424233256043511   Iteration 98 of 100, tot loss = 4.398696175643376, l1: 9.41554306114178e-05, l2: 0.00034571418721924955   Iteration 99 of 100, tot loss = 4.37756895176088, l1: 9.363798270704027e-05, l2: 0.0003441189127801353   Iteration 100 of 100, tot loss = 4.374931839704513, l1: 9.357457940495806e-05, l2: 0.00034391860484902283
   End of epoch 1240; saving model... 

Epoch 1241 of 2000
   Iteration 1 of 100, tot loss = 3.647789239883423, l1: 8.086385787464678e-05, l2: 0.0002839150547515601   Iteration 2 of 100, tot loss = 5.133833050727844, l1: 0.00011028937296941876, l2: 0.0004030939453514293   Iteration 3 of 100, tot loss = 4.741419712702434, l1: 0.000107820885508166, l2: 0.00036632109549827874   Iteration 4 of 100, tot loss = 4.9412320256233215, l1: 0.00011046335748687852, l2: 0.00038365985528798774   Iteration 5 of 100, tot loss = 4.404142904281616, l1: 0.00010212626366410405, l2: 0.0003382880357094109   Iteration 6 of 100, tot loss = 4.519687136014302, l1: 0.00010099799328600056, l2: 0.0003509707312332466   Iteration 7 of 100, tot loss = 4.679846729551043, l1: 0.0001012633230337607, l2: 0.0003667213604785502   Iteration 8 of 100, tot loss = 4.583395451307297, l1: 9.909781056194333e-05, l2: 0.00035924174153478816   Iteration 9 of 100, tot loss = 4.645685752232869, l1: 9.858724014419649e-05, l2: 0.0003659813438490447   Iteration 10 of 100, tot loss = 4.537251329421997, l1: 9.740767200128175e-05, l2: 0.0003563174686860293   Iteration 11 of 100, tot loss = 4.96573374488137, l1: 0.00010432904144346884, l2: 0.00039224434030157596   Iteration 12 of 100, tot loss = 4.93108316262563, l1: 0.00010420337457617279, l2: 0.0003889049500382195   Iteration 13 of 100, tot loss = 4.780302964724028, l1: 0.0001008072901570883, l2: 0.0003772230131569533   Iteration 14 of 100, tot loss = 4.699840732983181, l1: 0.00010011117722439979, l2: 0.0003698729009816556   Iteration 15 of 100, tot loss = 4.642157491048177, l1: 9.971945740592977e-05, l2: 0.00036449629600004605   Iteration 16 of 100, tot loss = 4.643769353628159, l1: 9.788563102119952e-05, l2: 0.00036649130834121024   Iteration 17 of 100, tot loss = 4.629211145288804, l1: 9.920706061701126e-05, l2: 0.00036371405910947085   Iteration 18 of 100, tot loss = 4.54375876320733, l1: 9.815021945137737e-05, l2: 0.00035622566226973303   Iteration 19 of 100, tot loss = 4.7248995931524975, l1: 0.0001001287547344538, l2: 0.00037236120796909456   Iteration 20 of 100, tot loss = 4.683987092971802, l1: 9.9822691117879e-05, l2: 0.00036857602099189536   Iteration 21 of 100, tot loss = 4.6292344729105634, l1: 9.976654525546889e-05, l2: 0.00036315690487667565   Iteration 22 of 100, tot loss = 4.7252678437666455, l1: 0.00010137174070802179, l2: 0.0003711550472706387   Iteration 23 of 100, tot loss = 4.792057431262473, l1: 0.00010341863209918222, l2: 0.0003757871138235635   Iteration 24 of 100, tot loss = 4.834757228692372, l1: 0.00010429205910137777, l2: 0.00037918366797384806   Iteration 25 of 100, tot loss = 4.852665119171142, l1: 0.00010508277016924695, l2: 0.00038018374587409196   Iteration 26 of 100, tot loss = 4.830781918305617, l1: 0.00010486798503104813, l2: 0.0003782102117279115   Iteration 27 of 100, tot loss = 4.815227314277932, l1: 0.00010584604724836363, l2: 0.000375676688876141   Iteration 28 of 100, tot loss = 4.813842296600342, l1: 0.00010586079848248378, l2: 0.00037552343565039337   Iteration 29 of 100, tot loss = 4.742960979198587, l1: 0.00010438067417045862, l2: 0.0003699154275919086   Iteration 30 of 100, tot loss = 4.667034347852071, l1: 0.00010328784943946327, l2: 0.0003634155887993984   Iteration 31 of 100, tot loss = 4.634784006303357, l1: 0.00010320722761999576, l2: 0.00036027117664249794   Iteration 32 of 100, tot loss = 4.659549325704575, l1: 0.00010322669686502195, l2: 0.00036272823990657344   Iteration 33 of 100, tot loss = 4.598517526279796, l1: 0.00010255289750851013, l2: 0.0003572988595503072   Iteration 34 of 100, tot loss = 4.623749950352837, l1: 0.00010229928739136085, l2: 0.0003600757113685284   Iteration 35 of 100, tot loss = 4.600004100799561, l1: 0.00010176142282684201, l2: 0.00035823899088427424   Iteration 36 of 100, tot loss = 4.61933512157864, l1: 0.00010167958316742443, l2: 0.00036025393233608664   Iteration 37 of 100, tot loss = 4.68112140088468, l1: 0.00010245128706210872, l2: 0.00036566085543961744   Iteration 38 of 100, tot loss = 4.768023152100413, l1: 0.00010395348860918985, l2: 0.0003728488280964819   Iteration 39 of 100, tot loss = 4.7243559666168995, l1: 0.00010286722886704434, l2: 0.000369568369560278   Iteration 40 of 100, tot loss = 4.666993552446366, l1: 0.00010196756593359169, l2: 0.0003647317906143144   Iteration 41 of 100, tot loss = 4.65881141220651, l1: 0.00010221123694663686, l2: 0.00036366990548785685   Iteration 42 of 100, tot loss = 4.667119678996858, l1: 0.00010218650918215557, l2: 0.0003645254598398294   Iteration 43 of 100, tot loss = 4.687300199686095, l1: 0.00010281192767569188, l2: 0.0003659180934042778   Iteration 44 of 100, tot loss = 4.638512372970581, l1: 0.00010202713780348527, l2: 0.0003618241005989892   Iteration 45 of 100, tot loss = 4.658825916714139, l1: 0.00010190565856949736, l2: 0.0003639769345884108   Iteration 46 of 100, tot loss = 4.67042554979739, l1: 0.0001021077982643518, l2: 0.000364934757944075   Iteration 47 of 100, tot loss = 4.660826997554048, l1: 0.00010197726836888754, l2: 0.0003641054326509859   Iteration 48 of 100, tot loss = 4.637264509995778, l1: 0.00010090552329226436, l2: 0.00036282092878536787   Iteration 49 of 100, tot loss = 4.586275241812881, l1: 0.00010010057659962271, l2: 0.00035852694893445897   Iteration 50 of 100, tot loss = 4.581219153404236, l1: 0.00010011088641476818, l2: 0.0003580110298935324   Iteration 51 of 100, tot loss = 4.596698382321526, l1: 0.00010024328694984718, l2: 0.0003594265521421809   Iteration 52 of 100, tot loss = 4.574668998901661, l1: 9.982221252777471e-05, l2: 0.0003576446878679025   Iteration 53 of 100, tot loss = 4.553591197391726, l1: 9.940582135898712e-05, l2: 0.0003559532990450707   Iteration 54 of 100, tot loss = 4.534781729733503, l1: 9.904866385989374e-05, l2: 0.00035442950967181887   Iteration 55 of 100, tot loss = 4.514967792684382, l1: 9.862112968800251e-05, l2: 0.0003528756502253765   Iteration 56 of 100, tot loss = 4.514014912503106, l1: 9.789247990608731e-05, l2: 0.00035350901193201674   Iteration 57 of 100, tot loss = 4.48027799840559, l1: 9.74116888404783e-05, l2: 0.0003506161112710017   Iteration 58 of 100, tot loss = 4.450353716981822, l1: 9.656441115707978e-05, l2: 0.00034847096058315244   Iteration 59 of 100, tot loss = 4.432115324472977, l1: 9.63441830131391e-05, l2: 0.00034686734937368184   Iteration 60 of 100, tot loss = 4.462395258744558, l1: 9.671842581155942e-05, l2: 0.0003495210999972187   Iteration 61 of 100, tot loss = 4.4759823259760125, l1: 9.671492223221366e-05, l2: 0.0003508833097675663   Iteration 62 of 100, tot loss = 4.490873540601423, l1: 9.720629186044642e-05, l2: 0.0003518810614966036   Iteration 63 of 100, tot loss = 4.481564510436285, l1: 9.661899613482612e-05, l2: 0.00035153745418591866   Iteration 64 of 100, tot loss = 4.490201476961374, l1: 9.703588602860691e-05, l2: 0.0003519842616697133   Iteration 65 of 100, tot loss = 4.474772834777832, l1: 9.684491401108411e-05, l2: 0.00035063236942872977   Iteration 66 of 100, tot loss = 4.460315682671287, l1: 9.649915889233604e-05, l2: 0.0003495324093488638   Iteration 67 of 100, tot loss = 4.4719820947789435, l1: 9.653878230132413e-05, l2: 0.000350659426889007   Iteration 68 of 100, tot loss = 4.44148490709417, l1: 9.628259335038499e-05, l2: 0.00034786589715034044   Iteration 69 of 100, tot loss = 4.430411777634552, l1: 9.609679640878154e-05, l2: 0.0003469443812112634   Iteration 70 of 100, tot loss = 4.4380425351006645, l1: 9.626872540268648e-05, l2: 0.0003475355273362116   Iteration 71 of 100, tot loss = 4.425636140393539, l1: 9.634106585585421e-05, l2: 0.0003462225474617247   Iteration 72 of 100, tot loss = 4.425591968827778, l1: 9.639078083030957e-05, l2: 0.00034616841508573596   Iteration 73 of 100, tot loss = 4.42336894714669, l1: 9.62613924719036e-05, l2: 0.0003460755012371284   Iteration 74 of 100, tot loss = 4.4472942255638745, l1: 9.662965198544584e-05, l2: 0.0003480997695225744   Iteration 75 of 100, tot loss = 4.493470106124878, l1: 9.747202139503012e-05, l2: 0.0003518749876335884   Iteration 76 of 100, tot loss = 4.516991179240377, l1: 9.791654749853096e-05, l2: 0.00035378256890046604   Iteration 77 of 100, tot loss = 4.502224278140377, l1: 9.756094170205125e-05, l2: 0.00035266148462413823   Iteration 78 of 100, tot loss = 4.5084392841045675, l1: 9.786435979409501e-05, l2: 0.0003529795669750549   Iteration 79 of 100, tot loss = 4.514197234865986, l1: 9.811769876573117e-05, l2: 0.0003533020228386232   Iteration 80 of 100, tot loss = 4.522639036178589, l1: 9.830966528170392e-05, l2: 0.00035395423674344784   Iteration 81 of 100, tot loss = 4.5519166522555885, l1: 9.878260023492377e-05, l2: 0.00035640906329111505   Iteration 82 of 100, tot loss = 4.572231769561768, l1: 9.910351741936898e-05, l2: 0.000358119657903444   Iteration 83 of 100, tot loss = 4.561571399849582, l1: 9.910893198369206e-05, l2: 0.000357048206433867   Iteration 84 of 100, tot loss = 4.579281792754219, l1: 9.890666478895582e-05, l2: 0.00035902151302295913   Iteration 85 of 100, tot loss = 4.616862726211548, l1: 9.931666701210334e-05, l2: 0.00036236960423754197   Iteration 86 of 100, tot loss = 4.6201128266578495, l1: 9.92704602161444e-05, l2: 0.0003627408212957752   Iteration 87 of 100, tot loss = 4.6424665642880845, l1: 9.987967395914141e-05, l2: 0.00036436698152603387   Iteration 88 of 100, tot loss = 4.638373854485425, l1: 9.955292303361189e-05, l2: 0.0003642844612014332   Iteration 89 of 100, tot loss = 4.631682259313176, l1: 9.935720340712414e-05, l2: 0.0003638110214156318   Iteration 90 of 100, tot loss = 4.618005365795559, l1: 9.926338324375036e-05, l2: 0.00036253715241198534   Iteration 91 of 100, tot loss = 4.606767002042833, l1: 9.931549904829628e-05, l2: 0.00036136120040312534   Iteration 92 of 100, tot loss = 4.5933622173641036, l1: 9.901266521015002e-05, l2: 0.0003603235557460246   Iteration 93 of 100, tot loss = 4.625748054955595, l1: 9.93978045630439e-05, l2: 0.00036317700008806645   Iteration 94 of 100, tot loss = 4.6673446158145335, l1: 0.00010008379560647572, l2: 0.0003666506652618719   Iteration 95 of 100, tot loss = 4.718141259645161, l1: 0.00010092726550187524, l2: 0.0003708868598455171   Iteration 96 of 100, tot loss = 4.7355773995320005, l1: 0.00010106696087556581, l2: 0.0003724907787727716   Iteration 97 of 100, tot loss = 4.6984170417195745, l1: 0.00010032451924773115, l2: 0.00036951718462172647   Iteration 98 of 100, tot loss = 4.699665801865714, l1: 0.00010057276327012116, l2: 0.00036939381651772776   Iteration 99 of 100, tot loss = 4.723502296389955, l1: 0.00010078173021831776, l2: 0.00037156849915239337   Iteration 100 of 100, tot loss = 4.760426037311554, l1: 0.00010146455340873217, l2: 0.0003745780504686991
   End of epoch 1241; saving model... 

Epoch 1242 of 2000
   Iteration 1 of 100, tot loss = 2.2697343826293945, l1: 4.050057759741321e-05, l2: 0.00018647286924533546   Iteration 2 of 100, tot loss = 3.4331183433532715, l1: 7.46835175959859e-05, l2: 0.00026862832601182163   Iteration 3 of 100, tot loss = 3.053069512049357, l1: 6.987771363734889e-05, l2: 0.00023542924706513682   Iteration 4 of 100, tot loss = 3.2717226147651672, l1: 7.764340170979267e-05, l2: 0.000249528864515014   Iteration 5 of 100, tot loss = 3.633335733413696, l1: 8.832483363221399e-05, l2: 0.0002750087413005531   Iteration 6 of 100, tot loss = 4.100466052691142, l1: 9.752853596485996e-05, l2: 0.00031251807619507116   Iteration 7 of 100, tot loss = 4.328803164618356, l1: 0.00010347236353222147, l2: 0.0003294079547881016   Iteration 8 of 100, tot loss = 4.152420252561569, l1: 9.786793270905036e-05, l2: 0.00031737409335619304   Iteration 9 of 100, tot loss = 3.870098657078213, l1: 9.186493520650806e-05, l2: 0.0002951449326549967   Iteration 10 of 100, tot loss = 3.9162659287452697, l1: 9.044879407156259e-05, l2: 0.0003011777997016907   Iteration 11 of 100, tot loss = 3.985880559140986, l1: 9.156592039454898e-05, l2: 0.0003070221335457807   Iteration 12 of 100, tot loss = 3.878299822409948, l1: 8.76108667095347e-05, l2: 0.0003002191127355521   Iteration 13 of 100, tot loss = 3.961399656075698, l1: 8.774720649503601e-05, l2: 0.00030839275747824175   Iteration 14 of 100, tot loss = 4.08700509582247, l1: 8.777481967367098e-05, l2: 0.00032092568913607726   Iteration 15 of 100, tot loss = 4.105332048734029, l1: 8.942978505122786e-05, l2: 0.0003211034170817584   Iteration 16 of 100, tot loss = 4.098315127193928, l1: 9.054821293830173e-05, l2: 0.00031928329735819716   Iteration 17 of 100, tot loss = 4.184107591124142, l1: 9.245330276077285e-05, l2: 0.00032595745185116194   Iteration 18 of 100, tot loss = 4.292513880464766, l1: 9.399255132949393e-05, l2: 0.0003352588343356426   Iteration 19 of 100, tot loss = 4.3580443294424756, l1: 9.653387410492685e-05, l2: 0.0003392705555980731   Iteration 20 of 100, tot loss = 4.386857563257218, l1: 9.683603966550435e-05, l2: 0.0003418497130041942   Iteration 21 of 100, tot loss = 4.375047280674889, l1: 9.689039801568946e-05, l2: 0.00034061432788370265   Iteration 22 of 100, tot loss = 4.407430502501401, l1: 9.647068873164244e-05, l2: 0.0003442723603008992   Iteration 23 of 100, tot loss = 4.310115435849065, l1: 9.481843928528099e-05, l2: 0.00033619310314584845   Iteration 24 of 100, tot loss = 4.365894868969917, l1: 9.598404494681745e-05, l2: 0.0003406054402148584   Iteration 25 of 100, tot loss = 4.327227387428284, l1: 9.52225575747434e-05, l2: 0.00033750017930287866   Iteration 26 of 100, tot loss = 4.327294354255383, l1: 9.49574217744297e-05, l2: 0.00033777201259419177   Iteration 27 of 100, tot loss = 4.278464401209796, l1: 9.41697495970099e-05, l2: 0.00033367668867059465   Iteration 28 of 100, tot loss = 4.306403802973883, l1: 9.553054444820321e-05, l2: 0.0003351098343305888   Iteration 29 of 100, tot loss = 4.3120369376807375, l1: 9.553944368360981e-05, l2: 0.00033566424902127095   Iteration 30 of 100, tot loss = 4.381358905633291, l1: 9.618107469577808e-05, l2: 0.00034195481518205874   Iteration 31 of 100, tot loss = 4.3330901015189385, l1: 9.593350172977924e-05, l2: 0.0003373755077313211   Iteration 32 of 100, tot loss = 4.395049136132002, l1: 9.548084710786497e-05, l2: 0.00034402406663502916   Iteration 33 of 100, tot loss = 4.426828366337401, l1: 9.583261216352129e-05, l2: 0.0003468502241461961   Iteration 34 of 100, tot loss = 4.421626774703755, l1: 9.576097545505036e-05, l2: 0.0003464017016299562   Iteration 35 of 100, tot loss = 4.382371926307679, l1: 9.51354943806239e-05, l2: 0.00034310169825662993   Iteration 36 of 100, tot loss = 4.419600033097797, l1: 9.648228246482581e-05, l2: 0.0003454777199496877   Iteration 37 of 100, tot loss = 4.423938077849311, l1: 9.626283507394553e-05, l2: 0.00034613097188805506   Iteration 38 of 100, tot loss = 4.376557641907742, l1: 9.503049573380083e-05, l2: 0.00034262526797216485   Iteration 39 of 100, tot loss = 4.380534089528597, l1: 9.586116310600072e-05, l2: 0.00034219224508911465   Iteration 40 of 100, tot loss = 4.421832671761512, l1: 9.673701815700042e-05, l2: 0.00034544624759291764   Iteration 41 of 100, tot loss = 4.374496090702895, l1: 9.581834651393497e-05, l2: 0.00034163126128543985   Iteration 42 of 100, tot loss = 4.4112287277267095, l1: 9.61102847386861e-05, l2: 0.000345012586746764   Iteration 43 of 100, tot loss = 4.429233143495959, l1: 9.671064651419603e-05, l2: 0.00034621266612978   Iteration 44 of 100, tot loss = 4.433596061034636, l1: 9.676983427198138e-05, l2: 0.00034658976999873465   Iteration 45 of 100, tot loss = 4.447429691420661, l1: 9.706431188129095e-05, l2: 0.00034767865500826803   Iteration 46 of 100, tot loss = 4.420699645643649, l1: 9.65033470255667e-05, l2: 0.0003455666149460265   Iteration 47 of 100, tot loss = 4.43233514085729, l1: 9.673619748122911e-05, l2: 0.00034649731427867044   Iteration 48 of 100, tot loss = 4.474769778549671, l1: 9.756829877005657e-05, l2: 0.0003499086766775387   Iteration 49 of 100, tot loss = 4.475708813083415, l1: 9.795052429414069e-05, l2: 0.00034962035418602125   Iteration 50 of 100, tot loss = 4.4714924788475034, l1: 9.779071944649332e-05, l2: 0.00034935852570924905   Iteration 51 of 100, tot loss = 4.455007924753077, l1: 9.747837255275169e-05, l2: 0.0003480224170512063   Iteration 52 of 100, tot loss = 4.467213298265751, l1: 9.747880596058586e-05, l2: 0.00034924252059471863   Iteration 53 of 100, tot loss = 4.41415481297475, l1: 9.656711322264218e-05, l2: 0.0003448483647552389   Iteration 54 of 100, tot loss = 4.444071994887458, l1: 9.719544484849191e-05, l2: 0.0003472117512299317   Iteration 55 of 100, tot loss = 4.439902682737871, l1: 9.714758052723483e-05, l2: 0.00034684268400517545   Iteration 56 of 100, tot loss = 4.450550500835691, l1: 9.74630091669886e-05, l2: 0.00034759203754219925   Iteration 57 of 100, tot loss = 4.4634896119435625, l1: 9.787998166294736e-05, l2: 0.00034846897605140777   Iteration 58 of 100, tot loss = 4.445042922579009, l1: 9.764612219316468e-05, l2: 0.00034685816692593826   Iteration 59 of 100, tot loss = 4.452897815381066, l1: 9.807156952851752e-05, l2: 0.0003472182085052906   Iteration 60 of 100, tot loss = 4.428226816654205, l1: 9.78294512606226e-05, l2: 0.000344993226826773   Iteration 61 of 100, tot loss = 4.412299691653643, l1: 9.783421791937264e-05, l2: 0.0003433957476250934   Iteration 62 of 100, tot loss = 4.461271643638611, l1: 9.896183990114819e-05, l2: 0.00034716532078868286   Iteration 63 of 100, tot loss = 4.449372386175488, l1: 9.861360290198631e-05, l2: 0.00034632363219446106   Iteration 64 of 100, tot loss = 4.451930683106184, l1: 9.880590323518845e-05, l2: 0.00034638716113022383   Iteration 65 of 100, tot loss = 4.486782004283024, l1: 9.922774692173474e-05, l2: 0.00034945044932940686   Iteration 66 of 100, tot loss = 4.4767227317347675, l1: 9.907609472231178e-05, l2: 0.0003485961742901991   Iteration 67 of 100, tot loss = 4.505660299044936, l1: 9.946898918220107e-05, l2: 0.00035109703631901337   Iteration 68 of 100, tot loss = 4.482439952738145, l1: 9.920387973954134e-05, l2: 0.00034904011141299955   Iteration 69 of 100, tot loss = 4.49544194124747, l1: 9.958770470288785e-05, l2: 0.0003499564852023943   Iteration 70 of 100, tot loss = 4.502823952266148, l1: 9.972612492025032e-05, l2: 0.00035055626555861505   Iteration 71 of 100, tot loss = 4.499729317678532, l1: 9.974559320589448e-05, l2: 0.00035022733424393406   Iteration 72 of 100, tot loss = 4.5356349216567144, l1: 0.00010028312822719777, l2: 0.00035328036018553475   Iteration 73 of 100, tot loss = 4.503401109617051, l1: 9.956878968969e-05, l2: 0.0003507713176675256   Iteration 74 of 100, tot loss = 4.519904748813526, l1: 9.986663698745778e-05, l2: 0.0003521238345333463   Iteration 75 of 100, tot loss = 4.495509684880575, l1: 9.912738993686313e-05, l2: 0.000350423575049111   Iteration 76 of 100, tot loss = 4.486776286049893, l1: 9.930996776874006e-05, l2: 0.0003493676573079459   Iteration 77 of 100, tot loss = 4.489118300475083, l1: 9.94447460919679e-05, l2: 0.0003494670800373333   Iteration 78 of 100, tot loss = 4.480196965046418, l1: 9.945767184641834e-05, l2: 0.0003485620208942773   Iteration 79 of 100, tot loss = 4.468969568421569, l1: 9.935984085535017e-05, l2: 0.0003475371122476645   Iteration 80 of 100, tot loss = 4.469839882850647, l1: 9.931373797371635e-05, l2: 0.00034767024617394783   Iteration 81 of 100, tot loss = 4.52439382341173, l1: 0.00010030943640568121, l2: 0.0003521299415611501   Iteration 82 of 100, tot loss = 4.537120627193916, l1: 0.000100385406900697, l2: 0.0003533266514373614   Iteration 83 of 100, tot loss = 4.540601058178638, l1: 0.00010035972739289324, l2: 0.00035370037396906885   Iteration 84 of 100, tot loss = 4.554455291657221, l1: 0.00010057187864731532, l2: 0.0003548736462800575   Iteration 85 of 100, tot loss = 4.533340493370505, l1: 0.00010007004799691083, l2: 0.0003532639969147139   Iteration 86 of 100, tot loss = 4.5183332770369775, l1: 9.991297507482624e-05, l2: 0.00035192034835724136   Iteration 87 of 100, tot loss = 4.509249292570969, l1: 9.959631229164721e-05, l2: 0.0003513286129240985   Iteration 88 of 100, tot loss = 4.516835396940058, l1: 9.91757357372659e-05, l2: 0.0003525078001819731   Iteration 89 of 100, tot loss = 4.512627210509911, l1: 9.92460754174174e-05, l2: 0.0003520166417197952   Iteration 90 of 100, tot loss = 4.502364524205526, l1: 9.885801927238289e-05, l2: 0.0003513784294126607   Iteration 91 of 100, tot loss = 4.509082752269703, l1: 9.87609608193532e-05, l2: 0.0003521473110675208   Iteration 92 of 100, tot loss = 4.507992526759272, l1: 9.889623201477504e-05, l2: 0.0003519030171604675   Iteration 93 of 100, tot loss = 4.499360233224849, l1: 9.871461960394437e-05, l2: 0.00035122140025612347   Iteration 94 of 100, tot loss = 4.490455693386971, l1: 9.865871354432262e-05, l2: 0.00035038685243133396   Iteration 95 of 100, tot loss = 4.508766761579012, l1: 9.901018071870663e-05, l2: 0.00035186649222037215   Iteration 96 of 100, tot loss = 4.512977545460065, l1: 9.921622980376317e-05, l2: 0.0003520815214415052   Iteration 97 of 100, tot loss = 4.488481379046883, l1: 9.864731740534123e-05, l2: 0.0003502008173771961   Iteration 98 of 100, tot loss = 4.509877516298878, l1: 9.888820579585534e-05, l2: 0.00035209954282380547   Iteration 99 of 100, tot loss = 4.510873211754693, l1: 9.906065558122161e-05, l2: 0.00035202666266640235   Iteration 100 of 100, tot loss = 4.520263824462891, l1: 9.93694876160589e-05, l2: 0.0003526568915549433
   End of epoch 1242; saving model... 

Epoch 1243 of 2000
   Iteration 1 of 100, tot loss = 6.363912105560303, l1: 0.0001503209350630641, l2: 0.0004860702611040324   Iteration 2 of 100, tot loss = 5.7324488162994385, l1: 0.00012947530558449216, l2: 0.0004437695606611669   Iteration 3 of 100, tot loss = 5.394083658854167, l1: 0.00012624088655381152, l2: 0.0004131674747137974   Iteration 4 of 100, tot loss = 4.49137881398201, l1: 0.00010883389131777221, l2: 0.00034030398637696635   Iteration 5 of 100, tot loss = 4.1192960977554325, l1: 0.00010147701905225404, l2: 0.0003104525923845358   Iteration 6 of 100, tot loss = 4.234391947587331, l1: 9.914046737928099e-05, l2: 0.0003242987259000074   Iteration 7 of 100, tot loss = 4.329501169068473, l1: 0.00010068518427682907, l2: 0.00033226493334430937   Iteration 8 of 100, tot loss = 4.3857506066560745, l1: 0.00010286354927302455, l2: 0.0003357115101607633   Iteration 9 of 100, tot loss = 4.3651010857688055, l1: 0.00010140502571529295, l2: 0.0003351050812246588   Iteration 10 of 100, tot loss = 4.299912369251251, l1: 0.0001021327745547751, l2: 0.00032785846051410774   Iteration 11 of 100, tot loss = 4.2085358988155015, l1: 0.00010005052403853783, l2: 0.0003208030651545745   Iteration 12 of 100, tot loss = 4.1141475935777025, l1: 9.637070616008714e-05, l2: 0.00031504405160376336   Iteration 13 of 100, tot loss = 4.115602062298701, l1: 9.673903864145708e-05, l2: 0.00031482116509533417   Iteration 14 of 100, tot loss = 4.201448959963662, l1: 9.782468623598106e-05, l2: 0.00032232020800750306   Iteration 15 of 100, tot loss = 4.291818086306254, l1: 9.77477291598916e-05, l2: 0.00033143407830114787   Iteration 16 of 100, tot loss = 4.2470258846879005, l1: 9.719949230202474e-05, l2: 0.0003275030944678292   Iteration 17 of 100, tot loss = 4.23131245024064, l1: 9.823784742312615e-05, l2: 0.000324893397405771   Iteration 18 of 100, tot loss = 4.110176768567827, l1: 9.493837306864507e-05, l2: 0.0003160793037548299   Iteration 19 of 100, tot loss = 4.055023689019053, l1: 9.470309248028666e-05, l2: 0.0003107992773661145   Iteration 20 of 100, tot loss = 4.146757954359055, l1: 9.533354768791469e-05, l2: 0.0003193422486219788   Iteration 21 of 100, tot loss = 4.1072287275677635, l1: 9.568929661846986e-05, l2: 0.00031503357742968506   Iteration 22 of 100, tot loss = 4.017941523681987, l1: 9.353041274483654e-05, l2: 0.0003082637409864798   Iteration 23 of 100, tot loss = 4.017051784888558, l1: 9.334903006674722e-05, l2: 0.00030835615009140304   Iteration 24 of 100, tot loss = 3.983279123902321, l1: 9.371426494908519e-05, l2: 0.0003046136483438507   Iteration 25 of 100, tot loss = 4.051012091636657, l1: 9.483658504905179e-05, l2: 0.0003102646247134544   Iteration 26 of 100, tot loss = 4.058101557768309, l1: 9.48902144535588e-05, l2: 0.0003109199429812949   Iteration 27 of 100, tot loss = 4.10189868344201, l1: 9.603307289243848e-05, l2: 0.0003141567967422479   Iteration 28 of 100, tot loss = 4.0503158782209665, l1: 9.533865133042647e-05, l2: 0.000309692938182187   Iteration 29 of 100, tot loss = 4.134021006781479, l1: 9.638710667033969e-05, l2: 0.00031701499593227394   Iteration 30 of 100, tot loss = 4.107763532797495, l1: 9.562522827764042e-05, l2: 0.0003151511261724712   Iteration 31 of 100, tot loss = 4.137037296448985, l1: 9.681061712927335e-05, l2: 0.0003168931131855765   Iteration 32 of 100, tot loss = 4.10278632864356, l1: 9.624582276046567e-05, l2: 0.00031403281104758207   Iteration 33 of 100, tot loss = 4.136228658936241, l1: 9.679060718609077e-05, l2: 0.00031683225979301795   Iteration 34 of 100, tot loss = 4.204214667572694, l1: 9.707088632721399e-05, l2: 0.0003233505817393854   Iteration 35 of 100, tot loss = 4.237343076297215, l1: 9.786057947037209e-05, l2: 0.0003258737296813966   Iteration 36 of 100, tot loss = 4.157581402195825, l1: 9.61073881424252e-05, l2: 0.0003196507536308167   Iteration 37 of 100, tot loss = 4.181848583994685, l1: 9.605975936063112e-05, l2: 0.0003221250997346549   Iteration 38 of 100, tot loss = 4.140584368454783, l1: 9.524235993969916e-05, l2: 0.0003188160771969706   Iteration 39 of 100, tot loss = 4.131539191955175, l1: 9.499532172459368e-05, l2: 0.00031815859778688697   Iteration 40 of 100, tot loss = 4.162934917211532, l1: 9.557298517393065e-05, l2: 0.000320720507443184   Iteration 41 of 100, tot loss = 4.230771547410546, l1: 9.660427957253599e-05, l2: 0.00032647287617891844   Iteration 42 of 100, tot loss = 4.3083168268203735, l1: 9.780121079183162e-05, l2: 0.0003330304732246857   Iteration 43 of 100, tot loss = 4.318684627843457, l1: 9.868264769888917e-05, l2: 0.0003331858166601769   Iteration 44 of 100, tot loss = 4.323302805423737, l1: 9.849935263032835e-05, l2: 0.0003338309298586947   Iteration 45 of 100, tot loss = 4.307752158906725, l1: 9.746977367386636e-05, l2: 0.0003333054444131752   Iteration 46 of 100, tot loss = 4.317489660304526, l1: 9.691778003521587e-05, l2: 0.0003348311875015497   Iteration 47 of 100, tot loss = 4.351506360033725, l1: 9.770810039357302e-05, l2: 0.00033744253612004536   Iteration 48 of 100, tot loss = 4.340794771909714, l1: 9.78953686778065e-05, l2: 0.00033618410937682103   Iteration 49 of 100, tot loss = 4.3251964899958395, l1: 9.777536214809219e-05, l2: 0.000334744287621496   Iteration 50 of 100, tot loss = 4.365091552734375, l1: 9.860984704573639e-05, l2: 0.0003378993092337623   Iteration 51 of 100, tot loss = 4.415764930201512, l1: 9.93591859108111e-05, l2: 0.0003422173076793186   Iteration 52 of 100, tot loss = 4.444422960281372, l1: 0.00010022857266169292, l2: 0.0003442137245796263   Iteration 53 of 100, tot loss = 4.436008399387576, l1: 9.978287932924259e-05, l2: 0.00034381796217101783   Iteration 54 of 100, tot loss = 4.462786718651101, l1: 0.00010057367506777626, l2: 0.00034570499890294203   Iteration 55 of 100, tot loss = 4.450759428197688, l1: 0.00010086769219593738, l2: 0.00034420825223523106   Iteration 56 of 100, tot loss = 4.474150555474417, l1: 0.0001013947191625318, l2: 0.00034602033805900386   Iteration 57 of 100, tot loss = 4.463118636817263, l1: 0.00010129179868830793, l2: 0.00034502006632177845   Iteration 58 of 100, tot loss = 4.409571729857346, l1: 0.0001000709835352609, l2: 0.00034088619062897814   Iteration 59 of 100, tot loss = 4.420563140157926, l1: 0.00010067883932467755, l2: 0.00034137747545975216   Iteration 60 of 100, tot loss = 4.444437074661255, l1: 0.00010135272417149584, l2: 0.00034309098446101417   Iteration 61 of 100, tot loss = 4.451344982522433, l1: 0.00010151883161895847, l2: 0.00034361566809582386   Iteration 62 of 100, tot loss = 4.461865140545752, l1: 0.0001015023505216607, l2: 0.0003446841647899184   Iteration 63 of 100, tot loss = 4.485631821647523, l1: 0.00010212482707824424, l2: 0.000346438355798397   Iteration 64 of 100, tot loss = 4.477395050227642, l1: 0.00010238196051659543, l2: 0.00034535754491571424   Iteration 65 of 100, tot loss = 4.499252678797795, l1: 0.00010233350862668326, l2: 0.0003475917596146106   Iteration 66 of 100, tot loss = 4.508177417697328, l1: 0.00010245026654350416, l2: 0.000348367475949503   Iteration 67 of 100, tot loss = 4.500627204553405, l1: 0.00010228598167391758, l2: 0.0003477767392873083   Iteration 68 of 100, tot loss = 4.478633466888876, l1: 0.00010177726466388456, l2: 0.0003460860828935812   Iteration 69 of 100, tot loss = 4.460541127384573, l1: 0.00010151030769718446, l2: 0.0003445438060621002   Iteration 70 of 100, tot loss = 4.449105303628104, l1: 0.00010138294793640462, l2: 0.00034352758344279467   Iteration 71 of 100, tot loss = 4.439563704208589, l1: 0.00010117674491353477, l2: 0.0003427796265230068   Iteration 72 of 100, tot loss = 4.440434442626105, l1: 0.00010102760729245751, l2: 0.00034301583754212415   Iteration 73 of 100, tot loss = 4.444576067467258, l1: 0.0001010097875136225, l2: 0.000343447819544315   Iteration 74 of 100, tot loss = 4.459007843120678, l1: 0.00010106252370491888, l2: 0.00034483826124955696   Iteration 75 of 100, tot loss = 4.457084096272786, l1: 0.00010095379232855824, l2: 0.00034475461822391175   Iteration 76 of 100, tot loss = 4.462706509389375, l1: 0.00010110170665269067, l2: 0.0003451689455304356   Iteration 77 of 100, tot loss = 4.493196883758941, l1: 0.0001012787740022192, l2: 0.00034804091504971534   Iteration 78 of 100, tot loss = 4.4842898692840185, l1: 0.00010138261537716832, l2: 0.0003470463723202463   Iteration 79 of 100, tot loss = 4.49096257475358, l1: 0.00010182173226192251, l2: 0.00034727452588459196   Iteration 80 of 100, tot loss = 4.520185360312462, l1: 0.00010223200329164683, l2: 0.0003497865335702954   Iteration 81 of 100, tot loss = 4.515396980591762, l1: 0.00010206361764209794, l2: 0.00034947608129400184   Iteration 82 of 100, tot loss = 4.532179771400079, l1: 0.00010230738531035745, l2: 0.00035091059216051134   Iteration 83 of 100, tot loss = 4.521412883896425, l1: 0.00010239349548243468, l2: 0.00034974779309887235   Iteration 84 of 100, tot loss = 4.499367838814145, l1: 0.00010205260232829633, l2: 0.00034788418192225175   Iteration 85 of 100, tot loss = 4.484558899262372, l1: 0.00010190020800698274, l2: 0.00034655568229294764   Iteration 86 of 100, tot loss = 4.500333578087563, l1: 0.00010239367580906283, l2: 0.0003476396823527974   Iteration 87 of 100, tot loss = 4.482063219465059, l1: 0.00010209927169288958, l2: 0.00034610705054432004   Iteration 88 of 100, tot loss = 4.475173579020933, l1: 0.00010216861884063374, l2: 0.0003453487394257205   Iteration 89 of 100, tot loss = 4.483800269244762, l1: 0.00010205915744980179, l2: 0.0003463208697130642   Iteration 90 of 100, tot loss = 4.465346556239658, l1: 0.00010165619084242886, l2: 0.0003448784651280019   Iteration 91 of 100, tot loss = 4.471298110354078, l1: 0.00010197845016251789, l2: 0.00034515136156607274   Iteration 92 of 100, tot loss = 4.471798018268917, l1: 0.00010200836775608047, l2: 0.00034517143483200294   Iteration 93 of 100, tot loss = 4.461695499317621, l1: 0.00010194895672142476, l2: 0.0003442205941668832   Iteration 94 of 100, tot loss = 4.462533983778446, l1: 0.00010183846140360866, l2: 0.0003444149379186243   Iteration 95 of 100, tot loss = 4.460531197096172, l1: 0.00010197291676855744, l2: 0.0003440802036876496   Iteration 96 of 100, tot loss = 4.465883118410905, l1: 0.00010185101113317312, l2: 0.00034473730124773283   Iteration 97 of 100, tot loss = 4.458103356902132, l1: 0.00010151376761886118, l2: 0.0003442965685984014   Iteration 98 of 100, tot loss = 4.457526095059453, l1: 0.0001015351320588985, l2: 0.0003442174780428438   Iteration 99 of 100, tot loss = 4.444691087260391, l1: 0.00010117312440986162, l2: 0.00034329598507023125   Iteration 100 of 100, tot loss = 4.4702616000175475, l1: 0.0001012878611436463, l2: 0.0003457382998749381
   End of epoch 1243; saving model... 

Epoch 1244 of 2000
   Iteration 1 of 100, tot loss = 5.867300987243652, l1: 7.502282096538693e-05, l2: 0.0005117072723805904   Iteration 2 of 100, tot loss = 5.7771241664886475, l1: 8.692145274835639e-05, l2: 0.0004907909460598603   Iteration 3 of 100, tot loss = 4.8372171719868975, l1: 8.635772246634588e-05, l2: 0.00039736398321110755   Iteration 4 of 100, tot loss = 4.5202441811561584, l1: 8.436376083409414e-05, l2: 0.0003676606465887744   Iteration 5 of 100, tot loss = 4.161721801757812, l1: 7.677949906792492e-05, l2: 0.00033939267159439624   Iteration 6 of 100, tot loss = 4.573880672454834, l1: 8.835761642937238e-05, l2: 0.0003690304438350722   Iteration 7 of 100, tot loss = 4.534226758139474, l1: 8.98597001131358e-05, l2: 0.0003635629712204848   Iteration 8 of 100, tot loss = 4.758744478225708, l1: 9.439509994990658e-05, l2: 0.00038147933810250834   Iteration 9 of 100, tot loss = 4.802320215437147, l1: 9.686425376761083e-05, l2: 0.00038336776196956635   Iteration 10 of 100, tot loss = 4.568599820137024, l1: 9.472599267610349e-05, l2: 0.00036213398416293783   Iteration 11 of 100, tot loss = 4.423206762834028, l1: 9.38928007433953e-05, l2: 0.0003484278700356795   Iteration 12 of 100, tot loss = 4.33523158232371, l1: 9.191596958165367e-05, l2: 0.00034160718496423215   Iteration 13 of 100, tot loss = 4.250934545810406, l1: 9.179323051984493e-05, l2: 0.00033330022079798463   Iteration 14 of 100, tot loss = 4.196908371789115, l1: 9.092578537612488e-05, l2: 0.0003287650490944673   Iteration 15 of 100, tot loss = 4.064124679565429, l1: 8.791258733253926e-05, l2: 0.0003184998786309734   Iteration 16 of 100, tot loss = 4.292246729135513, l1: 9.29852603803738e-05, l2: 0.00033623941180849215   Iteration 17 of 100, tot loss = 4.303223413579604, l1: 9.480931604390636e-05, l2: 0.000335513024337535   Iteration 18 of 100, tot loss = 4.382704496383667, l1: 9.528560662551576e-05, l2: 0.00034298484307429235   Iteration 19 of 100, tot loss = 4.3361313970465405, l1: 9.507986511667504e-05, l2: 0.00033853327515412514   Iteration 20 of 100, tot loss = 4.382780265808106, l1: 9.571916161803529e-05, l2: 0.0003425588663958479   Iteration 21 of 100, tot loss = 4.354516971678961, l1: 9.490923063519101e-05, l2: 0.00034054246844745996   Iteration 22 of 100, tot loss = 4.46117889881134, l1: 9.695584264012392e-05, l2: 0.00034916204921583727   Iteration 23 of 100, tot loss = 4.4559561791627305, l1: 9.781746674612488e-05, l2: 0.00034777815365131295   Iteration 24 of 100, tot loss = 4.463888933261235, l1: 9.847206001722952e-05, l2: 0.0003479168365932613   Iteration 25 of 100, tot loss = 4.441903333663941, l1: 9.835494740400463e-05, l2: 0.0003458353894529864   Iteration 26 of 100, tot loss = 4.442126961854788, l1: 9.922192363704268e-05, l2: 0.00034499077558463725   Iteration 27 of 100, tot loss = 4.390108391090676, l1: 9.866824230239554e-05, l2: 0.00034034259981665483   Iteration 28 of 100, tot loss = 4.41311160155705, l1: 9.867673647282313e-05, l2: 0.0003426344270077867   Iteration 29 of 100, tot loss = 4.368590190492827, l1: 9.814853045350776e-05, l2: 0.0003387104917582574   Iteration 30 of 100, tot loss = 4.370033915837606, l1: 9.855753160081804e-05, l2: 0.0003384458623865309   Iteration 31 of 100, tot loss = 4.319434942737702, l1: 9.797569181236829e-05, l2: 0.00033396780500263574   Iteration 32 of 100, tot loss = 4.356528677046299, l1: 9.889234092952393e-05, l2: 0.00033676052908049314   Iteration 33 of 100, tot loss = 4.340848713210135, l1: 9.843367760450664e-05, l2: 0.0003356511954211094   Iteration 34 of 100, tot loss = 4.3898988681681015, l1: 9.941381722455844e-05, l2: 0.0003395760710084099   Iteration 35 of 100, tot loss = 4.343467930385045, l1: 9.7814629927078e-05, l2: 0.0003365321646145146   Iteration 36 of 100, tot loss = 4.29146605067783, l1: 9.6287224854071e-05, l2: 0.0003328593819686729   Iteration 37 of 100, tot loss = 4.298048406033902, l1: 9.710635935629693e-05, l2: 0.00033269848318997065   Iteration 38 of 100, tot loss = 4.276950861278333, l1: 9.719942547809823e-05, l2: 0.00033049566263798624   Iteration 39 of 100, tot loss = 4.28450957322732, l1: 9.742467768657475e-05, l2: 0.00033102628255549533   Iteration 40 of 100, tot loss = 4.322695410251617, l1: 9.816274759941735e-05, l2: 0.0003341067953442689   Iteration 41 of 100, tot loss = 4.304059499647559, l1: 9.724827041060141e-05, l2: 0.0003331576817578114   Iteration 42 of 100, tot loss = 4.301639187903631, l1: 9.6254110238598e-05, l2: 0.0003339098101215703   Iteration 43 of 100, tot loss = 4.270219154136125, l1: 9.594306877345865e-05, l2: 0.0003310788485768446   Iteration 44 of 100, tot loss = 4.2925539504398, l1: 9.647285771114349e-05, l2: 0.00033278254010259513   Iteration 45 of 100, tot loss = 4.257626067267524, l1: 9.629352821826211e-05, l2: 0.0003294690812860305   Iteration 46 of 100, tot loss = 4.278468411901723, l1: 9.701712888668028e-05, l2: 0.00033082971418954674   Iteration 47 of 100, tot loss = 4.265983845325226, l1: 9.72543411022498e-05, l2: 0.00032934404512064807   Iteration 48 of 100, tot loss = 4.323147664467494, l1: 9.819942814222789e-05, l2: 0.00033411534089585376   Iteration 49 of 100, tot loss = 4.312496847035933, l1: 9.748182856128551e-05, l2: 0.0003337678591582962   Iteration 50 of 100, tot loss = 4.313237352371216, l1: 9.772848010470625e-05, l2: 0.00033359525812556965   Iteration 51 of 100, tot loss = 4.281308650970459, l1: 9.701166374154626e-05, l2: 0.0003311192039309033   Iteration 52 of 100, tot loss = 4.28721050115732, l1: 9.642928202643256e-05, l2: 0.00033229177083390264   Iteration 53 of 100, tot loss = 4.283721842855777, l1: 9.60652878862709e-05, l2: 0.00033230689916548863   Iteration 54 of 100, tot loss = 4.333434219713564, l1: 9.684460376404415e-05, l2: 0.0003364988218958455   Iteration 55 of 100, tot loss = 4.377904718572443, l1: 9.754765613417311e-05, l2: 0.00034024281935258343   Iteration 56 of 100, tot loss = 4.359262394053595, l1: 9.690270397706107e-05, l2: 0.0003390235391894488   Iteration 57 of 100, tot loss = 4.346060100354646, l1: 9.65568033918093e-05, l2: 0.00033804921079998867   Iteration 58 of 100, tot loss = 4.3723205615734235, l1: 9.709794521141106e-05, l2: 0.0003401341156608521   Iteration 59 of 100, tot loss = 4.361807261483144, l1: 9.698069772053652e-05, l2: 0.000339200033044632   Iteration 60 of 100, tot loss = 4.3646657506624855, l1: 9.666712788506024e-05, l2: 0.00033979945146711543   Iteration 61 of 100, tot loss = 4.421240787037084, l1: 9.778464711834219e-05, l2: 0.0003443394361071472   Iteration 62 of 100, tot loss = 4.430404436203741, l1: 9.805278404523813e-05, l2: 0.00034498766434544156   Iteration 63 of 100, tot loss = 4.423737234539455, l1: 9.795894671004221e-05, l2: 0.0003444147813651297   Iteration 64 of 100, tot loss = 4.4294453002512455, l1: 9.791149460625093e-05, l2: 0.00034503303959354525   Iteration 65 of 100, tot loss = 4.4317848682403564, l1: 9.768783610735017e-05, l2: 0.0003454906552528533   Iteration 66 of 100, tot loss = 4.4310053659207895, l1: 9.758415889695188e-05, l2: 0.00034551638236735016   Iteration 67 of 100, tot loss = 4.444214368934062, l1: 9.772250540215703e-05, l2: 0.0003466989359394439   Iteration 68 of 100, tot loss = 4.432417771395515, l1: 9.755776735640692e-05, l2: 0.00034568401443881583   Iteration 69 of 100, tot loss = 4.434830596481544, l1: 9.780568248917943e-05, l2: 0.000345677381593977   Iteration 70 of 100, tot loss = 4.4228843382426675, l1: 9.759310243160664e-05, l2: 0.0003446953358694113   Iteration 71 of 100, tot loss = 4.405313908214301, l1: 9.7242418812296e-05, l2: 0.00034328897665648287   Iteration 72 of 100, tot loss = 4.423927903175354, l1: 9.767056220274147e-05, l2: 0.00034472223261319514   Iteration 73 of 100, tot loss = 4.409433237493855, l1: 9.72346974714589e-05, l2: 0.00034370863043116276   Iteration 74 of 100, tot loss = 4.3802241795771835, l1: 9.653324077835596e-05, l2: 0.0003414891813205228   Iteration 75 of 100, tot loss = 4.378850456873576, l1: 9.684314112140176e-05, l2: 0.0003410419086382414   Iteration 76 of 100, tot loss = 4.396939073738299, l1: 9.72548858427265e-05, l2: 0.00034243902547658714   Iteration 77 of 100, tot loss = 4.405358782062283, l1: 9.772051653980471e-05, l2: 0.0003428153654744897   Iteration 78 of 100, tot loss = 4.3929010935318775, l1: 9.782458348640014e-05, l2: 0.0003414655297185676   Iteration 79 of 100, tot loss = 4.4247730683676805, l1: 9.811067355235249e-05, l2: 0.00034436663701876855   Iteration 80 of 100, tot loss = 4.400589191913605, l1: 9.741853609739337e-05, l2: 0.000342640386770654   Iteration 81 of 100, tot loss = 4.412559697657455, l1: 9.78959986600841e-05, l2: 0.000343359975010605   Iteration 82 of 100, tot loss = 4.464270649886712, l1: 9.857514971106245e-05, l2: 0.00034785191950859593   Iteration 83 of 100, tot loss = 4.447475895824202, l1: 9.792179603154872e-05, l2: 0.0003468257977125089   Iteration 84 of 100, tot loss = 4.4526166546912425, l1: 9.797333699506929e-05, l2: 0.00034728833253168325   Iteration 85 of 100, tot loss = 4.440464471368228, l1: 9.752621327232405e-05, l2: 0.00034652023785062793   Iteration 86 of 100, tot loss = 4.446244880210521, l1: 9.767106915438321e-05, l2: 0.00034695342240643925   Iteration 87 of 100, tot loss = 4.425113431338606, l1: 9.742434264191469e-05, l2: 0.0003450870040226085   Iteration 88 of 100, tot loss = 4.408910057761452, l1: 9.73022578860268e-05, l2: 0.0003435887515066001   Iteration 89 of 100, tot loss = 4.422818483931295, l1: 9.743775914879673e-05, l2: 0.00034484409304006087   Iteration 90 of 100, tot loss = 4.444331571790907, l1: 9.795020341698545e-05, l2: 0.00034648295792673404   Iteration 91 of 100, tot loss = 4.460192386920635, l1: 9.79926091811369e-05, l2: 0.0003480266337986565   Iteration 92 of 100, tot loss = 4.46011061253755, l1: 9.816770204412498e-05, l2: 0.00034784336315604617   Iteration 93 of 100, tot loss = 4.472846149116434, l1: 9.818552178487191e-05, l2: 0.00034909909706868194   Iteration 94 of 100, tot loss = 4.461929752471599, l1: 9.790551341714664e-05, l2: 0.0003482874656564239   Iteration 95 of 100, tot loss = 4.447418865404631, l1: 9.774156247945747e-05, l2: 0.00034700032786196585   Iteration 96 of 100, tot loss = 4.429437421262264, l1: 9.758246494584455e-05, l2: 0.0003453612810820535   Iteration 97 of 100, tot loss = 4.4324256931383585, l1: 9.756989540206989e-05, l2: 0.00034567267788794927   Iteration 98 of 100, tot loss = 4.4256997595028, l1: 9.741362043073145e-05, l2: 0.00034515635937224236   Iteration 99 of 100, tot loss = 4.435652886978303, l1: 9.759147026055173e-05, l2: 0.000345973822116532   Iteration 100 of 100, tot loss = 4.421749053001403, l1: 9.714819032524246e-05, l2: 0.0003450267185689881
   End of epoch 1244; saving model... 

Epoch 1245 of 2000
   Iteration 1 of 100, tot loss = 7.498640537261963, l1: 0.00014363379159476608, l2: 0.0006062302272766829   Iteration 2 of 100, tot loss = 6.272903919219971, l1: 0.00012543273987830617, l2: 0.0005018576339352876   Iteration 3 of 100, tot loss = 6.044633706410726, l1: 0.00012594297246929878, l2: 0.00047852038793886703   Iteration 4 of 100, tot loss = 5.786855697631836, l1: 0.00012503673679020721, l2: 0.0004536488268058747   Iteration 5 of 100, tot loss = 5.695411682128906, l1: 0.00012504487967817112, l2: 0.0004444962774869055   Iteration 6 of 100, tot loss = 5.475024461746216, l1: 0.00011850795878369051, l2: 0.0004289944821114962   Iteration 7 of 100, tot loss = 5.509648731776646, l1: 0.00011960036707543103, l2: 0.00043136450611720126   Iteration 8 of 100, tot loss = 5.486833930015564, l1: 0.0001209375286634895, l2: 0.00042774586472660303   Iteration 9 of 100, tot loss = 5.389532089233398, l1: 0.00012131425561771418, l2: 0.00041763895488758053   Iteration 10 of 100, tot loss = 5.26186990737915, l1: 0.00011914207207155413, l2: 0.00040704492130316794   Iteration 11 of 100, tot loss = 5.3721979748119, l1: 0.00011784279658141631, l2: 0.0004193770053627139   Iteration 12 of 100, tot loss = 5.217010458310445, l1: 0.00011564189480850473, l2: 0.00040605915516304475   Iteration 13 of 100, tot loss = 5.0487937193650465, l1: 0.00011312385546401716, l2: 0.00039175551948853984   Iteration 14 of 100, tot loss = 4.931234530040196, l1: 0.00011128683399874717, l2: 0.0003818366219223078   Iteration 15 of 100, tot loss = 4.926286888122559, l1: 0.00011095594139381622, l2: 0.0003816727490630001   Iteration 16 of 100, tot loss = 4.9029640853405, l1: 0.00011110571858807816, l2: 0.00037919069109193515   Iteration 17 of 100, tot loss = 4.756924993851605, l1: 0.00010748692083699793, l2: 0.0003682055787922924   Iteration 18 of 100, tot loss = 4.680632882648045, l1: 0.00010574973829433374, l2: 0.00036231354978452955   Iteration 19 of 100, tot loss = 4.7122928468804615, l1: 0.00010564401198184657, l2: 0.000365585274075305   Iteration 20 of 100, tot loss = 4.779414963722229, l1: 0.00010647247418091866, l2: 0.0003714690246852115   Iteration 21 of 100, tot loss = 4.702835764203753, l1: 0.00010372826450509906, l2: 0.0003665553134245177   Iteration 22 of 100, tot loss = 4.773153435100209, l1: 0.00010448015729542186, l2: 0.0003728351889135824   Iteration 23 of 100, tot loss = 4.726943378863127, l1: 0.00010322245725546964, l2: 0.0003694718837758283   Iteration 24 of 100, tot loss = 4.706855585177739, l1: 0.00010216434642037105, l2: 0.00036852121532623033   Iteration 25 of 100, tot loss = 4.73827712059021, l1: 0.0001026695164910052, l2: 0.0003711581986863166   Iteration 26 of 100, tot loss = 4.633601573797373, l1: 0.00010025420711966805, l2: 0.00036310595337104483   Iteration 27 of 100, tot loss = 4.7070233910172075, l1: 0.00010158020692567031, l2: 0.0003691221338360467   Iteration 28 of 100, tot loss = 4.773591262953622, l1: 0.00010344016118324362, l2: 0.0003739189669431653   Iteration 29 of 100, tot loss = 4.82612655902731, l1: 0.00010475605387031904, l2: 0.00037785660393198887   Iteration 30 of 100, tot loss = 4.762518811225891, l1: 0.00010368382014955083, l2: 0.00037256806341853614   Iteration 31 of 100, tot loss = 4.784664407853158, l1: 0.00010508717264935014, l2: 0.0003733792705574043   Iteration 32 of 100, tot loss = 4.770019181072712, l1: 0.00010525682228035294, l2: 0.0003717450977092085   Iteration 33 of 100, tot loss = 4.753062385501283, l1: 0.00010476689600512724, l2: 0.0003705393433902469   Iteration 34 of 100, tot loss = 4.742763189708485, l1: 0.00010449979163240641, l2: 0.0003697765286607832   Iteration 35 of 100, tot loss = 4.743664639336722, l1: 0.00010425881960795128, l2: 0.000370107644786393   Iteration 36 of 100, tot loss = 4.72683529721366, l1: 0.00010356991131427801, l2: 0.00036911361894453876   Iteration 37 of 100, tot loss = 4.73031057538213, l1: 0.0001033760593083964, l2: 0.0003696549986646435   Iteration 38 of 100, tot loss = 4.787400942099722, l1: 0.00010426362591954928, l2: 0.0003744764686225129   Iteration 39 of 100, tot loss = 4.791565106465266, l1: 0.00010470330483095052, l2: 0.00037445320608989836   Iteration 40 of 100, tot loss = 4.776972085237503, l1: 0.00010435908679937712, l2: 0.00037333812106226104   Iteration 41 of 100, tot loss = 4.794050792368447, l1: 0.00010443728317069344, l2: 0.0003749677956121315   Iteration 42 of 100, tot loss = 4.790319334893, l1: 0.00010475814685508209, l2: 0.0003742737861189415   Iteration 43 of 100, tot loss = 4.789981725604036, l1: 0.00010490281555428018, l2: 0.0003740953563761269   Iteration 44 of 100, tot loss = 4.777592360973358, l1: 0.00010486399332876317, l2: 0.00037289524169120176   Iteration 45 of 100, tot loss = 4.791518227259318, l1: 0.00010531769593298021, l2: 0.00037383412547771716   Iteration 46 of 100, tot loss = 4.794011535851852, l1: 0.00010536664163025662, l2: 0.00037403451057363543   Iteration 47 of 100, tot loss = 4.803052339148014, l1: 0.00010587702544921256, l2: 0.0003744282074237956   Iteration 48 of 100, tot loss = 4.787575945258141, l1: 0.00010569976787640674, l2: 0.0003730578255272121   Iteration 49 of 100, tot loss = 4.751695754576702, l1: 0.00010458772618333068, l2: 0.00037058184786975307   Iteration 50 of 100, tot loss = 4.737688336372376, l1: 0.00010472173409652896, l2: 0.0003690470979199745   Iteration 51 of 100, tot loss = 4.769578564400766, l1: 0.00010528998274251124, l2: 0.0003716678717561687   Iteration 52 of 100, tot loss = 4.742540652935322, l1: 0.00010465760659336901, l2: 0.0003695964565290174   Iteration 53 of 100, tot loss = 4.761412908446114, l1: 0.00010525024735439076, l2: 0.00037089104128272256   Iteration 54 of 100, tot loss = 4.778815260639897, l1: 0.00010543046300881542, l2: 0.00037245106149283756   Iteration 55 of 100, tot loss = 4.801307747580788, l1: 0.00010604926726294004, l2: 0.00037408150621393526   Iteration 56 of 100, tot loss = 4.819185188838413, l1: 0.00010652624028547766, l2: 0.0003753922774194507   Iteration 57 of 100, tot loss = 4.834687676346093, l1: 0.00010695817087380739, l2: 0.00037651059596611417   Iteration 58 of 100, tot loss = 4.815666079521179, l1: 0.00010658286467740505, l2: 0.00037498374249794167   Iteration 59 of 100, tot loss = 4.804417420241792, l1: 0.0001065085556454783, l2: 0.0003739331853358959   Iteration 60 of 100, tot loss = 4.840969876448313, l1: 0.0001071010147522126, l2: 0.0003769959723285865   Iteration 61 of 100, tot loss = 4.83491991777889, l1: 0.00010691032259244106, l2: 0.00037658166881749923   Iteration 62 of 100, tot loss = 4.8605726803502725, l1: 0.00010750110208567592, l2: 0.0003785561658010157   Iteration 63 of 100, tot loss = 4.831687931030515, l1: 0.00010682528308178816, l2: 0.00037634350952603634   Iteration 64 of 100, tot loss = 4.825388673692942, l1: 0.000106453956277619, l2: 0.00037608491084029083   Iteration 65 of 100, tot loss = 4.826391260440533, l1: 0.00010639033862389625, l2: 0.0003762487872826079   Iteration 66 of 100, tot loss = 4.819721124388955, l1: 0.00010599167867577776, l2: 0.00037598043369750184   Iteration 67 of 100, tot loss = 4.793790009484362, l1: 0.00010559768433362912, l2: 0.0003737813164132522   Iteration 68 of 100, tot loss = 4.808901523842531, l1: 0.0001061741225866546, l2: 0.00037471602990990505   Iteration 69 of 100, tot loss = 4.831637876621191, l1: 0.00010640333314675945, l2: 0.0003767604547370549   Iteration 70 of 100, tot loss = 4.848042954717363, l1: 0.00010656679774651171, l2: 0.00037823749756041384   Iteration 71 of 100, tot loss = 4.901270950344247, l1: 0.00010756335787320981, l2: 0.00038256373708989   Iteration 72 of 100, tot loss = 4.880614221096039, l1: 0.0001072278505994796, l2: 0.0003808335713983979   Iteration 73 of 100, tot loss = 4.930229245799861, l1: 0.00010811525812828659, l2: 0.0003849076659718452   Iteration 74 of 100, tot loss = 4.909541642343676, l1: 0.00010771571504773346, l2: 0.000383238448538012   Iteration 75 of 100, tot loss = 4.939415953954061, l1: 0.00010815548836641634, l2: 0.0003857861067323635   Iteration 76 of 100, tot loss = 4.9223267028206275, l1: 0.00010799761405092125, l2: 0.00038423505573423187   Iteration 77 of 100, tot loss = 4.932119933041659, l1: 0.00010852070030901436, l2: 0.00038469129192110686   Iteration 78 of 100, tot loss = 4.907523124645918, l1: 0.0001081923354575291, l2: 0.00038255997601365193   Iteration 79 of 100, tot loss = 4.9317938044101375, l1: 0.0001082533747223029, l2: 0.00038492600518261097   Iteration 80 of 100, tot loss = 4.941164296865463, l1: 0.00010815458572324132, l2: 0.00038596184331254337   Iteration 81 of 100, tot loss = 4.936011755907977, l1: 0.00010797047156144946, l2: 0.00038563070317908525   Iteration 82 of 100, tot loss = 4.94123628081345, l1: 0.00010813563246146503, l2: 0.0003859879945549637   Iteration 83 of 100, tot loss = 4.953201885682991, l1: 0.0001083810790757514, l2: 0.0003869391089594216   Iteration 84 of 100, tot loss = 4.937744072505406, l1: 0.00010811578050336157, l2: 0.0003856586261549854   Iteration 85 of 100, tot loss = 4.947415189182057, l1: 0.0001082909617894877, l2: 0.00038645055662046244   Iteration 86 of 100, tot loss = 4.937320731406988, l1: 0.00010766460717456729, l2: 0.00038606746560152145   Iteration 87 of 100, tot loss = 4.954917058177378, l1: 0.00010784380272567412, l2: 0.00038764790291282423   Iteration 88 of 100, tot loss = 4.937223038890145, l1: 0.00010758125323594537, l2: 0.0003861410505950599   Iteration 89 of 100, tot loss = 4.926326100745897, l1: 0.00010741767467796185, l2: 0.00038521493516281626   Iteration 90 of 100, tot loss = 4.954917396439447, l1: 0.00010760234084348971, l2: 0.000387889398552943   Iteration 91 of 100, tot loss = 4.935831300504915, l1: 0.0001070592725127526, l2: 0.00038652385730209384   Iteration 92 of 100, tot loss = 4.919141204460807, l1: 0.00010666766162182245, l2: 0.0003852464586721591   Iteration 93 of 100, tot loss = 4.940458256711242, l1: 0.00010671490842166047, l2: 0.0003873309170919901   Iteration 94 of 100, tot loss = 4.95386210908281, l1: 0.0001069817614947208, l2: 0.00038840444904053584   Iteration 95 of 100, tot loss = 4.9531129686455975, l1: 0.00010716115679028198, l2: 0.0003881501396934159   Iteration 96 of 100, tot loss = 4.979109088579814, l1: 0.00010762504492352794, l2: 0.00039028586388667463   Iteration 97 of 100, tot loss = 4.9798943480265505, l1: 0.00010777644368402238, l2: 0.00039021299114407607   Iteration 98 of 100, tot loss = 5.0039462216046395, l1: 0.00010815207838741302, l2: 0.00039224254370700303   Iteration 99 of 100, tot loss = 4.991382109998453, l1: 0.00010809897371059792, l2: 0.00039103923696493073   Iteration 100 of 100, tot loss = 5.021149995326996, l1: 0.00010867397737456486, l2: 0.00039344102187897077
   End of epoch 1245; saving model... 

Epoch 1246 of 2000
   Iteration 1 of 100, tot loss = 3.2042269706726074, l1: 7.944069511722773e-05, l2: 0.0002409819862805307   Iteration 2 of 100, tot loss = 3.664346694946289, l1: 9.910367225529626e-05, l2: 0.00026733099366538227   Iteration 3 of 100, tot loss = 4.552052021026611, l1: 0.00010170251819848393, l2: 0.0003535026723208527   Iteration 4 of 100, tot loss = 4.056793034076691, l1: 8.722956863493891e-05, l2: 0.00031844972909311764   Iteration 5 of 100, tot loss = 4.12801718711853, l1: 9.178654363495298e-05, l2: 0.00032101517135743054   Iteration 6 of 100, tot loss = 4.237628102302551, l1: 9.398821627352542e-05, l2: 0.0003297745934105478   Iteration 7 of 100, tot loss = 4.558430841990879, l1: 9.709147447470709e-05, l2: 0.0003587516112020239   Iteration 8 of 100, tot loss = 4.40653121471405, l1: 9.744229055286269e-05, l2: 0.0003432108314882498   Iteration 9 of 100, tot loss = 4.498106055789524, l1: 9.946970506765258e-05, l2: 0.00035034089685521193   Iteration 10 of 100, tot loss = 4.47733964920044, l1: 0.00010179317141592036, l2: 0.0003459407889749855   Iteration 11 of 100, tot loss = 4.562029751864347, l1: 0.00010355501481998627, l2: 0.0003526479572015391   Iteration 12 of 100, tot loss = 4.585318366686503, l1: 0.00010367367607007812, l2: 0.0003548581565458638   Iteration 13 of 100, tot loss = 4.511161914238563, l1: 0.0001035581621041414, l2: 0.0003475580261482929   Iteration 14 of 100, tot loss = 4.455516542707171, l1: 0.00010383503078108853, l2: 0.00034171662160328457   Iteration 15 of 100, tot loss = 4.430766963958741, l1: 0.00010343610653459715, l2: 0.0003396405877235035   Iteration 16 of 100, tot loss = 4.555382668972015, l1: 0.00010181371476392087, l2: 0.000353724550222978   Iteration 17 of 100, tot loss = 4.628650749430937, l1: 0.00010401592097717666, l2: 0.00035884915268979967   Iteration 18 of 100, tot loss = 4.6010880205366345, l1: 0.0001039514780940307, l2: 0.00035615732324206166   Iteration 19 of 100, tot loss = 4.572645262668007, l1: 0.00010374740917253995, l2: 0.000353517116500849   Iteration 20 of 100, tot loss = 4.54887273311615, l1: 0.00010324274426238844, l2: 0.0003516445285640657   Iteration 21 of 100, tot loss = 4.53292315346854, l1: 0.00010254847421449431, l2: 0.00035074383984985095   Iteration 22 of 100, tot loss = 4.612362384796143, l1: 0.00010381011634308379, l2: 0.00035742612089961767   Iteration 23 of 100, tot loss = 4.502272398575492, l1: 0.00010184214245426752, l2: 0.00034838509632011306   Iteration 24 of 100, tot loss = 4.528724829355876, l1: 0.00010056455024217333, l2: 0.0003523079303704435   Iteration 25 of 100, tot loss = 4.660074481964111, l1: 0.00010296096297679469, l2: 0.00036304648441728206   Iteration 26 of 100, tot loss = 4.614777069825393, l1: 0.00010225169955027432, l2: 0.0003592260064369139   Iteration 27 of 100, tot loss = 4.594705334416142, l1: 0.00010146020374829984, l2: 0.00035801032801693373   Iteration 28 of 100, tot loss = 4.515141470091684, l1: 0.00010011634575286215, l2: 0.00035139779928223494   Iteration 29 of 100, tot loss = 4.535401985563081, l1: 0.00010036392556904848, l2: 0.0003531762716914367   Iteration 30 of 100, tot loss = 4.494684815406799, l1: 9.996410735766404e-05, l2: 0.0003495043730557275   Iteration 31 of 100, tot loss = 4.544574914440032, l1: 0.00010080365515727129, l2: 0.00035365383495621744   Iteration 32 of 100, tot loss = 4.652917675673962, l1: 0.00010231129840576614, l2: 0.0003629804691627214   Iteration 33 of 100, tot loss = 4.616049672618057, l1: 0.00010152725537244061, l2: 0.0003600777122439026   Iteration 34 of 100, tot loss = 4.600561036783106, l1: 0.00010158870645682327, l2: 0.00035846739817369617   Iteration 35 of 100, tot loss = 4.595454563413347, l1: 0.00010169816918538085, l2: 0.00035784728866669217   Iteration 36 of 100, tot loss = 4.599416792392731, l1: 0.00010132403541421, l2: 0.00035861764586621173   Iteration 37 of 100, tot loss = 4.608029011133555, l1: 0.00010113374997131728, l2: 0.0003596691535062131   Iteration 38 of 100, tot loss = 4.607828585725081, l1: 0.00010121977002678537, l2: 0.0003595630907128859   Iteration 39 of 100, tot loss = 4.613429980400281, l1: 0.0001013771011807526, l2: 0.0003599658991263893   Iteration 40 of 100, tot loss = 4.655481821298599, l1: 0.0001018176419165684, l2: 0.0003637305420852499   Iteration 41 of 100, tot loss = 4.6442467817446085, l1: 0.00010216601491797851, l2: 0.0003622586653630317   Iteration 42 of 100, tot loss = 4.6229539314905805, l1: 0.00010200421006413221, l2: 0.000360291185470054   Iteration 43 of 100, tot loss = 4.724993212278499, l1: 0.00010344260734327267, l2: 0.00036905671444147564   Iteration 44 of 100, tot loss = 4.7594890215180135, l1: 0.00010350784081806937, l2: 0.0003724410625264599   Iteration 45 of 100, tot loss = 4.717326323191325, l1: 0.00010271861538058147, l2: 0.00036901401795653836   Iteration 46 of 100, tot loss = 4.701365191003551, l1: 0.00010237862174424505, l2: 0.0003677578978291105   Iteration 47 of 100, tot loss = 4.68622088432312, l1: 0.0001026651524180705, l2: 0.0003659569366062258   Iteration 48 of 100, tot loss = 4.70069966216882, l1: 0.00010342381059066004, l2: 0.00036664615618065   Iteration 49 of 100, tot loss = 4.730761883210163, l1: 0.00010370913366145664, l2: 0.0003693670554713783   Iteration 50 of 100, tot loss = 4.796938786506653, l1: 0.00010496961287572049, l2: 0.0003747242665849626   Iteration 51 of 100, tot loss = 4.772793559467091, l1: 0.0001046692481145774, l2: 0.00037261010855253713   Iteration 52 of 100, tot loss = 4.779493116415464, l1: 0.00010470412067101839, l2: 0.00037324519227975263   Iteration 53 of 100, tot loss = 4.8336970536214, l1: 0.00010528679874523561, l2: 0.0003780829072227034   Iteration 54 of 100, tot loss = 4.831316970012806, l1: 0.00010531122111219533, l2: 0.0003778204764886242   Iteration 55 of 100, tot loss = 4.793449115753174, l1: 0.00010463252481051975, l2: 0.00037471238703636284   Iteration 56 of 100, tot loss = 4.788611318383898, l1: 0.00010476645924687286, l2: 0.0003740946730041677   Iteration 57 of 100, tot loss = 4.793622485378332, l1: 0.00010474901117911366, l2: 0.0003746132373507543   Iteration 58 of 100, tot loss = 4.784627560911508, l1: 0.0001046496532769103, l2: 0.00037381310288890683   Iteration 59 of 100, tot loss = 4.7718734175471935, l1: 0.00010473801117090476, l2: 0.00037244933073298403   Iteration 60 of 100, tot loss = 4.7649423758188885, l1: 0.00010478314106876496, l2: 0.00037171109676516304   Iteration 61 of 100, tot loss = 4.775678267244433, l1: 0.00010531217961469055, l2: 0.00037225564754735986   Iteration 62 of 100, tot loss = 4.7641228014423005, l1: 0.00010475736546082302, l2: 0.0003716549150750882   Iteration 63 of 100, tot loss = 4.762639310624865, l1: 0.00010506606185882693, l2: 0.00037119786960945007   Iteration 64 of 100, tot loss = 4.752122610807419, l1: 0.00010515072028738359, l2: 0.000370061541616451   Iteration 65 of 100, tot loss = 4.780126505631667, l1: 0.00010525750694796443, l2: 0.0003727551451167808   Iteration 66 of 100, tot loss = 4.7471497564604785, l1: 0.00010487136916543187, l2: 0.00036984360772431967   Iteration 67 of 100, tot loss = 4.742466392801769, l1: 0.000104926777295525, l2: 0.0003693198631847722   Iteration 68 of 100, tot loss = 4.8008433019413665, l1: 0.00010534458092211828, l2: 0.00037473975060172104   Iteration 69 of 100, tot loss = 4.75921774947125, l1: 0.00010469439070405222, l2: 0.0003712273854931034   Iteration 70 of 100, tot loss = 4.7376418249947685, l1: 0.00010455718074808829, l2: 0.0003692070032619605   Iteration 71 of 100, tot loss = 4.740035285412426, l1: 0.00010439186079450555, l2: 0.00036961166905468773   Iteration 72 of 100, tot loss = 4.7288194133175745, l1: 0.00010405039231247631, l2: 0.0003688315500767203   Iteration 73 of 100, tot loss = 4.739449693732066, l1: 0.00010404183847466102, l2: 0.0003699031317597   Iteration 74 of 100, tot loss = 4.726505005681837, l1: 0.00010417595449079936, l2: 0.0003684745471046709   Iteration 75 of 100, tot loss = 4.714347645441691, l1: 0.00010430734313558787, l2: 0.0003671274227478231   Iteration 76 of 100, tot loss = 4.72304316884593, l1: 0.00010445087106123346, l2: 0.0003678534472113059   Iteration 77 of 100, tot loss = 4.725529797665485, l1: 0.00010432932038667503, l2: 0.0003682236607086712   Iteration 78 of 100, tot loss = 4.742131548050122, l1: 0.00010475399409561083, l2: 0.00036945916135431244   Iteration 79 of 100, tot loss = 4.746499342254445, l1: 0.00010459940317617375, l2: 0.0003700505316189691   Iteration 80 of 100, tot loss = 4.756342127919197, l1: 0.00010451361667946912, l2: 0.00037112059671926543   Iteration 81 of 100, tot loss = 4.785440789328681, l1: 0.00010525282198580465, l2: 0.00037329125718966723   Iteration 82 of 100, tot loss = 4.7810099444738245, l1: 0.0001053656670570135, l2: 0.0003727353275568997   Iteration 83 of 100, tot loss = 4.751697238669338, l1: 0.00010495228071890615, l2: 0.00037021744320960156   Iteration 84 of 100, tot loss = 4.752400196733928, l1: 0.00010476184789218852, l2: 0.0003704781721310047   Iteration 85 of 100, tot loss = 4.747656589395859, l1: 0.00010474579124574495, l2: 0.0003700198679272195   Iteration 86 of 100, tot loss = 4.764273069625677, l1: 0.00010481157952981872, l2: 0.00037161572778808647   Iteration 87 of 100, tot loss = 4.753121951530719, l1: 0.0001044150034765895, l2: 0.000370897191955344   Iteration 88 of 100, tot loss = 4.768069326877594, l1: 0.00010477903064880097, l2: 0.00037202790239958136   Iteration 89 of 100, tot loss = 4.755589238713297, l1: 0.000104591300860806, l2: 0.00037096762356709356   Iteration 90 of 100, tot loss = 4.765280840131972, l1: 0.00010486484784956297, l2: 0.0003716632363244167   Iteration 91 of 100, tot loss = 4.76033878326416, l1: 0.00010483696062762577, l2: 0.00037119691794262275   Iteration 92 of 100, tot loss = 4.74375247437021, l1: 0.00010464394978518375, l2: 0.0003697312979214906   Iteration 93 of 100, tot loss = 4.757244807417675, l1: 0.00010482767710752625, l2: 0.0003708968037909638   Iteration 94 of 100, tot loss = 4.725131926384378, l1: 0.000104303469576754, l2: 0.00036820972324695753   Iteration 95 of 100, tot loss = 4.729819095762152, l1: 0.0001043452128406467, l2: 0.0003686366967342206   Iteration 96 of 100, tot loss = 4.722324683020513, l1: 0.00010414864770306546, l2: 0.0003680838208310888   Iteration 97 of 100, tot loss = 4.740619422234211, l1: 0.00010432949574417002, l2: 0.00036973244667811724   Iteration 98 of 100, tot loss = 4.741217796899835, l1: 0.00010453033068262477, l2: 0.00036959144897397834   Iteration 99 of 100, tot loss = 4.744252974336797, l1: 0.00010469973788742503, l2: 0.0003697255595747118   Iteration 100 of 100, tot loss = 4.7620250403881075, l1: 0.00010461174941156059, l2: 0.0003715907543664798
   End of epoch 1246; saving model... 

Epoch 1247 of 2000
   Iteration 1 of 100, tot loss = 4.313658237457275, l1: 7.925155659904703e-05, l2: 0.0003521142643876374   Iteration 2 of 100, tot loss = 5.41337251663208, l1: 0.00010850497710634954, l2: 0.00043283228296786547   Iteration 3 of 100, tot loss = 5.6451036135355634, l1: 0.00011648835061350837, l2: 0.0004480220183419685   Iteration 4 of 100, tot loss = 5.747689485549927, l1: 0.00011831952178908978, l2: 0.0004564494302030653   Iteration 5 of 100, tot loss = 5.6105852127075195, l1: 0.00011546935711521655, l2: 0.00044558917288668455   Iteration 6 of 100, tot loss = 5.46036163965861, l1: 0.0001154353534123705, l2: 0.00043060081467653316   Iteration 7 of 100, tot loss = 5.332167421068464, l1: 0.0001136576704863858, l2: 0.00041955907779213576   Iteration 8 of 100, tot loss = 5.695709526538849, l1: 0.00011947939583478728, l2: 0.0004500915565586183   Iteration 9 of 100, tot loss = 5.36404554049174, l1: 0.00011587187974429171, l2: 0.0004205326759903174   Iteration 10 of 100, tot loss = 5.356930041313172, l1: 0.00011493098136270419, l2: 0.0004207620266242884   Iteration 11 of 100, tot loss = 5.081241130828857, l1: 0.00011032220407452604, l2: 0.00039780191383959556   Iteration 12 of 100, tot loss = 5.034958004951477, l1: 0.0001108324892508487, l2: 0.00039266331562733586   Iteration 13 of 100, tot loss = 5.000452408423791, l1: 0.00011113982607359783, l2: 0.00038890541831138904   Iteration 14 of 100, tot loss = 4.992675542831421, l1: 0.00011059592101706326, l2: 0.0003886716367560439   Iteration 15 of 100, tot loss = 4.864626773198446, l1: 0.00010978715678599353, l2: 0.00037667552339068304   Iteration 16 of 100, tot loss = 4.747806280851364, l1: 0.00010836943511094432, l2: 0.0003664111955004046   Iteration 17 of 100, tot loss = 4.742893639732809, l1: 0.00010797519344658427, l2: 0.0003663141722106101   Iteration 18 of 100, tot loss = 4.811726358201769, l1: 0.0001084767180954158, l2: 0.00037269592091130715   Iteration 19 of 100, tot loss = 4.775195121765137, l1: 0.00010818490907120959, l2: 0.0003693346041377242   Iteration 20 of 100, tot loss = 4.711554682254791, l1: 0.00010606316609482747, l2: 0.00036509230412775653   Iteration 21 of 100, tot loss = 4.73377419653393, l1: 0.00010675057481941102, l2: 0.0003666268464820903   Iteration 22 of 100, tot loss = 4.700017603960904, l1: 0.00010521952935960144, l2: 0.0003647822317857803   Iteration 23 of 100, tot loss = 4.697345277537471, l1: 0.00010532336771417329, l2: 0.0003644111616860913   Iteration 24 of 100, tot loss = 4.713310658931732, l1: 0.00010586718720636175, l2: 0.0003654638797646233   Iteration 25 of 100, tot loss = 4.630260343551636, l1: 0.00010434523341245949, l2: 0.0003586808015825227   Iteration 26 of 100, tot loss = 4.631395000677842, l1: 0.00010482719993048634, l2: 0.0003583123006347495   Iteration 27 of 100, tot loss = 4.673922847818445, l1: 0.00010577143634522886, l2: 0.00036162085005478865   Iteration 28 of 100, tot loss = 4.697640172072819, l1: 0.00010598537431048629, l2: 0.0003637786446363732   Iteration 29 of 100, tot loss = 4.618544373018988, l1: 0.0001045113975374863, l2: 0.00035734304150662803   Iteration 30 of 100, tot loss = 4.544552437464396, l1: 0.00010326922298797096, l2: 0.00035118602245347573   Iteration 31 of 100, tot loss = 4.538863812723467, l1: 0.00010331473012845362, l2: 0.000350571651999358   Iteration 32 of 100, tot loss = 4.526150569319725, l1: 0.00010294401158716937, l2: 0.00034967104647876113   Iteration 33 of 100, tot loss = 4.481895049413045, l1: 0.00010215835054898916, l2: 0.00034603115548661936   Iteration 34 of 100, tot loss = 4.515532402431264, l1: 0.00010167077283874867, l2: 0.00034988246882087826   Iteration 35 of 100, tot loss = 4.517505189350673, l1: 0.00010196090713309656, l2: 0.000349789612560666   Iteration 36 of 100, tot loss = 4.478592879242367, l1: 0.00010181188028784365, l2: 0.00034604740802832466   Iteration 37 of 100, tot loss = 4.47853177946967, l1: 0.00010147259525035986, l2: 0.0003463805836430323   Iteration 38 of 100, tot loss = 4.525745423216569, l1: 0.00010259050606308799, l2: 0.0003499840367064615   Iteration 39 of 100, tot loss = 4.533377812458919, l1: 0.00010160916728916793, l2: 0.00035172861363464157   Iteration 40 of 100, tot loss = 4.547555392980575, l1: 0.0001021604673951515, l2: 0.00035259507167211267   Iteration 41 of 100, tot loss = 4.542494686638436, l1: 0.0001023339039132158, l2: 0.0003519155636639902   Iteration 42 of 100, tot loss = 4.489301153591701, l1: 0.00010138171914842955, l2: 0.00034754839517907906   Iteration 43 of 100, tot loss = 4.540812542272168, l1: 0.00010282028368999098, l2: 0.0003512609691824764   Iteration 44 of 100, tot loss = 4.48525245623155, l1: 0.00010157758126958718, l2: 0.00034694766310911456   Iteration 45 of 100, tot loss = 4.443106593026055, l1: 0.00010092422898095618, l2: 0.0003433864285600268   Iteration 46 of 100, tot loss = 4.46697343432385, l1: 0.00010105085746782488, l2: 0.00034564648408179534   Iteration 47 of 100, tot loss = 4.476134914032956, l1: 0.00010080223070504639, l2: 0.0003468112585362681   Iteration 48 of 100, tot loss = 4.46171210706234, l1: 0.00010090721169338697, l2: 0.00034526399698127835   Iteration 49 of 100, tot loss = 4.416874437916036, l1: 9.963727836516135e-05, l2: 0.0003420501633794332   Iteration 50 of 100, tot loss = 4.399480986595154, l1: 9.88620725547662e-05, l2: 0.00034108602441847326   Iteration 51 of 100, tot loss = 4.437940228219126, l1: 9.958561121511237e-05, l2: 0.00034420840971756216   Iteration 52 of 100, tot loss = 4.416878296778752, l1: 9.865638346057564e-05, l2: 0.0003430314447121838   Iteration 53 of 100, tot loss = 4.416217992890556, l1: 9.855919697141837e-05, l2: 0.000343062600257086   Iteration 54 of 100, tot loss = 4.403024227530868, l1: 9.843913358054124e-05, l2: 0.00034186328735409514   Iteration 55 of 100, tot loss = 4.502772881767966, l1: 0.00010001697929427874, l2: 0.0003502603059380569   Iteration 56 of 100, tot loss = 4.501662471464702, l1: 9.954311307020751e-05, l2: 0.0003506231311933204   Iteration 57 of 100, tot loss = 4.4931760629018145, l1: 9.923310873298825e-05, l2: 0.0003500844948320535   Iteration 58 of 100, tot loss = 4.546363061872022, l1: 0.00010027435464194398, l2: 0.0003543619491994895   Iteration 59 of 100, tot loss = 4.582884962275877, l1: 0.00010123174483157789, l2: 0.00035705674889544817   Iteration 60 of 100, tot loss = 4.552474395434062, l1: 0.00010074823439936154, l2: 0.0003544992025126703   Iteration 61 of 100, tot loss = 4.541961419777792, l1: 0.00010049584146100478, l2: 0.00035370029766708004   Iteration 62 of 100, tot loss = 4.588266088116553, l1: 0.000101443883097325, l2: 0.0003573827228636571   Iteration 63 of 100, tot loss = 4.560598452885945, l1: 0.00010104960482937121, l2: 0.00035501023734151016   Iteration 64 of 100, tot loss = 4.541872892528772, l1: 0.00010093212301853782, l2: 0.0003532551634179981   Iteration 65 of 100, tot loss = 4.515310148092417, l1: 0.00010053612070176034, l2: 0.0003509948909945356   Iteration 66 of 100, tot loss = 4.53093975240534, l1: 0.00010051049199925426, l2: 0.00035258348053144857   Iteration 67 of 100, tot loss = 4.525278020260939, l1: 0.00010062538688123199, l2: 0.0003519024126103092   Iteration 68 of 100, tot loss = 4.572287720792434, l1: 0.00010128618102017379, l2: 0.00035594258887987747   Iteration 69 of 100, tot loss = 4.573797025542328, l1: 0.0001014479109300368, l2: 0.0003559317891536604   Iteration 70 of 100, tot loss = 4.577684463773455, l1: 0.00010125710188211608, l2: 0.0003565113419816563   Iteration 71 of 100, tot loss = 4.581525863056451, l1: 0.000101064551850407, l2: 0.000357088032153986   Iteration 72 of 100, tot loss = 4.605496333705054, l1: 0.00010149871771621595, l2: 0.0003590509127712317   Iteration 73 of 100, tot loss = 4.596107626614505, l1: 0.00010113981789638199, l2: 0.00035847094183475136   Iteration 74 of 100, tot loss = 4.588839079882647, l1: 0.00010099803998978255, l2: 0.00035788586526020507   Iteration 75 of 100, tot loss = 4.608983033498128, l1: 0.00010125193647885075, l2: 0.0003596463642315939   Iteration 76 of 100, tot loss = 4.58632029671418, l1: 0.0001007305792499162, l2: 0.0003579014481051433   Iteration 77 of 100, tot loss = 4.59304623479967, l1: 0.00010065940570074146, l2: 0.0003586452157242683   Iteration 78 of 100, tot loss = 4.596339106559753, l1: 0.00010096189511480193, l2: 0.00035867201334510284   Iteration 79 of 100, tot loss = 4.608659240263927, l1: 0.00010118833772508026, l2: 0.0003596775843820806   Iteration 80 of 100, tot loss = 4.614964988827706, l1: 0.00010154517131013562, l2: 0.00035995132602693045   Iteration 81 of 100, tot loss = 4.6341493777286855, l1: 0.00010191639012191445, l2: 0.0003614985462375859   Iteration 82 of 100, tot loss = 4.626690710463175, l1: 0.0001017457943817135, l2: 0.00036092327549917304   Iteration 83 of 100, tot loss = 4.648991805961333, l1: 0.00010230309070096384, l2: 0.00036259608887450164   Iteration 84 of 100, tot loss = 4.656343213149479, l1: 0.00010260685017523688, l2: 0.00036302746974840937   Iteration 85 of 100, tot loss = 4.641662620095646, l1: 0.00010250042259405531, l2: 0.0003616658381789046   Iteration 86 of 100, tot loss = 4.634282899457355, l1: 0.00010274497016812241, l2: 0.0003606833183872622   Iteration 87 of 100, tot loss = 4.616698448685394, l1: 0.00010248805776281261, l2: 0.00035918178572051825   Iteration 88 of 100, tot loss = 4.61027025363662, l1: 0.00010234529343671801, l2: 0.0003586817309207452   Iteration 89 of 100, tot loss = 4.595463308055749, l1: 0.00010213832505991725, l2: 0.0003574080045826817   Iteration 90 of 100, tot loss = 4.609847243626913, l1: 0.00010245301212610988, l2: 0.0003585317110668661   Iteration 91 of 100, tot loss = 4.587836168624542, l1: 0.00010216467481958527, l2: 0.0003566189407138154   Iteration 92 of 100, tot loss = 4.621974009534587, l1: 0.00010268599781620256, l2: 0.0003595114016523281   Iteration 93 of 100, tot loss = 4.61001895576395, l1: 0.00010236760918101076, l2: 0.00035863428489900926   Iteration 94 of 100, tot loss = 4.6155298517105425, l1: 0.00010230331579495736, l2: 0.00035924966768435937   Iteration 95 of 100, tot loss = 4.6127983093261715, l1: 0.00010240285237638378, l2: 0.0003588769770239627   Iteration 96 of 100, tot loss = 4.609692171216011, l1: 0.00010224502921118983, l2: 0.00035872418645036913   Iteration 97 of 100, tot loss = 4.633759046338268, l1: 0.00010231634741168968, l2: 0.000361059556058704   Iteration 98 of 100, tot loss = 4.649922283328309, l1: 0.00010254669771767316, l2: 0.00036244552979858746   Iteration 99 of 100, tot loss = 4.688786236926763, l1: 0.00010327973054378585, l2: 0.0003655988923238703   Iteration 100 of 100, tot loss = 4.680133635997772, l1: 0.00010321412344637792, l2: 0.00036479923946899364
   End of epoch 1247; saving model... 

Epoch 1248 of 2000
   Iteration 1 of 100, tot loss = 2.94179630279541, l1: 8.589312346884981e-05, l2: 0.00020828649576287717   Iteration 2 of 100, tot loss = 3.7708981037139893, l1: 9.985247015720233e-05, l2: 0.00027723734820028767   Iteration 3 of 100, tot loss = 4.104877630869548, l1: 9.568874763014416e-05, l2: 0.0003147990161475415   Iteration 4 of 100, tot loss = 4.7809077501297, l1: 0.00010161280806642026, l2: 0.0003764779794437345   Iteration 5 of 100, tot loss = 4.645829677581787, l1: 0.00010395931894890964, l2: 0.0003606236627092585   Iteration 6 of 100, tot loss = 4.720864375432332, l1: 0.0001070991565939039, l2: 0.0003649872839256811   Iteration 7 of 100, tot loss = 4.524418115615845, l1: 0.00010402697392107387, l2: 0.0003484148411579164   Iteration 8 of 100, tot loss = 4.690925985574722, l1: 0.00010457354437676258, l2: 0.00036451906089496333   Iteration 9 of 100, tot loss = 4.710626575681898, l1: 0.00010665957072180593, l2: 0.00036440309648039855   Iteration 10 of 100, tot loss = 4.701217293739319, l1: 0.00010593325569061563, l2: 0.00036418848467292266   Iteration 11 of 100, tot loss = 4.587069381367076, l1: 0.00010500595552994955, l2: 0.00035370099331802606   Iteration 12 of 100, tot loss = 4.493711551030477, l1: 0.00010224668767477851, l2: 0.0003471244793521085   Iteration 13 of 100, tot loss = 4.729602373563326, l1: 0.00010685300591061465, l2: 0.00036610724167146074   Iteration 14 of 100, tot loss = 5.002054963793073, l1: 0.00011184740027861803, l2: 0.0003883581032693785   Iteration 15 of 100, tot loss = 4.990343475341797, l1: 0.00011256514359653617, l2: 0.00038646920875180514   Iteration 16 of 100, tot loss = 4.979023575782776, l1: 0.00011303334667900344, l2: 0.0003848690166705637   Iteration 17 of 100, tot loss = 5.027261734008789, l1: 0.00011092653842163546, l2: 0.00039179964069797495   Iteration 18 of 100, tot loss = 5.01774345503913, l1: 0.00011007125916269918, l2: 0.0003917030901195378   Iteration 19 of 100, tot loss = 5.001214353661788, l1: 0.00011005333094784107, l2: 0.0003900681093862084   Iteration 20 of 100, tot loss = 5.043061447143555, l1: 0.00011156596810906194, l2: 0.00039274018272408286   Iteration 21 of 100, tot loss = 5.04287542615618, l1: 0.00011213478934396768, l2: 0.00039215275665767315   Iteration 22 of 100, tot loss = 5.022664265199141, l1: 0.000110138600906463, l2: 0.0003921278277464973   Iteration 23 of 100, tot loss = 4.902562483497288, l1: 0.00010816447763278595, l2: 0.0003820917734866152   Iteration 24 of 100, tot loss = 4.827014535665512, l1: 0.00010664918469653155, l2: 0.00037605227165234584   Iteration 25 of 100, tot loss = 4.757649822235107, l1: 0.00010567625169642269, l2: 0.00037008873361628504   Iteration 26 of 100, tot loss = 4.822116374969482, l1: 0.00010761022097610224, l2: 0.00037460141790618835   Iteration 27 of 100, tot loss = 4.912434171747278, l1: 0.00010861766457781885, l2: 0.0003826257528055942   Iteration 28 of 100, tot loss = 4.9411897999899725, l1: 0.00010862023425163767, l2: 0.0003854987459947422   Iteration 29 of 100, tot loss = 4.876472785555083, l1: 0.00010670743622226994, l2: 0.0003809398426553877   Iteration 30 of 100, tot loss = 4.89420436223348, l1: 0.00010582492795947473, l2: 0.00038359550865910326   Iteration 31 of 100, tot loss = 4.867175532925513, l1: 0.00010512634487954088, l2: 0.00038159120923674276   Iteration 32 of 100, tot loss = 4.8317297250032425, l1: 0.0001038709184513209, l2: 0.0003793020546254411   Iteration 33 of 100, tot loss = 4.83234843340787, l1: 0.00010448887648332554, l2: 0.00037874596741995913   Iteration 34 of 100, tot loss = 4.812718138975256, l1: 0.00010430651769531644, l2: 0.00037696529633488837   Iteration 35 of 100, tot loss = 4.811377443586077, l1: 0.00010419190169029337, l2: 0.00037694584253975856   Iteration 36 of 100, tot loss = 4.786742826302846, l1: 0.00010281410909050869, l2: 0.00037586017384051555   Iteration 37 of 100, tot loss = 4.741278287526724, l1: 0.0001027657643725114, l2: 0.0003713620645686279   Iteration 38 of 100, tot loss = 4.789628455513402, l1: 0.0001035733644369573, l2: 0.0003753894806753746   Iteration 39 of 100, tot loss = 4.78057137513772, l1: 0.00010368644050173223, l2: 0.0003743706971036796   Iteration 40 of 100, tot loss = 4.8074677109718325, l1: 0.0001035097224303172, l2: 0.0003772370480874088   Iteration 41 of 100, tot loss = 4.798351520445289, l1: 0.00010342194546843175, l2: 0.0003764132064733109   Iteration 42 of 100, tot loss = 4.82306939079648, l1: 0.00010378491649697978, l2: 0.00037852202201195596   Iteration 43 of 100, tot loss = 4.794852994209112, l1: 0.00010330063767384651, l2: 0.000376184661141705   Iteration 44 of 100, tot loss = 4.794261872768402, l1: 0.00010294178504078776, l2: 0.0003764844017992304   Iteration 45 of 100, tot loss = 4.801202440261841, l1: 0.00010333705043497806, l2: 0.0003767831923647059   Iteration 46 of 100, tot loss = 4.818289202192555, l1: 0.00010317546500869946, l2: 0.00037865345328337634   Iteration 47 of 100, tot loss = 4.786780266051597, l1: 0.00010292284949831268, l2: 0.00037575517521002034   Iteration 48 of 100, tot loss = 4.759271800518036, l1: 0.00010268185133099905, l2: 0.00037324532665176474   Iteration 49 of 100, tot loss = 4.748521036031295, l1: 0.00010267251179487045, l2: 0.0003721795902511447   Iteration 50 of 100, tot loss = 4.822670488357544, l1: 0.00010350860859034583, l2: 0.00037875843816436827   Iteration 51 of 100, tot loss = 4.82621031181485, l1: 0.00010371595000797956, l2: 0.00037890507990275235   Iteration 52 of 100, tot loss = 4.857074224031889, l1: 0.00010448769353388343, l2: 0.00038121972801701096   Iteration 53 of 100, tot loss = 4.8412064966165795, l1: 0.00010368406782497845, l2: 0.0003804365812168228   Iteration 54 of 100, tot loss = 4.842754311031765, l1: 0.00010349196505827692, l2: 0.0003807834652468079   Iteration 55 of 100, tot loss = 4.816148749264804, l1: 0.00010314461925934831, l2: 0.00037847025457515637   Iteration 56 of 100, tot loss = 4.803027706486838, l1: 0.00010305717517309989, l2: 0.00037724559401145337   Iteration 57 of 100, tot loss = 4.826292983272619, l1: 0.00010312475844106653, l2: 0.0003795045384066996   Iteration 58 of 100, tot loss = 4.803418804859293, l1: 0.00010265075239846643, l2: 0.00037769112657707444   Iteration 59 of 100, tot loss = 4.801594447281401, l1: 0.00010293154060738004, l2: 0.0003772279024676625   Iteration 60 of 100, tot loss = 4.8100104053815205, l1: 0.0001033345678782401, l2: 0.0003776664709827552   Iteration 61 of 100, tot loss = 4.843377390845877, l1: 0.00010404318927016231, l2: 0.00038029454824071925   Iteration 62 of 100, tot loss = 4.845952053223887, l1: 0.00010379533588063842, l2: 0.0003807998681065416   Iteration 63 of 100, tot loss = 4.823639790217082, l1: 0.00010374140840703769, l2: 0.0003786225690476833   Iteration 64 of 100, tot loss = 4.821581300348043, l1: 0.00010353506729643414, l2: 0.00037862306089664344   Iteration 65 of 100, tot loss = 4.812212991714477, l1: 0.00010333478276152164, l2: 0.0003778865144480593   Iteration 66 of 100, tot loss = 4.812982685638197, l1: 0.000102746559577733, l2: 0.00037855170728906876   Iteration 67 of 100, tot loss = 4.7793402920908, l1: 0.00010219547229057956, l2: 0.00037573855531005773   Iteration 68 of 100, tot loss = 4.778995790902306, l1: 0.00010224789783772875, l2: 0.0003756516797533122   Iteration 69 of 100, tot loss = 4.780348262925079, l1: 0.00010219856704824834, l2: 0.0003758362577502629   Iteration 70 of 100, tot loss = 4.794922913823809, l1: 0.0001026363562102363, l2: 0.0003768559335315201   Iteration 71 of 100, tot loss = 4.78196854322729, l1: 0.00010257169319821758, l2: 0.00037562515948157967   Iteration 72 of 100, tot loss = 4.787196175919639, l1: 0.00010281072193012935, l2: 0.0003759088942590299   Iteration 73 of 100, tot loss = 4.766340556210035, l1: 0.00010242845238368218, l2: 0.00037420560164881027   Iteration 74 of 100, tot loss = 4.740136352745262, l1: 0.00010211830620490316, l2: 0.00037189532756471907   Iteration 75 of 100, tot loss = 4.740853525797526, l1: 0.00010252782531703512, l2: 0.0003715575259411708   Iteration 76 of 100, tot loss = 4.725254046289544, l1: 0.00010235726040264126, l2: 0.00037016814301176436   Iteration 77 of 100, tot loss = 4.745543052623798, l1: 0.0001026364991119337, l2: 0.00037191780537288246   Iteration 78 of 100, tot loss = 4.745449836437519, l1: 0.00010264078315604143, l2: 0.0003719041995235528   Iteration 79 of 100, tot loss = 4.709352337861363, l1: 0.00010203037808297049, l2: 0.0003689048547280033   Iteration 80 of 100, tot loss = 4.716683985292912, l1: 0.00010211321891802072, l2: 0.00036955517898604737   Iteration 81 of 100, tot loss = 4.721466454458826, l1: 0.00010233760217993727, l2: 0.0003698090426221398   Iteration 82 of 100, tot loss = 4.688572946118145, l1: 0.000101628387888124, l2: 0.00036722890608205757   Iteration 83 of 100, tot loss = 4.684104757136609, l1: 0.00010156927544221068, l2: 0.0003668411998835628   Iteration 84 of 100, tot loss = 4.67177709653264, l1: 0.00010133763555045117, l2: 0.0003658400739714991   Iteration 85 of 100, tot loss = 4.650339290674995, l1: 0.00010118172756526345, l2: 0.00036385220146792775   Iteration 86 of 100, tot loss = 4.64512858972993, l1: 0.00010117715566255271, l2: 0.0003633357034211025   Iteration 87 of 100, tot loss = 4.664176903921982, l1: 0.00010137880285740188, l2: 0.0003650388877769268   Iteration 88 of 100, tot loss = 4.639272544871677, l1: 0.00010079970851703283, l2: 0.00036312754607272586   Iteration 89 of 100, tot loss = 4.642656032958727, l1: 0.00010119539042483502, l2: 0.00036307021300570016   Iteration 90 of 100, tot loss = 4.653460541036394, l1: 0.00010141969796677586, l2: 0.0003639263563349636   Iteration 91 of 100, tot loss = 4.629002087718838, l1: 0.00010088618388521014, l2: 0.00036201402506596603   Iteration 92 of 100, tot loss = 4.624984221613926, l1: 0.00010113499952833239, l2: 0.0003613634227962314   Iteration 93 of 100, tot loss = 4.632591405222493, l1: 0.00010111550751213556, l2: 0.0003621436330685092   Iteration 94 of 100, tot loss = 4.608155624663576, l1: 0.00010066915135038026, l2: 0.000360146411038202   Iteration 95 of 100, tot loss = 4.612216892995332, l1: 0.00010090279699254193, l2: 0.0003603188921452353   Iteration 96 of 100, tot loss = 4.6185239640374975, l1: 0.00010105375986313447, l2: 0.0003607986363931559   Iteration 97 of 100, tot loss = 4.625985498280869, l1: 0.00010106195686697076, l2: 0.0003615365928931872   Iteration 98 of 100, tot loss = 4.622848314898355, l1: 0.00010099367107019988, l2: 0.0003612911603674863   Iteration 99 of 100, tot loss = 4.620073664067972, l1: 0.00010102179805122579, l2: 0.0003609855681648384   Iteration 100 of 100, tot loss = 4.5977035796642305, l1: 0.00010061272245366126, l2: 0.0003591576353937853
   End of epoch 1248; saving model... 

Epoch 1249 of 2000
   Iteration 1 of 100, tot loss = 4.94550895690918, l1: 0.00010246465535601601, l2: 0.00039208628004416823   Iteration 2 of 100, tot loss = 5.23626971244812, l1: 0.0001199591533804778, l2: 0.0004036678437842056   Iteration 3 of 100, tot loss = 5.387461026509603, l1: 0.00012206963098530348, l2: 0.0004166764847468585   Iteration 4 of 100, tot loss = 4.974645495414734, l1: 0.00010483636560820742, l2: 0.0003926281933672726   Iteration 5 of 100, tot loss = 4.772084712982178, l1: 0.00010111322844750248, l2: 0.000376095250248909   Iteration 6 of 100, tot loss = 4.499944686889648, l1: 0.00010173702260847979, l2: 0.0003482574538793415   Iteration 7 of 100, tot loss = 4.34094272341047, l1: 9.97315151575354e-05, l2: 0.0003343627655080387   Iteration 8 of 100, tot loss = 4.424859344959259, l1: 9.982384062823257e-05, l2: 0.00034266210423083976   Iteration 9 of 100, tot loss = 4.57390218310886, l1: 0.00010114598060479491, l2: 0.0003562442434486002   Iteration 10 of 100, tot loss = 4.473263192176819, l1: 0.00010138195320905652, l2: 0.0003459443716565147   Iteration 11 of 100, tot loss = 4.402135133743286, l1: 9.866702384897508e-05, l2: 0.00034154649686322295   Iteration 12 of 100, tot loss = 4.168991794188817, l1: 9.330942915160752e-05, l2: 0.0003235897565900814   Iteration 13 of 100, tot loss = 4.113255840081435, l1: 9.201571503161595e-05, l2: 0.0003193098740526833   Iteration 14 of 100, tot loss = 4.083396749837058, l1: 9.115475170671874e-05, l2: 0.0003171849291123051   Iteration 15 of 100, tot loss = 4.179416378339131, l1: 9.466272482920128e-05, l2: 0.0003232789177369947   Iteration 16 of 100, tot loss = 4.176516242325306, l1: 9.277394997297961e-05, l2: 0.0003248776765758521   Iteration 17 of 100, tot loss = 4.151616552296807, l1: 9.202153438202324e-05, l2: 0.00032314012316740393   Iteration 18 of 100, tot loss = 4.227866232395172, l1: 9.31363584742131e-05, l2: 0.00032965026669747505   Iteration 19 of 100, tot loss = 4.249618611837688, l1: 9.421629759967083e-05, l2: 0.00033074556497587383   Iteration 20 of 100, tot loss = 4.260263448953628, l1: 9.508735474810237e-05, l2: 0.0003309389918285888   Iteration 21 of 100, tot loss = 4.235336536452884, l1: 9.449211852153807e-05, l2: 0.00032904153686970296   Iteration 22 of 100, tot loss = 4.18914920091629, l1: 9.390778342838695e-05, l2: 0.00032500713793772525   Iteration 23 of 100, tot loss = 4.223982246025749, l1: 9.314112226686278e-05, l2: 0.0003292571029463864   Iteration 24 of 100, tot loss = 4.243834371368091, l1: 9.45178909811754e-05, l2: 0.0003298655465187039   Iteration 25 of 100, tot loss = 4.179757494926452, l1: 9.22861356229987e-05, l2: 0.0003256896138191223   Iteration 26 of 100, tot loss = 4.181586517737462, l1: 9.32640646519408e-05, l2: 0.00032489458681084216   Iteration 27 of 100, tot loss = 4.162924488385518, l1: 9.3045960427305e-05, l2: 0.0003232464875543007   Iteration 28 of 100, tot loss = 4.30959831391062, l1: 9.464499310394916e-05, l2: 0.000336314838412883   Iteration 29 of 100, tot loss = 4.342370859507857, l1: 9.541426559986451e-05, l2: 0.00033882282100264625   Iteration 30 of 100, tot loss = 4.332182888189951, l1: 9.428672941188172e-05, l2: 0.00033893155923578886   Iteration 31 of 100, tot loss = 4.268193871744217, l1: 9.270180453313515e-05, l2: 0.00033411758270249854   Iteration 32 of 100, tot loss = 4.244175534695387, l1: 9.209210543303925e-05, l2: 0.0003323254472888948   Iteration 33 of 100, tot loss = 4.213858485221863, l1: 9.146877687960637e-05, l2: 0.00032991707143303233   Iteration 34 of 100, tot loss = 4.21524706658195, l1: 9.214806970489649e-05, l2: 0.0003293766370488276   Iteration 35 of 100, tot loss = 4.249279849869865, l1: 9.346718870801851e-05, l2: 0.00033146079637975034   Iteration 36 of 100, tot loss = 4.224031398693721, l1: 9.313081924725945e-05, l2: 0.0003292723207171851   Iteration 37 of 100, tot loss = 4.306343610222275, l1: 9.40525036689045e-05, l2: 0.0003365818572362784   Iteration 38 of 100, tot loss = 4.3632153994158696, l1: 9.536983653608906e-05, l2: 0.0003409517035163988   Iteration 39 of 100, tot loss = 4.353202346043709, l1: 9.57295678045529e-05, l2: 0.00033959066711712437   Iteration 40 of 100, tot loss = 4.367947867512703, l1: 9.670294584793738e-05, l2: 0.00034009184055321383   Iteration 41 of 100, tot loss = 4.341125560969841, l1: 9.676519598209912e-05, l2: 0.00033734735987353613   Iteration 42 of 100, tot loss = 4.359376603648776, l1: 9.703667421403917e-05, l2: 0.0003389009860776631   Iteration 43 of 100, tot loss = 4.3559366852738135, l1: 9.695582124296316e-05, l2: 0.00033863784669530255   Iteration 44 of 100, tot loss = 4.328458393161947, l1: 9.599180860029513e-05, l2: 0.00033685403005537495   Iteration 45 of 100, tot loss = 4.399115962452359, l1: 9.765265591947052e-05, l2: 0.0003422589397006151   Iteration 46 of 100, tot loss = 4.384028007154879, l1: 9.742307812975132e-05, l2: 0.00034097972219688415   Iteration 47 of 100, tot loss = 4.452986217559652, l1: 9.833234527948352e-05, l2: 0.00034696627606419807   Iteration 48 of 100, tot loss = 4.434067212045193, l1: 9.809289357084101e-05, l2: 0.0003453138269833289   Iteration 49 of 100, tot loss = 4.435630538025681, l1: 9.877654928440818e-05, l2: 0.00034478650373710815   Iteration 50 of 100, tot loss = 4.457186205387115, l1: 9.906152256007772e-05, l2: 0.0003466570971067995   Iteration 51 of 100, tot loss = 4.474801926051869, l1: 9.952570012412216e-05, l2: 0.0003479544909935737   Iteration 52 of 100, tot loss = 4.492716809877982, l1: 9.983307700167643e-05, l2: 0.0003494386020555304   Iteration 53 of 100, tot loss = 4.475802018957318, l1: 0.0001000958138045565, l2: 0.0003474843858539144   Iteration 54 of 100, tot loss = 4.474275688330333, l1: 9.997819405624695e-05, l2: 0.0003474493730916745   Iteration 55 of 100, tot loss = 4.503601561893117, l1: 0.00010091388348056089, l2: 0.0003494462718001821   Iteration 56 of 100, tot loss = 4.465467676520348, l1: 9.99028279043809e-05, l2: 0.0003466439385582427   Iteration 57 of 100, tot loss = 4.481213555001376, l1: 9.966390765233275e-05, l2: 0.00034845744672215036   Iteration 58 of 100, tot loss = 4.470559923813261, l1: 9.981912400439012e-05, l2: 0.0003472368673477643   Iteration 59 of 100, tot loss = 4.485859278905189, l1: 0.00010044300186070929, l2: 0.00034814292488035634   Iteration 60 of 100, tot loss = 4.4743871986866, l1: 9.994224910769844e-05, l2: 0.00034749646971855933   Iteration 61 of 100, tot loss = 4.498411121915598, l1: 0.00010022277173015564, l2: 0.00034961833944925885   Iteration 62 of 100, tot loss = 4.464002638093887, l1: 9.971960911746962e-05, l2: 0.0003466806538786078   Iteration 63 of 100, tot loss = 4.484125619842892, l1: 9.950009801301233e-05, l2: 0.000348912463301704   Iteration 64 of 100, tot loss = 4.4769137930125, l1: 9.930081449738282e-05, l2: 0.00034839056388591416   Iteration 65 of 100, tot loss = 4.504478504107548, l1: 9.978524282744799e-05, l2: 0.00035066260651756936   Iteration 66 of 100, tot loss = 4.49257043094346, l1: 9.967821676003619e-05, l2: 0.00034957882554877097   Iteration 67 of 100, tot loss = 4.5134388884501675, l1: 0.00010023886558761361, l2: 0.0003511050230399497   Iteration 68 of 100, tot loss = 4.521743677994785, l1: 0.00010042411213362357, l2: 0.0003517502555041574   Iteration 69 of 100, tot loss = 4.488499017729276, l1: 9.977829250746636e-05, l2: 0.0003490716090722792   Iteration 70 of 100, tot loss = 4.483407713685717, l1: 9.994874334162367e-05, l2: 0.0003483920279125284   Iteration 71 of 100, tot loss = 4.501334672242823, l1: 0.00010000797827698579, l2: 0.0003501254889759904   Iteration 72 of 100, tot loss = 4.516878901256455, l1: 0.00010061634859286844, l2: 0.0003510715408386507   Iteration 73 of 100, tot loss = 4.52479323785599, l1: 0.00010055367841244968, l2: 0.0003519256444558603   Iteration 74 of 100, tot loss = 4.52746719121933, l1: 0.00010057492712607004, l2: 0.00035217179086160014   Iteration 75 of 100, tot loss = 4.513381950060526, l1: 0.00010025903078106543, l2: 0.00035107916298632823   Iteration 76 of 100, tot loss = 4.504780977964401, l1: 9.994146890484874e-05, l2: 0.0003505366276888373   Iteration 77 of 100, tot loss = 4.4853342152261115, l1: 9.953632932640128e-05, l2: 0.00034899709106713244   Iteration 78 of 100, tot loss = 4.458861172199249, l1: 9.9274709683437e-05, l2: 0.00034661140633920114   Iteration 79 of 100, tot loss = 4.429908953135526, l1: 9.870827719318692e-05, l2: 0.0003442826168390604   Iteration 80 of 100, tot loss = 4.459430719912052, l1: 9.872006985460758e-05, l2: 0.00034722300060821   Iteration 81 of 100, tot loss = 4.46867400775721, l1: 9.90879795417753e-05, l2: 0.00034777941933206434   Iteration 82 of 100, tot loss = 4.450401301791028, l1: 9.886223024961243e-05, l2: 0.00034617789823664124   Iteration 83 of 100, tot loss = 4.456779515886882, l1: 9.911774886640192e-05, l2: 0.00034656020088951633   Iteration 84 of 100, tot loss = 4.450089714356831, l1: 9.888082257973674e-05, l2: 0.0003461281469623957   Iteration 85 of 100, tot loss = 4.44685369239134, l1: 9.872535729835577e-05, l2: 0.00034596001017181313   Iteration 86 of 100, tot loss = 4.428479240384212, l1: 9.809945042307359e-05, l2: 0.00034474847170924987   Iteration 87 of 100, tot loss = 4.414232959692505, l1: 9.800018179002914e-05, l2: 0.0003434231121095711   Iteration 88 of 100, tot loss = 4.386665044860407, l1: 9.761883037836841e-05, l2: 0.00034104767192647245   Iteration 89 of 100, tot loss = 4.360287533717209, l1: 9.702885360097293e-05, l2: 0.0003389998977111778   Iteration 90 of 100, tot loss = 4.328639116552141, l1: 9.632376588544704e-05, l2: 0.0003365401437299119   Iteration 91 of 100, tot loss = 4.3418168143911675, l1: 9.66381579688699e-05, l2: 0.00033754352153954353   Iteration 92 of 100, tot loss = 4.324985570233801, l1: 9.647721241850851e-05, l2: 0.0003360213426197373   Iteration 93 of 100, tot loss = 4.328753721329473, l1: 9.626448797550233e-05, l2: 0.0003366108825092294   Iteration 94 of 100, tot loss = 4.357292671152886, l1: 9.65178257134307e-05, l2: 0.0003392114398438741   Iteration 95 of 100, tot loss = 4.360714148220263, l1: 9.665384913157476e-05, l2: 0.00033941756410058586   Iteration 96 of 100, tot loss = 4.3465271679063635, l1: 9.654928794589068e-05, l2: 0.0003381034274146562   Iteration 97 of 100, tot loss = 4.34454529924491, l1: 9.655940587051482e-05, l2: 0.0003378951228067721   Iteration 98 of 100, tot loss = 4.322449753479082, l1: 9.620067579149059e-05, l2: 0.000336044298262782   Iteration 99 of 100, tot loss = 4.320918912839407, l1: 9.624806070768491e-05, l2: 0.0003358438295592093   Iteration 100 of 100, tot loss = 4.352948967218399, l1: 9.655465626565274e-05, l2: 0.0003387402398220729
   End of epoch 1249; saving model... 

Epoch 1250 of 2000
   Iteration 1 of 100, tot loss = 6.077941417694092, l1: 0.00010124810796696693, l2: 0.0005065460572950542   Iteration 2 of 100, tot loss = 5.282670736312866, l1: 0.00010057045801659115, l2: 0.00042769662104547024   Iteration 3 of 100, tot loss = 5.09984827041626, l1: 9.891722826675202e-05, l2: 0.00041106759454123676   Iteration 4 of 100, tot loss = 5.134019732475281, l1: 9.887267151498236e-05, l2: 0.00041452929144725204   Iteration 5 of 100, tot loss = 5.30783519744873, l1: 0.00010721321159508079, l2: 0.0004235703032463789   Iteration 6 of 100, tot loss = 5.583194176355998, l1: 0.00011666290568731104, l2: 0.0004416565061546862   Iteration 7 of 100, tot loss = 5.393057823181152, l1: 0.0001120548092460792, l2: 0.0004272509707204465   Iteration 8 of 100, tot loss = 5.478587210178375, l1: 0.00011311661000945605, l2: 0.0004347421090642456   Iteration 9 of 100, tot loss = 5.295843018425836, l1: 0.00011068746324680332, l2: 0.0004188968386087153   Iteration 10 of 100, tot loss = 5.543202018737793, l1: 0.00011528417380759492, l2: 0.00043903602636419237   Iteration 11 of 100, tot loss = 5.376583836295388, l1: 0.00011264297195752575, l2: 0.0004250154094982215   Iteration 12 of 100, tot loss = 5.390186270078023, l1: 0.00011134397209389135, l2: 0.0004276746525041138   Iteration 13 of 100, tot loss = 5.36247103030865, l1: 0.00011069225617505323, l2: 0.00042555484553584113   Iteration 14 of 100, tot loss = 5.219012141227722, l1: 0.0001085192322664495, l2: 0.000413381978952592   Iteration 15 of 100, tot loss = 5.138353617986043, l1: 0.00010701439362795403, l2: 0.000406820965387548   Iteration 16 of 100, tot loss = 4.9407782182097435, l1: 0.00010323108585907903, l2: 0.0003908467333531007   Iteration 17 of 100, tot loss = 5.0636708806542785, l1: 0.00010609102665672682, l2: 0.00040027606027091253   Iteration 18 of 100, tot loss = 5.0178495446840925, l1: 0.00010699540164043558, l2: 0.00039478955053103465   Iteration 19 of 100, tot loss = 5.2538736682189136, l1: 0.00011001182601352124, l2: 0.00041537553893010085   Iteration 20 of 100, tot loss = 5.148382622003555, l1: 0.00010688806178222876, l2: 0.0004079501988599077   Iteration 21 of 100, tot loss = 4.979732791582744, l1: 0.00010342045702383898, l2: 0.00039455282066704794   Iteration 22 of 100, tot loss = 4.948780130256306, l1: 0.00010256750564704734, l2: 0.0003923105058757673   Iteration 23 of 100, tot loss = 4.904945181763691, l1: 0.0001023032410834592, l2: 0.0003881912758199336   Iteration 24 of 100, tot loss = 4.977861300110817, l1: 0.00010382414954316725, l2: 0.0003939619776550292   Iteration 25 of 100, tot loss = 4.942195658683777, l1: 0.00010377888218499721, l2: 0.00039044068136718125   Iteration 26 of 100, tot loss = 4.88825229039559, l1: 0.00010332863334602174, l2: 0.0003854965935505998   Iteration 27 of 100, tot loss = 4.851503738650569, l1: 0.00010292071749939135, l2: 0.00038222965398475664   Iteration 28 of 100, tot loss = 4.733103283814022, l1: 0.00010042044479113039, l2: 0.00037288988122392245   Iteration 29 of 100, tot loss = 4.683496614982342, l1: 9.90626259439962e-05, l2: 0.00036928703399286765   Iteration 30 of 100, tot loss = 4.688184237480163, l1: 9.96619469030217e-05, l2: 0.000369156475305014   Iteration 31 of 100, tot loss = 4.64968353702176, l1: 9.86951850908583e-05, l2: 0.00036627316735722424   Iteration 32 of 100, tot loss = 4.678900048136711, l1: 9.986535303596611e-05, l2: 0.00036802465115215455   Iteration 33 of 100, tot loss = 4.643648913412383, l1: 9.881691827710408e-05, l2: 0.0003655479727471671   Iteration 34 of 100, tot loss = 4.6883078182444855, l1: 9.98054688620066e-05, l2: 0.00036902531321496046   Iteration 35 of 100, tot loss = 4.683700902121408, l1: 9.920826819974796e-05, l2: 0.00036916182247555946   Iteration 36 of 100, tot loss = 4.666792869567871, l1: 9.913925199119451e-05, l2: 0.00036754003607105307   Iteration 37 of 100, tot loss = 4.7442716005686165, l1: 0.00010085382687189062, l2: 0.00037357333447387745   Iteration 38 of 100, tot loss = 4.734917979491384, l1: 0.00010035632725415972, l2: 0.0003731354719141229   Iteration 39 of 100, tot loss = 4.77937329121125, l1: 0.00010098037110322203, l2: 0.000376956959184892   Iteration 40 of 100, tot loss = 4.810984516143799, l1: 0.00010172416377827176, l2: 0.0003793742902416852   Iteration 41 of 100, tot loss = 4.8170092861826825, l1: 0.00010157066935789771, l2: 0.000380130261984915   Iteration 42 of 100, tot loss = 4.869353283019293, l1: 0.0001023307899491296, l2: 0.00038460454189113253   Iteration 43 of 100, tot loss = 4.824313341185104, l1: 0.00010119422247868876, l2: 0.00038123711504361616   Iteration 44 of 100, tot loss = 4.932323997670954, l1: 0.00010321341844907941, l2: 0.0003900189845312112   Iteration 45 of 100, tot loss = 4.9312057601081, l1: 0.00010314379283550402, l2: 0.0003899767857445921   Iteration 46 of 100, tot loss = 4.955217921215555, l1: 0.00010329385804652702, l2: 0.00039222793771920766   Iteration 47 of 100, tot loss = 4.92696765635876, l1: 0.0001026784453604202, l2: 0.00039001832393547914   Iteration 48 of 100, tot loss = 4.863238513469696, l1: 0.00010139130533085942, l2: 0.00038493254957453854   Iteration 49 of 100, tot loss = 4.852153729419319, l1: 0.00010179838834434976, l2: 0.0003834169879805164   Iteration 50 of 100, tot loss = 4.808553791046142, l1: 0.00010123182524694129, l2: 0.00037962355723720977   Iteration 51 of 100, tot loss = 4.813960056678922, l1: 0.00010079595185565196, l2: 0.0003806000574567246   Iteration 52 of 100, tot loss = 4.872312784194946, l1: 0.00010155023189257078, l2: 0.00038568104985013354   Iteration 53 of 100, tot loss = 4.875875194117708, l1: 0.00010168749330024991, l2: 0.0003859000292854858   Iteration 54 of 100, tot loss = 4.865250304893211, l1: 0.00010196916398988967, l2: 0.000384555869391921   Iteration 55 of 100, tot loss = 4.832082353938709, l1: 0.00010146939351794902, l2: 0.000381738844523418   Iteration 56 of 100, tot loss = 4.832732979740415, l1: 0.00010202299680323839, l2: 0.0003812503040795231   Iteration 57 of 100, tot loss = 4.815889437993367, l1: 0.000102051323266463, l2: 0.0003795376234624551   Iteration 58 of 100, tot loss = 4.803711944612964, l1: 0.0001020073979252002, l2: 0.0003783637990197568   Iteration 59 of 100, tot loss = 4.836346137321602, l1: 0.00010235323800046344, l2: 0.0003812813785339516   Iteration 60 of 100, tot loss = 4.802882647514343, l1: 0.00010190733980077008, l2: 0.00037838092769864793   Iteration 61 of 100, tot loss = 4.7966152097358075, l1: 0.00010201683030349249, l2: 0.00037764469350088904   Iteration 62 of 100, tot loss = 4.773587169185761, l1: 0.00010180157457192939, l2: 0.00037555714527937403   Iteration 63 of 100, tot loss = 4.808318671726045, l1: 0.00010242790957572796, l2: 0.00037840396036035453   Iteration 64 of 100, tot loss = 4.793257288634777, l1: 0.0001017963439835512, l2: 0.0003775293874923591   Iteration 65 of 100, tot loss = 4.778339275946984, l1: 0.00010167043745661011, l2: 0.0003761634927986261   Iteration 66 of 100, tot loss = 4.774995276422212, l1: 0.00010132049183911559, l2: 0.0003761790385672639   Iteration 67 of 100, tot loss = 4.763193368911743, l1: 0.00010122622394979473, l2: 0.00037509311586114534   Iteration 68 of 100, tot loss = 4.7696961339782264, l1: 0.00010131181935481865, l2: 0.0003756577968006776   Iteration 69 of 100, tot loss = 4.773046469342882, l1: 0.00010171515305705395, l2: 0.00037558949617854574   Iteration 70 of 100, tot loss = 4.769038728305271, l1: 0.0001018745471290978, l2: 0.00037502932817525496   Iteration 71 of 100, tot loss = 4.811661575881528, l1: 0.00010232083765055302, l2: 0.00037884532257137347   Iteration 72 of 100, tot loss = 4.804970572392146, l1: 0.0001016781615261506, l2: 0.00037881889824752254   Iteration 73 of 100, tot loss = 4.803292859090518, l1: 0.00010185606087149721, l2: 0.00037847322757493014   Iteration 74 of 100, tot loss = 4.7725965106809465, l1: 0.0001014279695408658, l2: 0.00037583168405281203   Iteration 75 of 100, tot loss = 4.743073841730753, l1: 0.00010068582065287046, l2: 0.00037362156620171543   Iteration 76 of 100, tot loss = 4.718554179919393, l1: 0.00010023641214885222, l2: 0.00037161900830764543   Iteration 77 of 100, tot loss = 4.753228407401543, l1: 0.00010094293961097342, l2: 0.0003743799035354251   Iteration 78 of 100, tot loss = 4.765201376034663, l1: 0.00010133795321729477, l2: 0.00037518218698842713   Iteration 79 of 100, tot loss = 4.76862750174124, l1: 0.0001017200218346353, l2: 0.00037514273111511   Iteration 80 of 100, tot loss = 4.793712374567986, l1: 0.00010212323691121128, l2: 0.00037724800322394005   Iteration 81 of 100, tot loss = 4.814683693426627, l1: 0.00010237369326519619, l2: 0.0003790946791047679   Iteration 82 of 100, tot loss = 4.796733315398053, l1: 0.00010238170344985897, l2: 0.00037729163115017865   Iteration 83 of 100, tot loss = 4.791351852646793, l1: 0.00010234995378519364, l2: 0.0003767852344113986   Iteration 84 of 100, tot loss = 4.785266876220703, l1: 0.00010223366453352529, l2: 0.00037629302555398594   Iteration 85 of 100, tot loss = 4.791310377681957, l1: 0.00010244207861593595, l2: 0.0003766889619702638   Iteration 86 of 100, tot loss = 4.777644054834233, l1: 0.0001024206625896663, l2: 0.00037534374564000795   Iteration 87 of 100, tot loss = 4.776579169021256, l1: 0.00010233331633425297, l2: 0.00037532460338224647   Iteration 88 of 100, tot loss = 4.788931551304731, l1: 0.00010261417012522791, l2: 0.00037627898803872415   Iteration 89 of 100, tot loss = 4.7773240094774225, l1: 0.00010270038121765783, l2: 0.000375032022787033   Iteration 90 of 100, tot loss = 4.784558277659946, l1: 0.00010292359361806626, l2: 0.00037553223705294334   Iteration 91 of 100, tot loss = 4.803641625813076, l1: 0.0001032651472976815, l2: 0.00037709901796635956   Iteration 92 of 100, tot loss = 4.805516001970871, l1: 0.00010306615707162089, l2: 0.0003774854457847339   Iteration 93 of 100, tot loss = 4.790480536799277, l1: 0.00010296038868467784, l2: 0.0003760876676214168   Iteration 94 of 100, tot loss = 4.794725879709771, l1: 0.00010321552044262446, l2: 0.0003762570705340353   Iteration 95 of 100, tot loss = 4.776524322911313, l1: 0.00010312009525430193, l2: 0.00037453234016490906   Iteration 96 of 100, tot loss = 4.78949195643266, l1: 0.00010358968798603503, l2: 0.0003753595109780387   Iteration 97 of 100, tot loss = 4.767146521007891, l1: 0.00010330248753853459, l2: 0.000373412167954081   Iteration 98 of 100, tot loss = 4.774061453585722, l1: 0.00010352151407367236, l2: 0.00037388463437198474   Iteration 99 of 100, tot loss = 4.7792742011523, l1: 0.00010351705365029935, l2: 0.0003744103694293469   Iteration 100 of 100, tot loss = 4.782801892757416, l1: 0.00010342431542085251, l2: 0.00037485587679839225
   End of epoch 1250; saving model... 

Epoch 1251 of 2000
   Iteration 1 of 100, tot loss = 3.5131568908691406, l1: 5.485483416123316e-05, l2: 0.0002964608429465443   Iteration 2 of 100, tot loss = 5.26752233505249, l1: 0.00010923968875431456, l2: 0.00041751253593247384   Iteration 3 of 100, tot loss = 4.641993602116902, l1: 0.00010455863715227072, l2: 0.0003596407186705619   Iteration 4 of 100, tot loss = 4.295036971569061, l1: 9.295722429669695e-05, l2: 0.0003365464726812206   Iteration 5 of 100, tot loss = 4.372159719467163, l1: 9.627031467971393e-05, l2: 0.0003409456578083336   Iteration 6 of 100, tot loss = 4.481478333473206, l1: 9.624740838868699e-05, l2: 0.0003519004288439949   Iteration 7 of 100, tot loss = 4.604571785245623, l1: 9.806511297938414e-05, l2: 0.0003623920742289296   Iteration 8 of 100, tot loss = 4.749714881181717, l1: 0.00010443169003337971, l2: 0.0003705398085003253   Iteration 9 of 100, tot loss = 4.4615529378255205, l1: 9.861419918403651e-05, l2: 0.0003475411035146357   Iteration 10 of 100, tot loss = 4.318518948554993, l1: 9.423868759768083e-05, l2: 0.0003376132153789513   Iteration 11 of 100, tot loss = 4.224628513509577, l1: 9.231576404999942e-05, l2: 0.00033014709399801427   Iteration 12 of 100, tot loss = 4.239208320776622, l1: 9.376239783402222e-05, l2: 0.0003301584401924629   Iteration 13 of 100, tot loss = 4.20788497191209, l1: 9.497833301653512e-05, l2: 0.000325810169810071   Iteration 14 of 100, tot loss = 4.161036389214652, l1: 9.482995717137652e-05, l2: 0.00032127368675511595   Iteration 15 of 100, tot loss = 4.203304386138916, l1: 9.477548398232709e-05, l2: 0.0003255549595147992   Iteration 16 of 100, tot loss = 4.126664489507675, l1: 9.347341938337195e-05, l2: 0.000319193033647025   Iteration 17 of 100, tot loss = 4.17564582824707, l1: 9.366495300235008e-05, l2: 0.0003238996329909081   Iteration 18 of 100, tot loss = 4.2253612412346735, l1: 9.328569366415549e-05, l2: 0.0003292504342324618   Iteration 19 of 100, tot loss = 4.129416177147313, l1: 9.300321835326031e-05, l2: 0.0003199384028496417   Iteration 20 of 100, tot loss = 4.210603415966034, l1: 9.373445973324124e-05, l2: 0.00032732588369981385   Iteration 21 of 100, tot loss = 4.286621332168579, l1: 9.536876521971343e-05, l2: 0.000333293369511098   Iteration 22 of 100, tot loss = 4.311881769787181, l1: 9.546231608097018e-05, l2: 0.0003357258624005639   Iteration 23 of 100, tot loss = 4.2496122173641036, l1: 9.40417028133474e-05, l2: 0.0003309195200689947   Iteration 24 of 100, tot loss = 4.1754403710365295, l1: 9.269792008126387e-05, l2: 0.0003248461180191953   Iteration 25 of 100, tot loss = 4.265341587066651, l1: 9.525852947263047e-05, l2: 0.0003312756307423115   Iteration 26 of 100, tot loss = 4.295287260642419, l1: 9.67771822895604e-05, l2: 0.00033275154419243336   Iteration 27 of 100, tot loss = 4.300998387513338, l1: 9.643526303719867e-05, l2: 0.00033366457662648626   Iteration 28 of 100, tot loss = 4.2638651217733115, l1: 9.541087690325054e-05, l2: 0.0003309756357339211   Iteration 29 of 100, tot loss = 4.324615437408974, l1: 9.696322477197852e-05, l2: 0.000335498319553404   Iteration 30 of 100, tot loss = 4.3045422554016115, l1: 9.686023646888013e-05, l2: 0.0003335939904597277   Iteration 31 of 100, tot loss = 4.273176600856166, l1: 9.66545311741591e-05, l2: 0.00033066312983376725   Iteration 32 of 100, tot loss = 4.287295408546925, l1: 9.708866809887695e-05, l2: 0.0003316408742648491   Iteration 33 of 100, tot loss = 4.292119972633593, l1: 9.726279809414832e-05, l2: 0.0003319492014160267   Iteration 34 of 100, tot loss = 4.262340075829449, l1: 9.621016579330899e-05, l2: 0.0003300238446065444   Iteration 35 of 100, tot loss = 4.3517751898084365, l1: 9.731038236558172e-05, l2: 0.0003378671398552667   Iteration 36 of 100, tot loss = 4.421381321218279, l1: 9.849535793667503e-05, l2: 0.0003436427767964132   Iteration 37 of 100, tot loss = 4.445861036713059, l1: 9.923871992804059e-05, l2: 0.00034534738586296807   Iteration 38 of 100, tot loss = 4.409490459843686, l1: 9.884599306755455e-05, l2: 0.0003421030552616637   Iteration 39 of 100, tot loss = 4.430897565988394, l1: 9.857380642549683e-05, l2: 0.00034451595274731517   Iteration 40 of 100, tot loss = 4.491219580173492, l1: 9.907765161187854e-05, l2: 0.0003500443082884885   Iteration 41 of 100, tot loss = 4.468765107596793, l1: 9.849314789412679e-05, l2: 0.00034838336485824205   Iteration 42 of 100, tot loss = 4.494652135031564, l1: 9.870384036773993e-05, l2: 0.0003507613756006495   Iteration 43 of 100, tot loss = 4.510460831398188, l1: 9.887960254144313e-05, l2: 0.00035216648275097613   Iteration 44 of 100, tot loss = 4.498062908649445, l1: 9.930576611209703e-05, l2: 0.00035050052710258484   Iteration 45 of 100, tot loss = 4.468028280470106, l1: 9.888553936939893e-05, l2: 0.0003479172912193462   Iteration 46 of 100, tot loss = 4.469150584676991, l1: 9.8572468350935e-05, l2: 0.0003483425930228449   Iteration 47 of 100, tot loss = 4.495931726820926, l1: 9.935177374071382e-05, l2: 0.00035024140162660957   Iteration 48 of 100, tot loss = 4.506247291962306, l1: 9.967213903413115e-05, l2: 0.00035095259257407935   Iteration 49 of 100, tot loss = 4.487823544716348, l1: 9.942818783480218e-05, l2: 0.00034935416906777466   Iteration 50 of 100, tot loss = 4.478133325576782, l1: 9.978028188925236e-05, l2: 0.000348033053742256   Iteration 51 of 100, tot loss = 4.47421147776585, l1: 9.961399707975158e-05, l2: 0.0003478071534225498   Iteration 52 of 100, tot loss = 4.516997924217811, l1: 0.00010085943792476498, l2: 0.00035084035754642595   Iteration 53 of 100, tot loss = 4.540259001389989, l1: 0.00010145590972827467, l2: 0.00035256999275726177   Iteration 54 of 100, tot loss = 4.591553944128531, l1: 0.00010173111096028825, l2: 0.00035742428601089933   Iteration 55 of 100, tot loss = 4.609704511815852, l1: 0.00010201920204880562, l2: 0.0003589512518374249   Iteration 56 of 100, tot loss = 4.597161855016436, l1: 0.00010164990778451153, l2: 0.0003580662806988195   Iteration 57 of 100, tot loss = 4.6273087785955065, l1: 0.00010244287624485431, l2: 0.000360288004454209   Iteration 58 of 100, tot loss = 4.60517183254505, l1: 0.00010169868393608465, l2: 0.0003588185020911122   Iteration 59 of 100, tot loss = 4.617823895761522, l1: 0.00010134201751868004, l2: 0.0003604403752695485   Iteration 60 of 100, tot loss = 4.570237725973129, l1: 0.00010027944857332235, l2: 0.0003567443270488487   Iteration 61 of 100, tot loss = 4.618461079284793, l1: 0.00010082624293482083, l2: 0.0003610198678483148   Iteration 62 of 100, tot loss = 4.588657111890854, l1: 0.00010048323970442007, l2: 0.0003583824741766758   Iteration 63 of 100, tot loss = 4.597822145810203, l1: 0.00010099698337153457, l2: 0.0003587852337486332   Iteration 64 of 100, tot loss = 4.616717571392655, l1: 0.00010106087552230747, l2: 0.0003606108839449007   Iteration 65 of 100, tot loss = 4.615857573655935, l1: 0.00010133360058087139, l2: 0.00036025215880587126   Iteration 66 of 100, tot loss = 4.614253378275669, l1: 0.00010161031097981542, l2: 0.00035981502856664133   Iteration 67 of 100, tot loss = 4.579477536144541, l1: 0.00010113068639106159, l2: 0.00035681706916631216   Iteration 68 of 100, tot loss = 4.585039515705669, l1: 0.0001015141912349089, l2: 0.000356989761819611   Iteration 69 of 100, tot loss = 4.5553320058877915, l1: 0.0001008784856821876, l2: 0.0003546547164475761   Iteration 70 of 100, tot loss = 4.538623654842377, l1: 0.00010054593081544486, l2: 0.00035331643656328585   Iteration 71 of 100, tot loss = 4.5397663200405285, l1: 0.00010038209607878791, l2: 0.00035359453787850084   Iteration 72 of 100, tot loss = 4.561635548869769, l1: 0.00010055959425396092, l2: 0.0003556039626144209   Iteration 73 of 100, tot loss = 4.568735104717621, l1: 0.00010073616057562591, l2: 0.00035613735205627825   Iteration 74 of 100, tot loss = 4.5852313057796374, l1: 0.00010106465428229107, l2: 0.00035745847865630795   Iteration 75 of 100, tot loss = 4.622396311759949, l1: 0.00010170052735096154, l2: 0.0003605391059924538   Iteration 76 of 100, tot loss = 4.5947203495000535, l1: 0.00010129228791667724, l2: 0.00035817974908001674   Iteration 77 of 100, tot loss = 4.5661447682938014, l1: 0.00010061405857838992, l2: 0.0003560004204393715   Iteration 78 of 100, tot loss = 4.594754370359274, l1: 0.00010134867648631394, l2: 0.0003581267629693358   Iteration 79 of 100, tot loss = 4.597759124598926, l1: 0.00010158655664160011, l2: 0.0003581893583538624   Iteration 80 of 100, tot loss = 4.615718929469585, l1: 0.00010191152309744212, l2: 0.0003596603726691683   Iteration 81 of 100, tot loss = 4.652439928349153, l1: 0.00010242253685848684, l2: 0.0003628214588748107   Iteration 82 of 100, tot loss = 4.650454002182658, l1: 0.00010221027718571572, l2: 0.00036283512582020017   Iteration 83 of 100, tot loss = 4.658184854381056, l1: 0.00010245203748674984, l2: 0.00036336645061088476   Iteration 84 of 100, tot loss = 4.634148156359082, l1: 0.00010185400831341256, l2: 0.00036156080975723894   Iteration 85 of 100, tot loss = 4.614144514588749, l1: 0.0001016471879933413, l2: 0.00035976726589176584   Iteration 86 of 100, tot loss = 4.606953844081524, l1: 0.00010157857121707503, l2: 0.00035911681554226077   Iteration 87 of 100, tot loss = 4.606945148829756, l1: 0.00010142349760147112, l2: 0.00035927101940280573   Iteration 88 of 100, tot loss = 4.6548213213682175, l1: 0.00010219132383843481, l2: 0.00036329080996272916   Iteration 89 of 100, tot loss = 4.632433565814844, l1: 0.00010156856047472226, l2: 0.0003616747978075376   Iteration 90 of 100, tot loss = 4.631437393029531, l1: 0.00010151148797174554, l2: 0.00036163225271997767   Iteration 91 of 100, tot loss = 4.6455370423558, l1: 0.00010183579701793401, l2: 0.0003627179085614753   Iteration 92 of 100, tot loss = 4.6443140571532044, l1: 0.00010189311184228205, l2: 0.0003625382950817964   Iteration 93 of 100, tot loss = 4.652907731712506, l1: 0.00010195803883772153, l2: 0.0003633327354672014   Iteration 94 of 100, tot loss = 4.655042446674185, l1: 0.00010227804999344021, l2: 0.00036322619540597056   Iteration 95 of 100, tot loss = 4.637755096586127, l1: 0.0001020286333713509, l2: 0.00036174687694782683   Iteration 96 of 100, tot loss = 4.62970133498311, l1: 0.00010191321405272902, l2: 0.0003610569200039511   Iteration 97 of 100, tot loss = 4.6103851782906915, l1: 0.00010168203223975539, l2: 0.0003593564861617291   Iteration 98 of 100, tot loss = 4.61130697021679, l1: 0.00010149806721714486, l2: 0.00035963263018151784   Iteration 99 of 100, tot loss = 4.6204415969174315, l1: 0.00010156605625378128, l2: 0.00036047810407571795   Iteration 100 of 100, tot loss = 4.626518069505692, l1: 0.00010157347500353353, l2: 0.0003610783326439559
   End of epoch 1251; saving model... 

Epoch 1252 of 2000
   Iteration 1 of 100, tot loss = 4.596685409545898, l1: 0.00011284671199973673, l2: 0.0003468218201305717   Iteration 2 of 100, tot loss = 4.384451389312744, l1: 0.00011496067600091919, l2: 0.0003234844625694677   Iteration 3 of 100, tot loss = 3.7420694828033447, l1: 0.00010134761153797929, l2: 0.0002728593341695766   Iteration 4 of 100, tot loss = 4.373978078365326, l1: 0.0001050825412676204, l2: 0.0003323152632219717   Iteration 5 of 100, tot loss = 4.189363718032837, l1: 0.00010409185197204351, l2: 0.00031484451610594986   Iteration 6 of 100, tot loss = 3.8925968408584595, l1: 9.357336845520574e-05, l2: 0.00029568631240787607   Iteration 7 of 100, tot loss = 3.8240072386605397, l1: 8.819728931744717e-05, l2: 0.0002942034292833081   Iteration 8 of 100, tot loss = 4.168396890163422, l1: 9.375927584187593e-05, l2: 0.0003230804140912369   Iteration 9 of 100, tot loss = 4.356044398413764, l1: 9.407921768595568e-05, l2: 0.0003415252237270276   Iteration 10 of 100, tot loss = 4.292838168144226, l1: 9.312089678132906e-05, l2: 0.00033616292057558894   Iteration 11 of 100, tot loss = 4.225187149914828, l1: 9.1973471254195e-05, l2: 0.0003305452436589721   Iteration 12 of 100, tot loss = 4.046589871247609, l1: 8.766337092917335e-05, l2: 0.00031699561562466744   Iteration 13 of 100, tot loss = 4.176020237115713, l1: 8.844530916226526e-05, l2: 0.00032915671069461567   Iteration 14 of 100, tot loss = 4.077697208949497, l1: 8.622669936033032e-05, l2: 0.00032154301672043014   Iteration 15 of 100, tot loss = 4.339790916442871, l1: 9.154557701549493e-05, l2: 0.0003424335096497089   Iteration 16 of 100, tot loss = 4.473159044981003, l1: 9.565998175276036e-05, l2: 0.0003516559172567213   Iteration 17 of 100, tot loss = 4.430329911849078, l1: 9.552731802721289e-05, l2: 0.0003475056676988435   Iteration 18 of 100, tot loss = 4.429665830400255, l1: 9.555593209774492e-05, l2: 0.0003474106455946134   Iteration 19 of 100, tot loss = 4.481884203459087, l1: 9.726543622060777e-05, l2: 0.0003509229787404796   Iteration 20 of 100, tot loss = 4.603231263160706, l1: 9.90126120086643e-05, l2: 0.0003613105116528459   Iteration 21 of 100, tot loss = 4.4597642194657094, l1: 9.609607202167223e-05, l2: 0.00034988034756333077   Iteration 22 of 100, tot loss = 4.4549490646882495, l1: 9.497249993728474e-05, l2: 0.00035052240425350397   Iteration 23 of 100, tot loss = 4.34717901893284, l1: 9.271589395053604e-05, l2: 0.0003420020053137863   Iteration 24 of 100, tot loss = 4.279129296541214, l1: 9.146397799971358e-05, l2: 0.0003364489496865038   Iteration 25 of 100, tot loss = 4.278960008621215, l1: 9.267260902561248e-05, l2: 0.00033522338984766977   Iteration 26 of 100, tot loss = 4.215430030455956, l1: 9.164128669698794e-05, l2: 0.00032990171530400403   Iteration 27 of 100, tot loss = 4.195337895993833, l1: 9.108452677200721e-05, l2: 0.00032844926210128943   Iteration 28 of 100, tot loss = 4.133330302579062, l1: 9.03541947211904e-05, l2: 0.00032297883418712966   Iteration 29 of 100, tot loss = 4.180686926019603, l1: 9.172258708939149e-05, l2: 0.0003263461039750837   Iteration 30 of 100, tot loss = 4.2645982027053835, l1: 9.365470784056621e-05, l2: 0.0003328051124602401   Iteration 31 of 100, tot loss = 4.313206265049596, l1: 9.494299663328416e-05, l2: 0.00033637762868649236   Iteration 32 of 100, tot loss = 4.357154734432697, l1: 9.588298121343541e-05, l2: 0.00033983249227276247   Iteration 33 of 100, tot loss = 4.3825077504822705, l1: 9.561101350382987e-05, l2: 0.0003426397626312929   Iteration 34 of 100, tot loss = 4.425696043407216, l1: 9.69520941212335e-05, l2: 0.0003456175102602334   Iteration 35 of 100, tot loss = 4.448462792805263, l1: 9.655243089322799e-05, l2: 0.0003482938475956741   Iteration 36 of 100, tot loss = 4.4614633652899, l1: 9.75196289800806e-05, l2: 0.00034862670610083215   Iteration 37 of 100, tot loss = 4.469187704292503, l1: 9.81145878196568e-05, l2: 0.00034880418093667393   Iteration 38 of 100, tot loss = 4.48028932747088, l1: 9.808726693001135e-05, l2: 0.0003499416640184599   Iteration 39 of 100, tot loss = 4.457362419519669, l1: 9.790227919345339e-05, l2: 0.0003478339608707943   Iteration 40 of 100, tot loss = 4.439417392015457, l1: 9.756498038768769e-05, l2: 0.0003463767568973708   Iteration 41 of 100, tot loss = 4.390827399928395, l1: 9.625675179782642e-05, l2: 0.0003428259857705931   Iteration 42 of 100, tot loss = 4.416901338668096, l1: 9.732572765469307e-05, l2: 0.0003443644037317773   Iteration 43 of 100, tot loss = 4.492881741634635, l1: 9.888853868645573e-05, l2: 0.00035039963201365196   Iteration 44 of 100, tot loss = 4.545467918569392, l1: 9.998365310986463e-05, l2: 0.00035456313493010714   Iteration 45 of 100, tot loss = 4.590335792965359, l1: 0.00010047708549084038, l2: 0.0003585564892596772   Iteration 46 of 100, tot loss = 4.581833808318429, l1: 9.993318085237306e-05, l2: 0.0003582501960171731   Iteration 47 of 100, tot loss = 4.619596542196071, l1: 0.00010046469020729073, l2: 0.0003614949603983637   Iteration 48 of 100, tot loss = 4.557358202834924, l1: 9.925633980856219e-05, l2: 0.00035647947682567366   Iteration 49 of 100, tot loss = 4.538950966328991, l1: 9.880555674554875e-05, l2: 0.00035508953635367014   Iteration 50 of 100, tot loss = 4.50988222360611, l1: 9.848723908362445e-05, l2: 0.00035250097993412056   Iteration 51 of 100, tot loss = 4.530271597937042, l1: 9.841580286578676e-05, l2: 0.000354611353210274   Iteration 52 of 100, tot loss = 4.575730628692186, l1: 9.936394732134292e-05, l2: 0.00035820911090503126   Iteration 53 of 100, tot loss = 4.545499192093903, l1: 9.905824302926326e-05, l2: 0.0003554916715212078   Iteration 54 of 100, tot loss = 4.544634397383089, l1: 9.952094552756064e-05, l2: 0.00035494248987999054   Iteration 55 of 100, tot loss = 4.527621826258573, l1: 9.875895901297388e-05, l2: 0.0003540032192673111   Iteration 56 of 100, tot loss = 4.535431983215468, l1: 9.868605379129544e-05, l2: 0.00035485714072690044   Iteration 57 of 100, tot loss = 4.523352767291822, l1: 9.805006680963153e-05, l2: 0.00035428520628524813   Iteration 58 of 100, tot loss = 4.5144495820177015, l1: 9.803151404976861e-05, l2: 0.00035341344118933193   Iteration 59 of 100, tot loss = 4.5073332806765025, l1: 9.781911889519305e-05, l2: 0.0003529142062288267   Iteration 60 of 100, tot loss = 4.4618648568789165, l1: 9.688282601321892e-05, l2: 0.00034930365691252516   Iteration 61 of 100, tot loss = 4.476160006444962, l1: 9.728042331531827e-05, l2: 0.00035033557412775663   Iteration 62 of 100, tot loss = 4.485358411265958, l1: 9.751426654064187e-05, l2: 0.00035102157081353416   Iteration 63 of 100, tot loss = 4.498676591449314, l1: 9.79632542501869e-05, l2: 0.0003519044007084125   Iteration 64 of 100, tot loss = 4.547431204468012, l1: 9.91215335943707e-05, l2: 0.00035562158234370145   Iteration 65 of 100, tot loss = 4.535320476385263, l1: 9.910524162338473e-05, l2: 0.0003544268013613943   Iteration 66 of 100, tot loss = 4.524619543191158, l1: 9.88147656960681e-05, l2: 0.0003536471837530068   Iteration 67 of 100, tot loss = 4.497626269041603, l1: 9.856239791330534e-05, l2: 0.0003512002239498977   Iteration 68 of 100, tot loss = 4.490508535329034, l1: 9.822180115460309e-05, l2: 0.00035082904704562254   Iteration 69 of 100, tot loss = 4.470621147017548, l1: 9.809369183368172e-05, l2: 0.0003489684175397944   Iteration 70 of 100, tot loss = 4.465328574180603, l1: 9.798810060601681e-05, l2: 0.00034854475161409936   Iteration 71 of 100, tot loss = 4.466734573874675, l1: 9.797981820925152e-05, l2: 0.00034869363390870105   Iteration 72 of 100, tot loss = 4.472405443588893, l1: 9.80682176911311e-05, l2: 0.0003491723215726476   Iteration 73 of 100, tot loss = 4.448023593589051, l1: 9.734546610198864e-05, l2: 0.0003474568883363757   Iteration 74 of 100, tot loss = 4.4419092229894686, l1: 9.744553716626724e-05, l2: 0.0003467453800014384   Iteration 75 of 100, tot loss = 4.429756615956625, l1: 9.741317839749778e-05, l2: 0.00034556247827519353   Iteration 76 of 100, tot loss = 4.417272426580128, l1: 9.704474560796945e-05, l2: 0.0003446824920454099   Iteration 77 of 100, tot loss = 4.4095134611253615, l1: 9.692096708542002e-05, l2: 0.0003440303741402428   Iteration 78 of 100, tot loss = 4.44465326040219, l1: 9.710515036996908e-05, l2: 0.0003473601703799198   Iteration 79 of 100, tot loss = 4.440767891799347, l1: 9.738875289972211e-05, l2: 0.00034668803120347307   Iteration 80 of 100, tot loss = 4.442962855100632, l1: 9.726491716719466e-05, l2: 0.00034703136361713405   Iteration 81 of 100, tot loss = 4.419037568716355, l1: 9.674267377345076e-05, l2: 0.00034516107858328243   Iteration 82 of 100, tot loss = 4.419970678120125, l1: 9.668744616380824e-05, l2: 0.000345309617309462   Iteration 83 of 100, tot loss = 4.4259453652852985, l1: 9.657659910476194e-05, l2: 0.00034601793332370446   Iteration 84 of 100, tot loss = 4.426241752647218, l1: 9.647577202745847e-05, l2: 0.00034614839898408504   Iteration 85 of 100, tot loss = 4.456104668448953, l1: 9.715629755245411e-05, l2: 0.000348454165214207   Iteration 86 of 100, tot loss = 4.4520744417988976, l1: 9.731950499266278e-05, l2: 0.00034788793498084363   Iteration 87 of 100, tot loss = 4.443176000967793, l1: 9.739048832200532e-05, l2: 0.0003469271076697289   Iteration 88 of 100, tot loss = 4.43176334825429, l1: 9.724824096999576e-05, l2: 0.00034592809003119   Iteration 89 of 100, tot loss = 4.449366197157442, l1: 9.765417756201449e-05, l2: 0.0003472824388522435   Iteration 90 of 100, tot loss = 4.451164650917053, l1: 9.776815317713449e-05, l2: 0.00034734830898944187   Iteration 91 of 100, tot loss = 4.457224224949931, l1: 9.781678602519026e-05, l2: 0.00034790563326428777   Iteration 92 of 100, tot loss = 4.442553813042848, l1: 9.756863429451504e-05, l2: 0.0003466867439546268   Iteration 93 of 100, tot loss = 4.470577591209001, l1: 9.813976011887412e-05, l2: 0.00034891799574540125   Iteration 94 of 100, tot loss = 4.469219463936826, l1: 9.827335003486023e-05, l2: 0.0003486485933380925   Iteration 95 of 100, tot loss = 4.468967003571359, l1: 9.850298642720047e-05, l2: 0.0003483937110439384   Iteration 96 of 100, tot loss = 4.474889539182186, l1: 9.872264081423054e-05, l2: 0.00034876630987431173   Iteration 97 of 100, tot loss = 4.479811557789438, l1: 9.868490931544376e-05, l2: 0.00034929624317302704   Iteration 98 of 100, tot loss = 4.495175364066143, l1: 9.916406522780581e-05, l2: 0.00035035346816733424   Iteration 99 of 100, tot loss = 4.492073499795162, l1: 9.908918428030147e-05, l2: 0.00035011816296239636   Iteration 100 of 100, tot loss = 4.464816765785217, l1: 9.8514806413732e-05, l2: 0.00034796686748450156
   End of epoch 1252; saving model... 

Epoch 1253 of 2000
   Iteration 1 of 100, tot loss = 4.738203048706055, l1: 0.00011161256406921893, l2: 0.0003622077638283372   Iteration 2 of 100, tot loss = 4.559807538986206, l1: 0.00010521184958633967, l2: 0.0003507689107209444   Iteration 3 of 100, tot loss = 3.940920829772949, l1: 9.076215064851567e-05, l2: 0.0003033299435628578   Iteration 4 of 100, tot loss = 4.213980197906494, l1: 0.00010021978414442856, l2: 0.0003211782495782245   Iteration 5 of 100, tot loss = 4.507692432403564, l1: 0.00010893530998146161, l2: 0.00034183394454885275   Iteration 6 of 100, tot loss = 4.300562143325806, l1: 0.00010382242180639878, l2: 0.000326233801994628   Iteration 7 of 100, tot loss = 4.470553193773542, l1: 0.00010181579481078578, l2: 0.0003452395259435954   Iteration 8 of 100, tot loss = 4.279342025518417, l1: 9.78282450887491e-05, l2: 0.00033010595689120237   Iteration 9 of 100, tot loss = 4.1291476885477705, l1: 9.594737639417872e-05, l2: 0.000316967392185082   Iteration 10 of 100, tot loss = 4.161110353469849, l1: 9.494483238086105e-05, l2: 0.00032116620277520267   Iteration 11 of 100, tot loss = 4.013826305216009, l1: 9.32731491047889e-05, l2: 0.00030810948175547475   Iteration 12 of 100, tot loss = 4.105476160844167, l1: 9.435860980981185e-05, l2: 0.0003161890078141975   Iteration 13 of 100, tot loss = 4.150907534819383, l1: 9.661970733969401e-05, l2: 0.0003184710468093936   Iteration 14 of 100, tot loss = 4.187607543809073, l1: 9.899564559288722e-05, l2: 0.000319765111531264   Iteration 15 of 100, tot loss = 4.207638756434123, l1: 9.817822574405e-05, l2: 0.0003225856499436001   Iteration 16 of 100, tot loss = 4.4073003977537155, l1: 9.949827881428064e-05, l2: 0.0003412317601032555   Iteration 17 of 100, tot loss = 4.472265173407162, l1: 9.943490508971187e-05, l2: 0.00034779161112109087   Iteration 18 of 100, tot loss = 4.383716689215766, l1: 9.836740395662168e-05, l2: 0.00034000426401487656   Iteration 19 of 100, tot loss = 4.441902160644531, l1: 9.908000165045163e-05, l2: 0.00034511021620250846   Iteration 20 of 100, tot loss = 4.515531516075134, l1: 0.00010142298597202171, l2: 0.0003501301674987189   Iteration 21 of 100, tot loss = 4.468995786848522, l1: 0.0001006075887874301, l2: 0.00034629199182659035   Iteration 22 of 100, tot loss = 4.533371806144714, l1: 0.00010148464571102522, l2: 0.00035185253628614276   Iteration 23 of 100, tot loss = 4.585709582204404, l1: 0.00010195411438041407, l2: 0.0003566168458707145   Iteration 24 of 100, tot loss = 4.598574469486873, l1: 0.00010136431956198066, l2: 0.00035849312916980125   Iteration 25 of 100, tot loss = 4.584742555618286, l1: 0.00010092026961501688, l2: 0.00035755398799665274   Iteration 26 of 100, tot loss = 4.598263896428621, l1: 0.00010097784224154356, l2: 0.0003588485482256286   Iteration 27 of 100, tot loss = 4.5787590786262795, l1: 0.00010082216774915448, l2: 0.00035705374170060233   Iteration 28 of 100, tot loss = 4.559033947331565, l1: 0.00010032994110328477, l2: 0.00035557345524596585   Iteration 29 of 100, tot loss = 4.592702216115491, l1: 0.00010131280854191824, l2: 0.0003579574143353464   Iteration 30 of 100, tot loss = 4.537890926996867, l1: 0.00010016230274535095, l2: 0.0003536267914266015   Iteration 31 of 100, tot loss = 4.582584288812453, l1: 0.0001016084274705199, l2: 0.00035665000154216204   Iteration 32 of 100, tot loss = 4.589775651693344, l1: 0.0001020984884689824, l2: 0.0003568790762074059   Iteration 33 of 100, tot loss = 4.604312795581239, l1: 0.00010254646115230791, l2: 0.0003578848175521037   Iteration 34 of 100, tot loss = 4.620432475033929, l1: 0.00010278797280454241, l2: 0.00035925527438795306   Iteration 35 of 100, tot loss = 4.6152133396693635, l1: 0.00010302316451478483, l2: 0.0003584981688098716   Iteration 36 of 100, tot loss = 4.628826432757908, l1: 0.00010363797809986863, l2: 0.0003592446648528696   Iteration 37 of 100, tot loss = 4.584501704654178, l1: 0.00010274907116262191, l2: 0.0003557010991121265   Iteration 38 of 100, tot loss = 4.641960181688008, l1: 0.00010380083557193805, l2: 0.000360395183394614   Iteration 39 of 100, tot loss = 4.570938266240633, l1: 0.00010203483161137391, l2: 0.0003550589959232662   Iteration 40 of 100, tot loss = 4.594071075320244, l1: 0.00010283490091751446, l2: 0.00035657220796565526   Iteration 41 of 100, tot loss = 4.6618397148644055, l1: 0.00010362873430627898, l2: 0.0003625552387192573   Iteration 42 of 100, tot loss = 4.6400329413868135, l1: 0.0001032435342321627, l2: 0.0003607597616445717   Iteration 43 of 100, tot loss = 4.613611562307491, l1: 0.00010254353561999037, l2: 0.00035881762241208275   Iteration 44 of 100, tot loss = 4.570155531167984, l1: 0.00010155908082445173, l2: 0.0003554564738640858   Iteration 45 of 100, tot loss = 4.589984880553351, l1: 0.00010077194577509848, l2: 0.00035822654382273967   Iteration 46 of 100, tot loss = 4.604059776534205, l1: 0.00010072405134649117, l2: 0.00035968192748006675   Iteration 47 of 100, tot loss = 4.600477023327604, l1: 0.00010092007706840919, l2: 0.00035912762649922055   Iteration 48 of 100, tot loss = 4.621861952046554, l1: 0.00010073520578165092, l2: 0.00036145099056739127   Iteration 49 of 100, tot loss = 4.598978497544113, l1: 0.00010061169354119148, l2: 0.00035928615709833266   Iteration 50 of 100, tot loss = 4.592404153347015, l1: 0.000100493238933268, l2: 0.00035874717723345385   Iteration 51 of 100, tot loss = 4.577935749409246, l1: 0.00010039298214513681, l2: 0.00035740059370165876   Iteration 52 of 100, tot loss = 4.5934269955525036, l1: 0.00010052669627979273, l2: 0.00035881600370675395   Iteration 53 of 100, tot loss = 4.573097051314588, l1: 0.00010035671577909698, l2: 0.00035695299008676676   Iteration 54 of 100, tot loss = 4.582694356088285, l1: 0.00010092584226286918, l2: 0.0003573435934851187   Iteration 55 of 100, tot loss = 4.561134882406755, l1: 0.00010058189145638608, l2: 0.00035553159726656636   Iteration 56 of 100, tot loss = 4.5850632935762405, l1: 0.00010117107201170126, l2: 0.00035733525744165363   Iteration 57 of 100, tot loss = 4.58023736978832, l1: 0.00010103861074496403, l2: 0.0003569851266404026   Iteration 58 of 100, tot loss = 4.5746733135190505, l1: 0.00010085783105751034, l2: 0.00035660950028425855   Iteration 59 of 100, tot loss = 4.608428282252813, l1: 0.00010131978570686858, l2: 0.000359523042758248   Iteration 60 of 100, tot loss = 4.572461452086767, l1: 0.00010064264091245908, l2: 0.000356603504527205   Iteration 61 of 100, tot loss = 4.559569610924017, l1: 0.00010059495294537228, l2: 0.00035536200853950175   Iteration 62 of 100, tot loss = 4.590435714490952, l1: 0.0001011123675256855, l2: 0.00035793120473352893   Iteration 63 of 100, tot loss = 4.611069113489181, l1: 0.00010122735791752809, l2: 0.0003598795544045667   Iteration 64 of 100, tot loss = 4.611373754218221, l1: 0.000101016828068623, l2: 0.0003601205485210812   Iteration 65 of 100, tot loss = 4.6049983189656185, l1: 0.00010136117349387719, l2: 0.0003591386596851337   Iteration 66 of 100, tot loss = 4.633276702779712, l1: 0.00010218186694008622, l2: 0.00036114580430104536   Iteration 67 of 100, tot loss = 4.674023231463646, l1: 0.00010273157543573987, l2: 0.00036467074845750717   Iteration 68 of 100, tot loss = 4.683106725706773, l1: 0.00010282696963686784, l2: 0.00036548370382447233   Iteration 69 of 100, tot loss = 4.675113092298093, l1: 0.00010275166399272469, l2: 0.00036475964631223917   Iteration 70 of 100, tot loss = 4.669935936587198, l1: 0.00010262317035605, l2: 0.0003643704245665244   Iteration 71 of 100, tot loss = 4.676318868784837, l1: 0.00010276262114081912, l2: 0.00036486926690255565   Iteration 72 of 100, tot loss = 4.6769277734888925, l1: 0.00010280972648817826, l2: 0.0003648830521948791   Iteration 73 of 100, tot loss = 4.6870058611647725, l1: 0.0001030515754106614, l2: 0.00036564901190465446   Iteration 74 of 100, tot loss = 4.659591557206334, l1: 0.00010281560561624419, l2: 0.00036314355128451025   Iteration 75 of 100, tot loss = 4.63300709883372, l1: 0.00010263517823962805, l2: 0.00036066553264390677   Iteration 76 of 100, tot loss = 4.679025522972408, l1: 0.00010328411290653381, l2: 0.0003646184411895535   Iteration 77 of 100, tot loss = 4.757521466775374, l1: 0.0001044849267092612, l2: 0.0003712672225045427   Iteration 78 of 100, tot loss = 4.760904310605465, l1: 0.00010437219703490906, l2: 0.0003717182364944631   Iteration 79 of 100, tot loss = 4.743222681781914, l1: 0.00010416991864087921, l2: 0.0003701523519224263   Iteration 80 of 100, tot loss = 4.74432707875967, l1: 0.00010435356557536579, l2: 0.00037007914452260595   Iteration 81 of 100, tot loss = 4.746929649953489, l1: 0.00010468177514453994, l2: 0.00037001119225924076   Iteration 82 of 100, tot loss = 4.774589624346756, l1: 0.00010525958221966525, l2: 0.00037219938300351224   Iteration 83 of 100, tot loss = 4.766140166535435, l1: 0.00010521067088120617, l2: 0.00037140334856062456   Iteration 84 of 100, tot loss = 4.769793650933674, l1: 0.00010517711256224651, l2: 0.0003718022550553239   Iteration 85 of 100, tot loss = 4.763147779072033, l1: 0.00010535078142899746, l2: 0.000370963999268818   Iteration 86 of 100, tot loss = 4.768911199514256, l1: 0.00010555790094110497, l2: 0.0003713332216436758   Iteration 87 of 100, tot loss = 4.752792310440677, l1: 0.00010505765021662228, l2: 0.00037022158332923746   Iteration 88 of 100, tot loss = 4.743100181221962, l1: 0.00010499419737574168, l2: 0.0003693158232636051   Iteration 89 of 100, tot loss = 4.745371811845329, l1: 0.00010510620627888436, l2: 0.00036943097696299506   Iteration 90 of 100, tot loss = 4.770985416571299, l1: 0.00010538129608903546, l2: 0.00037171724785973006   Iteration 91 of 100, tot loss = 4.770863054872869, l1: 0.00010554590974876831, l2: 0.00037154039783162773   Iteration 92 of 100, tot loss = 4.773935327063436, l1: 0.00010557520438495608, l2: 0.0003718183305982561   Iteration 93 of 100, tot loss = 4.7594922678444975, l1: 0.000105303642911128, l2: 0.0003706455859418217   Iteration 94 of 100, tot loss = 4.76814246050855, l1: 0.00010525284763369858, l2: 0.000371561400707504   Iteration 95 of 100, tot loss = 4.801908366303695, l1: 0.00010576361026706812, l2: 0.0003744272290005986   Iteration 96 of 100, tot loss = 4.78963032240669, l1: 0.0001055266827355202, l2: 0.00037343635221986915   Iteration 97 of 100, tot loss = 4.8006749681590755, l1: 0.0001057009859632859, l2: 0.0003743665132337951   Iteration 98 of 100, tot loss = 4.794370027220979, l1: 0.00010552880563149561, l2: 0.0003739081996395633   Iteration 99 of 100, tot loss = 4.777237055277584, l1: 0.0001050632912151144, l2: 0.00037266041673696365   Iteration 100 of 100, tot loss = 4.801044951677323, l1: 0.0001055860902488348, l2: 0.00037451840689755044
   End of epoch 1253; saving model... 

Epoch 1254 of 2000
   Iteration 1 of 100, tot loss = 5.994112968444824, l1: 0.00012084774061804637, l2: 0.00047856359742581844   Iteration 2 of 100, tot loss = 4.745060205459595, l1: 8.552651888749097e-05, l2: 0.00038897951890248805   Iteration 3 of 100, tot loss = 4.987643241882324, l1: 9.799456771967623e-05, l2: 0.00040076976680817705   Iteration 4 of 100, tot loss = 5.449633955955505, l1: 0.00010606120940792607, l2: 0.0004389021996757947   Iteration 5 of 100, tot loss = 5.204468441009522, l1: 9.681127121439204e-05, l2: 0.0004236355831380934   Iteration 6 of 100, tot loss = 5.202787796656291, l1: 9.607138175245684e-05, l2: 0.0004242074113183965   Iteration 7 of 100, tot loss = 5.111858231680734, l1: 9.382025850105233e-05, l2: 0.00041736558029827265   Iteration 8 of 100, tot loss = 5.3113186955451965, l1: 0.0001016465876091388, l2: 0.0004294852951716166   Iteration 9 of 100, tot loss = 5.052394522560967, l1: 9.69318102256188e-05, l2: 0.0004083076556627121   Iteration 10 of 100, tot loss = 4.939258050918579, l1: 9.567675151629373e-05, l2: 0.00039824906561989335   Iteration 11 of 100, tot loss = 4.8982579057866875, l1: 9.736455897588961e-05, l2: 0.00039246124056675893   Iteration 12 of 100, tot loss = 4.892366727193196, l1: 9.683311509434134e-05, l2: 0.00039240356272784993   Iteration 13 of 100, tot loss = 5.083100979144756, l1: 0.00010112666104060526, l2: 0.00040718343762609246   Iteration 14 of 100, tot loss = 5.019807883671352, l1: 0.00010074794304923021, l2: 0.0004012328468629026   Iteration 15 of 100, tot loss = 4.9647442181905115, l1: 0.00010036300470043595, l2: 0.0003961114193468044   Iteration 16 of 100, tot loss = 5.013754636049271, l1: 0.00010194976903221686, l2: 0.00039942569856066257   Iteration 17 of 100, tot loss = 4.895702965119305, l1: 9.988952446934384e-05, l2: 0.00038968077655333807   Iteration 18 of 100, tot loss = 4.829257130622864, l1: 9.946233088461061e-05, l2: 0.00038346338745517034   Iteration 19 of 100, tot loss = 4.8598779000734025, l1: 9.958523227588127e-05, l2: 0.0003864025623586617   Iteration 20 of 100, tot loss = 4.981813585758209, l1: 9.987933408410754e-05, l2: 0.00039830202877055854   Iteration 21 of 100, tot loss = 4.924608662014916, l1: 9.89024559939502e-05, l2: 0.0003935584141540208   Iteration 22 of 100, tot loss = 5.022011951966719, l1: 0.00010166804879025386, l2: 0.00040053314957979387   Iteration 23 of 100, tot loss = 5.126197545424752, l1: 0.00010367387712113154, l2: 0.0004089458797471193   Iteration 24 of 100, tot loss = 5.165160755316417, l1: 0.00010422355777942964, l2: 0.0004122925201954786   Iteration 25 of 100, tot loss = 5.239034671783447, l1: 0.00010560167109360919, l2: 0.0004183017986360937   Iteration 26 of 100, tot loss = 5.210432125971868, l1: 0.00010605163417774468, l2: 0.0004149915811677392   Iteration 27 of 100, tot loss = 5.209019802234791, l1: 0.00010601463280316373, l2: 0.000414887350483763   Iteration 28 of 100, tot loss = 5.203182373728071, l1: 0.0001054578621863454, l2: 0.00041486037662252784   Iteration 29 of 100, tot loss = 5.105851905099277, l1: 0.00010436429498628874, l2: 0.0004062208970245937   Iteration 30 of 100, tot loss = 5.027695918083191, l1: 0.00010344451778413108, l2: 0.0003993250754623053   Iteration 31 of 100, tot loss = 4.998824465659357, l1: 0.00010285443521194881, l2: 0.00039702801328648124   Iteration 32 of 100, tot loss = 4.987845219671726, l1: 0.00010265687228638853, l2: 0.00039612765112906345   Iteration 33 of 100, tot loss = 5.002215305964152, l1: 0.00010341657197744952, l2: 0.0003968049594814974   Iteration 34 of 100, tot loss = 5.043342709541321, l1: 0.00010403658555279595, l2: 0.0004002976857009344   Iteration 35 of 100, tot loss = 5.026280995777675, l1: 0.0001043134175623501, l2: 0.00039831468311604115   Iteration 36 of 100, tot loss = 4.96965053346422, l1: 0.0001030070511155322, l2: 0.00039395800316318247   Iteration 37 of 100, tot loss = 4.935948745624439, l1: 0.00010302208102075383, l2: 0.00039057279433551674   Iteration 38 of 100, tot loss = 4.97500571451689, l1: 0.00010390205390745235, l2: 0.00039359851813826124   Iteration 39 of 100, tot loss = 5.060880575424585, l1: 0.0001054103200798496, l2: 0.00040067773825751664   Iteration 40 of 100, tot loss = 4.988605403900147, l1: 0.00010409450878796633, l2: 0.0003947660319681745   Iteration 41 of 100, tot loss = 5.017273135301544, l1: 0.0001046110630296625, l2: 0.00039711625034148554   Iteration 42 of 100, tot loss = 5.032185974575224, l1: 0.00010484184219551804, l2: 0.0003983767542411529   Iteration 43 of 100, tot loss = 5.0068247373714, l1: 0.00010440586223456532, l2: 0.00039627661097383255   Iteration 44 of 100, tot loss = 4.939099311828613, l1: 0.00010293776176695246, l2: 0.00039097216887801716   Iteration 45 of 100, tot loss = 4.943818971845839, l1: 0.00010354463738622144, l2: 0.0003908372587627835   Iteration 46 of 100, tot loss = 4.894878050555354, l1: 0.00010285294521726546, l2: 0.00038663485864400053   Iteration 47 of 100, tot loss = 4.885301148637812, l1: 0.00010284235124461392, l2: 0.0003856877620490466   Iteration 48 of 100, tot loss = 4.9041980256636934, l1: 0.00010323183475217472, l2: 0.0003871879671351053   Iteration 49 of 100, tot loss = 4.944748036715449, l1: 0.00010393911643590474, l2: 0.0003905356861650944   Iteration 50 of 100, tot loss = 4.967305617332459, l1: 0.00010466302337590605, l2: 0.0003920675377594307   Iteration 51 of 100, tot loss = 4.963188091913859, l1: 0.00010515620941113608, l2: 0.00039116259922236934   Iteration 52 of 100, tot loss = 4.960511588133299, l1: 0.00010505689295734136, l2: 0.00039099426570688735   Iteration 53 of 100, tot loss = 4.942985089320056, l1: 0.00010480139229363183, l2: 0.0003894971165243747   Iteration 54 of 100, tot loss = 4.950192623668247, l1: 0.00010523447943139925, l2: 0.00038978478254715875   Iteration 55 of 100, tot loss = 4.955420099605213, l1: 0.00010502237075177783, l2: 0.0003905196388421411   Iteration 56 of 100, tot loss = 4.973370862858636, l1: 0.00010529495271579694, l2: 0.0003920421331713442   Iteration 57 of 100, tot loss = 4.9586697837762665, l1: 0.0001051573246449902, l2: 0.0003907096530351657   Iteration 58 of 100, tot loss = 4.965526831561122, l1: 0.0001055710458002005, l2: 0.0003909816366323302   Iteration 59 of 100, tot loss = 4.949655835911379, l1: 0.0001054208965427479, l2: 0.00038954468643276997   Iteration 60 of 100, tot loss = 5.0014116883277895, l1: 0.00010597809402194495, l2: 0.00039416307457334674   Iteration 61 of 100, tot loss = 4.999367952346802, l1: 0.0001058934060102863, l2: 0.00039404338907038215   Iteration 62 of 100, tot loss = 4.988454053478856, l1: 0.00010547301632956782, l2: 0.00039337238863921696   Iteration 63 of 100, tot loss = 4.992943918894207, l1: 0.00010529478678354137, l2: 0.0003939996046635012   Iteration 64 of 100, tot loss = 4.992418218404055, l1: 0.00010511442030747276, l2: 0.0003941274007956963   Iteration 65 of 100, tot loss = 4.995373113338764, l1: 0.00010514518037850324, l2: 0.000394392130860629   Iteration 66 of 100, tot loss = 4.9650335781502, l1: 0.0001047204523550163, l2: 0.00039178290524322426   Iteration 67 of 100, tot loss = 4.982883357290012, l1: 0.00010532331850584736, l2: 0.0003929650176441603   Iteration 68 of 100, tot loss = 4.96170769368901, l1: 0.0001054615895554889, l2: 0.0003907091800904121   Iteration 69 of 100, tot loss = 4.920993255532307, l1: 0.00010470986666589759, l2: 0.00038738945917482823   Iteration 70 of 100, tot loss = 4.944106875147138, l1: 0.00010491661544490074, l2: 0.0003894940725461181   Iteration 71 of 100, tot loss = 4.944412678060397, l1: 0.00010513259488580958, l2: 0.00038930867318208266   Iteration 72 of 100, tot loss = 4.945336563719644, l1: 0.00010523460009102969, l2: 0.0003892990563245904   Iteration 73 of 100, tot loss = 4.947286419672508, l1: 0.00010535422667114651, l2: 0.00038937441542208806   Iteration 74 of 100, tot loss = 4.964583219708623, l1: 0.000105440185654663, l2: 0.0003910181364296256   Iteration 75 of 100, tot loss = 4.950904200871785, l1: 0.0001054816820154277, l2: 0.00038960873847827316   Iteration 76 of 100, tot loss = 4.950215593764656, l1: 0.00010551318413682135, l2: 0.0003895083757111263   Iteration 77 of 100, tot loss = 4.920534291824737, l1: 0.00010499022632698201, l2: 0.0003870632036440921   Iteration 78 of 100, tot loss = 4.899876441711035, l1: 0.00010467659964118726, l2: 0.00038531104506453115   Iteration 79 of 100, tot loss = 4.912228366996668, l1: 0.00010464650523904829, l2: 0.0003865763323626163   Iteration 80 of 100, tot loss = 4.918426364660263, l1: 0.00010474219243405969, l2: 0.00038710044445906534   Iteration 81 of 100, tot loss = 4.895018757125477, l1: 0.00010438885348978349, l2: 0.0003851130227472081   Iteration 82 of 100, tot loss = 4.8842467069625854, l1: 0.00010410260420474338, l2: 0.00038432206698727406   Iteration 83 of 100, tot loss = 4.927544211766806, l1: 0.00010506684838580978, l2: 0.00038768757304389885   Iteration 84 of 100, tot loss = 4.911488325822921, l1: 0.00010481187876264033, l2: 0.00038633695415531594   Iteration 85 of 100, tot loss = 4.904683817134184, l1: 0.00010483370022163453, l2: 0.0003856346819221097   Iteration 86 of 100, tot loss = 4.898869750111602, l1: 0.00010496479292267523, l2: 0.00038492218266392863   Iteration 87 of 100, tot loss = 4.925119901525563, l1: 0.00010557141967570067, l2: 0.0003869405709442565   Iteration 88 of 100, tot loss = 4.932215519926765, l1: 0.00010553523745991036, l2: 0.00038768631500054403   Iteration 89 of 100, tot loss = 4.951502063301172, l1: 0.00010572489432132478, l2: 0.00038942531257438775   Iteration 90 of 100, tot loss = 4.967079191737705, l1: 0.00010610814513509266, l2: 0.0003905997744166396   Iteration 91 of 100, tot loss = 4.996288773777721, l1: 0.00010660579437671413, l2: 0.00039302308341375664   Iteration 92 of 100, tot loss = 5.007624426613683, l1: 0.00010691687819654244, l2: 0.000393845564837608   Iteration 93 of 100, tot loss = 4.985880521035964, l1: 0.00010636252291958718, l2: 0.0003922255295115994   Iteration 94 of 100, tot loss = 4.981640057360872, l1: 0.00010623413711189332, l2: 0.00039192986878333613   Iteration 95 of 100, tot loss = 5.004333573893497, l1: 0.00010637227196160606, l2: 0.0003940610853820353   Iteration 96 of 100, tot loss = 5.018563188612461, l1: 0.00010686514625983061, l2: 0.00039499117262190947   Iteration 97 of 100, tot loss = 5.002731222467324, l1: 0.00010649066690845204, l2: 0.0003937824553337198   Iteration 98 of 100, tot loss = 5.003461577454392, l1: 0.00010665120587836266, l2: 0.000393694951656341   Iteration 99 of 100, tot loss = 5.0034952235944345, l1: 0.00010682993278617622, l2: 0.0003935195891612275   Iteration 100 of 100, tot loss = 5.023202364444733, l1: 0.00010714928270317614, l2: 0.0003951709532702807
   End of epoch 1254; saving model... 

Epoch 1255 of 2000
   Iteration 1 of 100, tot loss = 4.62575626373291, l1: 9.22816907404922e-05, l2: 0.00037029394297860563   Iteration 2 of 100, tot loss = 4.701017618179321, l1: 9.778465391718782e-05, l2: 0.0003723171103047207   Iteration 3 of 100, tot loss = 5.5833808581034345, l1: 0.00011093823074285562, l2: 0.0004473998463557412   Iteration 4 of 100, tot loss = 5.385265111923218, l1: 0.00010442680286359973, l2: 0.0004340997038525529   Iteration 5 of 100, tot loss = 5.036211252212524, l1: 9.753955237101763e-05, l2: 0.0004060815670527518   Iteration 6 of 100, tot loss = 4.677968422571818, l1: 9.331535936022799e-05, l2: 0.000374481479714935   Iteration 7 of 100, tot loss = 4.854720864977155, l1: 9.767999082604157e-05, l2: 0.00038779209301407846   Iteration 8 of 100, tot loss = 4.689699739217758, l1: 9.388033231516602e-05, l2: 0.00037508963941945694   Iteration 9 of 100, tot loss = 4.938341591093275, l1: 0.00010237671813229099, l2: 0.00039145744004700746   Iteration 10 of 100, tot loss = 5.088332724571228, l1: 0.00010480135752004572, l2: 0.0004040319152409211   Iteration 11 of 100, tot loss = 5.265065995129672, l1: 0.00010664284491213039, l2: 0.0004198637563438917   Iteration 12 of 100, tot loss = 5.29115508000056, l1: 0.00010860075235541444, l2: 0.00042051475611515343   Iteration 13 of 100, tot loss = 5.029865842599135, l1: 0.00010374531372620437, l2: 0.00039924127094519255   Iteration 14 of 100, tot loss = 4.841928098882947, l1: 0.00010183381086140539, l2: 0.0003823589992992181   Iteration 15 of 100, tot loss = 4.775548577308655, l1: 0.00010184210623265244, l2: 0.00037571275121687603   Iteration 16 of 100, tot loss = 4.774263195693493, l1: 0.00010151916353606794, l2: 0.0003759071578315343   Iteration 17 of 100, tot loss = 4.788197089644039, l1: 0.00010148602129501181, l2: 0.00037733369133681715   Iteration 18 of 100, tot loss = 4.699267711904314, l1: 9.985269429711884e-05, l2: 0.00037007408051673946   Iteration 19 of 100, tot loss = 4.655835283429999, l1: 9.930599500014643e-05, l2: 0.0003662775370206586   Iteration 20 of 100, tot loss = 4.612904685735702, l1: 9.753744325280422e-05, l2: 0.0003637530295236502   Iteration 21 of 100, tot loss = 4.714644766989208, l1: 9.990469403592648e-05, l2: 0.0003715597870593358   Iteration 22 of 100, tot loss = 4.692895493724129, l1: 9.958569270648613e-05, l2: 0.00036970386215464464   Iteration 23 of 100, tot loss = 4.69261784657188, l1: 0.00010012135048189363, l2: 0.0003691404385988236   Iteration 24 of 100, tot loss = 4.676799153288205, l1: 0.0001000100199538186, l2: 0.0003676699010005298   Iteration 25 of 100, tot loss = 4.789336543083191, l1: 0.00010290387013810687, l2: 0.00037602979049552234   Iteration 26 of 100, tot loss = 4.759095132350922, l1: 0.00010342168123035942, l2: 0.00037248783841138135   Iteration 27 of 100, tot loss = 4.813611714928238, l1: 0.00010490261950137749, l2: 0.00037645855843072276   Iteration 28 of 100, tot loss = 4.7798322056021005, l1: 0.00010451082393306674, l2: 0.00037347240296574976   Iteration 29 of 100, tot loss = 4.842626041379468, l1: 0.0001050442694796747, l2: 0.00037921834205164863   Iteration 30 of 100, tot loss = 4.8104702989260355, l1: 0.0001046943918481702, l2: 0.00037635264501053217   Iteration 31 of 100, tot loss = 4.755138324153039, l1: 0.00010377345660667596, l2: 0.00037174038175914075   Iteration 32 of 100, tot loss = 4.773204553872347, l1: 0.00010410573111130361, l2: 0.0003732147297341726   Iteration 33 of 100, tot loss = 4.790738586223487, l1: 0.00010476307932439147, l2: 0.00037431078486737203   Iteration 34 of 100, tot loss = 4.811855137348175, l1: 0.00010502665110620642, l2: 0.00037615886814070535   Iteration 35 of 100, tot loss = 4.798119357654026, l1: 0.00010484642002016439, l2: 0.00037496552104130385   Iteration 36 of 100, tot loss = 4.797233995464113, l1: 0.00010567948639719462, l2: 0.000374043918984373   Iteration 37 of 100, tot loss = 4.754159943477528, l1: 0.0001046733465242338, l2: 0.00037074265325114737   Iteration 38 of 100, tot loss = 4.791399758113058, l1: 0.00010522882161910132, l2: 0.0003739111592662276   Iteration 39 of 100, tot loss = 4.799790740013123, l1: 0.00010524693966428272, l2: 0.0003747321383203738   Iteration 40 of 100, tot loss = 4.830754581093788, l1: 0.00010524076578803943, l2: 0.0003778346967010293   Iteration 41 of 100, tot loss = 4.829613525693009, l1: 0.00010489788562664194, l2: 0.0003780634717436553   Iteration 42 of 100, tot loss = 4.81955266282672, l1: 0.00010494519881917985, l2: 0.00037701007247614184   Iteration 43 of 100, tot loss = 4.817435045574987, l1: 0.0001049671858666887, l2: 0.000376776323063559   Iteration 44 of 100, tot loss = 4.802604558792981, l1: 0.00010448058914004254, l2: 0.00037577987148489973   Iteration 45 of 100, tot loss = 4.849112354384529, l1: 0.00010525375457493485, l2: 0.00037965748601386114   Iteration 46 of 100, tot loss = 4.883008099120596, l1: 0.00010580249687811137, l2: 0.0003824983185673456   Iteration 47 of 100, tot loss = 4.950188974116711, l1: 0.00010682747798706802, l2: 0.0003881914240179306   Iteration 48 of 100, tot loss = 4.951509716610114, l1: 0.00010664102554377071, l2: 0.0003885099510322713   Iteration 49 of 100, tot loss = 4.922220475819646, l1: 0.000106655239802904, l2: 0.0003855668129494452   Iteration 50 of 100, tot loss = 4.910947763919831, l1: 0.00010655562371539418, l2: 0.00038453915796708314   Iteration 51 of 100, tot loss = 4.905545872800491, l1: 0.00010642342464302155, l2: 0.00038413116799704   Iteration 52 of 100, tot loss = 4.904410080267833, l1: 0.00010629757798596984, l2: 0.00038414343557983206   Iteration 53 of 100, tot loss = 4.889506945070231, l1: 0.00010622646526448903, l2: 0.0003827242350935022   Iteration 54 of 100, tot loss = 4.861307932270898, l1: 0.00010555305595550669, l2: 0.0003805777426653852   Iteration 55 of 100, tot loss = 4.8445397875525735, l1: 0.00010510690037997185, l2: 0.0003793470836667852   Iteration 56 of 100, tot loss = 4.875040750418391, l1: 0.00010558374424363137, l2: 0.000381920335972349   Iteration 57 of 100, tot loss = 4.850158162284315, l1: 0.00010490509291565487, l2: 0.00038011072830525797   Iteration 58 of 100, tot loss = 4.898733432950644, l1: 0.00010537856379132478, l2: 0.00038449478421986486   Iteration 59 of 100, tot loss = 4.90715036351802, l1: 0.00010557363535646523, l2: 0.00038514140563211956   Iteration 60 of 100, tot loss = 4.923058340946834, l1: 0.00010582803800692393, l2: 0.0003864778006876198   Iteration 61 of 100, tot loss = 4.88792947667544, l1: 0.00010525362885062827, l2: 0.0003835393230862091   Iteration 62 of 100, tot loss = 4.877595607311495, l1: 0.00010509712093710644, l2: 0.00038266244354740446   Iteration 63 of 100, tot loss = 4.851029390380496, l1: 0.00010450929455276179, l2: 0.0003805936483706954   Iteration 64 of 100, tot loss = 4.8728473130613565, l1: 0.00010459343678803634, l2: 0.0003826912986824027   Iteration 65 of 100, tot loss = 4.86153937303103, l1: 0.0001042821346625435, l2: 0.0003818718065024139   Iteration 66 of 100, tot loss = 4.842751501184521, l1: 0.00010405925780959836, l2: 0.00038021589584373447   Iteration 67 of 100, tot loss = 4.883035828818136, l1: 0.00010500562581652428, l2: 0.00038329796077607117   Iteration 68 of 100, tot loss = 4.87112946194761, l1: 0.00010490304259078316, l2: 0.00038220990737660457   Iteration 69 of 100, tot loss = 4.852214566175489, l1: 0.00010484108281451736, l2: 0.0003803803775699107   Iteration 70 of 100, tot loss = 4.839045529706137, l1: 0.00010488803574324786, l2: 0.00037901652082967174   Iteration 71 of 100, tot loss = 4.863661925557634, l1: 0.00010521194273909278, l2: 0.0003811542535426987   Iteration 72 of 100, tot loss = 4.899258870217535, l1: 0.00010562819630245536, l2: 0.00038429769503838744   Iteration 73 of 100, tot loss = 4.933546541488334, l1: 0.00010615875427204515, l2: 0.00038719590380947324   Iteration 74 of 100, tot loss = 4.914659089333302, l1: 0.00010609760593561569, l2: 0.0003853683067702789   Iteration 75 of 100, tot loss = 4.92759063243866, l1: 0.00010605531962937676, l2: 0.0003867037475962813   Iteration 76 of 100, tot loss = 4.973402244480033, l1: 0.00010661005241050397, l2: 0.0003907301769159906   Iteration 77 of 100, tot loss = 4.990076215236218, l1: 0.00010691851634697009, l2: 0.0003920891101082013   Iteration 78 of 100, tot loss = 4.999084455844684, l1: 0.00010706941094003223, l2: 0.00039283903961171373   Iteration 79 of 100, tot loss = 4.99824616426154, l1: 0.00010672370746438509, l2: 0.00039310091380707776   Iteration 80 of 100, tot loss = 4.982372932136059, l1: 0.00010676092674657411, l2: 0.00039147637126006883   Iteration 81 of 100, tot loss = 4.956211942213553, l1: 0.00010649505292135866, l2: 0.00038912614579686183   Iteration 82 of 100, tot loss = 4.919061028375858, l1: 0.00010585783452481293, l2: 0.00038604827266947435   Iteration 83 of 100, tot loss = 4.929816896656909, l1: 0.00010601660372241938, l2: 0.0003869650904397783   Iteration 84 of 100, tot loss = 4.937813442377817, l1: 0.00010614901956224847, l2: 0.0003876323292691571   Iteration 85 of 100, tot loss = 4.954541572402506, l1: 0.00010633682725122472, l2: 0.00038911733427084985   Iteration 86 of 100, tot loss = 4.9355259315912114, l1: 0.00010632069330144034, l2: 0.00038723190410501257   Iteration 87 of 100, tot loss = 4.921411718445262, l1: 0.00010606203143432704, l2: 0.00038607914456375073   Iteration 88 of 100, tot loss = 4.929716425863179, l1: 0.00010605368966439114, l2: 0.0003869179571748563   Iteration 89 of 100, tot loss = 4.919722895943717, l1: 0.00010606693958221369, l2: 0.00038590535388741473   Iteration 90 of 100, tot loss = 4.904117114014095, l1: 0.00010547218941307317, l2: 0.0003849395258132265   Iteration 91 of 100, tot loss = 4.914079148690779, l1: 0.00010561607112841947, l2: 0.0003857918476729377   Iteration 92 of 100, tot loss = 4.910836241815401, l1: 0.00010520222112182123, l2: 0.00038588140681816225   Iteration 93 of 100, tot loss = 4.911231088381942, l1: 0.00010525606468104588, l2: 0.00038586704793351113   Iteration 94 of 100, tot loss = 4.912902062243604, l1: 0.000105431407799652, l2: 0.00038585880262891146   Iteration 95 of 100, tot loss = 4.892633343997755, l1: 0.00010523786837247348, l2: 0.0003840254700901967   Iteration 96 of 100, tot loss = 4.897340497622888, l1: 0.00010548943699480635, l2: 0.0003842446171802294   Iteration 97 of 100, tot loss = 4.873621401098585, l1: 0.00010502329794539993, l2: 0.00038233884645040274   Iteration 98 of 100, tot loss = 4.873353555494425, l1: 0.00010505106454692562, l2: 0.00038228429555750397   Iteration 99 of 100, tot loss = 4.8453321998769585, l1: 0.00010453598988900959, l2: 0.0003799972345763253   Iteration 100 of 100, tot loss = 4.833987132310868, l1: 0.00010428560373838991, l2: 0.0003791131138859782
   End of epoch 1255; saving model... 

Epoch 1256 of 2000
   Iteration 1 of 100, tot loss = 5.907303333282471, l1: 0.00011809434363385662, l2: 0.0004726360202766955   Iteration 2 of 100, tot loss = 5.1209046840667725, l1: 0.00010931774522759952, l2: 0.00040277274092659354   Iteration 3 of 100, tot loss = 5.539642969767253, l1: 0.00011250911241707702, l2: 0.0004414552046606938   Iteration 4 of 100, tot loss = 5.340198636054993, l1: 0.00011149658348585945, l2: 0.0004225232914905064   Iteration 5 of 100, tot loss = 5.579462718963623, l1: 0.00011656730930553749, l2: 0.00044137896620668473   Iteration 6 of 100, tot loss = 5.776764790217082, l1: 0.0001212233097855157, l2: 0.00045645317004527897   Iteration 7 of 100, tot loss = 5.70680706841605, l1: 0.00011410999702223177, l2: 0.0004565707100222686   Iteration 8 of 100, tot loss = 5.66416072845459, l1: 0.00011697664194798563, l2: 0.0004494394306675531   Iteration 9 of 100, tot loss = 5.358321057425605, l1: 0.00011134523487675728, l2: 0.00042448686953220103   Iteration 10 of 100, tot loss = 5.188971829414368, l1: 0.00010937629049294628, l2: 0.00040952089184429494   Iteration 11 of 100, tot loss = 5.122481714595448, l1: 0.00010820228966291656, l2: 0.0004040458826983178   Iteration 12 of 100, tot loss = 5.132893184820811, l1: 0.0001105023329728283, l2: 0.00040278698967692134   Iteration 13 of 100, tot loss = 4.997498677327083, l1: 0.00010927826951956376, l2: 0.0003904716017691848   Iteration 14 of 100, tot loss = 4.965062260627747, l1: 0.00010888372915880089, l2: 0.00038762250083631704   Iteration 15 of 100, tot loss = 4.96474092801412, l1: 0.00010961975785903633, l2: 0.00038685433682985606   Iteration 16 of 100, tot loss = 5.160794243216515, l1: 0.00011311044090689393, l2: 0.0004029689825983951   Iteration 17 of 100, tot loss = 5.299485052333159, l1: 0.00011520208414260517, l2: 0.0004147464214303695   Iteration 18 of 100, tot loss = 5.266297062238057, l1: 0.00011572283498632412, l2: 0.00041090687025441893   Iteration 19 of 100, tot loss = 5.281715330324675, l1: 0.00011625030693769651, l2: 0.00041192122673811883   Iteration 20 of 100, tot loss = 5.163996338844299, l1: 0.00011450127822172362, l2: 0.0004018983556306921   Iteration 21 of 100, tot loss = 5.198269662402925, l1: 0.00011539406806973385, l2: 0.0004044328996401635   Iteration 22 of 100, tot loss = 5.196509924801913, l1: 0.00011523562980900434, l2: 0.0004044153637633744   Iteration 23 of 100, tot loss = 5.223139203113059, l1: 0.00011577098263407369, l2: 0.0004065429384354502   Iteration 24 of 100, tot loss = 5.306286454200745, l1: 0.00011739670602158488, l2: 0.0004132319384855994   Iteration 25 of 100, tot loss = 5.344076938629151, l1: 0.00011858706537168473, l2: 0.0004158206284046173   Iteration 26 of 100, tot loss = 5.3968425163855915, l1: 0.00011971193397095284, l2: 0.00041997231668434467   Iteration 27 of 100, tot loss = 5.448730998569065, l1: 0.00012074736662691942, l2: 0.0004241257319571795   Iteration 28 of 100, tot loss = 5.487274629729135, l1: 0.00011931622274070313, l2: 0.00042941123787646314   Iteration 29 of 100, tot loss = 5.436656310640532, l1: 0.00011830923828930626, l2: 0.0004253563900670872   Iteration 30 of 100, tot loss = 5.3896727403004965, l1: 0.00011735711765747207, l2: 0.0004216101534742241   Iteration 31 of 100, tot loss = 5.295192733887704, l1: 0.0001153837670684762, l2: 0.0004141355039469237   Iteration 32 of 100, tot loss = 5.2455818727612495, l1: 0.00011517754114720447, l2: 0.00040938064421425224   Iteration 33 of 100, tot loss = 5.168432250167385, l1: 0.00011377611813121334, l2: 0.00040306710500524127   Iteration 34 of 100, tot loss = 5.222942969378303, l1: 0.00011384055152816204, l2: 0.00040845374443212193   Iteration 35 of 100, tot loss = 5.159321798597063, l1: 0.0001122737090294582, l2: 0.00040365847027195353   Iteration 36 of 100, tot loss = 5.146936853726705, l1: 0.00011193204742691402, l2: 0.0004027616378152743   Iteration 37 of 100, tot loss = 5.185457654901453, l1: 0.00011184408199246595, l2: 0.00040670168409879143   Iteration 38 of 100, tot loss = 5.182911019576223, l1: 0.00011196163761365154, l2: 0.0004063294649026112   Iteration 39 of 100, tot loss = 5.216218019143129, l1: 0.00011272643883458864, l2: 0.0004088953644849169   Iteration 40 of 100, tot loss = 5.192251694202423, l1: 0.00011234935000175028, l2: 0.0004068758207722567   Iteration 41 of 100, tot loss = 5.147274459280619, l1: 0.00011088035552813557, l2: 0.0004038470917915153   Iteration 42 of 100, tot loss = 5.1730344749632335, l1: 0.00011148243408700052, l2: 0.0004058210158421259   Iteration 43 of 100, tot loss = 5.180722824362821, l1: 0.0001116253524953716, l2: 0.00040644693166710607   Iteration 44 of 100, tot loss = 5.119884111664512, l1: 0.00011081930014452982, l2: 0.00040116911292319526   Iteration 45 of 100, tot loss = 5.141542424096001, l1: 0.00011170630297985756, l2: 0.0004024479405618169   Iteration 46 of 100, tot loss = 5.130205154418945, l1: 0.00011181543184489863, l2: 0.00040120508429422245   Iteration 47 of 100, tot loss = 5.192213099053565, l1: 0.00011300524567288565, l2: 0.0004062160643367493   Iteration 48 of 100, tot loss = 5.192647000153859, l1: 0.00011271465685543565, l2: 0.00040655004310489556   Iteration 49 of 100, tot loss = 5.154758190622135, l1: 0.00011214807989522435, l2: 0.0004033277387973111   Iteration 50 of 100, tot loss = 5.141922912597656, l1: 0.00011193328042281792, l2: 0.00040225901029771193   Iteration 51 of 100, tot loss = 5.154820311303232, l1: 0.00011184262243035596, l2: 0.000403639408771186   Iteration 52 of 100, tot loss = 5.144961540515606, l1: 0.00011157415806687473, l2: 0.0004029219962831121   Iteration 53 of 100, tot loss = 5.1464326246729435, l1: 0.00011110237447044126, l2: 0.0004035408890318512   Iteration 54 of 100, tot loss = 5.123577872912089, l1: 0.00011031784931590125, l2: 0.0004020399390827623   Iteration 55 of 100, tot loss = 5.129451738704335, l1: 0.00011023958742787892, l2: 0.0004027055875419385   Iteration 56 of 100, tot loss = 5.142125883272716, l1: 0.00011021216700360778, l2: 0.000404000422869493   Iteration 57 of 100, tot loss = 5.131595130552325, l1: 0.0001103429452070901, l2: 0.00040281656950818406   Iteration 58 of 100, tot loss = 5.155066116102811, l1: 0.000110690458376635, l2: 0.0004048161542320078   Iteration 59 of 100, tot loss = 5.150639449135732, l1: 0.00011033389147102706, l2: 0.00040473005516351976   Iteration 60 of 100, tot loss = 5.153184751669566, l1: 0.00011049074031082758, l2: 0.00040482773692929186   Iteration 61 of 100, tot loss = 5.143359453951726, l1: 0.00011027574796347925, l2: 0.00040406019967927244   Iteration 62 of 100, tot loss = 5.142312146002246, l1: 0.00011025918020975715, l2: 0.0004039720367854311   Iteration 63 of 100, tot loss = 5.126289295771765, l1: 0.00010993973814442547, l2: 0.00040268919376161186   Iteration 64 of 100, tot loss = 5.152020912617445, l1: 0.00011051820990815031, l2: 0.0004046838832891808   Iteration 65 of 100, tot loss = 5.147487548681406, l1: 0.00011041439384616052, l2: 0.00040433436314252994   Iteration 66 of 100, tot loss = 5.168525417645772, l1: 0.00011081539648302802, l2: 0.00040603714759609744   Iteration 67 of 100, tot loss = 5.173042464612135, l1: 0.00011086258072920365, l2: 0.00040644166774790624   Iteration 68 of 100, tot loss = 5.186392724514008, l1: 0.00011119641117950189, l2: 0.0004074428633269717   Iteration 69 of 100, tot loss = 5.187409694643988, l1: 0.00011110960358696437, l2: 0.00040763136791571486   Iteration 70 of 100, tot loss = 5.203988814353943, l1: 0.00011149976167611645, l2: 0.00040889912197599186   Iteration 71 of 100, tot loss = 5.217999105722132, l1: 0.00011182426563939604, l2: 0.0004099756473114609   Iteration 72 of 100, tot loss = 5.20205882191658, l1: 0.00011160961195047194, l2: 0.0004085962724881635   Iteration 73 of 100, tot loss = 5.2359241884048675, l1: 0.00011232134888876766, l2: 0.000411271071978462   Iteration 74 of 100, tot loss = 5.2273483501898275, l1: 0.00011227510952488267, l2: 0.00041045972741542483   Iteration 75 of 100, tot loss = 5.204047123591105, l1: 0.00011198633204912766, l2: 0.00040841838247918837   Iteration 76 of 100, tot loss = 5.189084250676005, l1: 0.0001118554341940724, l2: 0.0004070529932122506   Iteration 77 of 100, tot loss = 5.1720135026163865, l1: 0.00011156140255905059, l2: 0.00040563994995167164   Iteration 78 of 100, tot loss = 5.166854415184412, l1: 0.00011144935669084915, l2: 0.0004052360870842583   Iteration 79 of 100, tot loss = 5.160418362557134, l1: 0.00011163646273485892, l2: 0.0004044053757407031   Iteration 80 of 100, tot loss = 5.13785465657711, l1: 0.00011123831627628533, l2: 0.00040254715149785626   Iteration 81 of 100, tot loss = 5.169290351278988, l1: 0.000111902438220568, l2: 0.000405026598507251   Iteration 82 of 100, tot loss = 5.151221286959764, l1: 0.00011129629212252727, l2: 0.00040382583800947476   Iteration 83 of 100, tot loss = 5.150931731764093, l1: 0.00011137131786013179, l2: 0.0004037218562857506   Iteration 84 of 100, tot loss = 5.143211909702846, l1: 0.00011118456114629571, l2: 0.0004031366307961954   Iteration 85 of 100, tot loss = 5.180634150785559, l1: 0.00011180134734946906, l2: 0.00040626206883804546   Iteration 86 of 100, tot loss = 5.202068295589713, l1: 0.0001118106598834518, l2: 0.00040839617069523653   Iteration 87 of 100, tot loss = 5.181811839684673, l1: 0.00011145054664122926, l2: 0.00040673063823755233   Iteration 88 of 100, tot loss = 5.148822911761024, l1: 0.0001107636792105702, l2: 0.0004041186130052665   Iteration 89 of 100, tot loss = 5.1488145362125355, l1: 0.00011040974980490963, l2: 0.00040447170498880317   Iteration 90 of 100, tot loss = 5.1394102017084755, l1: 0.00011038181235360551, l2: 0.0004035592087246995   Iteration 91 of 100, tot loss = 5.1411798524332575, l1: 0.00011055667626071506, l2: 0.00040356130969255224   Iteration 92 of 100, tot loss = 5.142061733681222, l1: 0.0001105981637801565, l2: 0.00040360801026311134   Iteration 93 of 100, tot loss = 5.127819850880613, l1: 0.00011041188902523299, l2: 0.00040237009678641833   Iteration 94 of 100, tot loss = 5.1167696080309275, l1: 0.00011012565250603849, l2: 0.000401551308738256   Iteration 95 of 100, tot loss = 5.139952077363667, l1: 0.00011047017994171351, l2: 0.0004035250282245933   Iteration 96 of 100, tot loss = 5.12443641324838, l1: 0.00011026116374068806, l2: 0.0004021824781072307   Iteration 97 of 100, tot loss = 5.107211555402303, l1: 0.0001099897328702392, l2: 0.00040073142331165557   Iteration 98 of 100, tot loss = 5.101350419375361, l1: 0.00011010861713397649, l2: 0.0004000264255280074   Iteration 99 of 100, tot loss = 5.094202711124613, l1: 0.0001100366166013768, l2: 0.000399383655284776   Iteration 100 of 100, tot loss = 5.087708125114441, l1: 0.00011021203677955781, l2: 0.000398558776214486
   End of epoch 1256; saving model... 

Epoch 1257 of 2000
   Iteration 1 of 100, tot loss = 3.5660061836242676, l1: 6.85371269355528e-05, l2: 0.00028806348564103246   Iteration 2 of 100, tot loss = 4.752672433853149, l1: 9.682601012173109e-05, l2: 0.000378441225620918   Iteration 3 of 100, tot loss = 4.726298650105794, l1: 0.00010796399874379858, l2: 0.0003646658636474361   Iteration 4 of 100, tot loss = 4.89973521232605, l1: 0.00011460612040536944, l2: 0.000375367388187442   Iteration 5 of 100, tot loss = 5.320004081726074, l1: 0.00012181498314021156, l2: 0.00041018541087396444   Iteration 6 of 100, tot loss = 5.061908880869548, l1: 0.00011297595240951826, l2: 0.00039321492658928037   Iteration 7 of 100, tot loss = 4.873667989458356, l1: 0.00011109856235894508, l2: 0.00037626822761792155   Iteration 8 of 100, tot loss = 5.13489043712616, l1: 0.00011679743056447478, l2: 0.00039669160833000205   Iteration 9 of 100, tot loss = 4.7390049828423395, l1: 0.00010748827440289056, l2: 0.00036641221959143877   Iteration 10 of 100, tot loss = 4.727789831161499, l1: 0.00010578067413007375, l2: 0.0003669983037980273   Iteration 11 of 100, tot loss = 4.657487219030207, l1: 0.00010621013676758263, l2: 0.00035953858010047537   Iteration 12 of 100, tot loss = 4.629732886950175, l1: 0.00010730173289630329, l2: 0.00035567155039946857   Iteration 13 of 100, tot loss = 4.582000292264498, l1: 0.00010602979930878904, l2: 0.00035217022433733713   Iteration 14 of 100, tot loss = 4.63884517124721, l1: 0.00010758783407384596, l2: 0.00035629667815685807   Iteration 15 of 100, tot loss = 4.464751704533895, l1: 0.0001033789723199637, l2: 0.0003430961941679319   Iteration 16 of 100, tot loss = 4.659248724579811, l1: 0.00010478720037099265, l2: 0.0003611376669141464   Iteration 17 of 100, tot loss = 4.565161382450777, l1: 0.00010306438622030648, l2: 0.00035345174803617685   Iteration 18 of 100, tot loss = 4.578977889484829, l1: 0.00010385279023871085, l2: 0.0003540449947144629   Iteration 19 of 100, tot loss = 4.56720599375273, l1: 0.0001037249976913077, l2: 0.00035299559883577257   Iteration 20 of 100, tot loss = 4.58069475889206, l1: 0.00010280067854182562, l2: 0.00035526879655662925   Iteration 21 of 100, tot loss = 4.624749490192959, l1: 0.00010306793843656557, l2: 0.0003594070093684076   Iteration 22 of 100, tot loss = 4.616701223633506, l1: 0.00010254375708675731, l2: 0.0003591263653519987   Iteration 23 of 100, tot loss = 4.619225325791732, l1: 0.00010234942554076122, l2: 0.00035957310794164306   Iteration 24 of 100, tot loss = 4.655088593562444, l1: 0.00010245424543124197, l2: 0.0003630546149603712   Iteration 25 of 100, tot loss = 4.608153839111328, l1: 0.00010172666879952885, l2: 0.000359088716795668   Iteration 26 of 100, tot loss = 4.645967465180617, l1: 0.00010273149340579179, l2: 0.0003618652553996071   Iteration 27 of 100, tot loss = 4.712934741267452, l1: 0.0001044732183530599, l2: 0.00036682025852820114   Iteration 28 of 100, tot loss = 4.702712110110691, l1: 0.00010505003735618499, l2: 0.0003652211765126724   Iteration 29 of 100, tot loss = 4.6925586338700915, l1: 0.00010479048407809199, l2: 0.00036446538207859827   Iteration 30 of 100, tot loss = 4.602430272102356, l1: 0.00010331409794162028, l2: 0.00035692893191784   Iteration 31 of 100, tot loss = 4.529815343118483, l1: 0.0001018778719064299, l2: 0.00035110366532594084   Iteration 32 of 100, tot loss = 4.590189523994923, l1: 0.00010323585433980043, l2: 0.0003557831009857182   Iteration 33 of 100, tot loss = 4.655170852487737, l1: 0.00010436757579692781, l2: 0.0003611495119526587   Iteration 34 of 100, tot loss = 4.721887216848486, l1: 0.0001059446057136965, l2: 0.0003662441180067082   Iteration 35 of 100, tot loss = 4.714736986160278, l1: 0.00010594086675804906, l2: 0.00036553283363381135   Iteration 36 of 100, tot loss = 4.722304575973087, l1: 0.00010575244717377548, l2: 0.000366478012438165   Iteration 37 of 100, tot loss = 4.751687468709172, l1: 0.00010636461052313648, l2: 0.00036880413795557077   Iteration 38 of 100, tot loss = 4.745131134986877, l1: 0.00010591746908367465, l2: 0.0003685956460111284   Iteration 39 of 100, tot loss = 4.68145168133271, l1: 0.00010495284816268116, l2: 0.0003631923217863704   Iteration 40 of 100, tot loss = 4.642141371965408, l1: 0.00010426790959172649, l2: 0.00035994622958241963   Iteration 41 of 100, tot loss = 4.631656454830635, l1: 0.00010400067300751532, l2: 0.0003591649739707752   Iteration 42 of 100, tot loss = 4.629447352318537, l1: 0.00010445270613889166, l2: 0.0003584920306734386   Iteration 43 of 100, tot loss = 4.607077465500942, l1: 0.00010438606384627218, l2: 0.0003563216840036127   Iteration 44 of 100, tot loss = 4.633609880100597, l1: 0.00010487867273861247, l2: 0.0003584823167660612   Iteration 45 of 100, tot loss = 4.611787202623155, l1: 0.00010484787344466895, l2: 0.00035633084868701793   Iteration 46 of 100, tot loss = 4.585687507753787, l1: 0.00010371324993763625, l2: 0.0003548555022976159   Iteration 47 of 100, tot loss = 4.64258843787173, l1: 0.00010459841839892373, l2: 0.0003596604269176246   Iteration 48 of 100, tot loss = 4.6296194444100065, l1: 0.00010450232646993148, l2: 0.0003584596191406793   Iteration 49 of 100, tot loss = 4.626731780110573, l1: 0.00010451933225600183, l2: 0.0003581538467373395   Iteration 50 of 100, tot loss = 4.644344515800476, l1: 0.00010476615389052313, l2: 0.0003596682980423793   Iteration 51 of 100, tot loss = 4.6867466954623955, l1: 0.00010497591738742045, l2: 0.0003636987524919723   Iteration 52 of 100, tot loss = 4.657383973781879, l1: 0.00010432283257754394, l2: 0.0003614155653989516   Iteration 53 of 100, tot loss = 4.660194909797524, l1: 0.00010432212407325114, l2: 0.00036169736775710197   Iteration 54 of 100, tot loss = 4.682565989317717, l1: 0.00010482261515992125, l2: 0.0003634339852767341   Iteration 55 of 100, tot loss = 4.652151146802035, l1: 0.00010421495740460655, l2: 0.00036100015885577624   Iteration 56 of 100, tot loss = 4.66477615918432, l1: 0.00010459281910308553, l2: 0.00036188479888161443   Iteration 57 of 100, tot loss = 4.6332893497065495, l1: 0.00010428872842008754, l2: 0.00035904020835898754   Iteration 58 of 100, tot loss = 4.730443292650683, l1: 0.00010571406597466257, l2: 0.00036733026404519854   Iteration 59 of 100, tot loss = 4.726554131103774, l1: 0.00010562872718102141, l2: 0.00036702668654408813   Iteration 60 of 100, tot loss = 4.71773826678594, l1: 0.00010546726777344399, l2: 0.00036630655983268905   Iteration 61 of 100, tot loss = 4.7127782047772016, l1: 0.00010525445449290049, l2: 0.0003660233668910462   Iteration 62 of 100, tot loss = 4.725348691786489, l1: 0.00010508837488399172, l2: 0.00036744649485392016   Iteration 63 of 100, tot loss = 4.737459375744774, l1: 0.00010507273951329157, l2: 0.0003686731984739059   Iteration 64 of 100, tot loss = 4.7171739637851715, l1: 0.00010503113463755653, l2: 0.0003666862621685141   Iteration 65 of 100, tot loss = 4.768325057396522, l1: 0.00010597989720946106, l2: 0.00037085260980977464   Iteration 66 of 100, tot loss = 4.7907476497418955, l1: 0.00010608615119770437, l2: 0.00037298861548840773   Iteration 67 of 100, tot loss = 4.817680970946355, l1: 0.00010619606652667176, l2: 0.0003755720325314732   Iteration 68 of 100, tot loss = 4.797596998074475, l1: 0.00010600477729466922, l2: 0.00037375492427278967   Iteration 69 of 100, tot loss = 4.795868054680202, l1: 0.00010603908979818158, l2: 0.00037354771748009693   Iteration 70 of 100, tot loss = 4.7789113794054305, l1: 0.00010537809913720203, l2: 0.0003725130406175075   Iteration 71 of 100, tot loss = 4.798159431403791, l1: 0.000105752423005982, l2: 0.0003740635219285868   Iteration 72 of 100, tot loss = 4.809575147098965, l1: 0.00010584065406267958, l2: 0.0003751168624148704   Iteration 73 of 100, tot loss = 4.799302800060952, l1: 0.00010542049215610572, l2: 0.00037450978990121147   Iteration 74 of 100, tot loss = 4.769698387867695, l1: 0.00010503777628253226, l2: 0.00037193206466130306   Iteration 75 of 100, tot loss = 4.759852574666341, l1: 0.00010496372526783185, l2: 0.0003710215344714622   Iteration 76 of 100, tot loss = 4.807345528351633, l1: 0.0001057355892427088, l2: 0.00037499896547830617   Iteration 77 of 100, tot loss = 4.832485013193899, l1: 0.00010579227455309592, l2: 0.00037745622821248973   Iteration 78 of 100, tot loss = 4.836529364952674, l1: 0.00010540514524155953, l2: 0.00037824779260867776   Iteration 79 of 100, tot loss = 4.819576396217829, l1: 0.00010482864672667347, l2: 0.0003771289945079037   Iteration 80 of 100, tot loss = 4.814195120334626, l1: 0.00010457784314894525, l2: 0.00037684167036786673   Iteration 81 of 100, tot loss = 4.806259814603829, l1: 0.00010452384841851143, l2: 0.00037610213405935575   Iteration 82 of 100, tot loss = 4.84663653955227, l1: 0.00010497426867404124, l2: 0.0003796893859810264   Iteration 83 of 100, tot loss = 4.848895308483078, l1: 0.000105333031975336, l2: 0.00037955650000788093   Iteration 84 of 100, tot loss = 4.85342135883513, l1: 0.00010539961645658227, l2: 0.00037994252072946565   Iteration 85 of 100, tot loss = 4.864826252881218, l1: 0.00010561067118781948, l2: 0.000380871955113595   Iteration 86 of 100, tot loss = 4.868129403092141, l1: 0.00010563019595581356, l2: 0.0003811827455349483   Iteration 87 of 100, tot loss = 4.8951646541727, l1: 0.00010597036841851338, l2: 0.00038354609853416766   Iteration 88 of 100, tot loss = 4.890900601040233, l1: 0.00010590145893729641, l2: 0.00038318860275797886   Iteration 89 of 100, tot loss = 4.883934921093202, l1: 0.0001054494714072063, l2: 0.00038294402229746154   Iteration 90 of 100, tot loss = 4.85404094060262, l1: 0.00010495376490224671, l2: 0.00038045033078459606   Iteration 91 of 100, tot loss = 4.866607052939279, l1: 0.00010493573023308232, l2: 0.0003817249769101861   Iteration 92 of 100, tot loss = 4.8709969468738725, l1: 0.00010494453738868722, l2: 0.0003821551589404091   Iteration 93 of 100, tot loss = 4.8668606563280985, l1: 0.00010511295527898236, l2: 0.00038157311197425367   Iteration 94 of 100, tot loss = 4.851831141938555, l1: 0.0001048137857380541, l2: 0.000380369330083882   Iteration 95 of 100, tot loss = 4.84369121852674, l1: 0.00010474079271692684, l2: 0.00037962833073259774   Iteration 96 of 100, tot loss = 4.855490272243817, l1: 0.00010512699501911509, l2: 0.00038042203429237514   Iteration 97 of 100, tot loss = 4.863672261385574, l1: 0.00010535243402664866, l2: 0.00038101479393722894   Iteration 98 of 100, tot loss = 4.845538788912248, l1: 0.00010513306321571486, l2: 0.00037942081749230166   Iteration 99 of 100, tot loss = 4.835198975572682, l1: 0.00010499192601377897, l2: 0.0003785279734093094   Iteration 100 of 100, tot loss = 4.8436633539199825, l1: 0.00010508358551305718, l2: 0.00037928275181911883
   End of epoch 1257; saving model... 

Epoch 1258 of 2000
   Iteration 1 of 100, tot loss = 5.983894348144531, l1: 0.0001114369006245397, l2: 0.0004869525146204978   Iteration 2 of 100, tot loss = 5.739734649658203, l1: 0.00011617664131335914, l2: 0.0004577968211378902   Iteration 3 of 100, tot loss = 5.757119337717692, l1: 0.00011820746294688433, l2: 0.00045750447316095233   Iteration 4 of 100, tot loss = 5.331742286682129, l1: 0.00011094221190433018, l2: 0.0004222320130793378   Iteration 5 of 100, tot loss = 5.574147891998291, l1: 0.00012024255120195449, l2: 0.0004371722345240414   Iteration 6 of 100, tot loss = 5.43427848815918, l1: 0.00011431710663600825, l2: 0.0004291107422128941   Iteration 7 of 100, tot loss = 5.2749247550964355, l1: 0.00011481775228665876, l2: 0.00041267472468981784   Iteration 8 of 100, tot loss = 5.020407617092133, l1: 0.00010958850270981202, l2: 0.00039245226071216166   Iteration 9 of 100, tot loss = 4.856976509094238, l1: 0.00010793411153523873, l2: 0.00037776354131185345   Iteration 10 of 100, tot loss = 4.959407615661621, l1: 0.00010924207381322049, l2: 0.00038669868663419036   Iteration 11 of 100, tot loss = 5.066072637384588, l1: 0.00010737867308886383, l2: 0.00039922859022309155   Iteration 12 of 100, tot loss = 5.110589623451233, l1: 0.00010591102606364682, l2: 0.00040514793605931726   Iteration 13 of 100, tot loss = 4.891392065928533, l1: 0.0001027548888277334, l2: 0.00038638431802750204   Iteration 14 of 100, tot loss = 5.032037275178092, l1: 0.00010591995097846458, l2: 0.0003972837730543688   Iteration 15 of 100, tot loss = 4.975958808263143, l1: 0.00010637415301365158, l2: 0.0003912217255371312   Iteration 16 of 100, tot loss = 4.918185368180275, l1: 0.00010557286304901936, l2: 0.0003862456724164076   Iteration 17 of 100, tot loss = 4.794120732475729, l1: 0.00010214292749022956, l2: 0.0003772691441768342   Iteration 18 of 100, tot loss = 4.809270355436537, l1: 0.00010171538492108489, l2: 0.00037921164847729314   Iteration 19 of 100, tot loss = 4.804985121676796, l1: 0.00010255708835565632, l2: 0.00037794142247692336   Iteration 20 of 100, tot loss = 4.900933265686035, l1: 0.00010170138702960684, l2: 0.00038839193803141825   Iteration 21 of 100, tot loss = 4.9411079088846845, l1: 0.00010269692020734684, l2: 0.0003914138686100376   Iteration 22 of 100, tot loss = 4.96614274111661, l1: 0.00010227535180588761, l2: 0.0003943389198287729   Iteration 23 of 100, tot loss = 5.061614285344663, l1: 0.00010470866064956088, l2: 0.00040145276390198296   Iteration 24 of 100, tot loss = 5.063133259614308, l1: 0.0001053608442210437, l2: 0.0004009524776241354   Iteration 25 of 100, tot loss = 5.0375189781188965, l1: 0.00010492243978660554, l2: 0.00039882945420686153   Iteration 26 of 100, tot loss = 4.945216417312622, l1: 0.0001035586596677044, l2: 0.0003909629785294573   Iteration 27 of 100, tot loss = 5.029936613859953, l1: 0.00010462758304870308, l2: 0.00039836607498323754   Iteration 28 of 100, tot loss = 5.071420703615461, l1: 0.00010592270674741095, l2: 0.00040121936139517596   Iteration 29 of 100, tot loss = 5.009443332409036, l1: 0.00010480638292361179, l2: 0.00039613794872601487   Iteration 30 of 100, tot loss = 4.979707972208659, l1: 0.00010394455190786782, l2: 0.0003940262433995182   Iteration 31 of 100, tot loss = 4.951955764524398, l1: 0.00010430501236940824, l2: 0.00039089056137468547   Iteration 32 of 100, tot loss = 5.030784144997597, l1: 0.00010599586448734044, l2: 0.00039708254735160153   Iteration 33 of 100, tot loss = 4.969162175149629, l1: 0.00010493989490593474, l2: 0.00039197632004013   Iteration 34 of 100, tot loss = 4.954852819442749, l1: 0.0001052331600101966, l2: 0.00039025211855700677   Iteration 35 of 100, tot loss = 4.901002699988229, l1: 0.00010378618067729154, l2: 0.00038631408616700876   Iteration 36 of 100, tot loss = 4.924341393841638, l1: 0.00010405542464771618, l2: 0.0003883787111489154   Iteration 37 of 100, tot loss = 4.898851098241033, l1: 0.00010391778522643035, l2: 0.00038596732121879687   Iteration 38 of 100, tot loss = 4.879177821309943, l1: 0.00010432036230524413, l2: 0.00038359741684919417   Iteration 39 of 100, tot loss = 4.884741722009121, l1: 0.00010499529628596532, l2: 0.00038347887255860347   Iteration 40 of 100, tot loss = 4.900246393680573, l1: 0.0001046140467224177, l2: 0.0003854105892969528   Iteration 41 of 100, tot loss = 4.929687814014714, l1: 0.00010527840250444303, l2: 0.00038769037538014987   Iteration 42 of 100, tot loss = 4.892451615560622, l1: 0.00010401335341503866, l2: 0.0003852318044491334   Iteration 43 of 100, tot loss = 4.878378834835319, l1: 0.00010423684757687476, l2: 0.00038360103221642675   Iteration 44 of 100, tot loss = 4.905751770192927, l1: 0.00010500605036544253, l2: 0.00038556912245853295   Iteration 45 of 100, tot loss = 4.926476913028293, l1: 0.00010454531285277981, l2: 0.00038810237407839546   Iteration 46 of 100, tot loss = 4.937039375305176, l1: 0.0001046528013039674, l2: 0.00038905113235713265   Iteration 47 of 100, tot loss = 4.970930738652006, l1: 0.00010510029011477836, l2: 0.0003919927796542129   Iteration 48 of 100, tot loss = 4.9326586574316025, l1: 0.0001045968826929311, l2: 0.0003886689792125253   Iteration 49 of 100, tot loss = 4.937758012693756, l1: 0.0001051591054271498, l2: 0.00038861669243636486   Iteration 50 of 100, tot loss = 4.974695200920105, l1: 0.0001057923801272409, l2: 0.0003916771372314543   Iteration 51 of 100, tot loss = 4.949785213844449, l1: 0.00010528497909499314, l2: 0.00038969353943898836   Iteration 52 of 100, tot loss = 4.910896755181826, l1: 0.00010464913590952235, l2: 0.00038644053683338617   Iteration 53 of 100, tot loss = 4.890241033626053, l1: 0.00010399153695786636, l2: 0.000385032563397819   Iteration 54 of 100, tot loss = 4.8662956511532816, l1: 0.0001036418390733026, l2: 0.0003829877233430226   Iteration 55 of 100, tot loss = 4.87710016857494, l1: 0.00010377122042965228, l2: 0.0003839387936750427   Iteration 56 of 100, tot loss = 4.842270676578794, l1: 0.00010339486035653473, l2: 0.00038083220429793334   Iteration 57 of 100, tot loss = 4.804237026917307, l1: 0.00010297749197394424, l2: 0.0003774462077815674   Iteration 58 of 100, tot loss = 4.797360070820512, l1: 0.00010307759676552137, l2: 0.00037665840726072806   Iteration 59 of 100, tot loss = 4.771748009374586, l1: 0.0001025352636534697, l2: 0.00037463953456115295   Iteration 60 of 100, tot loss = 4.747914139429728, l1: 0.00010210473368109282, l2: 0.00037268667753475405   Iteration 61 of 100, tot loss = 4.760801244954594, l1: 0.00010220481582094732, l2: 0.00037387530624744344   Iteration 62 of 100, tot loss = 4.733780876282723, l1: 0.00010187351120205881, l2: 0.0003715045738405728   Iteration 63 of 100, tot loss = 4.702075799306233, l1: 0.00010138812666853136, l2: 0.00036881945076553774   Iteration 64 of 100, tot loss = 4.694106318056583, l1: 0.0001013221817061094, l2: 0.0003680884476580104   Iteration 65 of 100, tot loss = 4.679064743335431, l1: 0.00010130132516049064, l2: 0.0003666051468686559   Iteration 66 of 100, tot loss = 4.656240329597935, l1: 0.00010084181887963716, l2: 0.000364782211721776   Iteration 67 of 100, tot loss = 4.667302754387927, l1: 0.00010084563364759574, l2: 0.0003658846394704488   Iteration 68 of 100, tot loss = 4.653083555838641, l1: 0.00010094479506840478, l2: 0.0003643635579518399   Iteration 69 of 100, tot loss = 4.641636409621308, l1: 0.00010073152979189679, l2: 0.00036343210861862946   Iteration 70 of 100, tot loss = 4.629613266672407, l1: 0.00010060975260525343, l2: 0.0003623515714675055   Iteration 71 of 100, tot loss = 4.620092140117162, l1: 0.00010033725517183791, l2: 0.0003616719563658887   Iteration 72 of 100, tot loss = 4.585922512743208, l1: 9.950353983691052e-05, l2: 0.0003590887091478281   Iteration 73 of 100, tot loss = 4.5841168312177265, l1: 9.947636688793116e-05, l2: 0.0003589353139818429   Iteration 74 of 100, tot loss = 4.566998636400378, l1: 9.926594493909097e-05, l2: 0.000357433916399935   Iteration 75 of 100, tot loss = 4.554184551239014, l1: 9.90427625462568e-05, l2: 0.0003563756903167814   Iteration 76 of 100, tot loss = 4.590520532507646, l1: 9.969517223477229e-05, l2: 0.0003593568793516361   Iteration 77 of 100, tot loss = 4.598725232211026, l1: 0.00010006787039395482, l2: 0.00035980465109822907   Iteration 78 of 100, tot loss = 4.580074557891259, l1: 9.980249622178514e-05, l2: 0.0003582049578723784   Iteration 79 of 100, tot loss = 4.579238526428802, l1: 9.980400808890869e-05, l2: 0.00035811984256540623   Iteration 80 of 100, tot loss = 4.59869102537632, l1: 9.976018932320586e-05, l2: 0.0003601089110816247   Iteration 81 of 100, tot loss = 4.634284734725952, l1: 0.00010017256634590147, l2: 0.00036325590540568726   Iteration 82 of 100, tot loss = 4.636446188135845, l1: 0.00010025676854683098, l2: 0.0003633878487318459   Iteration 83 of 100, tot loss = 4.626457987061466, l1: 0.00010019128770302387, l2: 0.0003624545094341107   Iteration 84 of 100, tot loss = 4.647273707957495, l1: 0.00010044188573894124, l2: 0.0003642854840325613   Iteration 85 of 100, tot loss = 4.641026987748988, l1: 0.00010029713829520487, l2: 0.0003638055594980388   Iteration 86 of 100, tot loss = 4.636067293411077, l1: 0.0001003101864795141, l2: 0.00036329654167909826   Iteration 87 of 100, tot loss = 4.635972891730824, l1: 0.00010033693038637685, l2: 0.0003632603575483543   Iteration 88 of 100, tot loss = 4.637590297243812, l1: 0.00010043637770319368, l2: 0.0003633226507107875   Iteration 89 of 100, tot loss = 4.639503245943048, l1: 0.00010042654284507591, l2: 0.0003635237803434158   Iteration 90 of 100, tot loss = 4.63415056069692, l1: 0.00010031676166464523, l2: 0.000363098293018993   Iteration 91 of 100, tot loss = 4.629423138859508, l1: 0.0001002587509551981, l2: 0.00036268356140411285   Iteration 92 of 100, tot loss = 4.606893599033356, l1: 9.991758883109509e-05, l2: 0.00036077176944514656   Iteration 93 of 100, tot loss = 4.622247057576334, l1: 0.0001001511225949407, l2: 0.0003620735820262663   Iteration 94 of 100, tot loss = 4.626630105870835, l1: 0.00010038883869747998, l2: 0.000362274171029059   Iteration 95 of 100, tot loss = 4.615319339852584, l1: 0.00010042232871024967, l2: 0.00036110960431151877   Iteration 96 of 100, tot loss = 4.628530852496624, l1: 0.00010016827191824025, l2: 0.000362684812292476   Iteration 97 of 100, tot loss = 4.650157886682098, l1: 0.00010030778146981377, l2: 0.0003647080063939901   Iteration 98 of 100, tot loss = 4.663086767099341, l1: 0.00010043304565461228, l2: 0.00036587563019046295   Iteration 99 of 100, tot loss = 4.663662686492458, l1: 0.00010028074092303421, l2: 0.00036608552674273725   Iteration 100 of 100, tot loss = 4.661081454753876, l1: 0.000100106136778777, l2: 0.00036600200750399383
   End of epoch 1258; saving model... 

Epoch 1259 of 2000
   Iteration 1 of 100, tot loss = 5.002391815185547, l1: 0.0001011956119327806, l2: 0.00039904360892251134   Iteration 2 of 100, tot loss = 4.804686069488525, l1: 9.703082105261274e-05, l2: 0.0003834378003375605   Iteration 3 of 100, tot loss = 5.216640154520671, l1: 0.00010187093236406024, l2: 0.00041979309753514826   Iteration 4 of 100, tot loss = 4.957708716392517, l1: 9.871887777990196e-05, l2: 0.0003970520047005266   Iteration 5 of 100, tot loss = 5.259558200836182, l1: 0.00010538351343711839, l2: 0.0004205723176710308   Iteration 6 of 100, tot loss = 5.329269806543986, l1: 0.0001080984099341246, l2: 0.0004248285840731114   Iteration 7 of 100, tot loss = 4.95558762550354, l1: 0.00010112885917936052, l2: 0.0003944299158839775   Iteration 8 of 100, tot loss = 4.79580157995224, l1: 9.904382295644609e-05, l2: 0.0003805363448918797   Iteration 9 of 100, tot loss = 5.10894415113661, l1: 0.00010093746823258698, l2: 0.00040995695679965947   Iteration 10 of 100, tot loss = 5.317638921737671, l1: 0.00010408670495962725, l2: 0.0004276771971490234   Iteration 11 of 100, tot loss = 5.191537683660334, l1: 0.00010398382172835144, l2: 0.0004151699567128989   Iteration 12 of 100, tot loss = 5.220569729804993, l1: 0.00010594720515655354, l2: 0.00041610977496020496   Iteration 13 of 100, tot loss = 5.294489347017729, l1: 0.00010632041700703737, l2: 0.0004231285241145927   Iteration 14 of 100, tot loss = 5.271036182131086, l1: 0.0001056083913551577, l2: 0.0004214952329805653   Iteration 15 of 100, tot loss = 5.4329863548278805, l1: 0.00010809897454843546, l2: 0.00043519966420717536   Iteration 16 of 100, tot loss = 5.499704033136368, l1: 0.00011057226356570027, l2: 0.00043939814167970326   Iteration 17 of 100, tot loss = 5.4274954515344955, l1: 0.0001116204439313151, l2: 0.00043112910284167705   Iteration 18 of 100, tot loss = 5.439715358946058, l1: 0.00011334043948510144, l2: 0.0004306310988290028   Iteration 19 of 100, tot loss = 5.383455050619025, l1: 0.00011214336355854022, l2: 0.0004262021424150781   Iteration 20 of 100, tot loss = 5.3871054410934445, l1: 0.0001116253733925987, l2: 0.00042708517285063865   Iteration 21 of 100, tot loss = 5.352847349076044, l1: 0.00011088206831898008, l2: 0.00042440267030282745   Iteration 22 of 100, tot loss = 5.3163276368921455, l1: 0.00010855070683068003, l2: 0.00042308206056160003   Iteration 23 of 100, tot loss = 5.348387116971224, l1: 0.00011005703392742282, l2: 0.00042478168008687055   Iteration 24 of 100, tot loss = 5.324348171552022, l1: 0.00010975320310535608, l2: 0.00042268161511553143   Iteration 25 of 100, tot loss = 5.290510959625244, l1: 0.00011015581549145281, l2: 0.0004188952804543078   Iteration 26 of 100, tot loss = 5.336189251679641, l1: 0.00011007940478381127, l2: 0.00042353951945327793   Iteration 27 of 100, tot loss = 5.414498293841326, l1: 0.00011034859171896069, l2: 0.0004311012367166027   Iteration 28 of 100, tot loss = 5.317963293620518, l1: 0.00010902649228228256, l2: 0.0004227698367945517   Iteration 29 of 100, tot loss = 5.33210146016088, l1: 0.00010978134252644819, l2: 0.0004234288047380938   Iteration 30 of 100, tot loss = 5.289175526301066, l1: 0.00010859472046528632, l2: 0.0004203228333305257   Iteration 31 of 100, tot loss = 5.266479046114029, l1: 0.00010781013711933948, l2: 0.00041883776852515554   Iteration 32 of 100, tot loss = 5.211487650871277, l1: 0.00010697541006265965, l2: 0.00041417335569349234   Iteration 33 of 100, tot loss = 5.225615804845637, l1: 0.00010694391830563264, l2: 0.00041561766314165044   Iteration 34 of 100, tot loss = 5.1825988432940315, l1: 0.0001065104102856233, l2: 0.00041174947460497017   Iteration 35 of 100, tot loss = 5.125705201285226, l1: 0.00010497992667036929, l2: 0.00040759059442539833   Iteration 36 of 100, tot loss = 5.136045866542393, l1: 0.00010551214978072999, l2: 0.0004080924372829031   Iteration 37 of 100, tot loss = 5.0816712186143205, l1: 0.00010421645955019001, l2: 0.00040395066300973395   Iteration 38 of 100, tot loss = 5.011818038789849, l1: 0.0001026363202790411, l2: 0.00039854548460999996   Iteration 39 of 100, tot loss = 5.020855689660097, l1: 0.00010306369725955077, l2: 0.0003990218735849246   Iteration 40 of 100, tot loss = 5.028363174200058, l1: 0.00010370630534453084, l2: 0.0003991300138295628   Iteration 41 of 100, tot loss = 5.042951264032504, l1: 0.00010398254220845269, l2: 0.0004003125863297411   Iteration 42 of 100, tot loss = 4.986534924734206, l1: 0.00010332329769707507, l2: 0.00039533019714456586   Iteration 43 of 100, tot loss = 5.0103249106296275, l1: 0.00010393318134024306, l2: 0.0003970993126385174   Iteration 44 of 100, tot loss = 5.0112257762388746, l1: 0.000104400695735504, l2: 0.00039672188392036003   Iteration 45 of 100, tot loss = 4.9859180662367075, l1: 0.00010457119464667307, l2: 0.0003940206137485802   Iteration 46 of 100, tot loss = 5.055954933166504, l1: 0.00010563406565621415, l2: 0.00039996142858517885   Iteration 47 of 100, tot loss = 5.051791728811061, l1: 0.00010578498257289423, l2: 0.00039939419118727144   Iteration 48 of 100, tot loss = 5.025845408439636, l1: 0.00010558195435805828, l2: 0.0003970025870027409   Iteration 49 of 100, tot loss = 5.014400219430729, l1: 0.00010562138406236713, l2: 0.00039581863841574105   Iteration 50 of 100, tot loss = 4.953945355415344, l1: 0.00010468406573636457, l2: 0.0003907104703830555   Iteration 51 of 100, tot loss = 4.905042143429027, l1: 0.00010349243629938356, l2: 0.0003870117789654828   Iteration 52 of 100, tot loss = 4.907632561830374, l1: 0.00010371888814776097, l2: 0.0003870443682759427   Iteration 53 of 100, tot loss = 4.8644062078224035, l1: 0.00010272225541862185, l2: 0.0003837183656561944   Iteration 54 of 100, tot loss = 4.882659046738236, l1: 0.00010309908867822271, l2: 0.00038516681713560873   Iteration 55 of 100, tot loss = 4.874954995242033, l1: 0.00010310128074541518, l2: 0.00038439421993891965   Iteration 56 of 100, tot loss = 4.869788595608303, l1: 0.00010341565679742157, l2: 0.0003835632038057416   Iteration 57 of 100, tot loss = 4.84626349231653, l1: 0.00010346185036612217, l2: 0.000381164500140585   Iteration 58 of 100, tot loss = 4.865012579950793, l1: 0.000104162171016375, l2: 0.00038233908833654857   Iteration 59 of 100, tot loss = 4.869907338740462, l1: 0.00010445165870636228, l2: 0.0003825390770127681   Iteration 60 of 100, tot loss = 4.889614542325337, l1: 0.00010489798332855572, l2: 0.0003840634723019321   Iteration 61 of 100, tot loss = 4.953577690437192, l1: 0.00010598625056922711, l2: 0.0003893715203033577   Iteration 62 of 100, tot loss = 4.923647588299167, l1: 0.0001054992956802204, l2: 0.00038686546500802283   Iteration 63 of 100, tot loss = 4.90644325528826, l1: 0.00010504717763055057, l2: 0.0003855971495680038   Iteration 64 of 100, tot loss = 4.9466022327542305, l1: 0.00010557650580267364, l2: 0.0003890837197104702   Iteration 65 of 100, tot loss = 4.906129774680505, l1: 0.00010483014109642961, l2: 0.00038578283834235315   Iteration 66 of 100, tot loss = 4.926865530736519, l1: 0.00010554200547851383, l2: 0.00038714454951166937   Iteration 67 of 100, tot loss = 4.9354219543400095, l1: 0.00010600507383385965, l2: 0.0003875371235085707   Iteration 68 of 100, tot loss = 4.94198272859349, l1: 0.00010613697395456256, l2: 0.00038806130054153446   Iteration 69 of 100, tot loss = 4.932302188182223, l1: 0.0001059796638888867, l2: 0.00038725055692602706   Iteration 70 of 100, tot loss = 4.951396264348712, l1: 0.0001060814441318923, l2: 0.00038905818426948304   Iteration 71 of 100, tot loss = 4.9399009053136265, l1: 0.00010589331495665788, l2: 0.0003880967775894276   Iteration 72 of 100, tot loss = 4.915039469798406, l1: 0.00010588128321210711, l2: 0.0003856226657161541   Iteration 73 of 100, tot loss = 4.941097909457063, l1: 0.00010641156629003801, l2: 0.00038769822655411516   Iteration 74 of 100, tot loss = 4.923385169055011, l1: 0.00010642254066526122, l2: 0.00038591597812892114   Iteration 75 of 100, tot loss = 4.896919991175333, l1: 0.00010585635100142099, l2: 0.00038383564989392954   Iteration 76 of 100, tot loss = 4.868994370887154, l1: 0.00010555708798308747, l2: 0.0003813423509678901   Iteration 77 of 100, tot loss = 4.88238534989295, l1: 0.00010560559956077184, l2: 0.0003826329374970118   Iteration 78 of 100, tot loss = 4.854771500978714, l1: 0.00010524537556659944, l2: 0.00038023177647772123   Iteration 79 of 100, tot loss = 4.867934284330923, l1: 0.00010525792251566508, l2: 0.00038153550785058473   Iteration 80 of 100, tot loss = 4.851014029979706, l1: 0.00010485473280823499, l2: 0.0003802466722845566   Iteration 81 of 100, tot loss = 4.845734937691394, l1: 0.00010491239425638564, l2: 0.00037966110171566226   Iteration 82 of 100, tot loss = 4.852725598870254, l1: 0.00010541603553908161, l2: 0.00037985652608501656   Iteration 83 of 100, tot loss = 4.84239309954356, l1: 0.00010527038422703508, l2: 0.00037896892768386975   Iteration 84 of 100, tot loss = 4.833474953969319, l1: 0.00010533774838265908, l2: 0.0003780097488613267   Iteration 85 of 100, tot loss = 4.823094926160924, l1: 0.00010536129005177987, l2: 0.00037694820442565663   Iteration 86 of 100, tot loss = 4.80066442489624, l1: 0.00010512535998206531, l2: 0.00037494108441616086   Iteration 87 of 100, tot loss = 4.808121785350229, l1: 0.00010521724567067509, l2: 0.0003755949348968657   Iteration 88 of 100, tot loss = 4.799823506311937, l1: 0.00010539946664390631, l2: 0.0003745828860039844   Iteration 89 of 100, tot loss = 4.81023749340786, l1: 0.00010591749533693034, l2: 0.00037510625562196326   Iteration 90 of 100, tot loss = 4.806062984466553, l1: 0.00010585267935109894, l2: 0.0003747536207406989   Iteration 91 of 100, tot loss = 4.796207619237376, l1: 0.00010576018262277152, l2: 0.00037386058110124585   Iteration 92 of 100, tot loss = 4.815806334433348, l1: 0.00010606461985488473, l2: 0.00037551601528083785   Iteration 93 of 100, tot loss = 4.805288232782836, l1: 0.00010574417437085011, l2: 0.00037478465071794206   Iteration 94 of 100, tot loss = 4.827099830546278, l1: 0.0001060638071376548, l2: 0.00037664617736888256   Iteration 95 of 100, tot loss = 4.831527047408255, l1: 0.00010597178422214807, l2: 0.00037718092192479065   Iteration 96 of 100, tot loss = 4.831877484917641, l1: 0.00010596720619560074, l2: 0.0003772205438205371   Iteration 97 of 100, tot loss = 4.8325869324281046, l1: 0.00010593584104838359, l2: 0.00037732285362807554   Iteration 98 of 100, tot loss = 4.826428554495987, l1: 0.0001058163986216437, l2: 0.00037682645794417593   Iteration 99 of 100, tot loss = 4.811520689665669, l1: 0.00010556405745452561, l2: 0.00037558801278749464   Iteration 100 of 100, tot loss = 4.828851039409638, l1: 0.0001056670849720831, l2: 0.00037721802000305614
   End of epoch 1259; saving model... 

Epoch 1260 of 2000
   Iteration 1 of 100, tot loss = 3.3020598888397217, l1: 8.897959924070165e-05, l2: 0.0002412263856967911   Iteration 2 of 100, tot loss = 4.078994393348694, l1: 9.55901232373435e-05, l2: 0.00031230929744197056   Iteration 3 of 100, tot loss = 4.102000633875529, l1: 9.735220131309082e-05, l2: 0.0003128478565486148   Iteration 4 of 100, tot loss = 3.9903998970985413, l1: 9.551661605655681e-05, l2: 0.00030352337125805207   Iteration 5 of 100, tot loss = 3.8641318798065187, l1: 8.570656209485605e-05, l2: 0.0003007066232385114   Iteration 6 of 100, tot loss = 3.8231738011042276, l1: 8.28856597460496e-05, l2: 0.0002994317207291412   Iteration 7 of 100, tot loss = 3.841592618397304, l1: 8.746478228463925e-05, l2: 0.0002966944781032258   Iteration 8 of 100, tot loss = 4.244121879339218, l1: 9.306593074143166e-05, l2: 0.00033134626028186176   Iteration 9 of 100, tot loss = 4.071508434083727, l1: 8.809732510902298e-05, l2: 0.00031905351918087236   Iteration 10 of 100, tot loss = 3.9461246967315673, l1: 8.649416813568677e-05, l2: 0.0003081183007452637   Iteration 11 of 100, tot loss = 3.98962909525091, l1: 8.87123569555115e-05, l2: 0.0003102505526674742   Iteration 12 of 100, tot loss = 4.015574137369792, l1: 9.063098059414187e-05, l2: 0.00031092643378845725   Iteration 13 of 100, tot loss = 4.047060746413011, l1: 9.249003750692982e-05, l2: 0.0003122160358963391   Iteration 14 of 100, tot loss = 4.125542368207659, l1: 9.286529815913777e-05, l2: 0.0003196889364127336   Iteration 15 of 100, tot loss = 4.297397549947103, l1: 9.613111784953313e-05, l2: 0.0003336086345370859   Iteration 16 of 100, tot loss = 4.304111510515213, l1: 9.626438827581296e-05, l2: 0.00033414675999665633   Iteration 17 of 100, tot loss = 4.3663214234744805, l1: 9.824553875232061e-05, l2: 0.00033838660216561575   Iteration 18 of 100, tot loss = 4.371450265248616, l1: 9.787690113524959e-05, l2: 0.0003392681230454602   Iteration 19 of 100, tot loss = 4.33946344726964, l1: 9.75210249108434e-05, l2: 0.00033642531764742573   Iteration 20 of 100, tot loss = 4.3734624028205875, l1: 9.936212900356623e-05, l2: 0.0003379841102287173   Iteration 21 of 100, tot loss = 4.303023633502779, l1: 9.741211605225024e-05, l2: 0.0003328902461189067   Iteration 22 of 100, tot loss = 4.271518013694069, l1: 9.69281916861126e-05, l2: 0.000330223607968285   Iteration 23 of 100, tot loss = 4.3175289941870645, l1: 9.83994538293463e-05, l2: 0.0003333534430408769   Iteration 24 of 100, tot loss = 4.238882303237915, l1: 9.745537499838974e-05, l2: 0.00032643285279239837   Iteration 25 of 100, tot loss = 4.3194895935058595, l1: 9.837970515945927e-05, l2: 0.00033356925239786506   Iteration 26 of 100, tot loss = 4.262909109775837, l1: 9.749120251329329e-05, l2: 0.000328799706879251   Iteration 27 of 100, tot loss = 4.223636115038836, l1: 9.645629271054295e-05, l2: 0.0003259073176498835   Iteration 28 of 100, tot loss = 4.231632198606219, l1: 9.645433497748204e-05, l2: 0.00032670888374143815   Iteration 29 of 100, tot loss = 4.251254131054056, l1: 9.671790231268147e-05, l2: 0.00032840750997099253   Iteration 30 of 100, tot loss = 4.308369509379069, l1: 9.777408195077442e-05, l2: 0.0003330628683518929   Iteration 31 of 100, tot loss = 4.4219601538873485, l1: 9.980334711614095e-05, l2: 0.00034239266730559567   Iteration 32 of 100, tot loss = 4.363761276006699, l1: 9.919507419908768e-05, l2: 0.0003371810521457519   Iteration 33 of 100, tot loss = 4.35676216356682, l1: 9.918264726310912e-05, l2: 0.0003364935678705065   Iteration 34 of 100, tot loss = 4.323236563626458, l1: 9.877864666872055e-05, l2: 0.0003335450087701354   Iteration 35 of 100, tot loss = 4.350836603982108, l1: 9.986539496042367e-05, l2: 0.0003352182642889342   Iteration 36 of 100, tot loss = 4.361513429217869, l1: 0.00010050789367394625, l2: 0.0003356434487310859   Iteration 37 of 100, tot loss = 4.416094612430882, l1: 0.00010042067328499429, l2: 0.00034118878790429115   Iteration 38 of 100, tot loss = 4.419696494152672, l1: 0.00010028867836808786, l2: 0.0003416809708333427   Iteration 39 of 100, tot loss = 4.419716492677346, l1: 0.00010046598356730568, l2: 0.0003415056652067086   Iteration 40 of 100, tot loss = 4.489761161804199, l1: 0.00010133919458894524, l2: 0.00034763692019623704   Iteration 41 of 100, tot loss = 4.475839486936244, l1: 0.00010122827832053257, l2: 0.00034635566904709286   Iteration 42 of 100, tot loss = 4.51874718211946, l1: 0.00010202545845892192, l2: 0.0003498492580893937   Iteration 43 of 100, tot loss = 4.459447078926619, l1: 0.00010079212251882784, l2: 0.0003451525838098077   Iteration 44 of 100, tot loss = 4.456810945814306, l1: 0.00010065964637430046, l2: 0.00034502144692603247   Iteration 45 of 100, tot loss = 4.449374882380168, l1: 0.00010026672114488772, l2: 0.00034467076539941544   Iteration 46 of 100, tot loss = 4.407402297724849, l1: 9.968981359396939e-05, l2: 0.00034105041445202556   Iteration 47 of 100, tot loss = 4.432226840485918, l1: 9.993490180312754e-05, l2: 0.00034328778131934316   Iteration 48 of 100, tot loss = 4.486035803953807, l1: 0.00010064535050939109, l2: 0.0003479582292129635   Iteration 49 of 100, tot loss = 4.476651814519142, l1: 0.00010045628732768819, l2: 0.00034720889365119975   Iteration 50 of 100, tot loss = 4.438064799308777, l1: 9.983761090552434e-05, l2: 0.00034396886883769184   Iteration 51 of 100, tot loss = 4.457671534781363, l1: 0.00010050305076326956, l2: 0.00034526410209033274   Iteration 52 of 100, tot loss = 4.435124475222367, l1: 0.00010027689915239954, l2: 0.0003432355476364207   Iteration 53 of 100, tot loss = 4.441362493443039, l1: 0.0001001114593785836, l2: 0.00034402478936585194   Iteration 54 of 100, tot loss = 4.39952094025082, l1: 9.940942335459922e-05, l2: 0.0003405426697874511   Iteration 55 of 100, tot loss = 4.397966354543512, l1: 9.925227707505904e-05, l2: 0.00034054435779001905   Iteration 56 of 100, tot loss = 4.3688956541674475, l1: 9.87565365773792e-05, l2: 0.0003381330279808026   Iteration 57 of 100, tot loss = 4.361176197988945, l1: 9.852692483834464e-05, l2: 0.0003375906942477613   Iteration 58 of 100, tot loss = 4.417666920300188, l1: 9.93918491013605e-05, l2: 0.0003423748421482742   Iteration 59 of 100, tot loss = 4.471429372237901, l1: 0.00010039903158400902, l2: 0.0003467439042435864   Iteration 60 of 100, tot loss = 4.511797539393107, l1: 0.00010095938050653785, l2: 0.0003502203714257727   Iteration 61 of 100, tot loss = 4.480439213455701, l1: 0.00010042734571530285, l2: 0.00034761657393499293   Iteration 62 of 100, tot loss = 4.468120455741882, l1: 0.00010043169752480612, l2: 0.0003463803462387483   Iteration 63 of 100, tot loss = 4.500446361208719, l1: 0.00010130417129922924, l2: 0.0003487404626664809   Iteration 64 of 100, tot loss = 4.552953127771616, l1: 0.00010178711011121777, l2: 0.00035350820076018863   Iteration 65 of 100, tot loss = 4.542244973549476, l1: 0.00010167401823519657, l2: 0.0003525504772999109   Iteration 66 of 100, tot loss = 4.543891997048349, l1: 0.00010171240721705765, l2: 0.00035267679049866274   Iteration 67 of 100, tot loss = 4.527623112521955, l1: 0.00010116526880040904, l2: 0.0003515970404520492   Iteration 68 of 100, tot loss = 4.574672060854295, l1: 0.00010175551452608917, l2: 0.00035571169021696896   Iteration 69 of 100, tot loss = 4.563625131828197, l1: 0.00010202911168259259, l2: 0.00035433340003626233   Iteration 70 of 100, tot loss = 4.555936993871416, l1: 0.00010196644351318744, l2: 0.0003536272546625696   Iteration 71 of 100, tot loss = 4.545007685540428, l1: 0.00010193111850473453, l2: 0.0003525696485385325   Iteration 72 of 100, tot loss = 4.537390516863929, l1: 0.00010179793268131714, l2: 0.000351941117413743   Iteration 73 of 100, tot loss = 4.521211859298079, l1: 0.00010121328817410969, l2: 0.00035090789646197314   Iteration 74 of 100, tot loss = 4.549193124513368, l1: 0.0001016656666694404, l2: 0.00035325364415011545   Iteration 75 of 100, tot loss = 4.54541379292806, l1: 0.00010137525052414276, l2: 0.00035316612707295765   Iteration 76 of 100, tot loss = 4.53715822884911, l1: 0.0001012403951168189, l2: 0.00035247542588254683   Iteration 77 of 100, tot loss = 4.5142156922972045, l1: 0.00010075571778791279, l2: 0.0003506658495216352   Iteration 78 of 100, tot loss = 4.536220110379732, l1: 0.00010128631994377475, l2: 0.00035233568894163443   Iteration 79 of 100, tot loss = 4.526937602441522, l1: 0.00010099226148921372, l2: 0.0003517014965491629   Iteration 80 of 100, tot loss = 4.54826166331768, l1: 0.00010107729299306812, l2: 0.00035374887120269704   Iteration 81 of 100, tot loss = 4.537442195562669, l1: 0.00010111651486281685, l2: 0.00035262770266674553   Iteration 82 of 100, tot loss = 4.530263610002471, l1: 0.00010108497367693373, l2: 0.00035194138515025103   Iteration 83 of 100, tot loss = 4.539144676851939, l1: 0.00010123149866514548, l2: 0.00035268296691005576   Iteration 84 of 100, tot loss = 4.520138834203992, l1: 0.00010090850805898386, l2: 0.00035110537334860837   Iteration 85 of 100, tot loss = 4.523594197104959, l1: 0.00010065749824314159, l2: 0.00035170191943453735   Iteration 86 of 100, tot loss = 4.518583045449367, l1: 0.000100464481790347, l2: 0.00035139382055413714   Iteration 87 of 100, tot loss = 4.519695569728983, l1: 0.00010071389666315564, l2: 0.00035125565806794483   Iteration 88 of 100, tot loss = 4.534402346069163, l1: 0.00010087120100169241, l2: 0.0003525690314166819   Iteration 89 of 100, tot loss = 4.541530960061577, l1: 0.0001012031337605355, l2: 0.00035294996011411   Iteration 90 of 100, tot loss = 4.528736639022827, l1: 0.00010104262485886769, l2: 0.00035183103706610077   Iteration 91 of 100, tot loss = 4.531909911187141, l1: 0.00010092093809642113, l2: 0.00035227005095553717   Iteration 92 of 100, tot loss = 4.5291265404742695, l1: 0.00010103617209413484, l2: 0.0003518764799240592   Iteration 93 of 100, tot loss = 4.550253073374431, l1: 0.00010119483242736458, l2: 0.00035383047298803645   Iteration 94 of 100, tot loss = 4.555983756450897, l1: 0.00010144839027216609, l2: 0.0003541499839790978   Iteration 95 of 100, tot loss = 4.57017943231683, l1: 0.00010160950512631404, l2: 0.00035540843673516066   Iteration 96 of 100, tot loss = 4.5759033759435015, l1: 0.0001018268654130831, l2: 0.00035576347075524   Iteration 97 of 100, tot loss = 4.54985333472183, l1: 0.0001012103602613596, l2: 0.0003537749717091258   Iteration 98 of 100, tot loss = 4.5402515153495635, l1: 0.0001011627313234886, l2: 0.000352862418590028   Iteration 99 of 100, tot loss = 4.5591874194867685, l1: 0.00010142902094157026, l2: 0.0003544897194704831   Iteration 100 of 100, tot loss = 4.535336866378784, l1: 0.00010106207813805668, l2: 0.0003524716068932321
   End of epoch 1260; saving model... 

Epoch 1261 of 2000
   Iteration 1 of 100, tot loss = 4.8213324546813965, l1: 0.00010726119216997176, l2: 0.0003748720628209412   Iteration 2 of 100, tot loss = 5.760730743408203, l1: 0.00011682753392960876, l2: 0.0004592455516103655   Iteration 3 of 100, tot loss = 4.831684827804565, l1: 0.00010156277130590752, l2: 0.0003816057239115859   Iteration 4 of 100, tot loss = 4.523826837539673, l1: 9.742300426296424e-05, l2: 0.0003549596913217101   Iteration 5 of 100, tot loss = 5.065738296508789, l1: 0.00010686211317079142, l2: 0.00039971171936485915   Iteration 6 of 100, tot loss = 5.38927976290385, l1: 0.00011358253808187631, l2: 0.00042534544142351177   Iteration 7 of 100, tot loss = 5.056355782917568, l1: 0.00010639983832204183, l2: 0.00039923574201696156   Iteration 8 of 100, tot loss = 5.236870914697647, l1: 0.00011034466206183424, l2: 0.0004133424281462794   Iteration 9 of 100, tot loss = 5.138576375113593, l1: 0.0001081389370180356, l2: 0.0004057186969374824   Iteration 10 of 100, tot loss = 5.152616333961487, l1: 0.00010840079412446357, l2: 0.00040686083812033755   Iteration 11 of 100, tot loss = 5.242331613193858, l1: 0.00011015111191558059, l2: 0.0004140820462701165   Iteration 12 of 100, tot loss = 4.972886184851329, l1: 0.00010499689718320344, l2: 0.0003922917179200643   Iteration 13 of 100, tot loss = 4.912843502484835, l1: 0.00010509327526401299, l2: 0.00038619107195140363   Iteration 14 of 100, tot loss = 4.891614760671343, l1: 0.00010521660442464054, l2: 0.00038394486909965053   Iteration 15 of 100, tot loss = 4.720956691106161, l1: 0.00010286678346650054, l2: 0.00036922888345240307   Iteration 16 of 100, tot loss = 4.709963694214821, l1: 0.00010328941516490886, l2: 0.00036770695351151517   Iteration 17 of 100, tot loss = 4.804077779545503, l1: 0.00010602339941268677, l2: 0.0003743843776379328   Iteration 18 of 100, tot loss = 4.903892027007209, l1: 0.00010765832202095125, l2: 0.000382730878805483   Iteration 19 of 100, tot loss = 4.808658110468011, l1: 0.00010484367923804951, l2: 0.0003760221305511598   Iteration 20 of 100, tot loss = 4.84421044588089, l1: 0.00010668245649867459, l2: 0.00037773858857690355   Iteration 21 of 100, tot loss = 4.990299667630877, l1: 0.00010920145775474208, l2: 0.0003898285095125348   Iteration 22 of 100, tot loss = 5.027460607615384, l1: 0.00010885446069799151, l2: 0.0003938916018920612   Iteration 23 of 100, tot loss = 5.046780783197154, l1: 0.00010823208345834206, l2: 0.000396445995252377   Iteration 24 of 100, tot loss = 5.0192395349343615, l1: 0.00010739754861788242, l2: 0.0003945264040036515   Iteration 25 of 100, tot loss = 5.049837636947632, l1: 0.00010803865807247348, l2: 0.000396945103420876   Iteration 26 of 100, tot loss = 5.055475152455843, l1: 0.00010835343947781858, l2: 0.0003971940756085902   Iteration 27 of 100, tot loss = 4.979874822828505, l1: 0.00010722478449072999, l2: 0.0003907626976595364   Iteration 28 of 100, tot loss = 4.928942067282541, l1: 0.00010685957710977943, l2: 0.0003860346296278294   Iteration 29 of 100, tot loss = 4.916427086139548, l1: 0.00010740859767699069, l2: 0.0003842341112462675   Iteration 30 of 100, tot loss = 4.9228326638539635, l1: 0.00010782187467460366, l2: 0.00038446139127093677   Iteration 31 of 100, tot loss = 4.902547405612085, l1: 0.00010658365722666795, l2: 0.00038367108256000303   Iteration 32 of 100, tot loss = 4.898801997303963, l1: 0.00010649202715740103, l2: 0.0003833881723949162   Iteration 33 of 100, tot loss = 4.848054907538674, l1: 0.00010571799805003332, l2: 0.00037908749205102635   Iteration 34 of 100, tot loss = 4.880147239741157, l1: 0.0001063206932218129, l2: 0.00038169403106782254   Iteration 35 of 100, tot loss = 4.877359519686018, l1: 0.00010557819238913778, l2: 0.0003821577598241025   Iteration 36 of 100, tot loss = 4.9114267230033875, l1: 0.00010644769796878488, l2: 0.00038469497465282783   Iteration 37 of 100, tot loss = 4.875911210034345, l1: 0.00010649494327702307, l2: 0.0003810961777187028   Iteration 38 of 100, tot loss = 4.857208214308086, l1: 0.00010690441434378859, l2: 0.0003788164073117322   Iteration 39 of 100, tot loss = 4.849529034052139, l1: 0.00010736240773649815, l2: 0.00037759049677660165   Iteration 40 of 100, tot loss = 4.932065808773041, l1: 0.00010902977764999377, l2: 0.0003841768051643157   Iteration 41 of 100, tot loss = 4.899042815696903, l1: 0.00010858022381775877, l2: 0.0003813240594065907   Iteration 42 of 100, tot loss = 4.88543819245838, l1: 0.00010777792487628868, l2: 0.0003807658965039688   Iteration 43 of 100, tot loss = 4.874848443408345, l1: 0.00010735927774708957, l2: 0.0003801255685217722   Iteration 44 of 100, tot loss = 4.850826003334739, l1: 0.00010705630839683263, l2: 0.00037802629396372305   Iteration 45 of 100, tot loss = 4.827481566535102, l1: 0.00010662255380237993, l2: 0.0003761256045739477   Iteration 46 of 100, tot loss = 4.804952631825986, l1: 0.00010654115399317917, l2: 0.0003739541106093067   Iteration 47 of 100, tot loss = 4.822585887097298, l1: 0.00010653974575379447, l2: 0.000375718844650769   Iteration 48 of 100, tot loss = 4.799746870994568, l1: 0.00010638490016390278, l2: 0.0003735897883719493   Iteration 49 of 100, tot loss = 4.836851090801005, l1: 0.00010679006300051697, l2: 0.00037689504780264913   Iteration 50 of 100, tot loss = 4.811814317703247, l1: 0.0001062485854345141, l2: 0.00037493284820811824   Iteration 51 of 100, tot loss = 4.861691306619083, l1: 0.0001073817276097971, l2: 0.0003787874048415973   Iteration 52 of 100, tot loss = 4.83628989641483, l1: 0.00010690290841571718, l2: 0.0003767260828821096   Iteration 53 of 100, tot loss = 4.783781335038959, l1: 0.00010592305337030546, l2: 0.0003724550816805963   Iteration 54 of 100, tot loss = 4.78982702449516, l1: 0.00010610059527809628, l2: 0.0003728821080854956   Iteration 55 of 100, tot loss = 4.802184820175171, l1: 0.00010652799709615382, l2: 0.00037369048513937744   Iteration 56 of 100, tot loss = 4.783001018421991, l1: 0.00010592928798100079, l2: 0.0003723708138880154   Iteration 57 of 100, tot loss = 4.764017786896019, l1: 0.00010529232237961863, l2: 0.00037110945656119537   Iteration 58 of 100, tot loss = 4.726018202715907, l1: 0.0001045499899693572, l2: 0.0003680518305467056   Iteration 59 of 100, tot loss = 4.6922245874243265, l1: 0.00010379328733059515, l2: 0.00036542917161753747   Iteration 60 of 100, tot loss = 4.667350141207377, l1: 0.0001034581177009386, l2: 0.0003632768966781441   Iteration 61 of 100, tot loss = 4.677304213164283, l1: 0.00010364623932353007, l2: 0.0003640841820074978   Iteration 62 of 100, tot loss = 4.643516359790679, l1: 0.00010297121298208367, l2: 0.0003613804231375276   Iteration 63 of 100, tot loss = 4.623895327250163, l1: 0.0001021832075142782, l2: 0.00036020632528345145   Iteration 64 of 100, tot loss = 4.617210149765015, l1: 0.00010234163545419506, l2: 0.00035937937991548097   Iteration 65 of 100, tot loss = 4.638922896751991, l1: 0.00010262926052046868, l2: 0.000361263029760896   Iteration 66 of 100, tot loss = 4.616897095333446, l1: 0.00010237404548564827, l2: 0.0003593156648231811   Iteration 67 of 100, tot loss = 4.6769556109584975, l1: 0.00010327856933199383, l2: 0.00036441699194256216   Iteration 68 of 100, tot loss = 4.669971679939943, l1: 0.00010309991231385057, l2: 0.00036389725595226455   Iteration 69 of 100, tot loss = 4.655487654865652, l1: 0.00010305584510206245, l2: 0.0003624929204174871   Iteration 70 of 100, tot loss = 4.6508508341653005, l1: 0.00010276578837614839, l2: 0.00036231929490376   Iteration 71 of 100, tot loss = 4.660506389510464, l1: 0.00010326309573326693, l2: 0.0003627875430794748   Iteration 72 of 100, tot loss = 4.664872758918339, l1: 0.00010305688389659433, l2: 0.0003634303916947425   Iteration 73 of 100, tot loss = 4.657686422948968, l1: 0.00010296542398601272, l2: 0.00036280321803307546   Iteration 74 of 100, tot loss = 4.678114491540033, l1: 0.0001036780572207676, l2: 0.00036413339222309406   Iteration 75 of 100, tot loss = 4.687852764129639, l1: 0.00010389467725569072, l2: 0.00036489059935168675   Iteration 76 of 100, tot loss = 4.672612714140039, l1: 0.00010389477895506915, l2: 0.00036336649243726   Iteration 77 of 100, tot loss = 4.65433044247813, l1: 0.00010353895574210926, l2: 0.00036189408860223777   Iteration 78 of 100, tot loss = 4.657225037232424, l1: 0.00010358132338949825, l2: 0.00036214118079395773   Iteration 79 of 100, tot loss = 4.700547643854648, l1: 0.00010415548461458434, l2: 0.00036589927949995697   Iteration 80 of 100, tot loss = 4.723377421498299, l1: 0.0001043219904659054, l2: 0.00036801575170102295   Iteration 81 of 100, tot loss = 4.732656793829835, l1: 0.00010479162395953338, l2: 0.0003684740554883409   Iteration 82 of 100, tot loss = 4.7159126531787035, l1: 0.00010458161736229469, l2: 0.00036700964793875225   Iteration 83 of 100, tot loss = 4.727890936725111, l1: 0.00010477338074418963, l2: 0.0003680157131888533   Iteration 84 of 100, tot loss = 4.725897462595077, l1: 0.0001045682069854506, l2: 0.00036802153963002464   Iteration 85 of 100, tot loss = 4.741897064096787, l1: 0.0001046508587023709, l2: 0.00036953884769258473   Iteration 86 of 100, tot loss = 4.741468315900758, l1: 0.00010498423948457606, l2: 0.0003691625923153858   Iteration 87 of 100, tot loss = 4.744415570949686, l1: 0.0001052866177449415, l2: 0.0003691549399859625   Iteration 88 of 100, tot loss = 4.743153135884892, l1: 0.00010523356877456536, l2: 0.00036908174545467114   Iteration 89 of 100, tot loss = 4.728738208835044, l1: 0.00010494216270201965, l2: 0.00036793165880103667   Iteration 90 of 100, tot loss = 4.7716236140992905, l1: 0.00010562226836757165, l2: 0.00037154009413724353   Iteration 91 of 100, tot loss = 4.7703735671200596, l1: 0.00010583080114844092, l2: 0.00037120655674748325   Iteration 92 of 100, tot loss = 4.8089732942373855, l1: 0.00010646059170381784, l2: 0.00037443673958794375   Iteration 93 of 100, tot loss = 4.809758583704631, l1: 0.00010626029364185118, l2: 0.00037471556667393696   Iteration 94 of 100, tot loss = 4.799148711752384, l1: 0.00010598305143890258, l2: 0.00037393182162517445   Iteration 95 of 100, tot loss = 4.776222532673886, l1: 0.00010563521680428255, l2: 0.00037198703840227897   Iteration 96 of 100, tot loss = 4.788850925862789, l1: 0.00010603635841259045, l2: 0.00037284873633325333   Iteration 97 of 100, tot loss = 4.775941423534118, l1: 0.00010574265312224616, l2: 0.0003718514911729612   Iteration 98 of 100, tot loss = 4.752499161934366, l1: 0.0001053423425868719, l2: 0.00036990757552284406   Iteration 99 of 100, tot loss = 4.760886254936758, l1: 0.00010542597440441817, l2: 0.0003706626527921551   Iteration 100 of 100, tot loss = 4.764412984848023, l1: 0.00010542979758611182, l2: 0.0003710115024296101
   End of epoch 1261; saving model... 

Epoch 1262 of 2000
   Iteration 1 of 100, tot loss = 4.63936710357666, l1: 7.02767792972736e-05, l2: 0.0003936599241569638   Iteration 2 of 100, tot loss = 3.522990345954895, l1: 6.982416016398929e-05, l2: 0.00028247487352928147   Iteration 3 of 100, tot loss = 4.144912481307983, l1: 8.93988932754534e-05, l2: 0.00032509237159198773   Iteration 4 of 100, tot loss = 5.028348743915558, l1: 0.00010974466385960113, l2: 0.00039309022031375207   Iteration 5 of 100, tot loss = 5.570287752151489, l1: 0.00011770520213758573, l2: 0.00043932358094025405   Iteration 6 of 100, tot loss = 5.354475299517314, l1: 0.0001117507466309083, l2: 0.00042369679188899073   Iteration 7 of 100, tot loss = 5.475692442485264, l1: 0.00011421097276200141, l2: 0.00043335827857455503   Iteration 8 of 100, tot loss = 5.477598279714584, l1: 0.00011392516807973152, l2: 0.0004338346716394881   Iteration 9 of 100, tot loss = 5.397019571728176, l1: 0.0001116692316524374, l2: 0.00042803273633277666   Iteration 10 of 100, tot loss = 5.144137883186341, l1: 0.00010987489222316071, l2: 0.0004045389054226689   Iteration 11 of 100, tot loss = 5.191619677977129, l1: 0.00011029161991742016, l2: 0.00040887035985096276   Iteration 12 of 100, tot loss = 5.149096190929413, l1: 0.00010659352422711284, l2: 0.0004083161050706015   Iteration 13 of 100, tot loss = 4.981258832491362, l1: 0.0001039029549037178, l2: 0.00039422293775714934   Iteration 14 of 100, tot loss = 4.876132079533169, l1: 0.0001038052505464293, l2: 0.00038380796572060457   Iteration 15 of 100, tot loss = 4.8307140350341795, l1: 0.00010453478074244534, l2: 0.0003785366308875382   Iteration 16 of 100, tot loss = 4.935636043548584, l1: 0.00010624665947034373, l2: 0.0003873169516737107   Iteration 17 of 100, tot loss = 4.973852718577666, l1: 0.00010863942160914816, l2: 0.0003887458553756861   Iteration 18 of 100, tot loss = 4.98071469200982, l1: 0.00010893264253455628, l2: 0.0003891388332704082   Iteration 19 of 100, tot loss = 4.855356505042629, l1: 0.0001058607057003476, l2: 0.00037967495154589415   Iteration 20 of 100, tot loss = 4.886552345752716, l1: 0.00010731674174166984, l2: 0.0003813385003013536   Iteration 21 of 100, tot loss = 4.905403443745205, l1: 0.00010555474779851335, l2: 0.0003849856050995489   Iteration 22 of 100, tot loss = 4.842036019672047, l1: 0.00010492739916530395, l2: 0.0003792762105480175   Iteration 23 of 100, tot loss = 4.76037973942964, l1: 0.00010369983378357416, l2: 0.0003723381479452972   Iteration 24 of 100, tot loss = 4.923886885245641, l1: 0.00010619743579809438, l2: 0.00038619126159270917   Iteration 25 of 100, tot loss = 4.9372021579742436, l1: 0.00010586194795905612, l2: 0.0003878582769539207   Iteration 26 of 100, tot loss = 4.976080756921035, l1: 0.00010620360276684201, l2: 0.00039140448251810786   Iteration 27 of 100, tot loss = 5.068564441468981, l1: 0.00010827680819568707, l2: 0.00039857964428072727   Iteration 28 of 100, tot loss = 5.130986511707306, l1: 0.0001086532092813286, l2: 0.00040444545032057377   Iteration 29 of 100, tot loss = 5.154332037629752, l1: 0.0001090086219031444, l2: 0.00040642459122142914   Iteration 30 of 100, tot loss = 5.110862183570862, l1: 0.00010910362701300376, l2: 0.0004019826010335237   Iteration 31 of 100, tot loss = 5.08939346959514, l1: 0.00010884611601208997, l2: 0.0004000932395067667   Iteration 32 of 100, tot loss = 5.01728305965662, l1: 0.000107546990307128, l2: 0.0003941813238270697   Iteration 33 of 100, tot loss = 5.0578905235637315, l1: 0.00010837673658953105, l2: 0.00039741232423252905   Iteration 34 of 100, tot loss = 5.0780640419791725, l1: 0.0001079573991292444, l2: 0.0003998490120626657   Iteration 35 of 100, tot loss = 5.132208068030221, l1: 0.00010885669294761361, l2: 0.0004043641209136695   Iteration 36 of 100, tot loss = 5.139126545853085, l1: 0.00010964626774997062, l2: 0.00040426639396981854   Iteration 37 of 100, tot loss = 5.07713550490302, l1: 0.00010825598498765455, l2: 0.00039945757197455277   Iteration 38 of 100, tot loss = 5.088850705247176, l1: 0.00010854309632697168, l2: 0.00040034198096499925   Iteration 39 of 100, tot loss = 5.057043876403418, l1: 0.00010813710543562849, l2: 0.00039756728913814115   Iteration 40 of 100, tot loss = 5.0713204681873325, l1: 0.0001079666445548355, l2: 0.00039916540881677063   Iteration 41 of 100, tot loss = 5.083619739951157, l1: 0.00010865304657050817, l2: 0.0003997089334894199   Iteration 42 of 100, tot loss = 5.058871274902707, l1: 0.00010805126008303237, l2: 0.0003978358730646072   Iteration 43 of 100, tot loss = 5.058206652486047, l1: 0.00010828941857082185, l2: 0.00039753125240040815   Iteration 44 of 100, tot loss = 5.067324101924896, l1: 0.00010843538811141943, l2: 0.00039829702853405087   Iteration 45 of 100, tot loss = 5.0309257984161375, l1: 0.00010789903309260909, l2: 0.00039519355276651264   Iteration 46 of 100, tot loss = 5.053426322729691, l1: 0.00010842656181763311, l2: 0.00039691607684210834   Iteration 47 of 100, tot loss = 5.1684421123342315, l1: 0.00011021750516250749, l2: 0.0004066267107152677   Iteration 48 of 100, tot loss = 5.143909176190694, l1: 0.0001096341162186339, l2: 0.0004047568063469953   Iteration 49 of 100, tot loss = 5.153240271977016, l1: 0.00010998203466368402, l2: 0.00040534199708454994   Iteration 50 of 100, tot loss = 5.144375400543213, l1: 0.00010948443559755106, l2: 0.0004049531088094227   Iteration 51 of 100, tot loss = 5.125660812153535, l1: 0.00010951173024688044, l2: 0.000403054355582058   Iteration 52 of 100, tot loss = 5.105064181181101, l1: 0.00010936955739173124, l2: 0.00040113686526847933   Iteration 53 of 100, tot loss = 5.079006073609838, l1: 0.00010895312969399317, l2: 0.00039894748199072156   Iteration 54 of 100, tot loss = 5.0309133088147195, l1: 0.00010802418135561445, l2: 0.00039506715364512747   Iteration 55 of 100, tot loss = 5.069102582064542, l1: 0.00010887975162900562, l2: 0.0003980305109342391   Iteration 56 of 100, tot loss = 5.04812416434288, l1: 0.00010882808010137524, l2: 0.00039598434055473523   Iteration 57 of 100, tot loss = 5.060069682305319, l1: 0.00010921506691374816, l2: 0.00039679190529951533   Iteration 58 of 100, tot loss = 5.102871685192503, l1: 0.00011006486257146804, l2: 0.0004002223105434778   Iteration 59 of 100, tot loss = 5.072832879373583, l1: 0.00010970392247407413, l2: 0.0003975793700546847   Iteration 60 of 100, tot loss = 5.064476732412974, l1: 0.00010981550373495944, l2: 0.00039663217418516673   Iteration 61 of 100, tot loss = 5.101430834316816, l1: 0.00011045134013955726, l2: 0.000399691747409887   Iteration 62 of 100, tot loss = 5.080765028153697, l1: 0.00011021500135792584, l2: 0.0003978615052126829   Iteration 63 of 100, tot loss = 5.056914991802639, l1: 0.00010990722575880927, l2: 0.00039578427695521407   Iteration 64 of 100, tot loss = 5.053414586931467, l1: 0.00010961953881860609, l2: 0.00039572192372361314   Iteration 65 of 100, tot loss = 5.033129156552828, l1: 0.00010921726929014907, l2: 0.00039409565013976626   Iteration 66 of 100, tot loss = 5.025543892022335, l1: 0.0001091021270192207, l2: 0.00039345226568085224   Iteration 67 of 100, tot loss = 5.039125157825983, l1: 0.00010918062893758918, l2: 0.0003947318903406832   Iteration 68 of 100, tot loss = 5.027256622033961, l1: 0.00010931405893857386, l2: 0.0003934116064235294   Iteration 69 of 100, tot loss = 5.02449855942657, l1: 0.00010956216003936446, l2: 0.0003928876991478213   Iteration 70 of 100, tot loss = 5.011640432902745, l1: 0.00010960965378866863, l2: 0.0003915543925748872   Iteration 71 of 100, tot loss = 5.026512998930166, l1: 0.00010976227395716911, l2: 0.0003928890292742618   Iteration 72 of 100, tot loss = 5.031905730565389, l1: 0.0001095268030439911, l2: 0.00039366377354276157   Iteration 73 of 100, tot loss = 5.050680905172269, l1: 0.00010955252513019943, l2: 0.0003955155688382634   Iteration 74 of 100, tot loss = 5.052192958625588, l1: 0.00010956646248224708, l2: 0.0003956528368714341   Iteration 75 of 100, tot loss = 5.055028928120931, l1: 0.00010964582048472949, l2: 0.000395857075539728   Iteration 76 of 100, tot loss = 5.0566609533209546, l1: 0.00010965621420741387, l2: 0.00039600988384336233   Iteration 77 of 100, tot loss = 5.049665816418536, l1: 0.00010959700970984572, l2: 0.0003953695750051456   Iteration 78 of 100, tot loss = 5.041876462789682, l1: 0.00010938685791655026, l2: 0.00039480079132585955   Iteration 79 of 100, tot loss = 5.0467033024075665, l1: 0.00010943465144631358, l2: 0.00039523568142889234   Iteration 80 of 100, tot loss = 5.034014636278153, l1: 0.00010913029332186852, l2: 0.0003942711729905568   Iteration 81 of 100, tot loss = 5.03313829280712, l1: 0.00010887753119782751, l2: 0.00039443630081268007   Iteration 82 of 100, tot loss = 5.059628475003127, l1: 0.00010912299716792626, l2: 0.000396839852683337   Iteration 83 of 100, tot loss = 5.066967786076558, l1: 0.00010927238962535903, l2: 0.000397424391016409   Iteration 84 of 100, tot loss = 5.038307950610206, l1: 0.0001087890081007139, l2: 0.00039504178875220185   Iteration 85 of 100, tot loss = 5.027436559340533, l1: 0.00010860682245145333, l2: 0.00039413683544433514   Iteration 86 of 100, tot loss = 5.0397865827693495, l1: 0.0001085028598449859, l2: 0.0003954758009735257   Iteration 87 of 100, tot loss = 5.024731937496141, l1: 0.00010829194292162503, l2: 0.00039418125354508257   Iteration 88 of 100, tot loss = 5.036349898034876, l1: 0.00010864370245723298, l2: 0.00039499129029123156   Iteration 89 of 100, tot loss = 5.006330637449629, l1: 0.0001082105195104652, l2: 0.0003924225471728073   Iteration 90 of 100, tot loss = 5.014660053782993, l1: 0.00010777678847565161, l2: 0.00039368921958763775   Iteration 91 of 100, tot loss = 5.00525607905545, l1: 0.00010786620903393038, l2: 0.00039265940146863585   Iteration 92 of 100, tot loss = 4.992115466491036, l1: 0.0001075529140586656, l2: 0.00039165863516087563   Iteration 93 of 100, tot loss = 4.99508790046938, l1: 0.00010774044928820654, l2: 0.0003917683433848984   Iteration 94 of 100, tot loss = 5.001448950868972, l1: 0.00010798345337729614, l2: 0.000392161444556513   Iteration 95 of 100, tot loss = 5.006427423577559, l1: 0.00010800982559694124, l2: 0.00039263291936980464   Iteration 96 of 100, tot loss = 5.028445765376091, l1: 0.0001084368991920807, l2: 0.00039440767992952413   Iteration 97 of 100, tot loss = 5.065020408827005, l1: 0.00010902193509499524, l2: 0.0003974801080947089   Iteration 98 of 100, tot loss = 5.054097774077435, l1: 0.00010850666204737072, l2: 0.00039690311767080115   Iteration 99 of 100, tot loss = 5.049784896349666, l1: 0.00010858200517079718, l2: 0.000396396486633081   Iteration 100 of 100, tot loss = 5.057571187019348, l1: 0.00010870184985833475, l2: 0.0003970552708778996
   End of epoch 1262; saving model... 

Epoch 1263 of 2000
   Iteration 1 of 100, tot loss = 6.728837966918945, l1: 0.00014752850984223187, l2: 0.000525355339050293   Iteration 2 of 100, tot loss = 5.227314114570618, l1: 0.0001144578491221182, l2: 0.00040827359771355987   Iteration 3 of 100, tot loss = 4.765679677327474, l1: 0.0001033252459213448, l2: 0.0003732427430804819   Iteration 4 of 100, tot loss = 4.837372064590454, l1: 9.814247459871694e-05, l2: 0.00038559475797228515   Iteration 5 of 100, tot loss = 4.466392898559571, l1: 9.390761842951179e-05, l2: 0.00035273168759886175   Iteration 6 of 100, tot loss = 4.673843781153361, l1: 9.930354281095788e-05, l2: 0.00036808084648024913   Iteration 7 of 100, tot loss = 4.931865010942731, l1: 0.0001050034022357847, l2: 0.00038818310505510975   Iteration 8 of 100, tot loss = 5.285316348075867, l1: 0.00011128650476166513, l2: 0.0004172451353952056   Iteration 9 of 100, tot loss = 5.586457146538629, l1: 0.00011671148563942147, l2: 0.00044193423269057856   Iteration 10 of 100, tot loss = 5.36806480884552, l1: 0.00011576127144508064, l2: 0.0004210452127153985   Iteration 11 of 100, tot loss = 5.215312784368342, l1: 0.0001145538090812889, l2: 0.00040697747317608446   Iteration 12 of 100, tot loss = 5.1732345422108965, l1: 0.00011347427547055607, l2: 0.0004038491812631643   Iteration 13 of 100, tot loss = 5.220793430621807, l1: 0.00011438206890418839, l2: 0.0004076972735884528   Iteration 14 of 100, tot loss = 5.024128266743252, l1: 0.00011175857850633162, l2: 0.0003906542478944175   Iteration 15 of 100, tot loss = 4.9890628496805824, l1: 0.00011206598380037273, l2: 0.00038684030005242676   Iteration 16 of 100, tot loss = 5.0081672966480255, l1: 0.00011177445685461862, l2: 0.0003890422731274157   Iteration 17 of 100, tot loss = 4.884927819756901, l1: 0.00010921638150809005, l2: 0.0003792764011236346   Iteration 18 of 100, tot loss = 4.914238466156854, l1: 0.00011068151878943253, l2: 0.0003807423304856962   Iteration 19 of 100, tot loss = 4.900949239730835, l1: 0.00011070511774098697, l2: 0.0003793898098305554   Iteration 20 of 100, tot loss = 4.8331461429595945, l1: 0.00010954549034067896, l2: 0.00037376912732725034   Iteration 21 of 100, tot loss = 4.874781199863979, l1: 0.00010948157640606431, l2: 0.00037799654680947283   Iteration 22 of 100, tot loss = 4.910805572162975, l1: 0.00010951895158293404, l2: 0.0003815616080167026   Iteration 23 of 100, tot loss = 4.857067688651707, l1: 0.00010875767886476672, l2: 0.0003769490912141602   Iteration 24 of 100, tot loss = 4.911263704299927, l1: 0.00011001055342300485, l2: 0.0003811158197398375   Iteration 25 of 100, tot loss = 4.828398084640503, l1: 0.0001092017293558456, l2: 0.00037363808136433365   Iteration 26 of 100, tot loss = 4.7827978409253635, l1: 0.00010846631854432277, l2: 0.0003698134678415954   Iteration 27 of 100, tot loss = 4.6714930887575505, l1: 0.0001061036223459437, l2: 0.00036104568883914636   Iteration 28 of 100, tot loss = 4.680420092173985, l1: 0.00010670859670166724, l2: 0.0003613334140806858   Iteration 29 of 100, tot loss = 4.721220246676741, l1: 0.00010788341349890006, l2: 0.0003642386130603223   Iteration 30 of 100, tot loss = 4.719786341985067, l1: 0.00010700708201814754, l2: 0.0003649715547605107   Iteration 31 of 100, tot loss = 4.728604255184051, l1: 0.00010619631258680695, l2: 0.0003666641151217083   Iteration 32 of 100, tot loss = 4.747646927833557, l1: 0.00010702563122322317, l2: 0.00036773906231246656   Iteration 33 of 100, tot loss = 4.805668411832867, l1: 0.000107850298236096, l2: 0.0003727165422980871   Iteration 34 of 100, tot loss = 4.767047075664296, l1: 0.00010693913431810764, l2: 0.00036976557235260876   Iteration 35 of 100, tot loss = 4.77333664894104, l1: 0.00010727365754844088, l2: 0.0003700600072209324   Iteration 36 of 100, tot loss = 4.760227991474999, l1: 0.00010694124891112249, l2: 0.0003690815493529145   Iteration 37 of 100, tot loss = 4.660171705323297, l1: 0.00010487596955499612, l2: 0.00036114120007983436   Iteration 38 of 100, tot loss = 4.6669237519565385, l1: 0.00010498962010866595, l2: 0.0003617027537793068   Iteration 39 of 100, tot loss = 4.6952513456344604, l1: 0.00010546100561324961, l2: 0.0003640641274008279   Iteration 40 of 100, tot loss = 4.684022429585457, l1: 0.00010502412487767288, l2: 0.0003633781165262917   Iteration 41 of 100, tot loss = 4.774903393373257, l1: 0.00010601925834368846, l2: 0.000371471078989732   Iteration 42 of 100, tot loss = 4.745779017607371, l1: 0.00010529094435609987, l2: 0.0003692869549489669   Iteration 43 of 100, tot loss = 4.779535540314608, l1: 0.00010611849769660229, l2: 0.00037183505476203333   Iteration 44 of 100, tot loss = 4.785021394491196, l1: 0.00010626921613782707, l2: 0.0003722329218527937   Iteration 45 of 100, tot loss = 4.79349958896637, l1: 0.0001065787895640824, l2: 0.0003727711669246977   Iteration 46 of 100, tot loss = 4.795105470263439, l1: 0.0001063254432032725, l2: 0.0003731851019952244   Iteration 47 of 100, tot loss = 4.7721225226179085, l1: 0.00010644804608594487, l2: 0.0003707642042192016   Iteration 48 of 100, tot loss = 4.728584460914135, l1: 0.00010597131654321856, l2: 0.00036688712740821455   Iteration 49 of 100, tot loss = 4.761103846588913, l1: 0.00010655546147787317, l2: 0.0003695549201030208   Iteration 50 of 100, tot loss = 4.766653568744659, l1: 0.00010653579789504875, l2: 0.0003701295563951135   Iteration 51 of 100, tot loss = 4.818768625165902, l1: 0.00010726696906631867, l2: 0.0003746098909508802   Iteration 52 of 100, tot loss = 4.819142619004617, l1: 0.00010737140144975946, l2: 0.00037454285820086417   Iteration 53 of 100, tot loss = 4.792548028927929, l1: 0.00010687721922569733, l2: 0.00037237758120029884   Iteration 54 of 100, tot loss = 4.752175633554105, l1: 0.00010640692784767857, l2: 0.0003688106331133491   Iteration 55 of 100, tot loss = 4.816378266161139, l1: 0.00010729797855178317, l2: 0.00037433984610040416   Iteration 56 of 100, tot loss = 4.816179745963642, l1: 0.00010698415573122995, l2: 0.000374633817045833   Iteration 57 of 100, tot loss = 4.805003787341871, l1: 0.00010665232120868096, l2: 0.00037384805603467514   Iteration 58 of 100, tot loss = 4.7919881939888, l1: 0.00010655201618521523, l2: 0.00037264680175777077   Iteration 59 of 100, tot loss = 4.7941534337350875, l1: 0.00010687842827252807, l2: 0.000372536913777891   Iteration 60 of 100, tot loss = 4.7805436789989475, l1: 0.00010667490939037331, l2: 0.0003713794569193851   Iteration 61 of 100, tot loss = 4.752585143339439, l1: 0.00010617349179200141, l2: 0.0003690850209124142   Iteration 62 of 100, tot loss = 4.709868871396588, l1: 0.00010529388354400233, l2: 0.0003656930022125673   Iteration 63 of 100, tot loss = 4.668424888262673, l1: 0.00010454361489835373, l2: 0.00036229887267663364   Iteration 64 of 100, tot loss = 4.655747098848224, l1: 0.0001046330921212757, l2: 0.0003609416162362322   Iteration 65 of 100, tot loss = 4.6380280843147865, l1: 0.000104452617289588, l2: 0.00035935018990690317   Iteration 66 of 100, tot loss = 4.597391704718272, l1: 0.00010369257297222221, l2: 0.0003560465962491986   Iteration 67 of 100, tot loss = 4.63692119762079, l1: 0.00010398891765308053, l2: 0.00035970320068508276   Iteration 68 of 100, tot loss = 4.618857823750552, l1: 0.00010323141272879987, l2: 0.00035865436845650784   Iteration 69 of 100, tot loss = 4.656045504238294, l1: 0.00010359956549887167, l2: 0.00036200498436591113   Iteration 70 of 100, tot loss = 4.657820979186467, l1: 0.00010386267276771833, l2: 0.0003619194242803912   Iteration 71 of 100, tot loss = 4.663208066577643, l1: 0.00010414982970244855, l2: 0.0003621709761767268   Iteration 72 of 100, tot loss = 4.676032309730847, l1: 0.00010401467923454927, l2: 0.0003635885506001715   Iteration 73 of 100, tot loss = 4.664572462643663, l1: 0.00010408789962203216, l2: 0.00036236934550425474   Iteration 74 of 100, tot loss = 4.650741449884467, l1: 0.000103970469501439, l2: 0.0003611036742301154   Iteration 75 of 100, tot loss = 4.641074500083923, l1: 0.00010375870299564364, l2: 0.0003603487457924833   Iteration 76 of 100, tot loss = 4.641226663401253, l1: 0.00010366297431717517, l2: 0.0003604596905030408   Iteration 77 of 100, tot loss = 4.651140716168787, l1: 0.00010361452969645344, l2: 0.00036149954063479197   Iteration 78 of 100, tot loss = 4.660876734134479, l1: 0.00010365818763183107, l2: 0.0003624294838981512   Iteration 79 of 100, tot loss = 4.6703751313535475, l1: 0.00010388192260760189, l2: 0.00036315558864322454   Iteration 80 of 100, tot loss = 4.665636946260929, l1: 0.00010396433604000776, l2: 0.0003625993569585262   Iteration 81 of 100, tot loss = 4.674819665190614, l1: 0.00010404714007170607, l2: 0.0003634348247964855   Iteration 82 of 100, tot loss = 4.667763938264149, l1: 0.00010364595689724211, l2: 0.0003631304354668118   Iteration 83 of 100, tot loss = 4.63701582385833, l1: 0.000103242797764784, l2: 0.0003604587831629822   Iteration 84 of 100, tot loss = 4.634240001440048, l1: 0.00010317797850369797, l2: 0.00036024602013640106   Iteration 85 of 100, tot loss = 4.658333906005411, l1: 0.00010342528938267873, l2: 0.0003624080995316891   Iteration 86 of 100, tot loss = 4.66085402771484, l1: 0.0001035210467772802, l2: 0.00036256435445000874   Iteration 87 of 100, tot loss = 4.64695995840533, l1: 0.00010345722857264576, l2: 0.0003612387655088785   Iteration 88 of 100, tot loss = 4.63144785572182, l1: 0.0001032733849487241, l2: 0.00035987139877265275   Iteration 89 of 100, tot loss = 4.642838841073968, l1: 0.0001037033881617349, l2: 0.00036058049394586906   Iteration 90 of 100, tot loss = 4.639290691746606, l1: 0.00010361969200150472, l2: 0.0003603093751331067   Iteration 91 of 100, tot loss = 4.676593768727648, l1: 0.00010407760223293175, l2: 0.0003635817721350487   Iteration 92 of 100, tot loss = 4.660826329303824, l1: 0.00010357379047174776, l2: 0.0003625088399491253   Iteration 93 of 100, tot loss = 4.664589342250619, l1: 0.00010377716730594115, l2: 0.00036268176430190405   Iteration 94 of 100, tot loss = 4.652905741904644, l1: 0.0001037658634657496, l2: 0.0003615247082284731   Iteration 95 of 100, tot loss = 4.641272583760713, l1: 0.00010355616257967133, l2: 0.0003605710932261948   Iteration 96 of 100, tot loss = 4.626523868491252, l1: 0.00010344101648721941, l2: 0.0003592113678981453   Iteration 97 of 100, tot loss = 4.641729655954027, l1: 0.00010381601521320468, l2: 0.00036035694788598947   Iteration 98 of 100, tot loss = 4.646043866264577, l1: 0.00010382183846763374, l2: 0.00036078254583562553   Iteration 99 of 100, tot loss = 4.6357153052031395, l1: 0.00010370736392838127, l2: 0.00035986416421082777   Iteration 100 of 100, tot loss = 4.6381116807460785, l1: 0.00010375603254942689, l2: 0.0003600551332056057
   End of epoch 1263; saving model... 

Epoch 1264 of 2000
   Iteration 1 of 100, tot loss = 3.6216084957122803, l1: 7.629224273841828e-05, l2: 0.0002858686202671379   Iteration 2 of 100, tot loss = 4.682867407798767, l1: 9.628566840547137e-05, l2: 0.00037200108636170626   Iteration 3 of 100, tot loss = 4.463828166325887, l1: 9.584531896204378e-05, l2: 0.0003505374964637061   Iteration 4 of 100, tot loss = 5.663075268268585, l1: 0.00011825622459582519, l2: 0.00044805130164604634   Iteration 5 of 100, tot loss = 5.4054121494293215, l1: 0.00011086491285823285, l2: 0.0004296762985177338   Iteration 6 of 100, tot loss = 5.267066518465678, l1: 0.00010930694406852126, l2: 0.0004173997052324315   Iteration 7 of 100, tot loss = 5.390468086515154, l1: 0.00011429017467889935, l2: 0.00042475662693115216   Iteration 8 of 100, tot loss = 5.162417262792587, l1: 0.00010941272557829507, l2: 0.0004068289963470306   Iteration 9 of 100, tot loss = 5.247737646102905, l1: 0.0001100377355922117, l2: 0.00041473602565626305   Iteration 10 of 100, tot loss = 5.390294480323791, l1: 0.00011223503170185723, l2: 0.00042679440812207756   Iteration 11 of 100, tot loss = 5.5813585844906894, l1: 0.00011512029066745362, l2: 0.000443015559788116   Iteration 12 of 100, tot loss = 5.425903876622518, l1: 0.00011096078681778938, l2: 0.000431629591427433   Iteration 13 of 100, tot loss = 5.2756096583146315, l1: 0.00010980052600363985, l2: 0.00041776043228590145   Iteration 14 of 100, tot loss = 5.065779668944223, l1: 0.00010623054160013063, l2: 0.00040034741791064983   Iteration 15 of 100, tot loss = 5.124166472752889, l1: 0.00010614052807795815, l2: 0.0004062761116074398   Iteration 16 of 100, tot loss = 5.128907844424248, l1: 0.00010609312971610052, l2: 0.0004067976487931446   Iteration 17 of 100, tot loss = 5.195944042766795, l1: 0.00010688260932939182, l2: 0.0004127117889070445   Iteration 18 of 100, tot loss = 5.239150298966302, l1: 0.00010680325613066088, l2: 0.000417111768911127   Iteration 19 of 100, tot loss = 5.249165095781025, l1: 0.00010703252356554578, l2: 0.0004178839804616904   Iteration 20 of 100, tot loss = 5.275004398822785, l1: 0.0001066698454451398, l2: 0.00042083059015567413   Iteration 21 of 100, tot loss = 5.18801076071603, l1: 0.00010627387503821713, l2: 0.00041252719668028435   Iteration 22 of 100, tot loss = 5.202721118927002, l1: 0.00010605851920527957, l2: 0.00041421358830782333   Iteration 23 of 100, tot loss = 5.120939202930616, l1: 0.00010483451146485649, l2: 0.00040725940577787065   Iteration 24 of 100, tot loss = 5.034128844738007, l1: 0.0001038636478369881, l2: 0.00039954923280068516   Iteration 25 of 100, tot loss = 4.9123523235321045, l1: 0.00010122952138772235, l2: 0.0003900057071587071   Iteration 26 of 100, tot loss = 5.025749325752258, l1: 0.00010247686581211523, l2: 0.0004000980647665878   Iteration 27 of 100, tot loss = 5.134179689266063, l1: 0.00010432562487915641, l2: 0.0004090923418213303   Iteration 28 of 100, tot loss = 5.218728840351105, l1: 0.00010566470284954579, l2: 0.000416208178648958   Iteration 29 of 100, tot loss = 5.20689241639499, l1: 0.0001055884214142209, l2: 0.00041510081791784614   Iteration 30 of 100, tot loss = 5.1329869349797566, l1: 0.00010527432483892577, l2: 0.00040802436657637976   Iteration 31 of 100, tot loss = 5.197313639425462, l1: 0.00010619181299379335, l2: 0.00041353954950482737   Iteration 32 of 100, tot loss = 5.142792709171772, l1: 0.0001054194717653445, l2: 0.00040885979751692503   Iteration 33 of 100, tot loss = 5.190347707632816, l1: 0.00010620698881349668, l2: 0.00041282778053727907   Iteration 34 of 100, tot loss = 5.204267873483546, l1: 0.00010668919875409782, l2: 0.0004137375872232951   Iteration 35 of 100, tot loss = 5.238304390226092, l1: 0.0001074170577339828, l2: 0.0004164133790514565   Iteration 36 of 100, tot loss = 5.230979727374183, l1: 0.00010714693235058803, l2: 0.0004159510382224754   Iteration 37 of 100, tot loss = 5.225534058905937, l1: 0.00010737123748923123, l2: 0.00041518216548374275   Iteration 38 of 100, tot loss = 5.1864758478967765, l1: 0.00010685210183605944, l2: 0.00041179548019212425   Iteration 39 of 100, tot loss = 5.200981830939268, l1: 0.00010723084247169587, l2: 0.000412867337730952   Iteration 40 of 100, tot loss = 5.169142919778824, l1: 0.00010656482609192608, l2: 0.00041034946298168506   Iteration 41 of 100, tot loss = 5.164019939376087, l1: 0.00010654451371675993, l2: 0.00040985747679170764   Iteration 42 of 100, tot loss = 5.1805265460695535, l1: 0.00010692499598094618, l2: 0.0004111276554924968   Iteration 43 of 100, tot loss = 5.170315215753955, l1: 0.00010736814519352561, l2: 0.00040966337296277893   Iteration 44 of 100, tot loss = 5.155890090899034, l1: 0.0001071202681156468, l2: 0.00040846873724314554   Iteration 45 of 100, tot loss = 5.138880268732707, l1: 0.00010714747234790896, l2: 0.0004067405505338684   Iteration 46 of 100, tot loss = 5.160672317380491, l1: 0.00010729874639250541, l2: 0.0004087684808283761   Iteration 47 of 100, tot loss = 5.107482681883142, l1: 0.00010603712151815342, l2: 0.0004047111421080425   Iteration 48 of 100, tot loss = 5.09638028840224, l1: 0.00010561412492885817, l2: 0.0004040238994396835   Iteration 49 of 100, tot loss = 5.106217593562846, l1: 0.00010557765266157174, l2: 0.00040504410245208714   Iteration 50 of 100, tot loss = 5.139841771125793, l1: 0.00010642516463121865, l2: 0.00040755900903604924   Iteration 51 of 100, tot loss = 5.1429333453084904, l1: 0.0001065650025089317, l2: 0.0004077283277486761   Iteration 52 of 100, tot loss = 5.107402861118317, l1: 0.00010610995776425206, l2: 0.0004046303242480812   Iteration 53 of 100, tot loss = 5.093234768453634, l1: 0.00010608064299306552, l2: 0.00040324282984003284   Iteration 54 of 100, tot loss = 5.099197745323181, l1: 0.00010650313675729126, l2: 0.0004034166340716183   Iteration 55 of 100, tot loss = 5.073930527947166, l1: 0.00010626151111368513, l2: 0.000401131538356739   Iteration 56 of 100, tot loss = 5.040083382810865, l1: 0.00010528818880369986, l2: 0.0003987201463522589   Iteration 57 of 100, tot loss = 5.053193703032377, l1: 0.00010551182308597817, l2: 0.0003998075441351128   Iteration 58 of 100, tot loss = 5.0683257250950255, l1: 0.0001054315837571831, l2: 0.0004014009855695647   Iteration 59 of 100, tot loss = 5.040997670868696, l1: 0.00010485439023626494, l2: 0.0003992453734469363   Iteration 60 of 100, tot loss = 5.03797793785731, l1: 0.00010469986312576415, l2: 0.0003990979273415481   Iteration 61 of 100, tot loss = 5.0327776807253475, l1: 0.00010450075790343317, l2: 0.00039877700686195226   Iteration 62 of 100, tot loss = 4.999306394207862, l1: 0.00010383832587620183, l2: 0.00039609231035067367   Iteration 63 of 100, tot loss = 5.044487400660439, l1: 0.00010420919473593433, l2: 0.00040023954236872555   Iteration 64 of 100, tot loss = 5.055055104196072, l1: 0.00010467175025041797, l2: 0.00040083375756694295   Iteration 65 of 100, tot loss = 5.037773664181049, l1: 0.00010451446623147394, l2: 0.00039926289800160493   Iteration 66 of 100, tot loss = 5.024046771454088, l1: 0.00010436543545272963, l2: 0.00039803923948167005   Iteration 67 of 100, tot loss = 5.038685859139286, l1: 0.00010477121645296271, l2: 0.00039909736738964193   Iteration 68 of 100, tot loss = 5.080633068785948, l1: 0.00010555474952578901, l2: 0.00040250855497159883   Iteration 69 of 100, tot loss = 5.073440935300744, l1: 0.00010506394523806681, l2: 0.0004022801461531713   Iteration 70 of 100, tot loss = 5.053206941059657, l1: 0.0001046964042221329, l2: 0.00040062428763901285   Iteration 71 of 100, tot loss = 5.050309214793461, l1: 0.00010469445198754662, l2: 0.00040033646686245283   Iteration 72 of 100, tot loss = 5.042891634835137, l1: 0.00010429287582761997, l2: 0.0003999962854221748   Iteration 73 of 100, tot loss = 5.010528139872093, l1: 0.00010354948324376194, l2: 0.0003975033285638496   Iteration 74 of 100, tot loss = 5.00549754580936, l1: 0.00010339746945278052, l2: 0.0003971522827349553   Iteration 75 of 100, tot loss = 5.009688981374105, l1: 0.00010335951330489479, l2: 0.00039760938263498245   Iteration 76 of 100, tot loss = 4.977427093606246, l1: 0.00010284360584927, l2: 0.0003948991013059736   Iteration 77 of 100, tot loss = 4.970183403461011, l1: 0.00010279763431187005, l2: 0.00039422070394366885   Iteration 78 of 100, tot loss = 4.985946111190013, l1: 0.00010327228240688176, l2: 0.00039532232711700577   Iteration 79 of 100, tot loss = 4.977924515929403, l1: 0.00010277636359700685, l2: 0.00039501608639684377   Iteration 80 of 100, tot loss = 4.964543715119362, l1: 0.00010265943824379064, l2: 0.00039379493173328226   Iteration 81 of 100, tot loss = 4.961336756929939, l1: 0.00010267509379097904, l2: 0.0003934585802647986   Iteration 82 of 100, tot loss = 4.9372637388182845, l1: 0.00010201119868902535, l2: 0.00039171517355276683   Iteration 83 of 100, tot loss = 4.9326796072075165, l1: 0.00010188209108239691, l2: 0.0003913858683123422   Iteration 84 of 100, tot loss = 4.936211086454845, l1: 0.00010197481017149541, l2: 0.0003916462973436518   Iteration 85 of 100, tot loss = 4.909976788128124, l1: 0.00010163713587269954, l2: 0.00038936054179965353   Iteration 86 of 100, tot loss = 4.907000838324081, l1: 0.00010146862879019165, l2: 0.00038923145434954527   Iteration 87 of 100, tot loss = 4.927456036381338, l1: 0.00010187608875995288, l2: 0.00039086951462728584   Iteration 88 of 100, tot loss = 4.954983936114744, l1: 0.00010202381566738372, l2: 0.0003934745772395135   Iteration 89 of 100, tot loss = 4.952986146626848, l1: 0.00010196978543176096, l2: 0.00039332882835388475   Iteration 90 of 100, tot loss = 4.977633452415466, l1: 0.00010201862735104644, l2: 0.0003957447173888795   Iteration 91 of 100, tot loss = 4.963180130654639, l1: 0.00010180420772835183, l2: 0.00039451380479686824   Iteration 92 of 100, tot loss = 4.966577973054803, l1: 0.00010200388469852244, l2: 0.0003946539121819154   Iteration 93 of 100, tot loss = 4.935871729286768, l1: 0.00010144455901814014, l2: 0.00039214261348098915   Iteration 94 of 100, tot loss = 4.95386343813957, l1: 0.00010169354553657653, l2: 0.0003936927981719256   Iteration 95 of 100, tot loss = 4.975634815818385, l1: 0.00010232478292125866, l2: 0.00039523869881553475   Iteration 96 of 100, tot loss = 4.97261702020963, l1: 0.00010238169409149123, l2: 0.0003948800079645783   Iteration 97 of 100, tot loss = 4.973146104321037, l1: 0.00010240696350048948, l2: 0.0003949076467740935   Iteration 98 of 100, tot loss = 4.9938070676764665, l1: 0.00010292910181825781, l2: 0.0003964516045573186   Iteration 99 of 100, tot loss = 4.967243994125212, l1: 0.00010270846957215015, l2: 0.00039401592941357366   Iteration 100 of 100, tot loss = 4.970987229347229, l1: 0.00010306222528015496, l2: 0.00039403649730957113
   End of epoch 1264; saving model... 

Epoch 1265 of 2000
   Iteration 1 of 100, tot loss = 3.9384524822235107, l1: 5.7875386119121686e-05, l2: 0.0003359698748681694   Iteration 2 of 100, tot loss = 4.01024067401886, l1: 7.963355346873868e-05, l2: 0.0003213905147276819   Iteration 3 of 100, tot loss = 3.7214688460032144, l1: 8.230077825525466e-05, l2: 0.0002898461098084226   Iteration 4 of 100, tot loss = 5.321544587612152, l1: 0.00010498422943783225, l2: 0.00042717024552985094   Iteration 5 of 100, tot loss = 5.261639165878296, l1: 0.00010411785406176932, l2: 0.0004220460745273158   Iteration 6 of 100, tot loss = 5.506681005160014, l1: 0.00010993106419239969, l2: 0.0004407370458163011   Iteration 7 of 100, tot loss = 5.587744338171823, l1: 0.00011237856285463619, l2: 0.0004463958799273574   Iteration 8 of 100, tot loss = 5.667487531900406, l1: 0.00011414300433898461, l2: 0.0004526057582552312   Iteration 9 of 100, tot loss = 5.874853054682414, l1: 0.00011945400304587868, l2: 0.0004680313140852377   Iteration 10 of 100, tot loss = 5.631312870979309, l1: 0.00011783112677221652, l2: 0.00044530017039505764   Iteration 11 of 100, tot loss = 5.666689894416115, l1: 0.00011963556111864322, l2: 0.00044703344015446913   Iteration 12 of 100, tot loss = 5.689291894435883, l1: 0.0001212722997176267, l2: 0.00044765689987495233   Iteration 13 of 100, tot loss = 5.622636850063618, l1: 0.0001205602678125545, l2: 0.00044170342936502915   Iteration 14 of 100, tot loss = 5.531690342085702, l1: 0.00011864205823388017, l2: 0.00043452698550286835   Iteration 15 of 100, tot loss = 5.339900922775269, l1: 0.0001150383136215775, l2: 0.00041895178825749707   Iteration 16 of 100, tot loss = 5.206124365329742, l1: 0.0001113191656259005, l2: 0.0004092932804269367   Iteration 17 of 100, tot loss = 5.139798585106345, l1: 0.00011037905770696371, l2: 0.00040360080910271365   Iteration 18 of 100, tot loss = 5.079548464881049, l1: 0.00010767664005268468, l2: 0.0004002782152383588   Iteration 19 of 100, tot loss = 5.090740053277266, l1: 0.00010756373414647227, l2: 0.0004015102787984927   Iteration 20 of 100, tot loss = 5.010761857032776, l1: 0.00010671550553524867, l2: 0.00039436068691429683   Iteration 21 of 100, tot loss = 5.070694787161691, l1: 0.00010706243483582512, l2: 0.00040000704895993254   Iteration 22 of 100, tot loss = 5.0104066133499146, l1: 0.00010646484390070492, l2: 0.00039457582236288795   Iteration 23 of 100, tot loss = 5.093093384867129, l1: 0.0001075184548466021, l2: 0.0004017908871426935   Iteration 24 of 100, tot loss = 4.980579247077306, l1: 0.0001061365367907759, l2: 0.000391921391080056   Iteration 25 of 100, tot loss = 4.962967958450317, l1: 0.00010683228756533936, l2: 0.0003894645115360618   Iteration 26 of 100, tot loss = 4.96553832751054, l1: 0.00010807851979804512, l2: 0.00038847531518863083   Iteration 27 of 100, tot loss = 4.969688212430036, l1: 0.00010876034044755485, l2: 0.0003882084835182737   Iteration 28 of 100, tot loss = 4.936273055417197, l1: 0.00010779107443730547, l2: 0.0003858362345324297   Iteration 29 of 100, tot loss = 4.922107063490769, l1: 0.00010798443314531047, l2: 0.00038422627579260234   Iteration 30 of 100, tot loss = 4.9701412439346315, l1: 0.00010897654986668689, l2: 0.0003880375773102666   Iteration 31 of 100, tot loss = 5.019395712883242, l1: 0.00010929596065844018, l2: 0.00039264361370324846   Iteration 32 of 100, tot loss = 4.992862991988659, l1: 0.00010924058528871683, l2: 0.00039004571681289235   Iteration 33 of 100, tot loss = 5.05229300441164, l1: 0.00011060575563919194, l2: 0.000394623547573715   Iteration 34 of 100, tot loss = 5.025481609737172, l1: 0.0001106205112163854, l2: 0.00039192765218424886   Iteration 35 of 100, tot loss = 4.987563351222447, l1: 0.00010964792205153831, l2: 0.00038910841519412186   Iteration 36 of 100, tot loss = 4.989802281061809, l1: 0.00010959088730386511, l2: 0.0003893893436826248   Iteration 37 of 100, tot loss = 5.010241959546064, l1: 0.00011006309167085517, l2: 0.0003909611078671406   Iteration 38 of 100, tot loss = 5.003021177492644, l1: 0.00010944368133143718, l2: 0.0003908584392200665   Iteration 39 of 100, tot loss = 5.039563827025584, l1: 0.00011016551434295252, l2: 0.00039379087124521343   Iteration 40 of 100, tot loss = 4.982547372579575, l1: 0.00010890921021200484, l2: 0.00038934552976570557   Iteration 41 of 100, tot loss = 4.958116566262594, l1: 0.00010875848486803745, l2: 0.0003870531739865816   Iteration 42 of 100, tot loss = 4.929834462347484, l1: 0.00010880816574042131, l2: 0.0003841752827394798   Iteration 43 of 100, tot loss = 4.9505442297735875, l1: 0.00010953328929581614, l2: 0.0003855211364489785   Iteration 44 of 100, tot loss = 4.943785391070626, l1: 0.000109474105722091, l2: 0.00038490443594954826   Iteration 45 of 100, tot loss = 4.936714728673299, l1: 0.00010944651148747653, l2: 0.0003842249632321505   Iteration 46 of 100, tot loss = 4.961062685303066, l1: 0.000109427587196241, l2: 0.00038667868363478664   Iteration 47 of 100, tot loss = 4.963633075673529, l1: 0.00010857517840935194, l2: 0.0003877881319296764   Iteration 48 of 100, tot loss = 4.91102501253287, l1: 0.00010784211766197889, l2: 0.00038326038611558033   Iteration 49 of 100, tot loss = 4.910445081944368, l1: 0.00010783904738728983, l2: 0.00038320546327111295   Iteration 50 of 100, tot loss = 4.934129376411438, l1: 0.00010792348854010925, l2: 0.0003854894518735819   Iteration 51 of 100, tot loss = 4.966626639459648, l1: 0.00010808353402478365, l2: 0.00038857913237092467   Iteration 52 of 100, tot loss = 4.951577493777642, l1: 0.00010791676644517932, l2: 0.0003872409850793282   Iteration 53 of 100, tot loss = 4.998953220979223, l1: 0.0001090792322669723, l2: 0.0003908160914126608   Iteration 54 of 100, tot loss = 4.996377958191766, l1: 0.00010951182654631945, l2: 0.0003901259713327616   Iteration 55 of 100, tot loss = 5.0264124740253795, l1: 0.00011005044652847573, l2: 0.0003925908031471243   Iteration 56 of 100, tot loss = 4.975160181522369, l1: 0.00010908177409874042, l2: 0.00038843424616581094   Iteration 57 of 100, tot loss = 4.984813690185547, l1: 0.0001093814885519494, l2: 0.00038909988187809   Iteration 58 of 100, tot loss = 4.976641638525601, l1: 0.000108944280108172, l2: 0.00038871988513635407   Iteration 59 of 100, tot loss = 4.9706115237737105, l1: 0.00010861408138564803, l2: 0.00038844707197020366   Iteration 60 of 100, tot loss = 4.95980220635732, l1: 0.00010816179601533805, l2: 0.00038781842595199124   Iteration 61 of 100, tot loss = 4.926873394700348, l1: 0.00010753223626229331, l2: 0.0003851551042972743   Iteration 62 of 100, tot loss = 4.950880112186555, l1: 0.00010830900036155307, l2: 0.0003867790123143594   Iteration 63 of 100, tot loss = 4.92130316249908, l1: 0.00010771231678691471, l2: 0.0003844180009475658   Iteration 64 of 100, tot loss = 4.917083818465471, l1: 0.00010783603568143008, l2: 0.00038387234781112056   Iteration 65 of 100, tot loss = 4.912806375210102, l1: 0.00010785863056438617, l2: 0.00038342200882303027   Iteration 66 of 100, tot loss = 4.884256034186392, l1: 0.0001072531042816329, l2: 0.0003811725008276743   Iteration 67 of 100, tot loss = 4.876454057978161, l1: 0.00010693802584289336, l2: 0.0003807073814790946   Iteration 68 of 100, tot loss = 4.867918277488036, l1: 0.00010677633134251023, l2: 0.0003800154978200547   Iteration 69 of 100, tot loss = 4.886620290037515, l1: 0.00010751200012316036, l2: 0.0003811500304753123   Iteration 70 of 100, tot loss = 4.9164798021316525, l1: 0.00010751499711269779, l2: 0.00038413298461819067   Iteration 71 of 100, tot loss = 4.935218337556006, l1: 0.0001079337302477322, l2: 0.00038558810569746025   Iteration 72 of 100, tot loss = 4.90952182147238, l1: 0.00010734414799016021, l2: 0.00038360803632207937   Iteration 73 of 100, tot loss = 4.900813543633239, l1: 0.00010730416866016179, l2: 0.0003827771878393035   Iteration 74 of 100, tot loss = 4.895071690146987, l1: 0.00010708957101136598, l2: 0.00038241760017086024   Iteration 75 of 100, tot loss = 4.960707960128784, l1: 0.00010812059820940098, l2: 0.00038795020004423955   Iteration 76 of 100, tot loss = 4.953986923945577, l1: 0.00010803007429977267, l2: 0.00038736861997198214   Iteration 77 of 100, tot loss = 4.9343627211335415, l1: 0.00010802731450492903, l2: 0.0003854089594966212   Iteration 78 of 100, tot loss = 4.910094499588013, l1: 0.00010761773871938483, l2: 0.00038339171311609115   Iteration 79 of 100, tot loss = 4.895855595793905, l1: 0.00010713920447772039, l2: 0.00038244635698568265   Iteration 80 of 100, tot loss = 4.933619040250778, l1: 0.00010756480360214482, l2: 0.00038579710180783875   Iteration 81 of 100, tot loss = 4.901883884712502, l1: 0.00010698793397032092, l2: 0.0003832004559053867   Iteration 82 of 100, tot loss = 4.914020108013618, l1: 0.00010708888001362339, l2: 0.00038431313246624864   Iteration 83 of 100, tot loss = 4.921855518616826, l1: 0.00010711125870296978, l2: 0.00038507429452287593   Iteration 84 of 100, tot loss = 4.914716067768278, l1: 0.00010689428624270174, l2: 0.0003845773220139866   Iteration 85 of 100, tot loss = 4.9364382575539985, l1: 0.00010725265310611576, l2: 0.0003863911740207935   Iteration 86 of 100, tot loss = 4.93898697786553, l1: 0.00010756344712463879, l2: 0.00038633525214526194   Iteration 87 of 100, tot loss = 4.925129405383406, l1: 0.00010761411441757676, l2: 0.0003848988275129305   Iteration 88 of 100, tot loss = 4.922376749190417, l1: 0.00010741125251198272, l2: 0.00038482642379345964   Iteration 89 of 100, tot loss = 4.897194827540537, l1: 0.00010697190017698416, l2: 0.00038274758403173786   Iteration 90 of 100, tot loss = 4.9117117272482975, l1: 0.00010690926339723066, l2: 0.00038426191102997916   Iteration 91 of 100, tot loss = 4.887756693494189, l1: 0.00010629539023144529, l2: 0.00038248028095739964   Iteration 92 of 100, tot loss = 4.885753693787948, l1: 0.00010612948897133997, l2: 0.0003824458823225501   Iteration 93 of 100, tot loss = 4.866540321739771, l1: 0.00010582703281968823, l2: 0.00038082700116955423   Iteration 94 of 100, tot loss = 4.86041992015027, l1: 0.00010536834285650253, l2: 0.00038067365088128544   Iteration 95 of 100, tot loss = 4.880337411478946, l1: 0.00010554886007172938, l2: 0.0003824848830271022   Iteration 96 of 100, tot loss = 4.902376227080822, l1: 0.00010601536515271921, l2: 0.00038422225967830553   Iteration 97 of 100, tot loss = 4.898317393568373, l1: 0.00010572395128583219, l2: 0.0003841077900172413   Iteration 98 of 100, tot loss = 4.9077326302625695, l1: 0.00010559571823324979, l2: 0.00038517754669277455   Iteration 99 of 100, tot loss = 4.884755141807325, l1: 0.00010535981030011728, l2: 0.0003831157057470821   Iteration 100 of 100, tot loss = 4.855938277244568, l1: 0.00010486220002349, l2: 0.0003807316297024954
   End of epoch 1265; saving model... 

Epoch 1266 of 2000
   Iteration 1 of 100, tot loss = 4.679988384246826, l1: 0.00012135567521909252, l2: 0.00034664315171539783   Iteration 2 of 100, tot loss = 3.8932079076766968, l1: 0.00010999062942573801, l2: 0.0002793301464407705   Iteration 3 of 100, tot loss = 4.641489267349243, l1: 0.0001145011920016259, l2: 0.0003496477293083444   Iteration 4 of 100, tot loss = 4.832796633243561, l1: 0.0001131394074036507, l2: 0.00037014025656390004   Iteration 5 of 100, tot loss = 5.065096807479859, l1: 0.00011244444758631288, l2: 0.00039406523283105346   Iteration 6 of 100, tot loss = 5.090733488400777, l1: 0.00011241658891473587, l2: 0.0003966567577056897   Iteration 7 of 100, tot loss = 5.210091216223581, l1: 0.00011257811274845153, l2: 0.0004084310099382752   Iteration 8 of 100, tot loss = 5.061983793973923, l1: 0.00011288301357126329, l2: 0.00039331536572717596   Iteration 9 of 100, tot loss = 4.995284689797296, l1: 0.0001139338597163765, l2: 0.0003855946108362534   Iteration 10 of 100, tot loss = 4.952293705940247, l1: 0.00011197369458386675, l2: 0.0003832556787529029   Iteration 11 of 100, tot loss = 4.762899853966453, l1: 0.00010898229877718471, l2: 0.00036730768889273435   Iteration 12 of 100, tot loss = 4.767648994922638, l1: 0.00010917894481584274, l2: 0.000367585955245886   Iteration 13 of 100, tot loss = 4.85916333932143, l1: 0.0001112911038664886, l2: 0.00037462522875732527   Iteration 14 of 100, tot loss = 4.925600681986127, l1: 0.00011315565487685879, l2: 0.00037940441065334847   Iteration 15 of 100, tot loss = 4.910072215398153, l1: 0.00011288248788332567, l2: 0.00037812473019585016   Iteration 16 of 100, tot loss = 4.822508618235588, l1: 0.00010979944045175216, l2: 0.0003724514190253103   Iteration 17 of 100, tot loss = 4.828657781376558, l1: 0.00010921919430770418, l2: 0.0003736465826959294   Iteration 18 of 100, tot loss = 4.731441060702006, l1: 0.00010837327782711427, l2: 0.0003647708266119783   Iteration 19 of 100, tot loss = 4.799253401003386, l1: 0.00011001068081608729, l2: 0.00036991465726475184   Iteration 20 of 100, tot loss = 4.7597452044487, l1: 0.00010859777139557992, l2: 0.00036737674672622235   Iteration 21 of 100, tot loss = 4.7474854446592785, l1: 0.00010857855453477463, l2: 0.0003661699884105474   Iteration 22 of 100, tot loss = 4.705309553579851, l1: 0.00010842609514260072, l2: 0.0003621048592983491   Iteration 23 of 100, tot loss = 4.782369831333989, l1: 0.00010848395828835909, l2: 0.0003697530242472725   Iteration 24 of 100, tot loss = 4.7392118374506635, l1: 0.00010835882646157795, l2: 0.00036556235621295247   Iteration 25 of 100, tot loss = 4.704279747009277, l1: 0.0001068398289498873, l2: 0.00036358814453706144   Iteration 26 of 100, tot loss = 4.904275454007662, l1: 0.00010913787851817548, l2: 0.00038128966787973273   Iteration 27 of 100, tot loss = 5.080418056911892, l1: 0.00011223241071328866, l2: 0.0003958093951007834   Iteration 28 of 100, tot loss = 5.082065463066101, l1: 0.00011192745166356741, l2: 0.0003962790942750871   Iteration 29 of 100, tot loss = 5.079539956717656, l1: 0.00011112010869165433, l2: 0.0003968338865449588   Iteration 30 of 100, tot loss = 5.045471477508545, l1: 0.00011047084917663597, l2: 0.0003940762981073931   Iteration 31 of 100, tot loss = 5.034424504926128, l1: 0.00010887943250465116, l2: 0.00039456301695486954   Iteration 32 of 100, tot loss = 4.925474688410759, l1: 0.00010695505920921278, l2: 0.00038559240874747047   Iteration 33 of 100, tot loss = 4.933537150874282, l1: 0.00010760331144521126, l2: 0.00038575040261176497   Iteration 34 of 100, tot loss = 4.97372628660763, l1: 0.00010789034679285524, l2: 0.00038948227986967303   Iteration 35 of 100, tot loss = 4.952628898620605, l1: 0.00010808748297027445, l2: 0.00038717540529822664   Iteration 36 of 100, tot loss = 4.907328685124715, l1: 0.00010720016184172386, l2: 0.00038353270550336066   Iteration 37 of 100, tot loss = 4.8908479922526595, l1: 0.00010762055053665126, l2: 0.000381464247963064   Iteration 38 of 100, tot loss = 4.922760875601518, l1: 0.00010890676946330227, l2: 0.0003833693174927152   Iteration 39 of 100, tot loss = 4.941698257739727, l1: 0.00010922594624273002, l2: 0.0003849438791616987   Iteration 40 of 100, tot loss = 4.949450051784515, l1: 0.00010929060772468802, l2: 0.00038565439681406135   Iteration 41 of 100, tot loss = 5.033192599691996, l1: 0.00011039027346513893, l2: 0.0003929289857141401   Iteration 42 of 100, tot loss = 4.9780196235293435, l1: 0.00010965228527008246, l2: 0.0003881496765340368   Iteration 43 of 100, tot loss = 4.982389139574628, l1: 0.00010954751362374355, l2: 0.0003886914001182155   Iteration 44 of 100, tot loss = 5.011954957788641, l1: 0.00011003727195202373, l2: 0.0003911582243745215   Iteration 45 of 100, tot loss = 4.973210165235732, l1: 0.0001094281403412525, l2: 0.00038789287662237054   Iteration 46 of 100, tot loss = 4.9520474931468135, l1: 0.00010884370709865597, l2: 0.000386361042847452   Iteration 47 of 100, tot loss = 4.932615036660052, l1: 0.00010843124851198034, l2: 0.00038483025607692595   Iteration 48 of 100, tot loss = 4.904489070177078, l1: 0.00010805385348551984, l2: 0.0003823950543543712   Iteration 49 of 100, tot loss = 4.926503775071125, l1: 0.00010847224235030993, l2: 0.00038417813555831657   Iteration 50 of 100, tot loss = 4.898179655075073, l1: 0.00010822115713381209, l2: 0.00038159680843818933   Iteration 51 of 100, tot loss = 4.867287953694661, l1: 0.00010766709444807002, l2: 0.00037906170073969694   Iteration 52 of 100, tot loss = 4.848071680619166, l1: 0.00010762587995626606, l2: 0.0003771812877787922   Iteration 53 of 100, tot loss = 4.786164805574237, l1: 0.00010625618473705747, l2: 0.0003723602955842408   Iteration 54 of 100, tot loss = 4.817195468478733, l1: 0.00010680988446067743, l2: 0.0003749096616013924   Iteration 55 of 100, tot loss = 4.828045810352672, l1: 0.00010725027400026606, l2: 0.0003755543067448095   Iteration 56 of 100, tot loss = 4.856578656605312, l1: 0.00010751596216453305, l2: 0.0003781419035086791   Iteration 57 of 100, tot loss = 4.840402741181223, l1: 0.00010760424029604907, l2: 0.00037643603412250643   Iteration 58 of 100, tot loss = 4.85173654967341, l1: 0.0001081125512177221, l2: 0.0003770611042925558   Iteration 59 of 100, tot loss = 4.845469171718015, l1: 0.00010776363724597997, l2: 0.0003767832801667421   Iteration 60 of 100, tot loss = 4.825250045458476, l1: 0.00010751742605255762, l2: 0.0003750075788654309   Iteration 61 of 100, tot loss = 4.860622632698934, l1: 0.00010823114208789298, l2: 0.0003778311216652195   Iteration 62 of 100, tot loss = 4.854840270934567, l1: 0.00010801377310042059, l2: 0.0003774702545929505   Iteration 63 of 100, tot loss = 4.844618282620869, l1: 0.00010763170679898135, l2: 0.00037683012176005704   Iteration 64 of 100, tot loss = 4.834485821425915, l1: 0.0001071587760179682, l2: 0.0003762898062404929   Iteration 65 of 100, tot loss = 4.87451177743765, l1: 0.00010748795085908988, l2: 0.00037996322666563524   Iteration 66 of 100, tot loss = 4.884496212005615, l1: 0.00010787214507052505, l2: 0.00038057747621099804   Iteration 67 of 100, tot loss = 4.860371258721423, l1: 0.00010718988485325044, l2: 0.0003788472411054214   Iteration 68 of 100, tot loss = 4.829654732171227, l1: 0.0001063490048823844, l2: 0.0003766164682019735   Iteration 69 of 100, tot loss = 4.843503761982572, l1: 0.00010670450417134249, l2: 0.00037764587181880125   Iteration 70 of 100, tot loss = 4.85713677746909, l1: 0.00010674921904865186, l2: 0.00037896445854650147   Iteration 71 of 100, tot loss = 4.8466108919869, l1: 0.00010649242150821907, l2: 0.00037816866732638617   Iteration 72 of 100, tot loss = 4.882298294040892, l1: 0.00010681836814304309, l2: 0.00038141146039076074   Iteration 73 of 100, tot loss = 4.88553340794289, l1: 0.00010659121795619634, l2: 0.00038196212201050723   Iteration 74 of 100, tot loss = 4.876382167274888, l1: 0.0001064466993026341, l2: 0.00038119151645947275   Iteration 75 of 100, tot loss = 4.865104290644328, l1: 0.00010617938062447744, l2: 0.0003803310474419656   Iteration 76 of 100, tot loss = 4.884750545024872, l1: 0.00010653131228441213, l2: 0.0003819437415107754   Iteration 77 of 100, tot loss = 4.91210355077471, l1: 0.0001068981290192422, l2: 0.00038431222596305555   Iteration 78 of 100, tot loss = 4.90350572573833, l1: 0.00010684158083817098, l2: 0.00038350899175827345   Iteration 79 of 100, tot loss = 4.921088885657395, l1: 0.00010723860138073811, l2: 0.0003848702867281081   Iteration 80 of 100, tot loss = 4.933475264906884, l1: 0.00010747636410997074, l2: 0.00038587116150665677   Iteration 81 of 100, tot loss = 4.942884448133869, l1: 0.00010753239536072978, l2: 0.00038675604868510644   Iteration 82 of 100, tot loss = 4.946618234238973, l1: 0.0001079098892660113, l2: 0.0003867519338133961   Iteration 83 of 100, tot loss = 4.932836885911873, l1: 0.00010761911199738115, l2: 0.0003856645763959702   Iteration 84 of 100, tot loss = 4.916149780863807, l1: 0.00010761827715690569, l2: 0.0003839967006829933   Iteration 85 of 100, tot loss = 4.904416446124806, l1: 0.00010742274783243534, l2: 0.000383018896493869   Iteration 86 of 100, tot loss = 4.913318986116454, l1: 0.0001076476664609292, l2: 0.00038368423236872416   Iteration 87 of 100, tot loss = 4.928638373298207, l1: 0.0001076306410026121, l2: 0.0003852331964867778   Iteration 88 of 100, tot loss = 4.935604401610115, l1: 0.0001076107903166055, l2: 0.0003859496501635559   Iteration 89 of 100, tot loss = 4.944477737619636, l1: 0.00010777868259681148, l2: 0.00038666909096534937   Iteration 90 of 100, tot loss = 4.96419297059377, l1: 0.00010801951943398713, l2: 0.00038839977714815176   Iteration 91 of 100, tot loss = 4.963853723400241, l1: 0.0001079799053863658, l2: 0.00038840546687504603   Iteration 92 of 100, tot loss = 4.943483435589334, l1: 0.00010758451093272964, l2: 0.00038676383266410204   Iteration 93 of 100, tot loss = 4.956818642154817, l1: 0.00010796850604306836, l2: 0.0003877133582449538   Iteration 94 of 100, tot loss = 5.004744651469778, l1: 0.00010853894061856021, l2: 0.00039193552413749745   Iteration 95 of 100, tot loss = 5.031434420535439, l1: 0.00010877531898777784, l2: 0.00039436812275150593   Iteration 96 of 100, tot loss = 5.0081278284390764, l1: 0.00010849661737211136, l2: 0.0003923161652134392   Iteration 97 of 100, tot loss = 5.002675007299049, l1: 0.00010846879466925268, l2: 0.0003917987060705536   Iteration 98 of 100, tot loss = 5.0200682026999335, l1: 0.0001088246880666405, l2: 0.0003931821319922016   Iteration 99 of 100, tot loss = 4.997058466227368, l1: 0.00010822914165798621, l2: 0.00039147670494540677   Iteration 100 of 100, tot loss = 5.013716623783112, l1: 0.00010843793341337005, l2: 0.0003929337290901458
   End of epoch 1266; saving model... 

Epoch 1267 of 2000
   Iteration 1 of 100, tot loss = 3.1590206623077393, l1: 6.344904249999672e-05, l2: 0.00025245299912057817   Iteration 2 of 100, tot loss = 5.113676428794861, l1: 0.0001089959405362606, l2: 0.0004023716755909845   Iteration 3 of 100, tot loss = 4.64329997698466, l1: 0.000100819085976885, l2: 0.0003635108975383143   Iteration 4 of 100, tot loss = 5.223404109477997, l1: 0.00010908008152910043, l2: 0.00041326032078359276   Iteration 5 of 100, tot loss = 4.732777309417725, l1: 9.951653046300635e-05, l2: 0.0003737611958058551   Iteration 6 of 100, tot loss = 4.486104806264241, l1: 9.397445440602799e-05, l2: 0.00035463602155990276   Iteration 7 of 100, tot loss = 4.61679778780256, l1: 9.495406681838046e-05, l2: 0.00036672570318582335   Iteration 8 of 100, tot loss = 4.752404510974884, l1: 9.563951698510209e-05, l2: 0.00037960092777211685   Iteration 9 of 100, tot loss = 4.651732232835558, l1: 9.375775860260344e-05, l2: 0.00037141546070213534   Iteration 10 of 100, tot loss = 4.414211463928223, l1: 9.012111695483327e-05, l2: 0.00035130002652294934   Iteration 11 of 100, tot loss = 4.416682676835493, l1: 9.094056837387721e-05, l2: 0.00035072769440541214   Iteration 12 of 100, tot loss = 4.584545334180196, l1: 9.380443346647856e-05, l2: 0.000364650094221967   Iteration 13 of 100, tot loss = 4.750153798323411, l1: 9.61090117925778e-05, l2: 0.0003789063641586556   Iteration 14 of 100, tot loss = 4.863234077181135, l1: 9.954528858153415e-05, l2: 0.00038677811556096586   Iteration 15 of 100, tot loss = 4.81494566599528, l1: 9.996502049034462e-05, l2: 0.0003815295446353654   Iteration 16 of 100, tot loss = 4.889299899339676, l1: 0.0001007641421892913, l2: 0.0003881658467435045   Iteration 17 of 100, tot loss = 5.001258737900677, l1: 0.00010404011631822762, l2: 0.00039608575410538297   Iteration 18 of 100, tot loss = 5.0157491366068525, l1: 0.0001040901972575941, l2: 0.00039748471398423944   Iteration 19 of 100, tot loss = 5.1050965158562915, l1: 0.00010658324218208068, l2: 0.00040392640757521515   Iteration 20 of 100, tot loss = 5.006396472454071, l1: 0.00010472828253114131, l2: 0.0003959113637392875   Iteration 21 of 100, tot loss = 5.076056673413231, l1: 0.00010520919680684095, l2: 0.0004023964692827403   Iteration 22 of 100, tot loss = 5.00819718837738, l1: 0.00010406477767074566, l2: 0.00039675493991870263   Iteration 23 of 100, tot loss = 4.945154086403225, l1: 0.00010298873752142991, l2: 0.0003915266696940703   Iteration 24 of 100, tot loss = 4.867240568002065, l1: 0.00010230543981985345, l2: 0.00038441861579485703   Iteration 25 of 100, tot loss = 4.737436270713806, l1: 0.00010012884245952592, l2: 0.00037361478374805303   Iteration 26 of 100, tot loss = 4.665997555622687, l1: 9.841194976336107e-05, l2: 0.0003681878051093708   Iteration 27 of 100, tot loss = 4.623705267906189, l1: 9.871542068948556e-05, l2: 0.0003636551048623881   Iteration 28 of 100, tot loss = 4.619245942149844, l1: 9.761736977712385e-05, l2: 0.0003643072226883045   Iteration 29 of 100, tot loss = 4.62151457112411, l1: 9.752269623528139e-05, l2: 0.0003646287589398181   Iteration 30 of 100, tot loss = 4.62841074069341, l1: 9.6709659313395e-05, l2: 0.00036613141322353233   Iteration 31 of 100, tot loss = 4.602593656509153, l1: 9.72725824648214e-05, l2: 0.0003629867823028396   Iteration 32 of 100, tot loss = 4.644872281700373, l1: 9.810429173739976e-05, l2: 0.0003663829365905258   Iteration 33 of 100, tot loss = 4.6383088899381235, l1: 9.762269650254342e-05, l2: 0.00036620819203866023   Iteration 34 of 100, tot loss = 4.577942844699411, l1: 9.625624839185654e-05, l2: 0.0003615380354698145   Iteration 35 of 100, tot loss = 4.520294179235186, l1: 9.533665236501422e-05, l2: 0.0003566927646586139   Iteration 36 of 100, tot loss = 4.5748132103019294, l1: 9.604344177205348e-05, l2: 0.00036143787898860563   Iteration 37 of 100, tot loss = 4.602104280446027, l1: 9.697165178540918e-05, l2: 0.0003632387752814621   Iteration 38 of 100, tot loss = 4.600057642710836, l1: 9.689137684625549e-05, l2: 0.0003631143860586331   Iteration 39 of 100, tot loss = 4.591805889056279, l1: 9.723192814794273e-05, l2: 0.0003619486595151755   Iteration 40 of 100, tot loss = 4.555099758505821, l1: 9.618801614124095e-05, l2: 0.0003593219586036867   Iteration 41 of 100, tot loss = 4.5200946301948735, l1: 9.576952714535476e-05, l2: 0.0003562399350169183   Iteration 42 of 100, tot loss = 4.514073658557165, l1: 9.638039842857757e-05, l2: 0.00035502696638494465   Iteration 43 of 100, tot loss = 4.4849880268407425, l1: 9.591906310497718e-05, l2: 0.00035257973853054694   Iteration 44 of 100, tot loss = 4.47532123056325, l1: 9.603169134855058e-05, l2: 0.0003515004312000449   Iteration 45 of 100, tot loss = 4.477562652693854, l1: 9.64149130999835e-05, l2: 0.00035134135169856667   Iteration 46 of 100, tot loss = 4.491866881432741, l1: 9.668225965983963e-05, l2: 0.0003525044274916265   Iteration 47 of 100, tot loss = 4.4635177749268555, l1: 9.66431836515883e-05, l2: 0.0003497085925389795   Iteration 48 of 100, tot loss = 4.418563507497311, l1: 9.554533903610718e-05, l2: 0.0003463110106167733   Iteration 49 of 100, tot loss = 4.4043611336727535, l1: 9.583013762698547e-05, l2: 0.00034460597475563005   Iteration 50 of 100, tot loss = 4.451565859317779, l1: 9.639855787099805e-05, l2: 0.0003487580266664736   Iteration 51 of 100, tot loss = 4.478721583590788, l1: 9.708021966884296e-05, l2: 0.00035079193670147407   Iteration 52 of 100, tot loss = 4.4685541689395905, l1: 9.73391713089838e-05, l2: 0.00034951624407907587   Iteration 53 of 100, tot loss = 4.4805422031654505, l1: 9.750458569746939e-05, l2: 0.00035054963395078577   Iteration 54 of 100, tot loss = 4.485466376498893, l1: 9.757749211092703e-05, l2: 0.00035096914534396665   Iteration 55 of 100, tot loss = 4.480605691129511, l1: 9.744634067332796e-05, l2: 0.00035061422835993157   Iteration 56 of 100, tot loss = 4.458525446908815, l1: 9.696321408877598e-05, l2: 0.0003488893305205108   Iteration 57 of 100, tot loss = 4.4489506483078, l1: 9.665620126996369e-05, l2: 0.0003482388634246921   Iteration 58 of 100, tot loss = 4.408158213927828, l1: 9.606810767607009e-05, l2: 0.0003447477133234096   Iteration 59 of 100, tot loss = 4.41625004501666, l1: 9.622808906666035e-05, l2: 0.00034539691508960737   Iteration 60 of 100, tot loss = 4.411643928289413, l1: 9.625156859935183e-05, l2: 0.0003449128234933596   Iteration 61 of 100, tot loss = 4.413937648788828, l1: 9.561052269657279e-05, l2: 0.0003457832415912636   Iteration 62 of 100, tot loss = 4.416732420844417, l1: 9.60198694590344e-05, l2: 0.0003456533719849352   Iteration 63 of 100, tot loss = 4.454247658214872, l1: 9.676738351579784e-05, l2: 0.00034865738185573485   Iteration 64 of 100, tot loss = 4.4657865185290575, l1: 9.723788627979957e-05, l2: 0.00034934076461468067   Iteration 65 of 100, tot loss = 4.462187944925748, l1: 9.707938353620613e-05, l2: 0.0003491394098651094   Iteration 66 of 100, tot loss = 4.450200667887023, l1: 9.67403889676048e-05, l2: 0.0003482796766382473   Iteration 67 of 100, tot loss = 4.453342653032559, l1: 9.694689184986749e-05, l2: 0.00034838737272692205   Iteration 68 of 100, tot loss = 4.452817434773726, l1: 9.677294784684337e-05, l2: 0.0003485087952011621   Iteration 69 of 100, tot loss = 4.456809404967488, l1: 9.649403406344219e-05, l2: 0.00034918690600560683   Iteration 70 of 100, tot loss = 4.465494552680425, l1: 9.702177475056877e-05, l2: 0.000349527680373285   Iteration 71 of 100, tot loss = 4.489608497686789, l1: 9.725046781478712e-05, l2: 0.0003517103825390837   Iteration 72 of 100, tot loss = 4.566304274731213, l1: 9.837375152225529e-05, l2: 0.00035825667772668466   Iteration 73 of 100, tot loss = 4.55378259861306, l1: 9.80071384979616e-05, l2: 0.00035737112323173053   Iteration 74 of 100, tot loss = 4.553578594246426, l1: 9.802756097554732e-05, l2: 0.0003573303006674364   Iteration 75 of 100, tot loss = 4.565125343004863, l1: 9.817915551442032e-05, l2: 0.00035833338100928815   Iteration 76 of 100, tot loss = 4.597009368632969, l1: 9.868749389409602e-05, l2: 0.0003610134450642171   Iteration 77 of 100, tot loss = 4.6108318598239455, l1: 9.90140993585024e-05, l2: 0.0003620690884228868   Iteration 78 of 100, tot loss = 4.605733845478449, l1: 9.914340560685676e-05, l2: 0.00036142998056348937   Iteration 79 of 100, tot loss = 4.637028547781933, l1: 9.957132936540779e-05, l2: 0.00036413152720572874   Iteration 80 of 100, tot loss = 4.627099274098873, l1: 9.944356097548734e-05, l2: 0.00036326636818557746   Iteration 81 of 100, tot loss = 4.627339644196593, l1: 9.93376972339185e-05, l2: 0.00036339626886830147   Iteration 82 of 100, tot loss = 4.625807923514668, l1: 9.922276379855735e-05, l2: 0.0003633580304378439   Iteration 83 of 100, tot loss = 4.61792170714183, l1: 9.885599532694814e-05, l2: 0.0003629361774775211   Iteration 84 of 100, tot loss = 4.611339442786717, l1: 9.873248754523783e-05, l2: 0.00036240145865511816   Iteration 85 of 100, tot loss = 4.5965190031949215, l1: 9.814094917280325e-05, l2: 0.00036151095290038295   Iteration 86 of 100, tot loss = 4.636922097483347, l1: 9.898006968331513e-05, l2: 0.00036471214150159815   Iteration 87 of 100, tot loss = 4.633080925064525, l1: 9.897779923396709e-05, l2: 0.0003643302949768994   Iteration 88 of 100, tot loss = 4.667796485803344, l1: 9.960414614728175e-05, l2: 0.00036717550409031736   Iteration 89 of 100, tot loss = 4.699017201916555, l1: 0.00010026032817151052, l2: 0.0003696413937330853   Iteration 90 of 100, tot loss = 4.681679125626882, l1: 9.993869870312563e-05, l2: 0.00036822921538259834   Iteration 91 of 100, tot loss = 4.6502730676106045, l1: 9.946629107107183e-05, l2: 0.000365561017143581   Iteration 92 of 100, tot loss = 4.655320339876672, l1: 9.977892133737559e-05, l2: 0.00036575311384259965   Iteration 93 of 100, tot loss = 4.674995828700322, l1: 0.00010037007172853606, l2: 0.0003671295124404032   Iteration 94 of 100, tot loss = 4.6663316044401615, l1: 9.992530907224369e-05, l2: 0.00036670785274064326   Iteration 95 of 100, tot loss = 4.682144746027495, l1: 0.00010035477056302816, l2: 0.000367859705798573   Iteration 96 of 100, tot loss = 4.69781952475508, l1: 0.00010053854650019882, l2: 0.00036924340762804303   Iteration 97 of 100, tot loss = 4.685564538867204, l1: 0.00010040516637448106, l2: 0.0003681512891264517   Iteration 98 of 100, tot loss = 4.681114086083004, l1: 0.0001002908543248813, l2: 0.00036782055558061833   Iteration 99 of 100, tot loss = 4.70472791098585, l1: 0.00010062782978756598, l2: 0.00036984496274280993   Iteration 100 of 100, tot loss = 4.712310668230057, l1: 0.00010089621850056574, l2: 0.00037033485001302323
   End of epoch 1267; saving model... 

Epoch 1268 of 2000
   Iteration 1 of 100, tot loss = 4.034800052642822, l1: 9.23132902244106e-05, l2: 0.0003111667465418577   Iteration 2 of 100, tot loss = 4.357876777648926, l1: 9.853076699073426e-05, l2: 0.00033725693356245756   Iteration 3 of 100, tot loss = 5.561838785807292, l1: 0.00012316122350360578, l2: 0.0004330226608241598   Iteration 4 of 100, tot loss = 5.470424771308899, l1: 0.00011964348050241824, l2: 0.0004273990052752197   Iteration 5 of 100, tot loss = 4.953895902633667, l1: 0.00011171290534548462, l2: 0.0003836766933090985   Iteration 6 of 100, tot loss = 5.198748151461284, l1: 0.00011561578624726583, l2: 0.00040425903474291164   Iteration 7 of 100, tot loss = 5.133407354354858, l1: 0.00011375772737665102, l2: 0.0003995830110008163   Iteration 8 of 100, tot loss = 5.296596497297287, l1: 0.00011374202767910901, l2: 0.000415917624195572   Iteration 9 of 100, tot loss = 5.242392990324232, l1: 0.00011084969850748571, l2: 0.00041338960161536105   Iteration 10 of 100, tot loss = 5.338957476615906, l1: 0.00011295793156023138, l2: 0.00042093781812582167   Iteration 11 of 100, tot loss = 5.242098960009488, l1: 0.00011223746167326516, l2: 0.00041197243792174214   Iteration 12 of 100, tot loss = 5.508186956246694, l1: 0.00011774357820589405, l2: 0.00043307512290387723   Iteration 13 of 100, tot loss = 5.606160732416006, l1: 0.00012070368281386506, l2: 0.00043991239642939315   Iteration 14 of 100, tot loss = 5.379900762013027, l1: 0.0001156236435885408, l2: 0.00042236643847510483   Iteration 15 of 100, tot loss = 5.251496537526449, l1: 0.00011154679377796128, l2: 0.00041360286510704704   Iteration 16 of 100, tot loss = 5.264374494552612, l1: 0.00011183856531715719, l2: 0.0004145988905293052   Iteration 17 of 100, tot loss = 5.1449823519762825, l1: 0.00010913918445379856, l2: 0.00040535905538811623   Iteration 18 of 100, tot loss = 5.0403795507219105, l1: 0.00010653449822307771, l2: 0.0003975034608932522   Iteration 19 of 100, tot loss = 5.025615717235365, l1: 0.00010743415125608935, l2: 0.00039512742583419343   Iteration 20 of 100, tot loss = 5.001262092590332, l1: 0.00010657515813363715, l2: 0.00039355105618597006   Iteration 21 of 100, tot loss = 5.07294952301752, l1: 0.00010799332466419964, l2: 0.0003993016303173222   Iteration 22 of 100, tot loss = 5.126274260607633, l1: 0.00010861244763046588, l2: 0.000404014980383429   Iteration 23 of 100, tot loss = 5.052318376043568, l1: 0.00010757480840370788, l2: 0.00039765703094536036   Iteration 24 of 100, tot loss = 5.1406336923440294, l1: 0.00010891974458597058, l2: 0.0004051436256607606   Iteration 25 of 100, tot loss = 5.113264093399048, l1: 0.00010847666300833225, l2: 0.0004028497479157522   Iteration 26 of 100, tot loss = 5.00483710032243, l1: 0.00010623059964788039, l2: 0.0003942531117130644   Iteration 27 of 100, tot loss = 5.157852835125393, l1: 0.00010768030336260347, l2: 0.0004081049830549293   Iteration 28 of 100, tot loss = 5.111204411302294, l1: 0.00010700292256972586, l2: 0.0004041175208320575   Iteration 29 of 100, tot loss = 5.040672828411234, l1: 0.00010627640472977936, l2: 0.00039779088020340764   Iteration 30 of 100, tot loss = 5.148348140716553, l1: 0.00010690524347107081, l2: 0.0004079295734603268   Iteration 31 of 100, tot loss = 5.138058985433271, l1: 0.00010707865963453397, l2: 0.0004067272412583172   Iteration 32 of 100, tot loss = 5.124844416975975, l1: 0.0001074792488680032, l2: 0.00040500519526176504   Iteration 33 of 100, tot loss = 5.07319248083866, l1: 0.00010681547417224272, l2: 0.00040050377587745476   Iteration 34 of 100, tot loss = 5.104222760480993, l1: 0.00010672999451344367, l2: 0.00040369228286983664   Iteration 35 of 100, tot loss = 5.04753897530692, l1: 0.00010603897629022998, l2: 0.0003987149227344032   Iteration 36 of 100, tot loss = 5.058169815275404, l1: 0.00010648207504952249, l2: 0.00039933490835311305   Iteration 37 of 100, tot loss = 5.084363550753207, l1: 0.00010724191334038797, l2: 0.0004011944441338749   Iteration 38 of 100, tot loss = 5.002687821262761, l1: 0.00010568272409727797, l2: 0.0003945860601226358   Iteration 39 of 100, tot loss = 5.017602202219841, l1: 0.00010539802259210354, l2: 0.0003963621998898303   Iteration 40 of 100, tot loss = 4.980720707774163, l1: 0.00010502564236958278, l2: 0.0003930464306904469   Iteration 41 of 100, tot loss = 4.992989537192554, l1: 0.00010554121204555353, l2: 0.00039375774355082796   Iteration 42 of 100, tot loss = 4.944755585420699, l1: 0.00010511817665593255, l2: 0.00038935738328811046   Iteration 43 of 100, tot loss = 4.977160950039709, l1: 0.00010626571053953105, l2: 0.0003914503853762696   Iteration 44 of 100, tot loss = 4.970016541806134, l1: 0.0001061567504207646, l2: 0.0003908449047726621   Iteration 45 of 100, tot loss = 4.944491606288486, l1: 0.00010562527604633943, l2: 0.000388823885198993   Iteration 46 of 100, tot loss = 5.001087642234305, l1: 0.00010693219661808816, l2: 0.00039317656742439243   Iteration 47 of 100, tot loss = 4.973912510466068, l1: 0.00010659860198118506, l2: 0.00039079264852842513   Iteration 48 of 100, tot loss = 4.995234457155068, l1: 0.00010674848423756582, l2: 0.00039277496004312223   Iteration 49 of 100, tot loss = 5.003502658435276, l1: 0.0001071079034828676, l2: 0.00039324236170113164   Iteration 50 of 100, tot loss = 5.0066212153434755, l1: 0.0001078189371037297, l2: 0.0003928431842359714   Iteration 51 of 100, tot loss = 5.002115845680237, l1: 0.00010807085616797136, l2: 0.00039214072788965103   Iteration 52 of 100, tot loss = 4.973094300581859, l1: 0.00010767964802373451, l2: 0.000389629781588715   Iteration 53 of 100, tot loss = 4.976394772529602, l1: 0.00010725495941994079, l2: 0.0003903845173003644   Iteration 54 of 100, tot loss = 4.988202415130757, l1: 0.00010760236896371848, l2: 0.00039121787159712294   Iteration 55 of 100, tot loss = 4.982933627475392, l1: 0.00010721301739315756, l2: 0.0003910803445085714   Iteration 56 of 100, tot loss = 4.969679161906242, l1: 0.0001073271457374046, l2: 0.0003896407691042571   Iteration 57 of 100, tot loss = 4.996238919726589, l1: 0.00010677251396516927, l2: 0.0003928513766017236   Iteration 58 of 100, tot loss = 4.996551647268492, l1: 0.00010675019991046204, l2: 0.00039290496351895854   Iteration 59 of 100, tot loss = 4.981094774553331, l1: 0.00010626723531582313, l2: 0.0003918422407261491   Iteration 60 of 100, tot loss = 5.000307728846868, l1: 0.00010656303602445405, l2: 0.0003934677357998832   Iteration 61 of 100, tot loss = 5.005941643089544, l1: 0.00010613990913252117, l2: 0.0003944542542859515   Iteration 62 of 100, tot loss = 5.054963698310237, l1: 0.00010664996079867706, l2: 0.00039884640770446086   Iteration 63 of 100, tot loss = 5.05086926997654, l1: 0.00010687037242210604, l2: 0.00039821655313033495   Iteration 64 of 100, tot loss = 5.062534818425775, l1: 0.00010684870142085856, l2: 0.00039940477950040076   Iteration 65 of 100, tot loss = 5.108511860554035, l1: 0.00010764833859866485, l2: 0.0004032028465119835   Iteration 66 of 100, tot loss = 5.094216384670951, l1: 0.00010772489401984565, l2: 0.0004016967434312612   Iteration 67 of 100, tot loss = 5.068383841372248, l1: 0.00010771399912069926, l2: 0.00039912438399763083   Iteration 68 of 100, tot loss = 5.023266474990284, l1: 0.00010676503916329239, l2: 0.0003955616071727812   Iteration 69 of 100, tot loss = 5.021246922188911, l1: 0.00010670951967873354, l2: 0.00039541517132117104   Iteration 70 of 100, tot loss = 4.974763737406049, l1: 0.000105871939584696, l2: 0.00039160443286943646   Iteration 71 of 100, tot loss = 5.025501751563918, l1: 0.00010611250276987204, l2: 0.0003964376707994182   Iteration 72 of 100, tot loss = 5.01973373691241, l1: 0.0001058762653277275, l2: 0.0003960971064063617   Iteration 73 of 100, tot loss = 5.03682568628494, l1: 0.0001062773391249596, l2: 0.00039740522752503216   Iteration 74 of 100, tot loss = 5.069609393944612, l1: 0.00010698236012005411, l2: 0.000399978577341834   Iteration 75 of 100, tot loss = 5.085855627059937, l1: 0.0001072483239598417, l2: 0.0004013372367868821   Iteration 76 of 100, tot loss = 5.058944673914659, l1: 0.0001065200198545905, l2: 0.00039937444556639284   Iteration 77 of 100, tot loss = 5.075116693199455, l1: 0.00010661989814819551, l2: 0.0004008917690575268   Iteration 78 of 100, tot loss = 5.036193924072461, l1: 0.00010605648976007405, l2: 0.0003975629006163217   Iteration 79 of 100, tot loss = 5.044670192501213, l1: 0.00010599968728352974, l2: 0.00039846733005428567   Iteration 80 of 100, tot loss = 5.033553275465965, l1: 0.00010616257850415423, l2: 0.0003971927470047376   Iteration 81 of 100, tot loss = 5.025012095769246, l1: 0.0001059852286822588, l2: 0.00039651597915418493   Iteration 82 of 100, tot loss = 5.031381406435153, l1: 0.0001059263937882284, l2: 0.00039721174515562315   Iteration 83 of 100, tot loss = 5.026861492409764, l1: 0.00010563873900493316, l2: 0.0003970474085745301   Iteration 84 of 100, tot loss = 5.010242152781713, l1: 0.00010565705651104162, l2: 0.0003953671571036379   Iteration 85 of 100, tot loss = 5.034656594781315, l1: 0.00010614144977058887, l2: 0.00039732420820441536   Iteration 86 of 100, tot loss = 5.094904858012532, l1: 0.0001070412914952274, l2: 0.0004024491924589478   Iteration 87 of 100, tot loss = 5.0734580823744855, l1: 0.00010670549651051219, l2: 0.000400640309792301   Iteration 88 of 100, tot loss = 5.062319882891395, l1: 0.00010683820883886338, l2: 0.0003993937774463451   Iteration 89 of 100, tot loss = 5.070148765371087, l1: 0.00010704338564302019, l2: 0.0003999714892334043   Iteration 90 of 100, tot loss = 5.051535150739881, l1: 0.00010670598931028508, l2: 0.000398447524154714   Iteration 91 of 100, tot loss = 5.020365644287277, l1: 0.00010600366007125997, l2: 0.00039603290273449744   Iteration 92 of 100, tot loss = 5.000637059626372, l1: 0.00010546092052327762, l2: 0.00039460278386958754   Iteration 93 of 100, tot loss = 4.996520811511624, l1: 0.00010554705608306923, l2: 0.00039410502343158166   Iteration 94 of 100, tot loss = 5.011259748580608, l1: 0.00010577906540036449, l2: 0.0003953469081836297   Iteration 95 of 100, tot loss = 5.00299489372655, l1: 0.00010589082410565185, l2: 0.000394408664004387   Iteration 96 of 100, tot loss = 5.014447192351024, l1: 0.00010603006743773828, l2: 0.0003954146504838718   Iteration 97 of 100, tot loss = 5.008156093125491, l1: 0.00010613395937342801, l2: 0.0003946816486296887   Iteration 98 of 100, tot loss = 5.028491239158475, l1: 0.00010648444282307945, l2: 0.00039636467973112454   Iteration 99 of 100, tot loss = 5.020972752811933, l1: 0.00010645142288242392, l2: 0.0003956458510741629   Iteration 100 of 100, tot loss = 5.008045036792755, l1: 0.00010643786004948197, l2: 0.00039436664257664234
   End of epoch 1268; saving model... 

Epoch 1269 of 2000
   Iteration 1 of 100, tot loss = 4.477811813354492, l1: 9.40165264182724e-05, l2: 0.0003537646553013474   Iteration 2 of 100, tot loss = 3.6339191198349, l1: 8.793269444140606e-05, l2: 0.00027545921329874545   Iteration 3 of 100, tot loss = 3.7748471101125083, l1: 9.356922237202525e-05, l2: 0.0002839154816077401   Iteration 4 of 100, tot loss = 3.8432371020317078, l1: 8.681956387590617e-05, l2: 0.00029750413523288444   Iteration 5 of 100, tot loss = 3.806928300857544, l1: 8.709821995580568e-05, l2: 0.0002935946045909077   Iteration 6 of 100, tot loss = 3.6616968711217246, l1: 8.540821606099296e-05, l2: 0.000280761465546675   Iteration 7 of 100, tot loss = 3.8037749358585904, l1: 8.757723740667902e-05, l2: 0.0002928002504631877   Iteration 8 of 100, tot loss = 4.01932755112648, l1: 9.134285937761888e-05, l2: 0.0003105898904323112   Iteration 9 of 100, tot loss = 4.280833906597561, l1: 9.48655377012781e-05, l2: 0.0003332178489977701   Iteration 10 of 100, tot loss = 4.315225100517273, l1: 9.836855169851333e-05, l2: 0.00033315395703539254   Iteration 11 of 100, tot loss = 4.4344612685116855, l1: 0.00010137892439326441, l2: 0.00034206720143133265   Iteration 12 of 100, tot loss = 4.477756400903066, l1: 0.00010264510516814578, l2: 0.0003451305310591124   Iteration 13 of 100, tot loss = 4.516476135987502, l1: 0.0001028568272326643, l2: 0.00034879078157246113   Iteration 14 of 100, tot loss = 4.658369285719735, l1: 0.00010438205260600495, l2: 0.00036145487268056186   Iteration 15 of 100, tot loss = 4.525000842412313, l1: 0.00010172083178379883, l2: 0.0003507792493716503   Iteration 16 of 100, tot loss = 4.584908053278923, l1: 0.00010186395275013638, l2: 0.0003566268487702473   Iteration 17 of 100, tot loss = 4.5076547370237465, l1: 0.00010041113872669965, l2: 0.0003503543309360633   Iteration 18 of 100, tot loss = 4.633890562587315, l1: 0.00010308339915354736, l2: 0.00036030565307656716   Iteration 19 of 100, tot loss = 4.583546625940423, l1: 0.00010210924703738113, l2: 0.0003562454114312698   Iteration 20 of 100, tot loss = 4.615129315853119, l1: 0.0001033273645589361, l2: 0.00035818556134472603   Iteration 21 of 100, tot loss = 4.583799294063023, l1: 0.00010252123736011396, l2: 0.0003558586876828312   Iteration 22 of 100, tot loss = 4.58281683921814, l1: 0.00010224503222905861, l2: 0.00035603664913320574   Iteration 23 of 100, tot loss = 4.605297586192256, l1: 0.0001030827381486154, l2: 0.0003574470180825776   Iteration 24 of 100, tot loss = 4.652122735977173, l1: 0.00010420251055620611, l2: 0.00036100976103625726   Iteration 25 of 100, tot loss = 4.681943454742432, l1: 0.00010466298001119867, l2: 0.00036353136354591695   Iteration 26 of 100, tot loss = 4.702597984900842, l1: 0.00010521199342642481, l2: 0.00036504780235610757   Iteration 27 of 100, tot loss = 4.708256244659424, l1: 0.00010547238128277024, l2: 0.0003653532395320427   Iteration 28 of 100, tot loss = 4.791087457111904, l1: 0.00010585689389180126, l2: 0.00037325184894143604   Iteration 29 of 100, tot loss = 4.697725180921884, l1: 0.00010361199283265862, l2: 0.00036616052264058644   Iteration 30 of 100, tot loss = 4.701375834147135, l1: 0.0001046119345119223, l2: 0.00036552564706653355   Iteration 31 of 100, tot loss = 4.826110286097372, l1: 0.00010645956752987038, l2: 0.0003761514596971533   Iteration 32 of 100, tot loss = 4.75295477360487, l1: 0.0001052831221386441, l2: 0.00037001235386924236   Iteration 33 of 100, tot loss = 4.7756382552060215, l1: 0.00010632694907006666, l2: 0.00037123687489480346   Iteration 34 of 100, tot loss = 4.686959624290466, l1: 0.00010465587333515835, l2: 0.0003640400874942048   Iteration 35 of 100, tot loss = 4.708650772912161, l1: 0.00010459844925208018, l2: 0.0003662666260165029   Iteration 36 of 100, tot loss = 4.721812744935353, l1: 0.00010530755773490657, l2: 0.00036687371458457265   Iteration 37 of 100, tot loss = 4.759018015217137, l1: 0.00010653460422700673, l2: 0.00036936719560479696   Iteration 38 of 100, tot loss = 4.715059882716129, l1: 0.00010573400715274099, l2: 0.0003657719797430266   Iteration 39 of 100, tot loss = 4.690650084079841, l1: 0.00010520162677261024, l2: 0.00036386338074524433   Iteration 40 of 100, tot loss = 4.69281085729599, l1: 0.00010562829556874931, l2: 0.0003636527901107911   Iteration 41 of 100, tot loss = 4.667145147556212, l1: 0.00010494130809802743, l2: 0.0003617732060175934   Iteration 42 of 100, tot loss = 4.603413851488204, l1: 0.000103908040251727, l2: 0.0003564333445475703   Iteration 43 of 100, tot loss = 4.61217094853867, l1: 0.00010399258293503853, l2: 0.0003572245115592946   Iteration 44 of 100, tot loss = 4.593656374649568, l1: 0.00010320091314497404, l2: 0.0003561647241771094   Iteration 45 of 100, tot loss = 4.56426694922977, l1: 0.00010295989373440129, l2: 0.00035346680120306297   Iteration 46 of 100, tot loss = 4.5284721929094065, l1: 0.00010256684064465252, l2: 0.00035028037856808743   Iteration 47 of 100, tot loss = 4.524965750410201, l1: 0.00010243751278315532, l2: 0.00035005906241341553   Iteration 48 of 100, tot loss = 4.56012378881375, l1: 0.00010287045824952656, l2: 0.0003531419209442295   Iteration 49 of 100, tot loss = 4.594046726518748, l1: 0.00010321867368326579, l2: 0.00035618599985336543   Iteration 50 of 100, tot loss = 4.607054765224457, l1: 0.00010316695101209916, l2: 0.000357538526004646   Iteration 51 of 100, tot loss = 4.646534309667699, l1: 0.00010390499793335467, l2: 0.0003607484329989472   Iteration 52 of 100, tot loss = 4.6418876716723805, l1: 0.00010392345673328516, l2: 0.0003602653106589479   Iteration 53 of 100, tot loss = 4.643957657634087, l1: 0.00010355901662787458, l2: 0.000360836749094679   Iteration 54 of 100, tot loss = 4.683097298498507, l1: 0.00010429434311205384, l2: 0.00036401538737764995   Iteration 55 of 100, tot loss = 4.66196167902513, l1: 0.00010410807193362747, l2: 0.00036208809655032713   Iteration 56 of 100, tot loss = 4.665156954101154, l1: 0.00010381710165217686, l2: 0.0003626985944720218   Iteration 57 of 100, tot loss = 4.67064264573549, l1: 0.00010392750590925284, l2: 0.00036313675937299993   Iteration 58 of 100, tot loss = 4.682174051630086, l1: 0.00010407815591014665, l2: 0.00036413925082687737   Iteration 59 of 100, tot loss = 4.704735402333534, l1: 0.00010486625695316973, l2: 0.000365607284840149   Iteration 60 of 100, tot loss = 4.704705383380254, l1: 0.00010509371822990943, l2: 0.0003653768214765781   Iteration 61 of 100, tot loss = 4.673804785384506, l1: 0.00010483459386779911, l2: 0.00036254588568957185   Iteration 62 of 100, tot loss = 4.668867197728926, l1: 0.00010493278727329696, l2: 0.0003619539335974672   Iteration 63 of 100, tot loss = 4.64065358563075, l1: 0.00010408841418601307, l2: 0.00035997694529156895   Iteration 64 of 100, tot loss = 4.647463781759143, l1: 0.00010400758401374333, l2: 0.0003607387950523844   Iteration 65 of 100, tot loss = 4.662602943640489, l1: 0.00010433834149108197, l2: 0.00036192195369109796   Iteration 66 of 100, tot loss = 4.69503007332484, l1: 0.00010445152987482619, l2: 0.0003650514781563261   Iteration 67 of 100, tot loss = 4.685811154877961, l1: 0.00010430239653127935, l2: 0.0003642787196440622   Iteration 68 of 100, tot loss = 4.696649682872436, l1: 0.00010473787327047081, l2: 0.0003649270958489711   Iteration 69 of 100, tot loss = 4.711284380028213, l1: 0.00010484078191676299, l2: 0.0003662876575169545   Iteration 70 of 100, tot loss = 4.7027725747653415, l1: 0.00010467859506856517, l2: 0.00036559866379580594   Iteration 71 of 100, tot loss = 4.706766556686079, l1: 0.00010449714697553107, l2: 0.00036617951048702055   Iteration 72 of 100, tot loss = 4.74726099272569, l1: 0.0001051520853757716, l2: 0.0003695740154701828   Iteration 73 of 100, tot loss = 4.73231486914909, l1: 0.00010509273632946233, l2: 0.00036813875230520403   Iteration 74 of 100, tot loss = 4.702521709171501, l1: 0.00010470302781008335, l2: 0.0003655491447126543   Iteration 75 of 100, tot loss = 4.7067459154129025, l1: 0.00010433539447452251, l2: 0.000366339198856925   Iteration 76 of 100, tot loss = 4.749068315091886, l1: 0.00010472893671631984, l2: 0.0003701778964329462   Iteration 77 of 100, tot loss = 4.748803273423926, l1: 0.00010474807159089205, l2: 0.0003701322575934328   Iteration 78 of 100, tot loss = 4.760422986287337, l1: 0.00010506575414090035, l2: 0.000370976546083768   Iteration 79 of 100, tot loss = 4.78680401663237, l1: 0.0001055762014989859, l2: 0.00037310420147944947   Iteration 80 of 100, tot loss = 4.766565476357937, l1: 0.0001052607366545999, l2: 0.00037139581218070816   Iteration 81 of 100, tot loss = 4.7879978035703115, l1: 0.00010542244485861127, l2: 0.0003733773364392282   Iteration 82 of 100, tot loss = 4.782357476106504, l1: 0.00010533352737760784, l2: 0.0003729022208107181   Iteration 83 of 100, tot loss = 4.763694770364876, l1: 0.00010499856308528997, l2: 0.00037137091444547186   Iteration 84 of 100, tot loss = 4.772493307079587, l1: 0.00010473723029593072, l2: 0.0003725121011181424   Iteration 85 of 100, tot loss = 4.771573730076061, l1: 0.00010471495496698052, l2: 0.0003724424185587422   Iteration 86 of 100, tot loss = 4.793131369490956, l1: 0.00010490422404430764, l2: 0.00037440891360030187   Iteration 87 of 100, tot loss = 4.795022193042711, l1: 0.00010469810187150242, l2: 0.00037480411842336943   Iteration 88 of 100, tot loss = 4.766169730912555, l1: 0.00010421302489372796, l2: 0.00037240394918957133   Iteration 89 of 100, tot loss = 4.750124978215507, l1: 0.00010427418805847175, l2: 0.00037073831072631763   Iteration 90 of 100, tot loss = 4.762379433049096, l1: 0.00010462157644926466, l2: 0.000371616368000913   Iteration 91 of 100, tot loss = 4.746343257663014, l1: 0.00010434126898462137, l2: 0.00037029305803026636   Iteration 92 of 100, tot loss = 4.783963671197062, l1: 0.00010483683534244926, l2: 0.00037355953258077574   Iteration 93 of 100, tot loss = 4.786903859466634, l1: 0.00010492463385866534, l2: 0.000373765752996288   Iteration 94 of 100, tot loss = 4.787586771427317, l1: 0.00010494896923773922, l2: 0.0003738097089189204   Iteration 95 of 100, tot loss = 4.780556730220193, l1: 0.0001049605369763939, l2: 0.0003730951370303764   Iteration 96 of 100, tot loss = 4.786540266126394, l1: 0.00010497954955705306, l2: 0.0003736744777900943   Iteration 97 of 100, tot loss = 4.771262204524168, l1: 0.00010472619010449973, l2: 0.00037240003086818556   Iteration 98 of 100, tot loss = 4.7479428040738005, l1: 0.00010428253643818161, l2: 0.0003705117444636072   Iteration 99 of 100, tot loss = 4.745483293677822, l1: 0.0001044241016827091, l2: 0.0003701242282539794   Iteration 100 of 100, tot loss = 4.751234413385391, l1: 0.00010445847387018148, l2: 0.0003706649682135321
   End of epoch 1269; saving model... 

Epoch 1270 of 2000
   Iteration 1 of 100, tot loss = 4.92043399810791, l1: 0.0001122375761042349, l2: 0.0003798058023676276   Iteration 2 of 100, tot loss = 4.393694281578064, l1: 0.00011197588173672557, l2: 0.0003273935435572639   Iteration 3 of 100, tot loss = 5.532615423202515, l1: 0.00012069195508956909, l2: 0.00043256958209288615   Iteration 4 of 100, tot loss = 6.205430567264557, l1: 0.00013168454097467475, l2: 0.0004888585172011517   Iteration 5 of 100, tot loss = 5.858657598495483, l1: 0.00012563065538415686, l2: 0.00046023510512895884   Iteration 6 of 100, tot loss = 5.687447587649028, l1: 0.00012088362806631874, l2: 0.00044786113236720365   Iteration 7 of 100, tot loss = 5.61682711328779, l1: 0.000122292509851312, l2: 0.0004393901991924005   Iteration 8 of 100, tot loss = 5.929053753614426, l1: 0.00012541343676275574, l2: 0.00046749193279538304   Iteration 9 of 100, tot loss = 5.6210108333163795, l1: 0.00011986763193918805, l2: 0.00044223344755462475   Iteration 10 of 100, tot loss = 5.351256108283996, l1: 0.00011654509580694139, l2: 0.00041858051117742433   Iteration 11 of 100, tot loss = 5.349645202810114, l1: 0.00011737549952654676, l2: 0.0004175890155073086   Iteration 12 of 100, tot loss = 5.565278430779775, l1: 0.0001200179431180004, l2: 0.00043650989270342205   Iteration 13 of 100, tot loss = 5.556072803644033, l1: 0.00011900566012347833, l2: 0.0004366016151974551   Iteration 14 of 100, tot loss = 5.456495812961033, l1: 0.00011499550912828584, l2: 0.00043065406602441466   Iteration 15 of 100, tot loss = 5.200445175170898, l1: 0.0001097377775295172, l2: 0.00041030673407173405   Iteration 16 of 100, tot loss = 5.271793693304062, l1: 0.00010985290805365366, l2: 0.0004173264542259858   Iteration 17 of 100, tot loss = 5.169010106255026, l1: 0.00010725543453351266, l2: 0.00040964556854519555   Iteration 18 of 100, tot loss = 5.346218215094672, l1: 0.0001099253396306368, l2: 0.0004246964745107107   Iteration 19 of 100, tot loss = 5.1570005354128385, l1: 0.00010660856653550199, l2: 0.0004090914796841772   Iteration 20 of 100, tot loss = 5.133785957098008, l1: 0.0001062231798641733, l2: 0.00040715540817473086   Iteration 21 of 100, tot loss = 5.185721959386553, l1: 0.00010686430340727038, l2: 0.00041170788574076836   Iteration 22 of 100, tot loss = 5.136609830639579, l1: 0.0001059806965548143, l2: 0.000407680280411362   Iteration 23 of 100, tot loss = 5.1372106541758, l1: 0.00010695997147906937, l2: 0.0004067610880947145   Iteration 24 of 100, tot loss = 5.0801839878161745, l1: 0.00010626453740769648, l2: 0.0004017538558400702   Iteration 25 of 100, tot loss = 5.03475597858429, l1: 0.00010529508013860322, l2: 0.00039818051271140576   Iteration 26 of 100, tot loss = 5.034298736315507, l1: 0.00010585311121338655, l2: 0.00039757675795744243   Iteration 27 of 100, tot loss = 4.975391462997154, l1: 0.00010520152149924629, l2: 0.0003923376207239926   Iteration 28 of 100, tot loss = 4.9679330459662845, l1: 0.0001051613265060171, l2: 0.0003916319744478512   Iteration 29 of 100, tot loss = 4.982487699081158, l1: 0.00010520884160510393, l2: 0.00039303992425316367   Iteration 30 of 100, tot loss = 4.9977223912874855, l1: 0.0001052828244913447, l2: 0.00039448941048855584   Iteration 31 of 100, tot loss = 4.942141698252771, l1: 0.00010473525887186219, l2: 0.0003894789068351289   Iteration 32 of 100, tot loss = 5.040448557585478, l1: 0.00010619765669162007, l2: 0.00039784719365343335   Iteration 33 of 100, tot loss = 5.041379520387361, l1: 0.00010615222146787511, l2: 0.00039798572481341773   Iteration 34 of 100, tot loss = 5.0898386976298164, l1: 0.00010695849039834952, l2: 0.0004020253729643574   Iteration 35 of 100, tot loss = 5.032679867744446, l1: 0.00010586748529541572, l2: 0.0003974004952137225   Iteration 36 of 100, tot loss = 5.019932544893688, l1: 0.00010578915872530261, l2: 0.0003962040900660213   Iteration 37 of 100, tot loss = 4.977471097095592, l1: 0.00010574017555021832, l2: 0.00039200692845042795   Iteration 38 of 100, tot loss = 4.950878692300696, l1: 0.00010529739809403269, l2: 0.0003897904661196088   Iteration 39 of 100, tot loss = 4.888713112244239, l1: 0.00010377607190420326, l2: 0.000385095234751367   Iteration 40 of 100, tot loss = 4.882148477435112, l1: 0.00010430142065160908, l2: 0.00038391342241084203   Iteration 41 of 100, tot loss = 4.832484079570305, l1: 0.00010334989680658753, l2: 0.00037989850623090186   Iteration 42 of 100, tot loss = 4.842403556619372, l1: 0.00010320430252856265, l2: 0.00038103604865249335   Iteration 43 of 100, tot loss = 4.829014453777047, l1: 0.00010325010767109085, l2: 0.00037965133311064524   Iteration 44 of 100, tot loss = 4.8550311625003815, l1: 0.0001036435539638412, l2: 0.00038185955748882736   Iteration 45 of 100, tot loss = 4.846635105874803, l1: 0.0001039901119333485, l2: 0.00038067339369768485   Iteration 46 of 100, tot loss = 4.887591597826584, l1: 0.0001039956468562393, l2: 0.0003847635079696572   Iteration 47 of 100, tot loss = 4.913345638741839, l1: 0.00010469729053550103, l2: 0.000386637268015778   Iteration 48 of 100, tot loss = 4.876118498543899, l1: 0.0001043626924304893, l2: 0.00038324915203702403   Iteration 49 of 100, tot loss = 4.849987514164983, l1: 0.00010438342575739347, l2: 0.00038061532036315804   Iteration 50 of 100, tot loss = 4.801622421741485, l1: 0.00010356545390095561, l2: 0.00037659678346244616   Iteration 51 of 100, tot loss = 4.804493859702466, l1: 0.00010404525742427829, l2: 0.00037640412412264255   Iteration 52 of 100, tot loss = 4.799096623292336, l1: 0.0001032915175668537, l2: 0.00037661814013307984   Iteration 53 of 100, tot loss = 4.791822489702477, l1: 0.0001032404764390656, l2: 0.0003759417683435253   Iteration 54 of 100, tot loss = 4.7851426270273, l1: 0.00010305507789275402, l2: 0.00037545918059078286   Iteration 55 of 100, tot loss = 4.758228256485679, l1: 0.00010255378345556726, l2: 0.000373269037862139   Iteration 56 of 100, tot loss = 4.750368967652321, l1: 0.00010227181415497657, l2: 0.00037276507828210015   Iteration 57 of 100, tot loss = 4.728748260882863, l1: 0.00010151899212423974, l2: 0.0003713558296766949   Iteration 58 of 100, tot loss = 4.742987540261499, l1: 0.00010237139188042232, l2: 0.00037192735858141156   Iteration 59 of 100, tot loss = 4.69139336327375, l1: 0.0001013776383873754, l2: 0.00036776169440985295   Iteration 60 of 100, tot loss = 4.686560269196828, l1: 0.00010153938674193342, l2: 0.00036711663681974945   Iteration 61 of 100, tot loss = 4.6762913211447295, l1: 0.0001012395977206742, l2: 0.0003663895307700379   Iteration 62 of 100, tot loss = 4.686407900625659, l1: 0.00010115996373051237, l2: 0.0003674808228219439   Iteration 63 of 100, tot loss = 4.699168125788371, l1: 0.00010113830325685235, l2: 0.0003687785053718096   Iteration 64 of 100, tot loss = 4.666346676647663, l1: 0.00010065509707146703, l2: 0.0003659795670500898   Iteration 65 of 100, tot loss = 4.696174394167387, l1: 0.00010102048746865385, l2: 0.00036859694787730963   Iteration 66 of 100, tot loss = 4.699777241909143, l1: 0.00010106075712952368, l2: 0.0003689169635904501   Iteration 67 of 100, tot loss = 4.6820498224514635, l1: 0.00010115104614941181, l2: 0.00036705393255549246   Iteration 68 of 100, tot loss = 4.697631464285009, l1: 0.00010135206278338206, l2: 0.0003684110796551549   Iteration 69 of 100, tot loss = 4.687229961588763, l1: 0.00010111477330202183, l2: 0.0003676082191489421   Iteration 70 of 100, tot loss = 4.741695639065334, l1: 0.00010205939439141989, l2: 0.00037211016585518206   Iteration 71 of 100, tot loss = 4.746018493679208, l1: 0.0001019480105114012, l2: 0.000372653835269452   Iteration 72 of 100, tot loss = 4.73809536629253, l1: 0.00010160940706151046, l2: 0.0003722001258413204   Iteration 73 of 100, tot loss = 4.723059758748094, l1: 0.00010147386338970024, l2: 0.00037083210868518545   Iteration 74 of 100, tot loss = 4.753594540260933, l1: 0.00010186517327973568, l2: 0.00037349427686815426   Iteration 75 of 100, tot loss = 4.7492169062296545, l1: 0.00010188615929412966, l2: 0.00037303552768814066   Iteration 76 of 100, tot loss = 4.761554172164516, l1: 0.00010189953134517724, l2: 0.0003742558822666883   Iteration 77 of 100, tot loss = 4.760934377645517, l1: 0.00010162009404028778, l2: 0.00037447334034368396   Iteration 78 of 100, tot loss = 4.784786707315689, l1: 0.00010204949462487816, l2: 0.00037642917297302914   Iteration 79 of 100, tot loss = 4.774163955374609, l1: 0.00010213413706072901, l2: 0.00037528225552302467   Iteration 80 of 100, tot loss = 4.7614545911550525, l1: 0.00010184296525039827, l2: 0.00037430249103636014   Iteration 81 of 100, tot loss = 4.744483615145271, l1: 0.0001016766620500956, l2: 0.00037277169650662   Iteration 82 of 100, tot loss = 4.767702524254962, l1: 0.0001019763053258563, l2: 0.00037479394423582294   Iteration 83 of 100, tot loss = 4.770628822855203, l1: 0.00010215514444701194, l2: 0.0003749077350002186   Iteration 84 of 100, tot loss = 4.763436564377376, l1: 0.00010212040693399363, l2: 0.00037422324697087916   Iteration 85 of 100, tot loss = 4.764860251370599, l1: 0.00010203556337407517, l2: 0.0003744504592545769   Iteration 86 of 100, tot loss = 4.742274081984232, l1: 0.00010175274622079721, l2: 0.000372474659491482   Iteration 87 of 100, tot loss = 4.739723301481926, l1: 0.00010196777452358804, l2: 0.00037200455339732525   Iteration 88 of 100, tot loss = 4.709020414135673, l1: 0.00010152724875793369, l2: 0.00036937479026991326   Iteration 89 of 100, tot loss = 4.7079357029346935, l1: 0.00010139529731940854, l2: 0.000369398270470739   Iteration 90 of 100, tot loss = 4.7198440180884464, l1: 0.00010175318699718142, l2: 0.0003702312127441271   Iteration 91 of 100, tot loss = 4.71975155715104, l1: 0.0001017036719841289, l2: 0.00037027148137785043   Iteration 92 of 100, tot loss = 4.729305225869884, l1: 0.00010181365747564047, l2: 0.0003711168628006303   Iteration 93 of 100, tot loss = 4.727209865405995, l1: 0.00010145608847710474, l2: 0.000371264895738443   Iteration 94 of 100, tot loss = 4.726195264369883, l1: 0.00010142461921551581, l2: 0.0003711949048397884   Iteration 95 of 100, tot loss = 4.707940631163748, l1: 0.00010126988751213312, l2: 0.00036952417316282857   Iteration 96 of 100, tot loss = 4.717150228718917, l1: 0.00010139903770323144, l2: 0.00037031598230896634   Iteration 97 of 100, tot loss = 4.712197792898748, l1: 0.00010132354149503691, l2: 0.0003698962347848418   Iteration 98 of 100, tot loss = 4.699541449546814, l1: 0.0001013042511120296, l2: 0.0003686498909088669   Iteration 99 of 100, tot loss = 4.698259543890905, l1: 0.00010139323605490247, l2: 0.0003684327152447135   Iteration 100 of 100, tot loss = 4.7085406422615055, l1: 0.00010165135077841114, l2: 0.000369202710717218
   End of epoch 1270; saving model... 

Epoch 1271 of 2000
   Iteration 1 of 100, tot loss = 4.52867317199707, l1: 0.00010599703091429546, l2: 0.0003468702780082822   Iteration 2 of 100, tot loss = 4.030226349830627, l1: 9.558362944517285e-05, l2: 0.0003074389969697222   Iteration 3 of 100, tot loss = 4.3226052125295, l1: 9.804228951300804e-05, l2: 0.0003342182220270236   Iteration 4 of 100, tot loss = 4.383946359157562, l1: 9.703621617518365e-05, l2: 0.000341358405421488   Iteration 5 of 100, tot loss = 4.548677682876587, l1: 0.0001026391750201583, l2: 0.0003522285842336714   Iteration 6 of 100, tot loss = 4.455536921819051, l1: 0.0001026587112088843, l2: 0.0003428949712542817   Iteration 7 of 100, tot loss = 4.496352059500558, l1: 0.00010231366052591641, l2: 0.00034732153289951384   Iteration 8 of 100, tot loss = 4.487451672554016, l1: 0.00010149344598175958, l2: 0.00034725170917226933   Iteration 9 of 100, tot loss = 4.414325926038954, l1: 0.0001004766090773046, l2: 0.0003409559722058475   Iteration 10 of 100, tot loss = 4.362741303443909, l1: 0.00010112570453202352, l2: 0.0003351484163431451   Iteration 11 of 100, tot loss = 4.214315110986883, l1: 9.903058459961109e-05, l2: 0.00032240091770125383   Iteration 12 of 100, tot loss = 4.2348601420720415, l1: 9.855517540321064e-05, l2: 0.0003249308283557184   Iteration 13 of 100, tot loss = 4.38427628003634, l1: 0.00010127310815732926, l2: 0.00033715451046681177   Iteration 14 of 100, tot loss = 4.2558073827198575, l1: 9.784554380790464e-05, l2: 0.0003277351851076154   Iteration 15 of 100, tot loss = 4.335929822921753, l1: 0.0001000294064094002, l2: 0.00033356356628549597   Iteration 16 of 100, tot loss = 4.213628798723221, l1: 9.786224836716428e-05, l2: 0.0003235006224713288   Iteration 17 of 100, tot loss = 4.3051994828616875, l1: 9.988328640568344e-05, l2: 0.0003306366510086638   Iteration 18 of 100, tot loss = 4.331782897313436, l1: 0.00010005598394652932, l2: 0.0003331222962717422   Iteration 19 of 100, tot loss = 4.420735032934892, l1: 0.00010134076498951272, l2: 0.00034073273050843884   Iteration 20 of 100, tot loss = 4.497129011154175, l1: 0.00010292541883245576, l2: 0.00034678747615544123   Iteration 21 of 100, tot loss = 4.658552646636963, l1: 0.00010476948262380791, l2: 0.0003610857765168129   Iteration 22 of 100, tot loss = 4.654798290946267, l1: 0.00010395713450799866, l2: 0.0003615226900861175   Iteration 23 of 100, tot loss = 4.61980446525242, l1: 0.00010456259390238024, l2: 0.0003574178478194644   Iteration 24 of 100, tot loss = 4.623539884885152, l1: 0.00010504662410918779, l2: 0.00035730735908146016   Iteration 25 of 100, tot loss = 4.56754976272583, l1: 0.00010362852277467028, l2: 0.0003531264478806406   Iteration 26 of 100, tot loss = 4.609964444087102, l1: 0.0001037203888140189, l2: 0.0003572760492366237   Iteration 27 of 100, tot loss = 4.682815127902561, l1: 0.00010420966363098059, l2: 0.0003640718436886177   Iteration 28 of 100, tot loss = 4.727685451507568, l1: 0.00010416180962887925, l2: 0.00036860673052225526   Iteration 29 of 100, tot loss = 4.693834674769435, l1: 0.00010364283737325077, l2: 0.00036574062461772097   Iteration 30 of 100, tot loss = 4.653491687774658, l1: 0.00010200845135841519, l2: 0.00036334071191959085   Iteration 31 of 100, tot loss = 4.658619126965923, l1: 0.00010091965809659732, l2: 0.0003649422489557295   Iteration 32 of 100, tot loss = 4.653317913413048, l1: 0.00010155317954740894, l2: 0.0003637786066974513   Iteration 33 of 100, tot loss = 4.6522471977002695, l1: 0.00010222351027716121, l2: 0.00036300120480132824   Iteration 34 of 100, tot loss = 4.649093726102044, l1: 0.00010205996102954755, l2: 0.0003628494074184667   Iteration 35 of 100, tot loss = 4.72019488470895, l1: 0.0001032579769214083, l2: 0.0003687615073951227   Iteration 36 of 100, tot loss = 4.845417009459601, l1: 0.00010528498144897942, l2: 0.00037925671495031565   Iteration 37 of 100, tot loss = 4.8526920370153475, l1: 0.0001056904493944367, l2: 0.00037957875080079446   Iteration 38 of 100, tot loss = 4.816790486636915, l1: 0.00010449280470263482, l2: 0.0003771862408795737   Iteration 39 of 100, tot loss = 4.804867713879316, l1: 0.00010451647251148899, l2: 0.00037597029586322606   Iteration 40 of 100, tot loss = 4.804226511716843, l1: 0.00010443468718221994, l2: 0.0003759879611607175   Iteration 41 of 100, tot loss = 4.778425862149494, l1: 0.00010420824037282728, l2: 0.000373634342927622   Iteration 42 of 100, tot loss = 4.767800007547651, l1: 0.0001040204336147456, l2: 0.0003727595647519809   Iteration 43 of 100, tot loss = 4.841672847437304, l1: 0.00010559576515143542, l2: 0.00037857151771679003   Iteration 44 of 100, tot loss = 4.878718338229439, l1: 0.00010573914807371858, l2: 0.00038213268337792465   Iteration 45 of 100, tot loss = 4.845751598146227, l1: 0.0001052177566937947, l2: 0.0003793574009452843   Iteration 46 of 100, tot loss = 4.841029037599978, l1: 0.00010561061595660209, l2: 0.0003784922851279945   Iteration 47 of 100, tot loss = 4.861905417543777, l1: 0.00010589818230847471, l2: 0.0003802923571090511   Iteration 48 of 100, tot loss = 4.857390716671944, l1: 0.00010564275309358588, l2: 0.0003800963161969169   Iteration 49 of 100, tot loss = 4.838702449993211, l1: 0.00010578167422529671, l2: 0.00037808856827073863   Iteration 50 of 100, tot loss = 4.8164910793304445, l1: 0.00010526432168262546, l2: 0.0003763847838854417   Iteration 51 of 100, tot loss = 4.821314250721651, l1: 0.00010522714479057137, l2: 0.0003769042777290166   Iteration 52 of 100, tot loss = 4.829150493328388, l1: 0.00010579478972571311, l2: 0.00037712025648663536   Iteration 53 of 100, tot loss = 4.841316223144531, l1: 0.00010590605254621105, l2: 0.000378225566511798   Iteration 54 of 100, tot loss = 4.805312779214647, l1: 0.00010538507336200887, l2: 0.00037514620177076994   Iteration 55 of 100, tot loss = 4.795943741364913, l1: 0.00010510174595930783, l2: 0.00037449262586464595   Iteration 56 of 100, tot loss = 4.778641662427357, l1: 0.00010461317853176817, l2: 0.00037325098544118063   Iteration 57 of 100, tot loss = 4.807225281732125, l1: 0.00010512400107823818, l2: 0.00037559852480945554   Iteration 58 of 100, tot loss = 4.856890666073766, l1: 0.00010634423026580219, l2: 0.0003793448345747712   Iteration 59 of 100, tot loss = 4.8308144141051725, l1: 0.00010576749461460306, l2: 0.00037731394498269626   Iteration 60 of 100, tot loss = 4.8560036540031435, l1: 0.00010589617395453387, l2: 0.00037970418998156674   Iteration 61 of 100, tot loss = 4.85634678309081, l1: 0.00010596275489183799, l2: 0.00037967192212341077   Iteration 62 of 100, tot loss = 4.8583631400139105, l1: 0.00010580495918768594, l2: 0.00038003135353304265   Iteration 63 of 100, tot loss = 4.8655679074544755, l1: 0.00010573227723759215, l2: 0.0003808245119649828   Iteration 64 of 100, tot loss = 4.847682666033506, l1: 0.0001052608875511396, l2: 0.0003795073778292135   Iteration 65 of 100, tot loss = 4.844943358347966, l1: 0.00010520098344736302, l2: 0.0003792933512998458   Iteration 66 of 100, tot loss = 4.8083045988371875, l1: 0.00010429685172504944, l2: 0.00037653360711622304   Iteration 67 of 100, tot loss = 4.790367304389156, l1: 0.00010391942976014828, l2: 0.0003751172999496947   Iteration 68 of 100, tot loss = 4.779266946456012, l1: 0.00010401571511037593, l2: 0.00037391097901169867   Iteration 69 of 100, tot loss = 4.74154007607612, l1: 0.00010338997284028058, l2: 0.00037076403416416514   Iteration 70 of 100, tot loss = 4.733672308921814, l1: 0.00010350203156122007, l2: 0.0003698651984450407   Iteration 71 of 100, tot loss = 4.701227963810235, l1: 0.0001030230328389837, l2: 0.0003670997627433652   Iteration 72 of 100, tot loss = 4.719383335775799, l1: 0.00010311379800340446, l2: 0.00036882453504303057   Iteration 73 of 100, tot loss = 4.721517892733012, l1: 0.00010280476092466166, l2: 0.0003693470275284697   Iteration 74 of 100, tot loss = 4.7465907721906095, l1: 0.00010349032576463337, l2: 0.0003711687506340142   Iteration 75 of 100, tot loss = 4.765982764561971, l1: 0.00010361225383045772, l2: 0.00037298602207253375   Iteration 76 of 100, tot loss = 4.76731591789346, l1: 0.00010358178741099484, l2: 0.00037314980344442457   Iteration 77 of 100, tot loss = 4.751361150246162, l1: 0.00010343579215968221, l2: 0.000371700321708189   Iteration 78 of 100, tot loss = 4.75064872778379, l1: 0.00010340919508076369, l2: 0.0003716556765049553   Iteration 79 of 100, tot loss = 4.7263831518873385, l1: 0.0001030972852265533, l2: 0.00036954102872670453   Iteration 80 of 100, tot loss = 4.762781605124474, l1: 0.00010393185275461291, l2: 0.00037234630653983916   Iteration 81 of 100, tot loss = 4.8019885280985894, l1: 0.00010455189647158769, l2: 0.0003756469555406107   Iteration 82 of 100, tot loss = 4.816724384703288, l1: 0.00010479435398493235, l2: 0.0003768780833768954   Iteration 83 of 100, tot loss = 4.802674503211516, l1: 0.00010469461477737515, l2: 0.00037557283466883813   Iteration 84 of 100, tot loss = 4.805680050736382, l1: 0.00010458746423648249, l2: 0.0003759805400650727   Iteration 85 of 100, tot loss = 4.810947976392858, l1: 0.00010463713072514271, l2: 0.00037645766621126846   Iteration 86 of 100, tot loss = 4.784749976424283, l1: 0.00010435016850161171, l2: 0.0003741248285730889   Iteration 87 of 100, tot loss = 4.774867405836609, l1: 0.00010421056169601714, l2: 0.0003732761785310383   Iteration 88 of 100, tot loss = 4.780209099704569, l1: 0.00010404784684810279, l2: 0.00037397306253048805   Iteration 89 of 100, tot loss = 4.826753415418475, l1: 0.00010473194634777316, l2: 0.00037794339446366626   Iteration 90 of 100, tot loss = 4.828412000338236, l1: 0.00010484954998699121, l2: 0.00037799164953563984   Iteration 91 of 100, tot loss = 4.824316818635542, l1: 0.00010473968091441298, l2: 0.00037769200027649096   Iteration 92 of 100, tot loss = 4.808332298112952, l1: 0.0001041410307310719, l2: 0.00037669219850415726   Iteration 93 of 100, tot loss = 4.800747553507487, l1: 0.00010413012921250606, l2: 0.0003759446258153466   Iteration 94 of 100, tot loss = 4.786027657224777, l1: 0.0001040325709293532, l2: 0.00037457019433086896   Iteration 95 of 100, tot loss = 4.792043512745908, l1: 0.00010429225983802768, l2: 0.0003749120908507489   Iteration 96 of 100, tot loss = 4.776115549107392, l1: 0.0001037699120918963, l2: 0.0003738416424615328   Iteration 97 of 100, tot loss = 4.788651581892033, l1: 0.0001040979397057714, l2: 0.00037476721837199727   Iteration 98 of 100, tot loss = 4.753882814426811, l1: 0.0001033872428465378, l2: 0.00037200103839978157   Iteration 99 of 100, tot loss = 4.734240659559616, l1: 0.00010315240988439664, l2: 0.00037027165591199364   Iteration 100 of 100, tot loss = 4.7441002202034, l1: 0.00010320277746359352, l2: 0.0003712072444614023
   End of epoch 1271; saving model... 

Epoch 1272 of 2000
   Iteration 1 of 100, tot loss = 5.373055458068848, l1: 0.00012822021380998194, l2: 0.0004090853326488286   Iteration 2 of 100, tot loss = 4.849447727203369, l1: 0.00011011940296157263, l2: 0.0003748253657249734   Iteration 3 of 100, tot loss = 4.379612445831299, l1: 0.00010640387336025015, l2: 0.0003315573655224095   Iteration 4 of 100, tot loss = 4.247961401939392, l1: 0.00010714010386436712, l2: 0.00031765603489475325   Iteration 5 of 100, tot loss = 4.2014364242553714, l1: 0.00010650418698787689, l2: 0.00031363945454359053   Iteration 6 of 100, tot loss = 4.512554327646892, l1: 0.00010467848187545314, l2: 0.0003465769502023856   Iteration 7 of 100, tot loss = 4.6152938434055875, l1: 0.00010560257422704516, l2: 0.0003559268038121185   Iteration 8 of 100, tot loss = 4.718269884586334, l1: 0.00010914417816820787, l2: 0.0003626828038250096   Iteration 9 of 100, tot loss = 4.859575695461697, l1: 0.0001100884870134501, l2: 0.0003758690792084154   Iteration 10 of 100, tot loss = 4.6893247842788695, l1: 0.00010719869751483203, l2: 0.0003617337759351358   Iteration 11 of 100, tot loss = 4.514936772259799, l1: 0.00010194302012678236, l2: 0.000349550653481856   Iteration 12 of 100, tot loss = 4.451666533946991, l1: 9.979565948015079e-05, l2: 0.0003453709917569843   Iteration 13 of 100, tot loss = 4.374467134475708, l1: 9.812699024153587e-05, l2: 0.0003393197212762271   Iteration 14 of 100, tot loss = 4.307021703038897, l1: 9.789595943792457e-05, l2: 0.0003328062095014112   Iteration 15 of 100, tot loss = 4.447691742579142, l1: 9.831991192186252e-05, l2: 0.0003464492619968951   Iteration 16 of 100, tot loss = 4.416027128696442, l1: 9.751725929163513e-05, l2: 0.0003440854543441674   Iteration 17 of 100, tot loss = 4.419425235075109, l1: 9.8478864375091e-05, l2: 0.0003434636609574013   Iteration 18 of 100, tot loss = 4.496909856796265, l1: 9.939927662748637e-05, l2: 0.00035029171138174005   Iteration 19 of 100, tot loss = 4.529796876405415, l1: 9.899877822087881e-05, l2: 0.0003539809120794464   Iteration 20 of 100, tot loss = 4.540680408477783, l1: 9.8885106126545e-05, l2: 0.00035518293734639885   Iteration 21 of 100, tot loss = 4.661022663116455, l1: 0.00010092695759210204, l2: 0.00036517530956882096   Iteration 22 of 100, tot loss = 4.6076392043720595, l1: 0.00010026085874415003, l2: 0.00036050306252119214   Iteration 23 of 100, tot loss = 4.736224464748217, l1: 0.00010301255584319891, l2: 0.0003706098903658921   Iteration 24 of 100, tot loss = 4.719749410947164, l1: 0.00010341349540491744, l2: 0.0003685614453085388   Iteration 25 of 100, tot loss = 4.699766597747803, l1: 0.00010177078569540753, l2: 0.00036820587352849544   Iteration 26 of 100, tot loss = 4.632071760984568, l1: 0.00010017673272299222, l2: 0.00036303044236354675   Iteration 27 of 100, tot loss = 4.594245398486102, l1: 9.882946404913027e-05, l2: 0.0003605950747711446   Iteration 28 of 100, tot loss = 4.621171082769122, l1: 9.883027191140823e-05, l2: 0.0003632868366756676   Iteration 29 of 100, tot loss = 4.6171662396398085, l1: 9.853574612924573e-05, l2: 0.00036318087900169836   Iteration 30 of 100, tot loss = 4.614310185114543, l1: 9.798837660734231e-05, l2: 0.0003634426427500633   Iteration 31 of 100, tot loss = 4.7121208713900655, l1: 0.00010029871707722064, l2: 0.00037091337059879856   Iteration 32 of 100, tot loss = 4.817550793290138, l1: 0.00010192872787229135, l2: 0.00037982635103617213   Iteration 33 of 100, tot loss = 4.87418981031938, l1: 0.0001026562703455883, l2: 0.00038476270988625896   Iteration 34 of 100, tot loss = 4.88281512260437, l1: 0.00010312681029865737, l2: 0.00038515470163581674   Iteration 35 of 100, tot loss = 4.891919694628034, l1: 0.00010352881093110357, l2: 0.0003856631582103936   Iteration 36 of 100, tot loss = 4.906512035263909, l1: 0.00010423924848307959, l2: 0.00038641195351374336   Iteration 37 of 100, tot loss = 4.974251334731643, l1: 0.00010516998480778893, l2: 0.00039225514715449333   Iteration 38 of 100, tot loss = 4.915260233377156, l1: 0.00010440621228649673, l2: 0.00038711980979362696   Iteration 39 of 100, tot loss = 4.985789696375529, l1: 0.00010567315192206595, l2: 0.0003929058159253775   Iteration 40 of 100, tot loss = 5.051073235273361, l1: 0.00010632779139996273, l2: 0.00039877952985989396   Iteration 41 of 100, tot loss = 5.051728428863898, l1: 0.00010542850249316316, l2: 0.00039974433865409527   Iteration 42 of 100, tot loss = 5.101306773367382, l1: 0.00010635183865642397, l2: 0.00040377883713171327   Iteration 43 of 100, tot loss = 5.106956254604251, l1: 0.00010598465180306067, l2: 0.0004047109724461036   Iteration 44 of 100, tot loss = 5.117114235054363, l1: 0.0001066553324397484, l2: 0.00040505608955175955   Iteration 45 of 100, tot loss = 5.053325425253974, l1: 0.00010558097387224229, l2: 0.00039975156703601694   Iteration 46 of 100, tot loss = 5.0272329320078315, l1: 0.00010544585482262926, l2: 0.0003972774369606708   Iteration 47 of 100, tot loss = 5.013510272857991, l1: 0.00010515237149678448, l2: 0.0003961986546934721   Iteration 48 of 100, tot loss = 4.989755233128865, l1: 0.00010480880928298575, l2: 0.0003941667130978506   Iteration 49 of 100, tot loss = 5.00220200480247, l1: 0.00010525444163156825, l2: 0.00039496575749469734   Iteration 50 of 100, tot loss = 5.0054248046875, l1: 0.0001054294963250868, l2: 0.00039511298266006636   Iteration 51 of 100, tot loss = 5.0169551606271785, l1: 0.00010611356793008014, l2: 0.00039558194654162827   Iteration 52 of 100, tot loss = 4.9879646347119255, l1: 0.0001052691025576608, l2: 0.00039352735928752314   Iteration 53 of 100, tot loss = 5.042486627146883, l1: 0.00010643519539680367, l2: 0.0003978134655143257   Iteration 54 of 100, tot loss = 5.039163399625708, l1: 0.00010597509417156862, l2: 0.0003979412439447414   Iteration 55 of 100, tot loss = 5.087431365793401, l1: 0.00010679484932387079, l2: 0.0004019482852361927   Iteration 56 of 100, tot loss = 5.111631167786462, l1: 0.00010706993167072401, l2: 0.00040409318249398245   Iteration 57 of 100, tot loss = 5.059780656245717, l1: 0.00010600787426116305, l2: 0.00039997018884924617   Iteration 58 of 100, tot loss = 5.061154373760881, l1: 0.00010616695325207447, l2: 0.0003999484818858287   Iteration 59 of 100, tot loss = 5.053452653399969, l1: 0.00010624674597253896, l2: 0.00039909851724495007   Iteration 60 of 100, tot loss = 5.051031335194906, l1: 0.00010643455849882837, l2: 0.00039866857332526706   Iteration 61 of 100, tot loss = 5.025555520761208, l1: 0.00010574762098735473, l2: 0.000396807929243297   Iteration 62 of 100, tot loss = 4.988059124638958, l1: 0.00010529441398478323, l2: 0.00039351149649930096   Iteration 63 of 100, tot loss = 5.016101000800965, l1: 0.00010584565207532179, l2: 0.0003957644461594995   Iteration 64 of 100, tot loss = 5.0375778786838055, l1: 0.00010647342196534737, l2: 0.00039728436422592495   Iteration 65 of 100, tot loss = 5.041322565078735, l1: 0.00010689502299870723, l2: 0.00039723723222358297   Iteration 66 of 100, tot loss = 5.018108241485827, l1: 0.00010655641171703765, l2: 0.00039525441102203774   Iteration 67 of 100, tot loss = 4.990977194771838, l1: 0.00010655058891887763, l2: 0.00039254712942577623   Iteration 68 of 100, tot loss = 5.02761569443871, l1: 0.00010678566725212423, l2: 0.0003959759010553278   Iteration 69 of 100, tot loss = 5.023114162942638, l1: 0.00010675494685647604, l2: 0.0003955564683090652   Iteration 70 of 100, tot loss = 5.033965573992048, l1: 0.00010715112085953088, l2: 0.0003962454350715104   Iteration 71 of 100, tot loss = 5.035160904199305, l1: 0.00010700984001258047, l2: 0.0003965062490569323   Iteration 72 of 100, tot loss = 5.016756739881304, l1: 0.00010639681537819949, l2: 0.000395278857265819   Iteration 73 of 100, tot loss = 4.994104215543564, l1: 0.00010593569200017095, l2: 0.00039347472831077415   Iteration 74 of 100, tot loss = 4.969659405785638, l1: 0.00010591367223438479, l2: 0.0003910522671602037   Iteration 75 of 100, tot loss = 4.949725529352824, l1: 0.0001055964997309881, l2: 0.0003893760519955928   Iteration 76 of 100, tot loss = 4.954048636712526, l1: 0.00010566479585050777, l2: 0.00038974006690152335   Iteration 77 of 100, tot loss = 4.957237519227065, l1: 0.00010565265417083881, l2: 0.00039007109661689393   Iteration 78 of 100, tot loss = 4.92381568138416, l1: 0.00010495031398395673, l2: 0.00038743125314328773   Iteration 79 of 100, tot loss = 4.898584758179097, l1: 0.00010434668517599318, l2: 0.0003855117895829055   Iteration 80 of 100, tot loss = 4.896025449037552, l1: 0.00010448420775901467, l2: 0.0003851183362712618   Iteration 81 of 100, tot loss = 4.915660363656503, l1: 0.00010504938005778744, l2: 0.00038651665485820466   Iteration 82 of 100, tot loss = 4.920479669803527, l1: 0.0001047478394533888, l2: 0.0003873001267315774   Iteration 83 of 100, tot loss = 4.942809955183282, l1: 0.0001050604561940593, l2: 0.00038922053806663277   Iteration 84 of 100, tot loss = 4.961148091724941, l1: 0.00010555699897113733, l2: 0.0003905578092339316   Iteration 85 of 100, tot loss = 4.952474111669204, l1: 0.00010557189290172091, l2: 0.0003896755173199755   Iteration 86 of 100, tot loss = 4.959234060243118, l1: 0.00010550768718869307, l2: 0.0003904157178070353   Iteration 87 of 100, tot loss = 4.938525969954743, l1: 0.00010503569923792216, l2: 0.00038881689690096284   Iteration 88 of 100, tot loss = 4.9363289502534, l1: 0.00010472154351439464, l2: 0.00038891135045560077   Iteration 89 of 100, tot loss = 4.923425564605199, l1: 0.00010475740251177138, l2: 0.000387585152913764   Iteration 90 of 100, tot loss = 4.901006280051337, l1: 0.00010452419887264518, l2: 0.00038557642821817554   Iteration 91 of 100, tot loss = 4.894063331268646, l1: 0.00010449122755045, l2: 0.000384915104375098   Iteration 92 of 100, tot loss = 4.871870427027993, l1: 0.00010383640951856606, l2: 0.00038335063215092066   Iteration 93 of 100, tot loss = 4.904161512210805, l1: 0.0001042668117142673, l2: 0.0003861493380334709   Iteration 94 of 100, tot loss = 4.935914453039778, l1: 0.00010479213713242256, l2: 0.00038879930674583907   Iteration 95 of 100, tot loss = 4.909355976707057, l1: 0.00010431369584311094, l2: 0.0003866219003133378   Iteration 96 of 100, tot loss = 4.888167229791482, l1: 0.0001041724330358799, l2: 0.0003846442885636255   Iteration 97 of 100, tot loss = 4.8812475917265585, l1: 0.00010409115585042452, l2: 0.00038403360195309115   Iteration 98 of 100, tot loss = 4.889150646268105, l1: 0.00010428493676174964, l2: 0.00038463012621220086   Iteration 99 of 100, tot loss = 4.90515217395744, l1: 0.00010455563409263362, l2: 0.0003859595813955453   Iteration 100 of 100, tot loss = 4.901311185359955, l1: 0.0001044714986346662, l2: 0.00038565961804124525
   End of epoch 1272; saving model... 

Epoch 1273 of 2000
   Iteration 1 of 100, tot loss = 5.372599124908447, l1: 0.00010608250886434689, l2: 0.0004311773809604347   Iteration 2 of 100, tot loss = 5.464592456817627, l1: 0.00011294204159639776, l2: 0.00043351721251383424   Iteration 3 of 100, tot loss = 4.2661120891571045, l1: 8.858182385059384e-05, l2: 0.00033802938802788657   Iteration 4 of 100, tot loss = 3.9416202306747437, l1: 8.467429961456219e-05, l2: 0.0003094877247349359   Iteration 5 of 100, tot loss = 4.496830940246582, l1: 9.279684236389585e-05, l2: 0.0003568862623069435   Iteration 6 of 100, tot loss = 4.259818156560262, l1: 9.041575625208982e-05, l2: 0.0003355660689218591   Iteration 7 of 100, tot loss = 4.354589189801898, l1: 9.13045104035908e-05, l2: 0.0003441544166500015   Iteration 8 of 100, tot loss = 4.4030978083610535, l1: 9.570952943249722e-05, l2: 0.0003446002592681907   Iteration 9 of 100, tot loss = 4.377538098229302, l1: 9.709736049343419e-05, l2: 0.0003406564582190994   Iteration 10 of 100, tot loss = 4.47486457824707, l1: 9.891978443192784e-05, l2: 0.00034856667916756124   Iteration 11 of 100, tot loss = 4.53648666902022, l1: 9.926671224424022e-05, l2: 0.00035438195812854576   Iteration 12 of 100, tot loss = 4.573786695798238, l1: 0.00010191735752111224, l2: 0.00035546131160420674   Iteration 13 of 100, tot loss = 4.754495437328632, l1: 0.00010379945692525675, l2: 0.0003716500852113733   Iteration 14 of 100, tot loss = 4.922217641557966, l1: 0.00010590199105666085, l2: 0.00038631977180817297   Iteration 15 of 100, tot loss = 4.766044410069783, l1: 0.00010408423719733643, l2: 0.0003725202016842862   Iteration 16 of 100, tot loss = 4.6537967920303345, l1: 0.00010234789374408138, l2: 0.0003630317833085428   Iteration 17 of 100, tot loss = 4.664640622980454, l1: 0.00010169741439954926, l2: 0.00036476664536166936   Iteration 18 of 100, tot loss = 4.649875561396281, l1: 0.00010273924494362695, l2: 0.00036224830788948265   Iteration 19 of 100, tot loss = 4.688922480532997, l1: 0.00010266845397142645, l2: 0.0003662237914364883   Iteration 20 of 100, tot loss = 4.740720129013061, l1: 0.00010108846563525731, l2: 0.00037298354363883844   Iteration 21 of 100, tot loss = 4.69951513835362, l1: 0.00010158689543632569, l2: 0.0003683646161031599   Iteration 22 of 100, tot loss = 4.72366603938016, l1: 0.00010128318949269173, l2: 0.0003710834114992229   Iteration 23 of 100, tot loss = 4.655880471934443, l1: 0.00010065031702928853, l2: 0.00036493772716747355   Iteration 24 of 100, tot loss = 4.7102201382319135, l1: 0.00010146494075039907, l2: 0.0003695570715838888   Iteration 25 of 100, tot loss = 4.6223180389404295, l1: 9.958649956388399e-05, l2: 0.00036264530208427457   Iteration 26 of 100, tot loss = 4.647332539925208, l1: 0.00010022954190320049, l2: 0.00036450370960385323   Iteration 27 of 100, tot loss = 4.591354502571954, l1: 9.920833771401603e-05, l2: 0.000359927110063533   Iteration 28 of 100, tot loss = 4.6679471135139465, l1: 0.00010146897095962361, l2: 0.00036532573826012334   Iteration 29 of 100, tot loss = 4.639562417720926, l1: 0.00010155517344989269, l2: 0.00036240106645232903   Iteration 30 of 100, tot loss = 4.725593797365824, l1: 0.00010336477328867963, l2: 0.0003691946049608911   Iteration 31 of 100, tot loss = 4.716300079899449, l1: 0.0001029309731224672, l2: 0.00036869903335407856   Iteration 32 of 100, tot loss = 4.661179505288601, l1: 0.00010152556467346585, l2: 0.0003645923839030729   Iteration 33 of 100, tot loss = 4.630298426657012, l1: 0.00010123968957376553, l2: 0.00036179015164221215   Iteration 34 of 100, tot loss = 4.630955864401424, l1: 0.00010101761900467168, l2: 0.0003620779654524727   Iteration 35 of 100, tot loss = 4.625924205780029, l1: 0.00010000574974193504, l2: 0.00036258666950743644   Iteration 36 of 100, tot loss = 4.600439588228862, l1: 9.983085237763589e-05, l2: 0.0003602131049168141   Iteration 37 of 100, tot loss = 4.6170573234558105, l1: 0.0001008943453033115, l2: 0.00036081138524742847   Iteration 38 of 100, tot loss = 4.669658234244899, l1: 0.00010184911767091283, l2: 0.00036511670355378697   Iteration 39 of 100, tot loss = 4.637141331648215, l1: 0.00010121833446856517, l2: 0.0003624957966027208   Iteration 40 of 100, tot loss = 4.603525185585022, l1: 0.00010057797235276666, l2: 0.00035977454426756595   Iteration 41 of 100, tot loss = 4.618678965219638, l1: 0.00010058490842333768, l2: 0.00036128298642125163   Iteration 42 of 100, tot loss = 4.656264906837826, l1: 0.00010080321910555497, l2: 0.00036482326998209046   Iteration 43 of 100, tot loss = 4.619156942811123, l1: 0.00010003373583543201, l2: 0.0003618819569372897   Iteration 44 of 100, tot loss = 4.664086975834587, l1: 0.00010044126206180822, l2: 0.00036596743401870214   Iteration 45 of 100, tot loss = 4.629372061623467, l1: 9.984740309947584e-05, l2: 0.00036308980197645723   Iteration 46 of 100, tot loss = 4.632400466048199, l1: 9.995189762064356e-05, l2: 0.0003632881479975565   Iteration 47 of 100, tot loss = 4.64710385241407, l1: 0.00010079260383908478, l2: 0.00036391778044065736   Iteration 48 of 100, tot loss = 4.634433661897977, l1: 0.00010055410143650079, l2: 0.00036288926397295046   Iteration 49 of 100, tot loss = 4.660277361772498, l1: 0.00010097916711689619, l2: 0.00036504856852472435   Iteration 50 of 100, tot loss = 4.623361325263977, l1: 0.00010007637974922546, l2: 0.0003622597520006821   Iteration 51 of 100, tot loss = 4.599635367299996, l1: 9.988959266361325e-05, l2: 0.0003600739428828306   Iteration 52 of 100, tot loss = 4.58519290960752, l1: 0.00010019267462596942, l2: 0.00035832661524182186   Iteration 53 of 100, tot loss = 4.63009677743012, l1: 0.00010102021069713992, l2: 0.000361989465880401   Iteration 54 of 100, tot loss = 4.720036312385842, l1: 0.00010197690965098984, l2: 0.00037002671969174925   Iteration 55 of 100, tot loss = 4.725862034884366, l1: 0.00010249207791110332, l2: 0.00037009412348693743   Iteration 56 of 100, tot loss = 4.722941032477787, l1: 0.0001025309060683607, l2: 0.0003697631951321715   Iteration 57 of 100, tot loss = 4.706740534096434, l1: 0.00010245972366088576, l2: 0.0003682143277521327   Iteration 58 of 100, tot loss = 4.70664370059967, l1: 0.0001024398350574333, l2: 0.0003682245326766359   Iteration 59 of 100, tot loss = 4.709258172471644, l1: 0.00010246859399219028, l2: 0.0003684572204631769   Iteration 60 of 100, tot loss = 4.764127743244171, l1: 0.0001034014173152779, l2: 0.000373011355016691   Iteration 61 of 100, tot loss = 4.818664906454868, l1: 0.00010439081818388286, l2: 0.0003774756705006737   Iteration 62 of 100, tot loss = 4.808610635419046, l1: 0.00010448118783026603, l2: 0.00037637987359982706   Iteration 63 of 100, tot loss = 4.801412828384884, l1: 0.00010427618020450667, l2: 0.00037586510029902296   Iteration 64 of 100, tot loss = 4.8208754397928715, l1: 0.00010491902799003583, l2: 0.00037716851375080296   Iteration 65 of 100, tot loss = 4.81732508952801, l1: 0.00010478272888576612, l2: 0.0003769497776654764   Iteration 66 of 100, tot loss = 4.821233883048549, l1: 0.00010509188161284493, l2: 0.00037703150398253155   Iteration 67 of 100, tot loss = 4.803838039512065, l1: 0.00010493225087898213, l2: 0.0003754515504923218   Iteration 68 of 100, tot loss = 4.8233495670206405, l1: 0.00010553713881937951, l2: 0.00037679781491854503   Iteration 69 of 100, tot loss = 4.8309857188791465, l1: 0.0001054093449636349, l2: 0.0003776892237668938   Iteration 70 of 100, tot loss = 4.838831302097866, l1: 0.00010558500829834624, l2: 0.0003782981191761792   Iteration 71 of 100, tot loss = 4.837229896599139, l1: 0.000105828158807462, l2: 0.00037789482846331426   Iteration 72 of 100, tot loss = 4.821556852923499, l1: 0.00010561188496972641, l2: 0.0003765437977563124   Iteration 73 of 100, tot loss = 4.834725118663213, l1: 0.00010600042400230998, l2: 0.0003774720858080848   Iteration 74 of 100, tot loss = 4.861078101235467, l1: 0.00010673531643398157, l2: 0.00037937249116775756   Iteration 75 of 100, tot loss = 4.869991099039714, l1: 0.00010671733335281412, l2: 0.0003802817741719385   Iteration 76 of 100, tot loss = 4.887047497849715, l1: 0.00010718928368939822, l2: 0.0003815154636150079   Iteration 77 of 100, tot loss = 4.896838262483671, l1: 0.00010736651364078915, l2: 0.0003823173102354268   Iteration 78 of 100, tot loss = 4.902027661983784, l1: 0.0001073352430420271, l2: 0.0003828675204810376   Iteration 79 of 100, tot loss = 4.874408462379552, l1: 0.00010692950336651617, l2: 0.0003805113401885816   Iteration 80 of 100, tot loss = 4.881503415107727, l1: 0.00010703488378567272, l2: 0.00038111545491119613   Iteration 81 of 100, tot loss = 4.874090995317624, l1: 0.00010690087138856243, l2: 0.00038050822493258037   Iteration 82 of 100, tot loss = 4.863820360928047, l1: 0.00010676934675202796, l2: 0.00037961268620764294   Iteration 83 of 100, tot loss = 4.889817094228354, l1: 0.00010723806382928335, l2: 0.00038174364252257197   Iteration 84 of 100, tot loss = 4.889314577693031, l1: 0.00010735332545577659, l2: 0.00038157812949585993   Iteration 85 of 100, tot loss = 4.891934658499325, l1: 0.00010748805704197901, l2: 0.00038170540612875756   Iteration 86 of 100, tot loss = 4.863588477289954, l1: 0.00010703638487706653, l2: 0.00037932245999569244   Iteration 87 of 100, tot loss = 4.909944051983713, l1: 0.00010796167938595359, l2: 0.00038303272318946003   Iteration 88 of 100, tot loss = 4.894798839634115, l1: 0.00010739060280684498, l2: 0.0003820892788181399   Iteration 89 of 100, tot loss = 4.9605866630425615, l1: 0.00010842132591217328, l2: 0.0003876373388613522   Iteration 90 of 100, tot loss = 4.948175658120049, l1: 0.00010838898269867059, l2: 0.00038642858158305496   Iteration 91 of 100, tot loss = 4.949864806709709, l1: 0.00010843489908038864, l2: 0.00038655158013980426   Iteration 92 of 100, tot loss = 4.962732947391013, l1: 0.00010864810369441884, l2: 0.00038762518907669403   Iteration 93 of 100, tot loss = 4.973600474737024, l1: 0.00010905047340828034, l2: 0.0003883095718832606   Iteration 94 of 100, tot loss = 4.967843867362814, l1: 0.00010887717302892347, l2: 0.00038790721195487384   Iteration 95 of 100, tot loss = 4.974261856079101, l1: 0.00010877147636308923, l2: 0.0003886547075939904   Iteration 96 of 100, tot loss = 4.975450287262599, l1: 0.00010900959845609275, l2: 0.0003885354288589345   Iteration 97 of 100, tot loss = 4.966360087247239, l1: 0.00010891500432146281, l2: 0.00038772100296481177   Iteration 98 of 100, tot loss = 4.9464342715788865, l1: 0.0001084657516392846, l2: 0.00038617767398934624   Iteration 99 of 100, tot loss = 4.9265708176776615, l1: 0.0001080743175848373, l2: 0.00038458276251942446   Iteration 100 of 100, tot loss = 4.965754778385162, l1: 0.000108569080439338, l2: 0.0003880063959513791
   End of epoch 1273; saving model... 

Epoch 1274 of 2000
   Iteration 1 of 100, tot loss = 7.5136799812316895, l1: 0.00013338288408704102, l2: 0.0006179850897751749   Iteration 2 of 100, tot loss = 5.436503052711487, l1: 0.00012087224968126975, l2: 0.0004227780518704094   Iteration 3 of 100, tot loss = 5.083893060684204, l1: 0.00011598020743501063, l2: 0.00039240909488095593   Iteration 4 of 100, tot loss = 5.107734382152557, l1: 0.00011291355804132763, l2: 0.00039785987974028103   Iteration 5 of 100, tot loss = 5.169345235824585, l1: 0.00011227245995542035, l2: 0.0004046620597364381   Iteration 6 of 100, tot loss = 4.700991153717041, l1: 0.00010312840034506128, l2: 0.0003669707099713075   Iteration 7 of 100, tot loss = 4.414172240665981, l1: 9.688919713620894e-05, l2: 0.0003445280246004196   Iteration 8 of 100, tot loss = 4.800590872764587, l1: 0.00010578838464425644, l2: 0.00037427069582918193   Iteration 9 of 100, tot loss = 4.87293455335829, l1: 0.00010763082617712725, l2: 0.0003796626260736957   Iteration 10 of 100, tot loss = 4.675920295715332, l1: 0.0001042032556142658, l2: 0.00036338876961963253   Iteration 11 of 100, tot loss = 4.543172337792137, l1: 0.00010286028290548447, l2: 0.00035145694776226514   Iteration 12 of 100, tot loss = 4.735845585664113, l1: 0.00010613782797008753, l2: 0.0003674467261589598   Iteration 13 of 100, tot loss = 4.728401019023015, l1: 0.00010778149589896202, l2: 0.00036505860155627417   Iteration 14 of 100, tot loss = 4.959763373647418, l1: 0.00011229991650907323, l2: 0.00038367641744636264   Iteration 15 of 100, tot loss = 4.981486876805623, l1: 0.00011368858783195416, l2: 0.00038446009566541763   Iteration 16 of 100, tot loss = 4.899607375264168, l1: 0.00011305082671242417, l2: 0.0003769099066630588   Iteration 17 of 100, tot loss = 4.9618396057802086, l1: 0.00011449000457479783, l2: 0.00038169395017629377   Iteration 18 of 100, tot loss = 5.003964119487339, l1: 0.00011400390495610837, l2: 0.0003863925028579413   Iteration 19 of 100, tot loss = 5.0653321743011475, l1: 0.00011489149083469161, l2: 0.00039164172439517354   Iteration 20 of 100, tot loss = 5.071989905834198, l1: 0.00011509683063195552, l2: 0.00039210215836646964   Iteration 21 of 100, tot loss = 5.015023662930443, l1: 0.00011275671706590358, l2: 0.0003887456474504212   Iteration 22 of 100, tot loss = 5.025522708892822, l1: 0.00011283595515785485, l2: 0.0003897163134586828   Iteration 23 of 100, tot loss = 5.041919418003248, l1: 0.0001118491570156513, l2: 0.00039234278285774684   Iteration 24 of 100, tot loss = 4.991807649532954, l1: 0.00011233775239816168, l2: 0.00038684301095296786   Iteration 25 of 100, tot loss = 4.931878099441528, l1: 0.00011135620821733028, l2: 0.00038183159951586275   Iteration 26 of 100, tot loss = 5.000309293086712, l1: 0.00011142713209175362, l2: 0.00038860379460787115   Iteration 27 of 100, tot loss = 4.974630364665279, l1: 0.00011184377702496325, l2: 0.0003856192574689717   Iteration 28 of 100, tot loss = 5.03016242810658, l1: 0.00011219254286386006, l2: 0.000390823699230428   Iteration 29 of 100, tot loss = 4.96387060757341, l1: 0.00011164199632134866, l2: 0.0003847450641152481   Iteration 30 of 100, tot loss = 4.932623688379923, l1: 0.0001119461531440417, l2: 0.00038131621598343674   Iteration 31 of 100, tot loss = 4.964114727512483, l1: 0.00011226283658611317, l2: 0.00038414863698680196   Iteration 32 of 100, tot loss = 4.920885749161243, l1: 0.00011148182284159702, l2: 0.0003806067529694701   Iteration 33 of 100, tot loss = 4.932883688897798, l1: 0.00011163236437520634, l2: 0.00038165600474740404   Iteration 34 of 100, tot loss = 4.941571943900165, l1: 0.00011247559664565521, l2: 0.0003816815985962475   Iteration 35 of 100, tot loss = 4.898148911339896, l1: 0.00011156428975352485, l2: 0.00037825060218373046   Iteration 36 of 100, tot loss = 4.913444697856903, l1: 0.00011182149521320955, l2: 0.00037952297417278815   Iteration 37 of 100, tot loss = 4.868242218687728, l1: 0.00011159333331994964, l2: 0.00037523088827288737   Iteration 38 of 100, tot loss = 4.922714314962688, l1: 0.00011260127767424197, l2: 0.00037967015439215556   Iteration 39 of 100, tot loss = 4.976907002620208, l1: 0.00011337454522547957, l2: 0.00038431615617950086   Iteration 40 of 100, tot loss = 4.994952303171158, l1: 0.00011338386066199746, l2: 0.0003861113709717756   Iteration 41 of 100, tot loss = 4.957768370465534, l1: 0.00011304840740106046, l2: 0.0003827284313823528   Iteration 42 of 100, tot loss = 4.9465104057675315, l1: 0.00011269427912858581, l2: 0.0003819567635738557   Iteration 43 of 100, tot loss = 4.961987661760907, l1: 0.0001135681934186344, l2: 0.0003826305744655127   Iteration 44 of 100, tot loss = 4.92867940122431, l1: 0.00011274790631432552, l2: 0.00038012003543289296   Iteration 45 of 100, tot loss = 4.911190986633301, l1: 0.00011248583549685362, l2: 0.0003786332649825555   Iteration 46 of 100, tot loss = 4.961651791696963, l1: 0.00011353808329896192, l2: 0.000382627097639235   Iteration 47 of 100, tot loss = 4.959091440160224, l1: 0.00011346875312193198, l2: 0.00038244039287166473   Iteration 48 of 100, tot loss = 4.98232306043307, l1: 0.00011389759644468238, l2: 0.0003843347109674748   Iteration 49 of 100, tot loss = 4.98619439650555, l1: 0.00011435413480099595, l2: 0.00038426530598520246   Iteration 50 of 100, tot loss = 4.945371685028076, l1: 0.0001133592271071393, l2: 0.0003811779426177964   Iteration 51 of 100, tot loss = 4.912892084495694, l1: 0.00011275641080311627, l2: 0.00037853279911146006   Iteration 52 of 100, tot loss = 4.922203444517576, l1: 0.00011301768860833433, l2: 0.0003792026573836875   Iteration 53 of 100, tot loss = 4.910906256369825, l1: 0.00011223308230807372, l2: 0.0003788575446584596   Iteration 54 of 100, tot loss = 4.8605029582977295, l1: 0.00011124017240675769, l2: 0.000374810124749611   Iteration 55 of 100, tot loss = 4.8487079186873006, l1: 0.00011116902533103712, l2: 0.00037370176790070466   Iteration 56 of 100, tot loss = 4.824032868657794, l1: 0.00011032758350536045, l2: 0.0003720757046851629   Iteration 57 of 100, tot loss = 4.863069542667322, l1: 0.0001107012611643305, l2: 0.0003756056950742117   Iteration 58 of 100, tot loss = 4.846523950839865, l1: 0.00011081459116871099, l2: 0.00037383780576858734   Iteration 59 of 100, tot loss = 4.812074285442546, l1: 0.00011049635898000246, l2: 0.0003707110716554857   Iteration 60 of 100, tot loss = 4.781375634670257, l1: 0.00010993551786668832, l2: 0.00036820204768446275   Iteration 61 of 100, tot loss = 4.743803497220649, l1: 0.0001092870137296411, l2: 0.00036509333800555007   Iteration 62 of 100, tot loss = 4.775311266222308, l1: 0.00010981958439515438, l2: 0.00036771154414201455   Iteration 63 of 100, tot loss = 4.780396979952616, l1: 0.00011002611894842371, l2: 0.00036801358071407154   Iteration 64 of 100, tot loss = 4.761645872145891, l1: 0.00010970647093699881, l2: 0.00036645811815105844   Iteration 65 of 100, tot loss = 4.76048390681927, l1: 0.00010974794103486392, l2: 0.00036630045192746015   Iteration 66 of 100, tot loss = 4.7558312090960415, l1: 0.00010993809971268726, l2: 0.00036564502321804565   Iteration 67 of 100, tot loss = 4.728689983709534, l1: 0.00010955134125855585, l2: 0.0003633176588189246   Iteration 68 of 100, tot loss = 4.7506632594501275, l1: 0.00011007375141001402, l2: 0.00036499257625751747   Iteration 69 of 100, tot loss = 4.728633994641512, l1: 0.00010939439849555156, l2: 0.0003634690023634745   Iteration 70 of 100, tot loss = 4.693008705547878, l1: 0.00010864057445100375, l2: 0.00036066029736373043   Iteration 71 of 100, tot loss = 4.671409445749203, l1: 0.00010839555239137656, l2: 0.0003587453934350904   Iteration 72 of 100, tot loss = 4.670013341638777, l1: 0.00010828275824476602, l2: 0.0003587185775460158   Iteration 73 of 100, tot loss = 4.6503833124082385, l1: 0.0001078712967645428, l2: 0.0003571670358101136   Iteration 74 of 100, tot loss = 4.646996527104764, l1: 0.00010785194172058254, l2: 0.00035684771255300557   Iteration 75 of 100, tot loss = 4.621922127405802, l1: 0.00010711130501780038, l2: 0.0003550809094061454   Iteration 76 of 100, tot loss = 4.614139252587369, l1: 0.00010679628097645228, l2: 0.00035461764568217884   Iteration 77 of 100, tot loss = 4.625460250037057, l1: 0.00010690127473699486, l2: 0.00035564475140969766   Iteration 78 of 100, tot loss = 4.638526986806821, l1: 0.0001069354690956131, l2: 0.00035691723072280485   Iteration 79 of 100, tot loss = 4.626458946662613, l1: 0.00010665849945728985, l2: 0.00035598739625037284   Iteration 80 of 100, tot loss = 4.606621697545052, l1: 0.00010657010097929742, l2: 0.0003540920695741079   Iteration 81 of 100, tot loss = 4.610765777988198, l1: 0.00010658157595942073, l2: 0.000354495002093357   Iteration 82 of 100, tot loss = 4.61873940432944, l1: 0.00010675524310677358, l2: 0.0003551186973487436   Iteration 83 of 100, tot loss = 4.607648269239679, l1: 0.00010623808575693115, l2: 0.000354526741030424   Iteration 84 of 100, tot loss = 4.619945066315787, l1: 0.00010635842530367275, l2: 0.00035563608097228486   Iteration 85 of 100, tot loss = 4.613041956284467, l1: 0.00010617719637914835, l2: 0.000355126998928266   Iteration 86 of 100, tot loss = 4.590778849845709, l1: 0.00010568657130216855, l2: 0.00035339131356395653   Iteration 87 of 100, tot loss = 4.587102045958069, l1: 0.00010579925851064103, l2: 0.0003529109461192877   Iteration 88 of 100, tot loss = 4.625846499746496, l1: 0.00010610580027952727, l2: 0.0003564788495912746   Iteration 89 of 100, tot loss = 4.61448140626543, l1: 0.0001058196863146457, l2: 0.0003556284543030027   Iteration 90 of 100, tot loss = 4.624536508984036, l1: 0.00010568453362793661, l2: 0.00035676911761078775   Iteration 91 of 100, tot loss = 4.607637732893556, l1: 0.00010540834861515847, l2: 0.0003553554250994329   Iteration 92 of 100, tot loss = 4.602496776891791, l1: 0.00010514196369710945, l2: 0.00035510771466879703   Iteration 93 of 100, tot loss = 4.616971849113383, l1: 0.0001054087341437617, l2: 0.0003562884513847029   Iteration 94 of 100, tot loss = 4.61141281178657, l1: 0.00010545609551085595, l2: 0.00035568518626647306   Iteration 95 of 100, tot loss = 4.609907323435733, l1: 0.000105269560069581, l2: 0.00035572117301757984   Iteration 96 of 100, tot loss = 4.622897885739803, l1: 0.00010519319850269919, l2: 0.00035709659080869943   Iteration 97 of 100, tot loss = 4.653080215159151, l1: 0.00010581962281499615, l2: 0.00035948839897323475   Iteration 98 of 100, tot loss = 4.6459163719294025, l1: 0.00010581731335058029, l2: 0.0003587743242173836   Iteration 99 of 100, tot loss = 4.64755412544867, l1: 0.00010603349522106594, l2: 0.00035872191765152784   Iteration 100 of 100, tot loss = 4.642307770252228, l1: 0.00010583547184069175, l2: 0.0003583953056659084
   End of epoch 1274; saving model... 

Epoch 1275 of 2000
   Iteration 1 of 100, tot loss = 5.114282131195068, l1: 0.00012996705481782556, l2: 0.0003814611991401762   Iteration 2 of 100, tot loss = 5.22819185256958, l1: 0.00013469527038978413, l2: 0.00038812395359855145   Iteration 3 of 100, tot loss = 4.9077738126118975, l1: 0.00012325452795873085, l2: 0.0003675228702680518   Iteration 4 of 100, tot loss = 5.002737641334534, l1: 0.00012154807882325258, l2: 0.0003787257010117173   Iteration 5 of 100, tot loss = 4.948429584503174, l1: 0.00011347492254571989, l2: 0.00038136804359965024   Iteration 6 of 100, tot loss = 4.9489602247873945, l1: 0.0001086933819654708, l2: 0.00038620264967903495   Iteration 7 of 100, tot loss = 4.991058281489781, l1: 0.00011016412255620318, l2: 0.0003889417151055698   Iteration 8 of 100, tot loss = 4.84142604470253, l1: 0.00010708523677749326, l2: 0.00037705737486248836   Iteration 9 of 100, tot loss = 4.790486892064412, l1: 0.00010487489473436856, l2: 0.0003741738004868643   Iteration 10 of 100, tot loss = 4.738297724723816, l1: 0.00010484331578481942, l2: 0.00036898646503686906   Iteration 11 of 100, tot loss = 5.040622689507225, l1: 0.00011042816649106416, l2: 0.0003936341117051515   Iteration 12 of 100, tot loss = 4.816318035125732, l1: 0.00010503326181302934, l2: 0.00037659855055001873   Iteration 13 of 100, tot loss = 4.612857085007888, l1: 0.00010028742410493299, l2: 0.0003609982925879124   Iteration 14 of 100, tot loss = 4.652907984597342, l1: 0.00010233415481967054, l2: 0.00036295665216000216   Iteration 15 of 100, tot loss = 4.656077734629313, l1: 0.00010221424793902164, l2: 0.00036339353440174215   Iteration 16 of 100, tot loss = 4.572708964347839, l1: 0.00010233819057248184, l2: 0.00035493271479936084   Iteration 17 of 100, tot loss = 4.653994644389433, l1: 0.00010406354532974278, l2: 0.0003613359303391703   Iteration 18 of 100, tot loss = 4.694328281614515, l1: 0.00010540912585889196, l2: 0.00036402371430691954   Iteration 19 of 100, tot loss = 4.711358145663612, l1: 0.00010641188074307712, l2: 0.0003647239454470477   Iteration 20 of 100, tot loss = 4.744015717506409, l1: 0.00010795176749525126, l2: 0.0003664498137368355   Iteration 21 of 100, tot loss = 4.726283890860421, l1: 0.00010667248335223467, l2: 0.00036595591518562287   Iteration 22 of 100, tot loss = 4.721222270618785, l1: 0.00010706193147184835, l2: 0.0003650603036724285   Iteration 23 of 100, tot loss = 4.776390407396399, l1: 0.00010792094310644366, l2: 0.0003697181040795682   Iteration 24 of 100, tot loss = 4.802453180154164, l1: 0.0001088391212154723, l2: 0.0003714062034608408   Iteration 25 of 100, tot loss = 4.791696186065674, l1: 0.00010925680311629548, l2: 0.0003699128207517788   Iteration 26 of 100, tot loss = 4.739134476735042, l1: 0.00010852445652394985, l2: 0.0003653889966349547   Iteration 27 of 100, tot loss = 4.6397690243191185, l1: 0.0001069383754468247, l2: 0.0003570385322543896   Iteration 28 of 100, tot loss = 4.560144705431802, l1: 0.00010476330836744247, l2: 0.0003512511674281476   Iteration 29 of 100, tot loss = 4.489124528292952, l1: 0.00010328540796261474, l2: 0.0003456270496826619   Iteration 30 of 100, tot loss = 4.464740745226542, l1: 0.00010288167153097069, l2: 0.0003435924076863254   Iteration 31 of 100, tot loss = 4.446608912560247, l1: 0.00010253201241627516, l2: 0.00034212888379941785   Iteration 32 of 100, tot loss = 4.507245138287544, l1: 0.00010333912246096588, l2: 0.00034738539670797763   Iteration 33 of 100, tot loss = 4.588061650594075, l1: 0.00010480180582927653, l2: 0.0003540043650322001   Iteration 34 of 100, tot loss = 4.551372745457818, l1: 0.00010406955548473085, l2: 0.0003510677248842138   Iteration 35 of 100, tot loss = 4.557448611940656, l1: 0.00010366620075988717, l2: 0.00035207866541376074   Iteration 36 of 100, tot loss = 4.605840557151371, l1: 0.00010392272593485864, l2: 0.0003566613344850743   Iteration 37 of 100, tot loss = 4.509320700490797, l1: 0.00010200405036763137, l2: 0.0003489280240279874   Iteration 38 of 100, tot loss = 4.56552770576979, l1: 0.00010252513646647505, l2: 0.0003540276388536624   Iteration 39 of 100, tot loss = 4.5951345303119755, l1: 0.00010253490724538166, l2: 0.00035697855081451003   Iteration 40 of 100, tot loss = 4.587202945351601, l1: 0.00010288208586644032, l2: 0.0003558382142728078   Iteration 41 of 100, tot loss = 4.560271370701674, l1: 0.00010254278840431829, l2: 0.00035348435471305715   Iteration 42 of 100, tot loss = 4.6124333909579684, l1: 0.00010388047745176923, l2: 0.0003573628684015213   Iteration 43 of 100, tot loss = 4.556784128033837, l1: 0.000102644865681975, l2: 0.00035303355405123387   Iteration 44 of 100, tot loss = 4.538993334228342, l1: 0.00010270612512447934, l2: 0.00035119321463835974   Iteration 45 of 100, tot loss = 4.535234151946174, l1: 0.00010257777491157564, l2: 0.000350945646480088   Iteration 46 of 100, tot loss = 4.522717208965965, l1: 0.00010298452149942497, l2: 0.0003492872054503886   Iteration 47 of 100, tot loss = 4.514311295874576, l1: 0.00010343729894536428, l2: 0.0003479938361821003   Iteration 48 of 100, tot loss = 4.562045040229957, l1: 0.00010454041110582087, l2: 0.0003516640977068164   Iteration 49 of 100, tot loss = 4.533722821547061, l1: 0.0001039294705207089, l2: 0.00034944281681103406   Iteration 50 of 100, tot loss = 4.544454748630524, l1: 0.00010355161190091167, l2: 0.00035089386816252957   Iteration 51 of 100, tot loss = 4.564021701906242, l1: 0.00010397240946665133, l2: 0.00035242976546036444   Iteration 52 of 100, tot loss = 4.591913665716465, l1: 0.00010428547599216332, l2: 0.00035490589498994476   Iteration 53 of 100, tot loss = 4.601174095891556, l1: 0.00010442878453243295, l2: 0.0003556886294973083   Iteration 54 of 100, tot loss = 4.690394924746619, l1: 0.00010563435885328713, l2: 0.00036340513812589323   Iteration 55 of 100, tot loss = 4.712915379350836, l1: 0.00010539498266139576, l2: 0.00036589655964317815   Iteration 56 of 100, tot loss = 4.717183464339802, l1: 0.0001058598594292042, l2: 0.00036585849128251927   Iteration 57 of 100, tot loss = 4.718135442650109, l1: 0.00010585188979268678, l2: 0.0003659616582138513   Iteration 58 of 100, tot loss = 4.712639818931448, l1: 0.00010610644042979653, l2: 0.00036515754505546345   Iteration 59 of 100, tot loss = 4.667780445793928, l1: 0.00010506394213179948, l2: 0.00036171410576602193   Iteration 60 of 100, tot loss = 4.678787976503372, l1: 0.00010537593079789076, l2: 0.0003625028695144768   Iteration 61 of 100, tot loss = 4.719846856398661, l1: 0.00010587254309641259, l2: 0.00036611214538246055   Iteration 62 of 100, tot loss = 4.7410827202181665, l1: 0.00010619085291945075, l2: 0.00036791742193951994   Iteration 63 of 100, tot loss = 4.714227103051686, l1: 0.00010578816808346246, l2: 0.00036563454526162425   Iteration 64 of 100, tot loss = 4.741156565025449, l1: 0.00010608262027744786, l2: 0.00036803303953547584   Iteration 65 of 100, tot loss = 4.707967096108657, l1: 0.00010545080227669902, l2: 0.00036534591065271973   Iteration 66 of 100, tot loss = 4.693515457890251, l1: 0.00010563491190568487, l2: 0.00036371663725620954   Iteration 67 of 100, tot loss = 4.677631842556284, l1: 0.00010555499324960801, l2: 0.00036220819419580027   Iteration 68 of 100, tot loss = 4.665735632181168, l1: 0.00010551146442594472, l2: 0.0003610621021353462   Iteration 69 of 100, tot loss = 4.651446689730105, l1: 0.00010525843116329929, l2: 0.00035988624100961414   Iteration 70 of 100, tot loss = 4.670850346769606, l1: 0.00010541436256192225, l2: 0.00036167067581638026   Iteration 71 of 100, tot loss = 4.673589157386565, l1: 0.00010523366705763718, l2: 0.00036212525190703547   Iteration 72 of 100, tot loss = 4.649966725044781, l1: 0.00010477474299679873, l2: 0.0003602219327755544   Iteration 73 of 100, tot loss = 4.6331142023818135, l1: 0.0001043371731823642, l2: 0.00035897425062115596   Iteration 74 of 100, tot loss = 4.64741424128816, l1: 0.00010452571124816943, l2: 0.0003602157160679092   Iteration 75 of 100, tot loss = 4.62388068040212, l1: 0.0001041252330954497, l2: 0.00035826283807788664   Iteration 76 of 100, tot loss = 4.611524991298976, l1: 0.00010427317082710368, l2: 0.0003568793310573949   Iteration 77 of 100, tot loss = 4.596296054976327, l1: 0.00010399389826819639, l2: 0.0003556357098512001   Iteration 78 of 100, tot loss = 4.602647211307135, l1: 0.00010394966580534879, l2: 0.0003563150574523323   Iteration 79 of 100, tot loss = 4.604414926299566, l1: 0.00010424321686813631, l2: 0.00035619827772452367   Iteration 80 of 100, tot loss = 4.587108506262302, l1: 0.00010390689994892455, l2: 0.00035480395263221   Iteration 81 of 100, tot loss = 4.597281015949485, l1: 0.00010416311501844407, l2: 0.00035556498905974383   Iteration 82 of 100, tot loss = 4.578725699971362, l1: 0.00010356301435222565, l2: 0.0003543095580669881   Iteration 83 of 100, tot loss = 4.588841525905104, l1: 0.00010363748918619871, l2: 0.0003552466660916121   Iteration 84 of 100, tot loss = 4.581771360976355, l1: 0.00010311501951253463, l2: 0.0003550621192213536   Iteration 85 of 100, tot loss = 4.590961791487301, l1: 0.00010336767907788538, l2: 0.00035572850328582505   Iteration 86 of 100, tot loss = 4.593801241974498, l1: 0.00010329000326956428, l2: 0.00035609012436822903   Iteration 87 of 100, tot loss = 4.609263868167482, l1: 0.00010371369399376556, l2: 0.00035721269674029136   Iteration 88 of 100, tot loss = 4.604523868723349, l1: 0.0001037393727032891, l2: 0.00035671301810163476   Iteration 89 of 100, tot loss = 4.613140988885687, l1: 0.00010383020708104596, l2: 0.00035748389617244324   Iteration 90 of 100, tot loss = 4.612278910477956, l1: 0.00010381678164574421, l2: 0.00035741111391366254   Iteration 91 of 100, tot loss = 4.616287282535008, l1: 0.00010382723145424849, l2: 0.0003578015013852484   Iteration 92 of 100, tot loss = 4.628193366786708, l1: 0.00010387420811837174, l2: 0.0003589451332474077   Iteration 93 of 100, tot loss = 4.6225437284797755, l1: 0.00010401514885816924, l2: 0.0003582392286437924   Iteration 94 of 100, tot loss = 4.642114176395092, l1: 0.00010436558984517259, l2: 0.0003598458318632998   Iteration 95 of 100, tot loss = 4.677052652208428, l1: 0.00010506251373475319, l2: 0.0003626427557288767   Iteration 96 of 100, tot loss = 4.658679625640313, l1: 0.00010486858135057749, l2: 0.0003609993854449082   Iteration 97 of 100, tot loss = 4.6658835570836805, l1: 0.000104907787280138, l2: 0.0003616805727678812   Iteration 98 of 100, tot loss = 4.672045163962306, l1: 0.00010478357744061064, l2: 0.00036242094366960894   Iteration 99 of 100, tot loss = 4.700300449072713, l1: 0.00010518799947499476, l2: 0.0003648420499321403   Iteration 100 of 100, tot loss = 4.720636469125748, l1: 0.00010536186964600347, l2: 0.00036670178138592746
   End of epoch 1275; saving model... 

Epoch 1276 of 2000
   Iteration 1 of 100, tot loss = 2.8292148113250732, l1: 5.429593147709966e-05, l2: 0.00022862553305458277   Iteration 2 of 100, tot loss = 3.2333444356918335, l1: 6.407464752555825e-05, l2: 0.00025925979571184143   Iteration 3 of 100, tot loss = 4.203171491622925, l1: 7.765806852451836e-05, l2: 0.0003426590847084299   Iteration 4 of 100, tot loss = 3.994055449962616, l1: 8.062592132773716e-05, l2: 0.0003187796210113447   Iteration 5 of 100, tot loss = 4.9546849727630615, l1: 9.564597130520269e-05, l2: 0.00039982251764740794   Iteration 6 of 100, tot loss = 5.06654139359792, l1: 0.00010363728142692707, l2: 0.0004030168517298686   Iteration 7 of 100, tot loss = 5.266120195388794, l1: 0.00011046864113138457, l2: 0.00041614337117477717   Iteration 8 of 100, tot loss = 5.1228470504283905, l1: 0.00010915621533058584, l2: 0.0004031284825032344   Iteration 9 of 100, tot loss = 5.011444913016425, l1: 0.00010749888234487217, l2: 0.00039364560345549963   Iteration 10 of 100, tot loss = 4.755241179466248, l1: 0.00010172193724429235, l2: 0.0003738021739991382   Iteration 11 of 100, tot loss = 4.83037560636347, l1: 0.00010381462528708984, l2: 0.00037922292937185955   Iteration 12 of 100, tot loss = 4.757874608039856, l1: 0.00010100638549677872, l2: 0.00037478106969501823   Iteration 13 of 100, tot loss = 4.705962107731746, l1: 9.849026313391872e-05, l2: 0.0003721059407465733   Iteration 14 of 100, tot loss = 4.554504615919931, l1: 9.588330535085074e-05, l2: 0.0003595671504237024   Iteration 15 of 100, tot loss = 4.404155731201172, l1: 9.363898182831083e-05, l2: 0.0003467765859871482   Iteration 16 of 100, tot loss = 4.396974176168442, l1: 9.292406184613355e-05, l2: 0.00034677335224841954   Iteration 17 of 100, tot loss = 4.396759033203125, l1: 9.243797574205982e-05, l2: 0.00034723792358657676   Iteration 18 of 100, tot loss = 4.4408407476213245, l1: 9.279743042295902e-05, l2: 0.00035128664118625846   Iteration 19 of 100, tot loss = 4.47542637272885, l1: 9.421445122968994e-05, l2: 0.0003533281828583169   Iteration 20 of 100, tot loss = 4.478838062286377, l1: 9.373495304316748e-05, l2: 0.00035414884987403636   Iteration 21 of 100, tot loss = 4.554820582980201, l1: 9.548105056940888e-05, l2: 0.00036000100676790767   Iteration 22 of 100, tot loss = 4.517499967054888, l1: 9.637205419659784e-05, l2: 0.0003553779421384785   Iteration 23 of 100, tot loss = 4.451464393864507, l1: 9.665846300777048e-05, l2: 0.00034848797646270174   Iteration 24 of 100, tot loss = 4.4657953679561615, l1: 9.763031387895656e-05, l2: 0.0003489492228254676   Iteration 25 of 100, tot loss = 4.391704139709472, l1: 9.653181827161461e-05, l2: 0.00034263859561178833   Iteration 26 of 100, tot loss = 4.393832518504216, l1: 9.656705683133063e-05, l2: 0.00034281619446119294   Iteration 27 of 100, tot loss = 4.4193764792548285, l1: 9.711009328451905e-05, l2: 0.0003448275550639395   Iteration 28 of 100, tot loss = 4.479414701461792, l1: 9.836304655306906e-05, l2: 0.00034957842529235805   Iteration 29 of 100, tot loss = 4.493921263464566, l1: 9.873014240493548e-05, l2: 0.0003506619847532169   Iteration 30 of 100, tot loss = 4.470008007685343, l1: 9.7937289441082e-05, l2: 0.00034906351211247964   Iteration 31 of 100, tot loss = 4.44020644310982, l1: 9.812758493823029e-05, l2: 0.0003458930601012863   Iteration 32 of 100, tot loss = 4.511918477714062, l1: 0.00010007465107264579, l2: 0.0003511171967147675   Iteration 33 of 100, tot loss = 4.523005044821537, l1: 0.00010022574706729783, l2: 0.00035207475770166087   Iteration 34 of 100, tot loss = 4.480069181498359, l1: 9.94619145143695e-05, l2: 0.00034854500357623156   Iteration 35 of 100, tot loss = 4.452567768096924, l1: 9.924881492874452e-05, l2: 0.0003460079618629866   Iteration 36 of 100, tot loss = 4.442737539609273, l1: 9.917444064437102e-05, l2: 0.000345099312754529   Iteration 37 of 100, tot loss = 4.451571657850936, l1: 9.922641864348505e-05, l2: 0.00034593074618700286   Iteration 38 of 100, tot loss = 4.417639274346201, l1: 9.919312172936962e-05, l2: 0.0003425708047488067   Iteration 39 of 100, tot loss = 4.434026039563692, l1: 9.927776110812258e-05, l2: 0.0003441248424142265   Iteration 40 of 100, tot loss = 4.47066622376442, l1: 0.00010002789094869513, l2: 0.0003470387306151679   Iteration 41 of 100, tot loss = 4.486338993398155, l1: 0.00010066762426016262, l2: 0.0003479662732499447   Iteration 42 of 100, tot loss = 4.440288157690139, l1: 9.970079848980753e-05, l2: 0.0003443280158015633   Iteration 43 of 100, tot loss = 4.456185573755309, l1: 9.98738794086696e-05, l2: 0.00034574467687445236   Iteration 44 of 100, tot loss = 4.519570534879511, l1: 0.00010089645217125177, l2: 0.0003510606008851689   Iteration 45 of 100, tot loss = 4.577834765116374, l1: 0.00010170920140808448, l2: 0.000356074273521598   Iteration 46 of 100, tot loss = 4.655309946640678, l1: 0.00010238836330245249, l2: 0.0003631426291616188   Iteration 47 of 100, tot loss = 4.592731549384746, l1: 0.00010107746547486812, l2: 0.0003581956871710916   Iteration 48 of 100, tot loss = 4.585791649917762, l1: 0.00010117584201907448, l2: 0.0003574033207769389   Iteration 49 of 100, tot loss = 4.5589262928281515, l1: 0.00010071096388558971, l2: 0.0003551816634895584   Iteration 50 of 100, tot loss = 4.529464399814605, l1: 0.00010020248511864338, l2: 0.0003527439531171694   Iteration 51 of 100, tot loss = 4.550112670543147, l1: 0.00010090736581261416, l2: 0.0003541038992504279   Iteration 52 of 100, tot loss = 4.49811297425857, l1: 9.988471762111518e-05, l2: 0.0003499265778653073   Iteration 53 of 100, tot loss = 4.531293565372251, l1: 0.00010055470220494207, l2: 0.00035257465211577927   Iteration 54 of 100, tot loss = 4.5427928257871555, l1: 0.0001012965533138615, l2: 0.0003529827261917052   Iteration 55 of 100, tot loss = 4.53760395266793, l1: 0.00010158475647172468, l2: 0.00035217563616408206   Iteration 56 of 100, tot loss = 4.563769521457808, l1: 0.00010166554251814628, l2: 0.00035471140680393934   Iteration 57 of 100, tot loss = 4.543537459875408, l1: 0.0001017122142417193, l2: 0.00035264152903804194   Iteration 58 of 100, tot loss = 4.509102576765521, l1: 0.00010119209350548396, l2: 0.000349718161330735   Iteration 59 of 100, tot loss = 4.544412986706879, l1: 0.0001016608694661014, l2: 0.0003527804266491715   Iteration 60 of 100, tot loss = 4.56940216422081, l1: 0.00010198764575761743, l2: 0.00035495256815920584   Iteration 61 of 100, tot loss = 4.599932680364515, l1: 0.00010210894583533595, l2: 0.00035788432039419706   Iteration 62 of 100, tot loss = 4.607887116170699, l1: 0.0001022198414448799, l2: 0.0003585688680158778   Iteration 63 of 100, tot loss = 4.673051699759468, l1: 0.00010341888252970954, l2: 0.0003638862854420459   Iteration 64 of 100, tot loss = 4.684855321422219, l1: 0.00010393484626547433, l2: 0.0003645506837983703   Iteration 65 of 100, tot loss = 4.6697028435193575, l1: 0.00010410606437989582, l2: 0.0003628642179627115   Iteration 66 of 100, tot loss = 4.67753818360242, l1: 0.00010392305116648927, l2: 0.0003638307651713011   Iteration 67 of 100, tot loss = 4.661342782760734, l1: 0.00010380751488959667, l2: 0.000362326761789328   Iteration 68 of 100, tot loss = 4.641127391773112, l1: 0.00010326656132854302, l2: 0.0003608461760022157   Iteration 69 of 100, tot loss = 4.633946904237719, l1: 0.00010320542397507317, l2: 0.0003601892645035506   Iteration 70 of 100, tot loss = 4.608059939316341, l1: 0.00010296117824119782, l2: 0.00035784481380168084   Iteration 71 of 100, tot loss = 4.5896116461552365, l1: 0.00010264458958285822, l2: 0.00035631657323085137   Iteration 72 of 100, tot loss = 4.578123057881991, l1: 0.00010273041122369857, l2: 0.0003550818924446099   Iteration 73 of 100, tot loss = 4.573810327542971, l1: 0.00010250832814538264, l2: 0.000354872702115394   Iteration 74 of 100, tot loss = 4.584908393589226, l1: 0.00010299134363948897, l2: 0.0003554994935355451   Iteration 75 of 100, tot loss = 4.563470196723938, l1: 0.00010276275764529904, l2: 0.0003535842600588997   Iteration 76 of 100, tot loss = 4.554154887011177, l1: 0.00010279468810231744, l2: 0.00035262079855831533   Iteration 77 of 100, tot loss = 4.564534647124154, l1: 0.00010327636024252475, l2: 0.00035317710263642495   Iteration 78 of 100, tot loss = 4.553941862705426, l1: 0.00010334583884030461, l2: 0.000352048345704157   Iteration 79 of 100, tot loss = 4.591040673135202, l1: 0.00010371927285782073, l2: 0.00035538479296749905   Iteration 80 of 100, tot loss = 4.614017947018146, l1: 0.00010438771796543733, l2: 0.0003570140754163731   Iteration 81 of 100, tot loss = 4.615945717434824, l1: 0.000104548213579462, l2: 0.0003570463572324104   Iteration 82 of 100, tot loss = 4.597345362349254, l1: 0.00010424827961265337, l2: 0.00035548625565730264   Iteration 83 of 100, tot loss = 4.589021687048027, l1: 0.00010412815403886116, l2: 0.0003547740138363071   Iteration 84 of 100, tot loss = 4.604849002190998, l1: 0.0001043015201416676, l2: 0.00035618337885049794   Iteration 85 of 100, tot loss = 4.599207635486827, l1: 0.00010401081072974621, l2: 0.00035590995154480504   Iteration 86 of 100, tot loss = 4.598351660162904, l1: 0.00010427751984152141, l2: 0.0003555576450010047   Iteration 87 of 100, tot loss = 4.581322046532028, l1: 0.00010379786636939957, l2: 0.00035433433685671194   Iteration 88 of 100, tot loss = 4.587116833437573, l1: 0.00010379817302402279, l2: 0.00035491350933046885   Iteration 89 of 100, tot loss = 4.5771016552207175, l1: 0.00010356117004601891, l2: 0.00035414899443668636   Iteration 90 of 100, tot loss = 4.5884991394148935, l1: 0.00010372840576261903, l2: 0.00035512150708301405   Iteration 91 of 100, tot loss = 4.600869782678374, l1: 0.00010414333495306322, l2: 0.00035594364229697163   Iteration 92 of 100, tot loss = 4.603469792915427, l1: 0.00010433358685353913, l2: 0.00035601339170724435   Iteration 93 of 100, tot loss = 4.605869002239679, l1: 0.00010432799774540028, l2: 0.00035625890192140136   Iteration 94 of 100, tot loss = 4.614354063855841, l1: 0.00010436455952313214, l2: 0.0003570708465710421   Iteration 95 of 100, tot loss = 4.59233046958321, l1: 0.00010378516882682513, l2: 0.0003554478779398395   Iteration 96 of 100, tot loss = 4.571726720780134, l1: 0.00010341881087091072, l2: 0.00035375386111506185   Iteration 97 of 100, tot loss = 4.582194295126138, l1: 0.00010366932260647857, l2: 0.0003545501067773583   Iteration 98 of 100, tot loss = 4.590780293454929, l1: 0.0001037212202892097, l2: 0.00035535680885635774   Iteration 99 of 100, tot loss = 4.618897328473101, l1: 0.00010411152955687915, l2: 0.00035777820316566664   Iteration 100 of 100, tot loss = 4.64284649014473, l1: 0.00010452266629727091, l2: 0.0003597619826905429
   End of epoch 1276; saving model... 

Epoch 1277 of 2000
   Iteration 1 of 100, tot loss = 7.6474738121032715, l1: 0.00015838099352549762, l2: 0.0006063663749955595   Iteration 2 of 100, tot loss = 6.279669523239136, l1: 0.00011300207552267238, l2: 0.0005149648495716974   Iteration 3 of 100, tot loss = 5.255269845326741, l1: 0.00010245936330951129, l2: 0.00042306760830494267   Iteration 4 of 100, tot loss = 5.538923621177673, l1: 0.00011268564594502095, l2: 0.000441206713730935   Iteration 5 of 100, tot loss = 5.324604797363281, l1: 0.00010948403942165897, l2: 0.00042297643958590927   Iteration 6 of 100, tot loss = 4.960275769233704, l1: 0.00010386882543874283, l2: 0.0003921587534326439   Iteration 7 of 100, tot loss = 4.823520966938564, l1: 0.00010176409081655688, l2: 0.0003805880052303629   Iteration 8 of 100, tot loss = 4.941976994276047, l1: 0.00010546785506448941, l2: 0.00038872984441695735   Iteration 9 of 100, tot loss = 5.040032201343113, l1: 0.00010668119971847368, l2: 0.00039732202033822733   Iteration 10 of 100, tot loss = 5.032670903205871, l1: 0.00010741295191110111, l2: 0.00039585413760505616   Iteration 11 of 100, tot loss = 5.019405776804144, l1: 0.00010698854153319685, l2: 0.00039495203368873757   Iteration 12 of 100, tot loss = 5.1116742094357805, l1: 0.00010958795064652804, l2: 0.000401579466900633   Iteration 13 of 100, tot loss = 5.016630191069383, l1: 0.00010796586194863686, l2: 0.00039369715467238653   Iteration 14 of 100, tot loss = 4.89476067679269, l1: 0.00010500440060111162, l2: 0.00038447166610110017   Iteration 15 of 100, tot loss = 5.039177576700847, l1: 0.00010581055636672924, l2: 0.000398107198998332   Iteration 16 of 100, tot loss = 5.247755646705627, l1: 0.00010982850653817877, l2: 0.0004149470551055856   Iteration 17 of 100, tot loss = 5.347705027636359, l1: 0.00011241226542658885, l2: 0.0004223582355360336   Iteration 18 of 100, tot loss = 5.173200262917413, l1: 0.00010861181807639595, l2: 0.00040870820683064975   Iteration 19 of 100, tot loss = 5.120166678177683, l1: 0.00010860332909750899, l2: 0.00040341333767987395   Iteration 20 of 100, tot loss = 5.169922280311584, l1: 0.00010971681767841801, l2: 0.00040727540908847004   Iteration 21 of 100, tot loss = 5.1923429398309615, l1: 0.00011026510626487878, l2: 0.00040896918480506255   Iteration 22 of 100, tot loss = 5.064712340181524, l1: 0.00010727548066246196, l2: 0.000399195751180576   Iteration 23 of 100, tot loss = 5.0694742099098535, l1: 0.00010565130477239701, l2: 0.00040129611443262547   Iteration 24 of 100, tot loss = 5.0350678861141205, l1: 0.00010619720690859442, l2: 0.0003973095814823561   Iteration 25 of 100, tot loss = 5.17247353553772, l1: 0.00010847470824955963, l2: 0.0004087726428406313   Iteration 26 of 100, tot loss = 5.195734088237469, l1: 0.0001083065993812992, l2: 0.00041126680522351165   Iteration 27 of 100, tot loss = 5.295096141320688, l1: 0.00011055522824150373, l2: 0.00041895438173216663   Iteration 28 of 100, tot loss = 5.221939384937286, l1: 0.00010963138876312379, l2: 0.00041256254553445615   Iteration 29 of 100, tot loss = 5.168029875590883, l1: 0.0001096272077622165, l2: 0.00040717577563743266   Iteration 30 of 100, tot loss = 5.131424339612325, l1: 0.0001080723456349612, l2: 0.00040507008525310085   Iteration 31 of 100, tot loss = 5.124056562300651, l1: 0.00010818485093149056, l2: 0.00040422080248038493   Iteration 32 of 100, tot loss = 5.091128535568714, l1: 0.00010778169882996735, l2: 0.0004013311513517692   Iteration 33 of 100, tot loss = 5.059401100332087, l1: 0.00010655640995316912, l2: 0.0003993836973236422   Iteration 34 of 100, tot loss = 5.044556807069218, l1: 0.00010652818405908286, l2: 0.00039792749457239336   Iteration 35 of 100, tot loss = 5.020595393862043, l1: 0.0001066984204434058, l2: 0.0003953611174698121   Iteration 36 of 100, tot loss = 5.023070726129744, l1: 0.00010708351217342877, l2: 0.0003952235585731816   Iteration 37 of 100, tot loss = 4.95929927439303, l1: 0.00010624910420439257, l2: 0.0003896808213195638   Iteration 38 of 100, tot loss = 4.94956846613633, l1: 0.00010633206894501774, l2: 0.0003886247757595572   Iteration 39 of 100, tot loss = 4.929145232225076, l1: 0.00010646470325763147, l2: 0.00038644981769044907   Iteration 40 of 100, tot loss = 4.9007574915885925, l1: 0.00010626069006320904, l2: 0.0003838150569208665   Iteration 41 of 100, tot loss = 4.888297976517096, l1: 0.00010620809766029508, l2: 0.00038262169840802416   Iteration 42 of 100, tot loss = 4.884187800543649, l1: 0.00010598630294788607, l2: 0.00038243247538394785   Iteration 43 of 100, tot loss = 4.880999387696732, l1: 0.00010604397733871782, l2: 0.0003820559599431405   Iteration 44 of 100, tot loss = 4.924685825001109, l1: 0.00010727524411115816, l2: 0.0003851933364454784   Iteration 45 of 100, tot loss = 4.924786557091607, l1: 0.00010757001687630286, l2: 0.00038490863678614714   Iteration 46 of 100, tot loss = 4.975437278332918, l1: 0.00010818749979080167, l2: 0.00038935622717911866   Iteration 47 of 100, tot loss = 4.928086194586246, l1: 0.00010753312295840241, l2: 0.00038527549545409434   Iteration 48 of 100, tot loss = 4.904859557747841, l1: 0.00010715172940460131, l2: 0.00038333422556509805   Iteration 49 of 100, tot loss = 4.8679957195204135, l1: 0.00010689046349416829, l2: 0.0003799091076017452   Iteration 50 of 100, tot loss = 4.9584533214569095, l1: 0.00010844371230632532, l2: 0.00038740161777241157   Iteration 51 of 100, tot loss = 4.955618344101251, l1: 0.00010790443611411177, l2: 0.00038765739579913296   Iteration 52 of 100, tot loss = 4.923957421229436, l1: 0.00010728291894898015, l2: 0.000385112820647639   Iteration 53 of 100, tot loss = 5.017202881147277, l1: 0.00010877653300478826, l2: 0.0003929437532963864   Iteration 54 of 100, tot loss = 4.980062935087416, l1: 0.00010786390595495943, l2: 0.00039014238562365926   Iteration 55 of 100, tot loss = 4.956873447244818, l1: 0.0001074716935611584, l2: 0.0003882156494496898   Iteration 56 of 100, tot loss = 4.98036772438458, l1: 0.00010804393111486985, l2: 0.00038999283944057036   Iteration 57 of 100, tot loss = 4.955264342458625, l1: 0.00010770631023772519, l2: 0.0003878201221181243   Iteration 58 of 100, tot loss = 4.956156500454607, l1: 0.00010790038479548269, l2: 0.00038771526356115295   Iteration 59 of 100, tot loss = 4.954339277946342, l1: 0.00010778750571663002, l2: 0.0003876464206614863   Iteration 60 of 100, tot loss = 4.95231081644694, l1: 0.00010728777754896631, l2: 0.0003879433022423958   Iteration 61 of 100, tot loss = 4.9343495759807645, l1: 0.00010721147507448207, l2: 0.00038622348086496237   Iteration 62 of 100, tot loss = 4.959543904950542, l1: 0.00010780488868946239, l2: 0.00038814950018985976   Iteration 63 of 100, tot loss = 4.971341859726679, l1: 0.0001080763131973245, l2: 0.00038905787126471597   Iteration 64 of 100, tot loss = 4.955547463148832, l1: 0.00010778066211969417, l2: 0.00038777408281021053   Iteration 65 of 100, tot loss = 4.904316828801082, l1: 0.00010673512459526626, l2: 0.00038369655689726083   Iteration 66 of 100, tot loss = 4.92912582917647, l1: 0.0001071048217537199, l2: 0.00038580776052902257   Iteration 67 of 100, tot loss = 4.897974373689339, l1: 0.00010668293516709358, l2: 0.0003831145015228957   Iteration 68 of 100, tot loss = 4.866086328730864, l1: 0.00010594305711171782, l2: 0.000380665575101054   Iteration 69 of 100, tot loss = 4.861650460008262, l1: 0.00010603930591521245, l2: 0.00038012573972135186   Iteration 70 of 100, tot loss = 4.902608415058681, l1: 0.00010675880423929942, l2: 0.00038350203644118403   Iteration 71 of 100, tot loss = 4.886250257492065, l1: 0.00010646449838122221, l2: 0.00038216052667900237   Iteration 72 of 100, tot loss = 4.860815392600165, l1: 0.00010590786203263431, l2: 0.0003801736764014802   Iteration 73 of 100, tot loss = 4.839209592505677, l1: 0.00010578956166744769, l2: 0.0003781313966076234   Iteration 74 of 100, tot loss = 4.843438216157861, l1: 0.00010557742801186553, l2: 0.0003787663926895884   Iteration 75 of 100, tot loss = 4.8940778255462645, l1: 0.00010664079415922363, l2: 0.0003827669873135164   Iteration 76 of 100, tot loss = 4.8922744079640035, l1: 0.00010697935316497215, l2: 0.00038224808648871996   Iteration 77 of 100, tot loss = 4.899114822412466, l1: 0.00010658017038019955, l2: 0.00038333131131177843   Iteration 78 of 100, tot loss = 4.916924265714792, l1: 0.00010677850113074199, l2: 0.00038491392479045317   Iteration 79 of 100, tot loss = 4.905896630468248, l1: 0.00010671760012687455, l2: 0.00038387206227583455   Iteration 80 of 100, tot loss = 4.890609449148178, l1: 0.00010657702541720937, l2: 0.00038248391883826115   Iteration 81 of 100, tot loss = 4.858029656940037, l1: 0.00010583755463386173, l2: 0.0003799654102889982   Iteration 82 of 100, tot loss = 4.857611426493016, l1: 0.00010579804824729076, l2: 0.00037996309330948154   Iteration 83 of 100, tot loss = 4.843529405364071, l1: 0.0001056186240941359, l2: 0.00037873431539488397   Iteration 84 of 100, tot loss = 4.816905393486931, l1: 0.00010527773238642286, l2: 0.0003764128059195909   Iteration 85 of 100, tot loss = 4.791078177620383, l1: 0.00010486785566386329, l2: 0.0003742399610072265   Iteration 86 of 100, tot loss = 4.795351219731708, l1: 0.00010503005991225554, l2: 0.00037450506098075703   Iteration 87 of 100, tot loss = 4.828743794868732, l1: 0.00010542922082722412, l2: 0.0003774451573938667   Iteration 88 of 100, tot loss = 4.828138495033437, l1: 0.00010529927224276683, l2: 0.00037751457636080556   Iteration 89 of 100, tot loss = 4.847205593344871, l1: 0.00010555952803259245, l2: 0.00037916103038502586   Iteration 90 of 100, tot loss = 4.8401228030522665, l1: 0.000105754318105432, l2: 0.000378257961549227   Iteration 91 of 100, tot loss = 4.8255038392412795, l1: 0.00010525439666797775, l2: 0.00037729598669422235   Iteration 92 of 100, tot loss = 4.8369486409684885, l1: 0.00010540633012050935, l2: 0.00037828853348163767   Iteration 93 of 100, tot loss = 4.827239072451028, l1: 0.00010504380618165728, l2: 0.000377680100369147   Iteration 94 of 100, tot loss = 4.811197831275615, l1: 0.00010466808433851296, l2: 0.0003764516978333902   Iteration 95 of 100, tot loss = 4.80797626846715, l1: 0.0001046000953417541, l2: 0.00037619753075935144   Iteration 96 of 100, tot loss = 4.811495500306289, l1: 0.00010483384157093194, l2: 0.00037631570785379154   Iteration 97 of 100, tot loss = 4.828338876212995, l1: 0.00010503353949332581, l2: 0.0003778003472726811   Iteration 98 of 100, tot loss = 4.870963979740532, l1: 0.0001058943127682884, l2: 0.00038120208461935235   Iteration 99 of 100, tot loss = 4.866517199410333, l1: 0.00010550730138390343, l2: 0.00038114441820359175   Iteration 100 of 100, tot loss = 4.885336019992828, l1: 0.00010579681165836518, l2: 0.0003827367899066303
   End of epoch 1277; saving model... 

Epoch 1278 of 2000
   Iteration 1 of 100, tot loss = 4.455992698669434, l1: 0.00011098713002866134, l2: 0.0003346121229697019   Iteration 2 of 100, tot loss = 3.9361863136291504, l1: 0.00010774869224405847, l2: 0.00028586992993950844   Iteration 3 of 100, tot loss = 5.01762851079305, l1: 0.00012268263041429842, l2: 0.00037908020506923396   Iteration 4 of 100, tot loss = 5.3457159996032715, l1: 0.00012734560550597962, l2: 0.0004072259762324393   Iteration 5 of 100, tot loss = 5.36680040359497, l1: 0.0001268869251362048, l2: 0.00040979310870170593   Iteration 6 of 100, tot loss = 5.358370701471965, l1: 0.0001263072493505509, l2: 0.0004095298160488407   Iteration 7 of 100, tot loss = 5.468394483838763, l1: 0.00012317586931333478, l2: 0.0004236635785283787   Iteration 8 of 100, tot loss = 5.665159583091736, l1: 0.00012214303296786966, l2: 0.0004443729267222807   Iteration 9 of 100, tot loss = 5.713432100084093, l1: 0.00012043700876852704, l2: 0.0004509062014727129   Iteration 10 of 100, tot loss = 5.790346384048462, l1: 0.00012189307453809306, l2: 0.0004571415658574551   Iteration 11 of 100, tot loss = 5.7854532328518955, l1: 0.00012055078365268524, l2: 0.0004579945419788022   Iteration 12 of 100, tot loss = 5.738038976987203, l1: 0.00012092548240616452, l2: 0.0004528784168845353   Iteration 13 of 100, tot loss = 5.805502047905555, l1: 0.0001224300699854771, l2: 0.00045812013782919024   Iteration 14 of 100, tot loss = 5.680871895381382, l1: 0.00012226274546784616, l2: 0.00044582444700478973   Iteration 15 of 100, tot loss = 5.617730331420899, l1: 0.00012071707121018941, l2: 0.0004410559660755098   Iteration 16 of 100, tot loss = 5.638114780187607, l1: 0.00012122771886424744, l2: 0.00044258376510697417   Iteration 17 of 100, tot loss = 5.7088093757629395, l1: 0.00012266763480832144, l2: 0.00044821330811828375   Iteration 18 of 100, tot loss = 5.62446223364936, l1: 0.00012110903076568825, l2: 0.0004413371967125891   Iteration 19 of 100, tot loss = 5.4602963422474105, l1: 0.0001185815800502161, l2: 0.00042744805824967394   Iteration 20 of 100, tot loss = 5.382109868526459, l1: 0.00011761163841583766, l2: 0.00042059935294673777   Iteration 21 of 100, tot loss = 5.257079203923543, l1: 0.00011547587477945767, l2: 0.0004102320505938094   Iteration 22 of 100, tot loss = 5.279240727424622, l1: 0.00011535046922455712, l2: 0.0004125736086280085   Iteration 23 of 100, tot loss = 5.305829307307368, l1: 0.00011490338004912938, l2: 0.0004156795577095498   Iteration 24 of 100, tot loss = 5.4031912585099535, l1: 0.00011589373661990976, l2: 0.00042442539597686846   Iteration 25 of 100, tot loss = 5.301292238235473, l1: 0.00011427995399571955, l2: 0.0004158492764690891   Iteration 26 of 100, tot loss = 5.264088163009057, l1: 0.00011322932074947927, l2: 0.00041317950112548156   Iteration 27 of 100, tot loss = 5.243092757684213, l1: 0.00011386895952185753, l2: 0.00041044032130028226   Iteration 28 of 100, tot loss = 5.210959255695343, l1: 0.00011392996982197343, l2: 0.0004071659604960587   Iteration 29 of 100, tot loss = 5.105631335028287, l1: 0.00011154982988453663, l2: 0.00039901330782068445   Iteration 30 of 100, tot loss = 5.068164698282877, l1: 0.00011025626263290178, l2: 0.00039656021108385174   Iteration 31 of 100, tot loss = 5.060662069628315, l1: 0.00011027721707397441, l2: 0.00039578899328086164   Iteration 32 of 100, tot loss = 5.003765106201172, l1: 0.00010845131794212648, l2: 0.0003919251957995584   Iteration 33 of 100, tot loss = 4.9297066096103555, l1: 0.00010730836562067975, l2: 0.0003856622982614984   Iteration 34 of 100, tot loss = 4.93480134711546, l1: 0.00010766722302565434, l2: 0.00038581291514941873   Iteration 35 of 100, tot loss = 4.958990008490426, l1: 0.0001080712466708584, l2: 0.00038782775684792015   Iteration 36 of 100, tot loss = 4.994405541155073, l1: 0.00010893860972929461, l2: 0.00039050194674119767   Iteration 37 of 100, tot loss = 4.994004552428787, l1: 0.00010872809729630103, l2: 0.0003906723593260992   Iteration 38 of 100, tot loss = 5.033952844770331, l1: 0.00010864327228582145, l2: 0.0003947520132274612   Iteration 39 of 100, tot loss = 4.972887895046136, l1: 0.00010788479556093733, l2: 0.0003894039949115652   Iteration 40 of 100, tot loss = 4.943712520599365, l1: 0.00010795947264341521, l2: 0.00038641178034595216   Iteration 41 of 100, tot loss = 4.982791749442496, l1: 0.00010884180869097913, l2: 0.00038943736652116766   Iteration 42 of 100, tot loss = 4.952810872168768, l1: 0.00010798122379388328, l2: 0.0003872998640872538   Iteration 43 of 100, tot loss = 5.039883741112643, l1: 0.00010923248471023494, l2: 0.00039475589048463937   Iteration 44 of 100, tot loss = 5.011680814352903, l1: 0.00010847089031978447, l2: 0.00039269719193477863   Iteration 45 of 100, tot loss = 5.0063003698984785, l1: 0.00010840841269883741, l2: 0.00039222162514407604   Iteration 46 of 100, tot loss = 5.03158844553906, l1: 0.00010821082983254556, l2: 0.0003949480147718492   Iteration 47 of 100, tot loss = 5.046880787991463, l1: 0.00010864253326485112, l2: 0.00039604554532412834   Iteration 48 of 100, tot loss = 5.089182073871295, l1: 0.00010958374809888483, l2: 0.0003993344592648403   Iteration 49 of 100, tot loss = 5.096314853551436, l1: 0.00010950288300851494, l2: 0.00040012860209776127   Iteration 50 of 100, tot loss = 5.074960627555847, l1: 0.00010902430840360466, l2: 0.0003984717541607097   Iteration 51 of 100, tot loss = 5.045834228104236, l1: 0.00010809764731390055, l2: 0.00039648577541240727   Iteration 52 of 100, tot loss = 5.121572223993448, l1: 0.0001090824633558021, l2: 0.00040307475977057876   Iteration 53 of 100, tot loss = 5.11545360763118, l1: 0.00010945820649108357, l2: 0.0004020871551716173   Iteration 54 of 100, tot loss = 5.111352271503872, l1: 0.0001092051001318463, l2: 0.00040193012736617   Iteration 55 of 100, tot loss = 5.124143838882446, l1: 0.00010972147872301072, l2: 0.0004026929048863663   Iteration 56 of 100, tot loss = 5.12601186973708, l1: 0.00011000655552249685, l2: 0.00040259463120102216   Iteration 57 of 100, tot loss = 5.1317632700267595, l1: 0.00010992642337601355, l2: 0.00040324990384346036   Iteration 58 of 100, tot loss = 5.103886871502318, l1: 0.00010958221581200508, l2: 0.0004008064715890214   Iteration 59 of 100, tot loss = 5.13240399199017, l1: 0.0001104102551742749, l2: 0.00040283014403924474   Iteration 60 of 100, tot loss = 5.143975055217743, l1: 0.00011050372553048268, l2: 0.0004038937797304243   Iteration 61 of 100, tot loss = 5.1293990299349925, l1: 0.00011006954821927823, l2: 0.0004028703546968335   Iteration 62 of 100, tot loss = 5.090113312967362, l1: 0.00010964232927606997, l2: 0.00039936900195184976   Iteration 63 of 100, tot loss = 5.091658641421605, l1: 0.00010957152104339681, l2: 0.0003995943434780375   Iteration 64 of 100, tot loss = 5.066434126347303, l1: 0.00010926047974635367, l2: 0.00039738293298796634   Iteration 65 of 100, tot loss = 5.082125843488253, l1: 0.00010927748768993366, l2: 0.00039893509685778275   Iteration 66 of 100, tot loss = 5.076126160043659, l1: 0.00010918243418872765, l2: 0.0003984301817554021   Iteration 67 of 100, tot loss = 5.101440404778097, l1: 0.00010930985081358813, l2: 0.0004008341899468327   Iteration 68 of 100, tot loss = 5.084815130514257, l1: 0.00010903596402760526, l2: 0.0003994455490850241   Iteration 69 of 100, tot loss = 5.104741891225179, l1: 0.00010912529920671554, l2: 0.0004013488895894177   Iteration 70 of 100, tot loss = 5.113435370581491, l1: 0.00010940977907531695, l2: 0.00040193375773794416   Iteration 71 of 100, tot loss = 5.123717193872157, l1: 0.00010940514362054798, l2: 0.0004029665753068271   Iteration 72 of 100, tot loss = 5.120080557134417, l1: 0.00010938131350081271, l2: 0.0004026267419653272   Iteration 73 of 100, tot loss = 5.128112806032782, l1: 0.00010969471637907514, l2: 0.0004031165640112268   Iteration 74 of 100, tot loss = 5.104710830224527, l1: 0.00010908480392056329, l2: 0.00040138627888597046   Iteration 75 of 100, tot loss = 5.104886411031087, l1: 0.0001093461794759302, l2: 0.00040114246114778023   Iteration 76 of 100, tot loss = 5.087526029662082, l1: 0.00010897781914153663, l2: 0.00039977478330624044   Iteration 77 of 100, tot loss = 5.063114026924232, l1: 0.00010860491115577192, l2: 0.000397706490883894   Iteration 78 of 100, tot loss = 5.062213466717647, l1: 0.00010859543129836078, l2: 0.00039762591507333593   Iteration 79 of 100, tot loss = 5.069245643253568, l1: 0.00010891173850366244, l2: 0.0003980128252967202   Iteration 80 of 100, tot loss = 5.051334726810455, l1: 0.00010859121043722552, l2: 0.00039654226166021547   Iteration 81 of 100, tot loss = 5.064872635735406, l1: 0.00010860142217019148, l2: 0.00039788584094092156   Iteration 82 of 100, tot loss = 5.103704417624125, l1: 0.00010937111379994681, l2: 0.0004009993277666182   Iteration 83 of 100, tot loss = 5.0937162192471055, l1: 0.0001093361415659745, l2: 0.00040003548043847354   Iteration 84 of 100, tot loss = 5.093697587649028, l1: 0.00010966715529191009, l2: 0.00039970260349080143   Iteration 85 of 100, tot loss = 5.100874536177692, l1: 0.00010970692515804651, l2: 0.00040038052864153597   Iteration 86 of 100, tot loss = 5.088939833086591, l1: 0.00010957607645291001, l2: 0.0003993179074483166   Iteration 87 of 100, tot loss = 5.078444009539725, l1: 0.00010941665166829734, l2: 0.00039842775012580573   Iteration 88 of 100, tot loss = 5.106680859218944, l1: 0.0001098507448825165, l2: 0.00040081734220042233   Iteration 89 of 100, tot loss = 5.066783866185821, l1: 0.00010902616707150291, l2: 0.00039765222059900274   Iteration 90 of 100, tot loss = 5.078087153699663, l1: 0.000108926562340154, l2: 0.0003988821535636412   Iteration 91 of 100, tot loss = 5.094610281043, l1: 0.00010947460180102429, l2: 0.00039998642631082357   Iteration 92 of 100, tot loss = 5.081583324981772, l1: 0.0001094811815045946, l2: 0.00039867715096751573   Iteration 93 of 100, tot loss = 5.069716600961582, l1: 0.00010935615692462933, l2: 0.00039761550312984473   Iteration 94 of 100, tot loss = 5.073536196921734, l1: 0.0001096696856969155, l2: 0.0003976839340813309   Iteration 95 of 100, tot loss = 5.060516444005464, l1: 0.00010962870338086137, l2: 0.00039642294110595495   Iteration 96 of 100, tot loss = 5.055997293442488, l1: 0.00010969871558093776, l2: 0.00039590101406853745   Iteration 97 of 100, tot loss = 5.054911904728289, l1: 0.00010967224289677538, l2: 0.00039581894778860155   Iteration 98 of 100, tot loss = 5.050993818409589, l1: 0.00010968109036555837, l2: 0.0003954182918408021   Iteration 99 of 100, tot loss = 5.055748073741643, l1: 0.00010997149409817983, l2: 0.0003956033134006312   Iteration 100 of 100, tot loss = 5.0711645567417145, l1: 0.00011035875162633602, l2: 0.0003967577045114012
   End of epoch 1278; saving model... 

Epoch 1279 of 2000
   Iteration 1 of 100, tot loss = 6.40043306350708, l1: 0.00011099051334895194, l2: 0.0005290528060868382   Iteration 2 of 100, tot loss = 4.801646828651428, l1: 9.883789243758656e-05, l2: 0.000381326797651127   Iteration 3 of 100, tot loss = 4.989618539810181, l1: 0.00011029699089704081, l2: 0.0003886648628395051   Iteration 4 of 100, tot loss = 4.793178379535675, l1: 0.00010578072942735162, l2: 0.0003735371137736365   Iteration 5 of 100, tot loss = 5.047541666030884, l1: 0.00011418328649597243, l2: 0.00039057089015841483   Iteration 6 of 100, tot loss = 4.599834044774373, l1: 0.00010313078200852033, l2: 0.00035685262992046773   Iteration 7 of 100, tot loss = 4.69914790562221, l1: 0.00010276336096077492, l2: 0.00036715143193889944   Iteration 8 of 100, tot loss = 4.970674693584442, l1: 0.00010841697212526924, l2: 0.0003886505000991747   Iteration 9 of 100, tot loss = 5.010066509246826, l1: 0.00010754497513213816, l2: 0.0003934616817989283   Iteration 10 of 100, tot loss = 4.925641012191773, l1: 0.00010499784584681038, l2: 0.00038756626017857345   Iteration 11 of 100, tot loss = 4.946392059326172, l1: 0.00010769331559210762, l2: 0.0003869458953638307   Iteration 12 of 100, tot loss = 5.003539562225342, l1: 0.00010954012668662472, l2: 0.0003908138338980886   Iteration 13 of 100, tot loss = 4.886578981692974, l1: 0.00010654273529116136, l2: 0.00038211516668805137   Iteration 14 of 100, tot loss = 4.771688801901681, l1: 0.00010598882077569474, l2: 0.0003711800610679867   Iteration 15 of 100, tot loss = 4.812843100229899, l1: 0.00010529291976126842, l2: 0.0003759913922597965   Iteration 16 of 100, tot loss = 4.811491072177887, l1: 0.00010501159908926638, l2: 0.0003761375101021258   Iteration 17 of 100, tot loss = 4.906107706182143, l1: 0.00010794022241230671, l2: 0.00038267055060714483   Iteration 18 of 100, tot loss = 4.764746997091505, l1: 0.00010445622238977294, l2: 0.0003720184799426028   Iteration 19 of 100, tot loss = 4.756116528260081, l1: 0.00010490521641546174, l2: 0.0003707064369289008   Iteration 20 of 100, tot loss = 4.754825079441071, l1: 0.00010409069145680405, l2: 0.0003713918173161801   Iteration 21 of 100, tot loss = 4.690080313455491, l1: 0.00010288473034355168, l2: 0.00036612330240175304   Iteration 22 of 100, tot loss = 4.6695556965741245, l1: 0.00010347351847237654, l2: 0.00036348205321701244   Iteration 23 of 100, tot loss = 4.558997745099275, l1: 0.00010146386051454338, l2: 0.00035443591617007297   Iteration 24 of 100, tot loss = 4.521661152442296, l1: 0.00010085163330586511, l2: 0.0003513144850633883   Iteration 25 of 100, tot loss = 4.484939203262329, l1: 0.00010056679937406443, l2: 0.00034792712423950436   Iteration 26 of 100, tot loss = 4.523995298605699, l1: 0.00010175207815056917, l2: 0.000350647454839558   Iteration 27 of 100, tot loss = 4.618220567703247, l1: 0.00010276100093270307, l2: 0.0003590610587572748   Iteration 28 of 100, tot loss = 4.735852335180555, l1: 0.00010477043114016331, l2: 0.00036881480627926067   Iteration 29 of 100, tot loss = 4.758681091768988, l1: 0.00010473635898208534, l2: 0.00037113175394238326   Iteration 30 of 100, tot loss = 4.717713554700215, l1: 0.00010463564285601024, l2: 0.0003671357165634011   Iteration 31 of 100, tot loss = 4.623643886658453, l1: 0.00010242134838687977, l2: 0.00035994304409776363   Iteration 32 of 100, tot loss = 4.607060845941305, l1: 0.00010240103506475862, l2: 0.00035830505248668487   Iteration 33 of 100, tot loss = 4.629232793143301, l1: 0.00010217445180222222, l2: 0.000360748829817484   Iteration 34 of 100, tot loss = 4.70348081518622, l1: 0.00010274002338786992, l2: 0.0003676080604611129   Iteration 35 of 100, tot loss = 4.71297995362963, l1: 0.00010283519181289844, l2: 0.00036846280522045814   Iteration 36 of 100, tot loss = 4.716768592596054, l1: 0.00010271397271329381, l2: 0.00036896288858972385   Iteration 37 of 100, tot loss = 4.7383016734509855, l1: 0.00010351905134361791, l2: 0.00037031111699164013   Iteration 38 of 100, tot loss = 4.722098096420891, l1: 0.00010391074293262423, l2: 0.00036829906802220077   Iteration 39 of 100, tot loss = 4.75750998655955, l1: 0.00010506248453076786, l2: 0.0003706885144287625   Iteration 40 of 100, tot loss = 4.74002535045147, l1: 0.00010517792943574023, l2: 0.00036882460590277334   Iteration 41 of 100, tot loss = 4.67542081053664, l1: 0.00010419892049224184, l2: 0.00036334316104241624   Iteration 42 of 100, tot loss = 4.668864366554079, l1: 0.00010409722398478716, l2: 0.0003627892140121687   Iteration 43 of 100, tot loss = 4.640747061995572, l1: 0.00010330439731343325, l2: 0.000360770310074914   Iteration 44 of 100, tot loss = 4.604837788776918, l1: 0.00010306978947483003, l2: 0.00035741399055537346   Iteration 45 of 100, tot loss = 4.579048461384243, l1: 0.00010270420963125718, l2: 0.0003552006376493308   Iteration 46 of 100, tot loss = 4.623140747132509, l1: 0.00010390985461645354, l2: 0.0003584042209220807   Iteration 47 of 100, tot loss = 4.596911554640912, l1: 0.0001035072615350339, l2: 0.0003561838944888416   Iteration 48 of 100, tot loss = 4.612205110490322, l1: 0.00010430067656367707, l2: 0.00035691983430297114   Iteration 49 of 100, tot loss = 4.603485742393805, l1: 0.00010402404215562214, l2: 0.0003563245314135387   Iteration 50 of 100, tot loss = 4.62810921907425, l1: 0.00010412200761493295, l2: 0.0003586889134021476   Iteration 51 of 100, tot loss = 4.640910534297719, l1: 0.00010409400576962998, l2: 0.00035999704713878385   Iteration 52 of 100, tot loss = 4.678374329438577, l1: 0.00010456850465673667, l2: 0.0003632689275456449   Iteration 53 of 100, tot loss = 4.70770604880351, l1: 0.00010541330897776445, l2: 0.0003653572957945179   Iteration 54 of 100, tot loss = 4.717497598241876, l1: 0.00010547455814149644, l2: 0.0003662752009134878   Iteration 55 of 100, tot loss = 4.75643768527291, l1: 0.00010621867602987384, l2: 0.00036942509151148523   Iteration 56 of 100, tot loss = 4.754710778594017, l1: 0.00010608239413808665, l2: 0.00036938868288416415   Iteration 57 of 100, tot loss = 4.727197251821819, l1: 0.00010517482957782289, l2: 0.0003675448947193983   Iteration 58 of 100, tot loss = 4.746137863603131, l1: 0.00010530631434762497, l2: 0.00036930747115422553   Iteration 59 of 100, tot loss = 4.729811512817771, l1: 0.0001053957748808898, l2: 0.0003675853757590245   Iteration 60 of 100, tot loss = 4.726285431782404, l1: 0.00010538245296629612, l2: 0.0003672460894449614   Iteration 61 of 100, tot loss = 4.696958356216306, l1: 0.00010489416797740812, l2: 0.0003648016670505043   Iteration 62 of 100, tot loss = 4.698007185612956, l1: 0.00010486596605210986, l2: 0.00036493475143347054   Iteration 63 of 100, tot loss = 4.73283196441711, l1: 0.0001050531115782048, l2: 0.00036823008364687365   Iteration 64 of 100, tot loss = 4.726285481825471, l1: 0.0001051693509452889, l2: 0.0003674591962408158   Iteration 65 of 100, tot loss = 4.719772252669701, l1: 0.00010539786565529469, l2: 0.0003665793588599907   Iteration 66 of 100, tot loss = 4.719648016221596, l1: 0.00010524474893111002, l2: 0.000366720052129081   Iteration 67 of 100, tot loss = 4.717078545200291, l1: 0.00010547809387077646, l2: 0.000366229759859366   Iteration 68 of 100, tot loss = 4.731953699799145, l1: 0.00010570301289108637, l2: 0.00036749235604314464   Iteration 69 of 100, tot loss = 4.717204455016316, l1: 0.00010560941355188201, l2: 0.00036611103079875636   Iteration 70 of 100, tot loss = 4.741420359270913, l1: 0.00010579527869205256, l2: 0.00036834675577535693   Iteration 71 of 100, tot loss = 4.758214448539304, l1: 0.00010612187917347551, l2: 0.00036969956405736296   Iteration 72 of 100, tot loss = 4.746612060401175, l1: 0.00010597104564011615, l2: 0.0003686901586156131   Iteration 73 of 100, tot loss = 4.742743336991088, l1: 0.00010578507788348561, l2: 0.00036848925392156184   Iteration 74 of 100, tot loss = 4.73322003113257, l1: 0.00010551753350867059, l2: 0.00036780446739489764   Iteration 75 of 100, tot loss = 4.712987780570984, l1: 0.00010515022295294329, l2: 0.0003661485530513649   Iteration 76 of 100, tot loss = 4.72010285760227, l1: 0.00010520920027614154, l2: 0.00036680108333760135   Iteration 77 of 100, tot loss = 4.707581376100515, l1: 0.00010497294876243702, l2: 0.00036578518682455814   Iteration 78 of 100, tot loss = 4.726020120657408, l1: 0.00010502483937904262, l2: 0.0003675771711128525   Iteration 79 of 100, tot loss = 4.716651742971396, l1: 0.00010468816184630335, l2: 0.0003669770109199011   Iteration 80 of 100, tot loss = 4.718641014397145, l1: 0.00010474325281393249, l2: 0.00036712084711325587   Iteration 81 of 100, tot loss = 4.719605859415031, l1: 0.00010462452005020079, l2: 0.0003673360644578129   Iteration 82 of 100, tot loss = 4.707983663896235, l1: 0.00010454662209135344, l2: 0.00036625174266746176   Iteration 83 of 100, tot loss = 4.699390547821321, l1: 0.00010463628500310076, l2: 0.00036530276807445855   Iteration 84 of 100, tot loss = 4.687509120929809, l1: 0.00010448136050572863, l2: 0.00036426954968538626   Iteration 85 of 100, tot loss = 4.679940373757306, l1: 0.00010453919496606378, l2: 0.00036345484053633886   Iteration 86 of 100, tot loss = 4.693808199361313, l1: 0.00010482130435479502, l2: 0.00036455951323110044   Iteration 87 of 100, tot loss = 4.675759431959569, l1: 0.00010430929966066193, l2: 0.00036326664129951864   Iteration 88 of 100, tot loss = 4.661352844400839, l1: 0.00010408713788737342, l2: 0.00036204814436628527   Iteration 89 of 100, tot loss = 4.633440720901061, l1: 0.00010348961684817521, l2: 0.0003598544531138195   Iteration 90 of 100, tot loss = 4.624957549571991, l1: 0.00010325776800325709, l2: 0.000359237984877028   Iteration 91 of 100, tot loss = 4.632505148321718, l1: 0.00010317553972197278, l2: 0.00036007497292702795   Iteration 92 of 100, tot loss = 4.695718074622362, l1: 0.00010411694913035835, l2: 0.00036545485652269514   Iteration 93 of 100, tot loss = 4.682902865512396, l1: 0.00010411654601670971, l2: 0.0003641737386852162   Iteration 94 of 100, tot loss = 4.6837482312892345, l1: 0.00010400363367188355, l2: 0.0003643711879316896   Iteration 95 of 100, tot loss = 4.666652976839166, l1: 0.00010354012694350738, l2: 0.00036312516941076244   Iteration 96 of 100, tot loss = 4.6687825582921505, l1: 0.0001037720398547511, l2: 0.00036310621468752896   Iteration 97 of 100, tot loss = 4.66599941868143, l1: 0.00010370347058831368, l2: 0.00036289646982133736   Iteration 98 of 100, tot loss = 4.646846913561529, l1: 0.00010330059751363386, l2: 0.00036138409231338005   Iteration 99 of 100, tot loss = 4.644343184702324, l1: 0.00010353750034060207, l2: 0.00036089681632783864   Iteration 100 of 100, tot loss = 4.656765562295914, l1: 0.00010386248246504692, l2: 0.00036181407209369355
   End of epoch 1279; saving model... 

Epoch 1280 of 2000
   Iteration 1 of 100, tot loss = 6.368983745574951, l1: 0.00014156870020087808, l2: 0.0004953296738676727   Iteration 2 of 100, tot loss = 5.179583311080933, l1: 0.00011845002882182598, l2: 0.0003995082952314988   Iteration 3 of 100, tot loss = 5.476563453674316, l1: 0.00013031594183606407, l2: 0.00041734039162596065   Iteration 4 of 100, tot loss = 4.839682042598724, l1: 0.0001151278120232746, l2: 0.0003688403812702745   Iteration 5 of 100, tot loss = 5.066127061843872, l1: 0.00011877085780724883, l2: 0.0003878418414387852   Iteration 6 of 100, tot loss = 5.067799210548401, l1: 0.0001177224621642381, l2: 0.00038905744440853596   Iteration 7 of 100, tot loss = 5.397500821522304, l1: 0.0001226797778924395, l2: 0.00041707028867676854   Iteration 8 of 100, tot loss = 5.542913347482681, l1: 0.0001230078505614074, l2: 0.00043128346442244947   Iteration 9 of 100, tot loss = 5.3950799571143255, l1: 0.00011879523663083091, l2: 0.00042071273977247375   Iteration 10 of 100, tot loss = 5.243505477905273, l1: 0.00011602814338402822, l2: 0.00040832238737493755   Iteration 11 of 100, tot loss = 5.091060096567327, l1: 0.00011256208844398233, l2: 0.00039654390738261014   Iteration 12 of 100, tot loss = 5.302944481372833, l1: 0.00011600227662711404, l2: 0.00041429215828732896   Iteration 13 of 100, tot loss = 5.235114336013794, l1: 0.00011447427039876438, l2: 0.0004090371501381294   Iteration 14 of 100, tot loss = 5.2041972534997125, l1: 0.00011455600958600241, l2: 0.0004058637049248708   Iteration 15 of 100, tot loss = 5.192865292231242, l1: 0.00011534749016088124, l2: 0.0004039390264855077   Iteration 16 of 100, tot loss = 5.459121212363243, l1: 0.00011871653032358154, l2: 0.00042719558223325294   Iteration 17 of 100, tot loss = 5.360672866596895, l1: 0.00011737879685824737, l2: 0.0004186884821940433   Iteration 18 of 100, tot loss = 5.341579702165392, l1: 0.00011799405547208153, l2: 0.000416163906467975   Iteration 19 of 100, tot loss = 5.362023202996505, l1: 0.0001178041550845496, l2: 0.0004183981570684792   Iteration 20 of 100, tot loss = 5.366286492347717, l1: 0.00011777312101912685, l2: 0.00041885551909217613   Iteration 21 of 100, tot loss = 5.417875812167213, l1: 0.00011821905943742466, l2: 0.0004235685129450368   Iteration 22 of 100, tot loss = 5.303522662682966, l1: 0.00011593008259131403, l2: 0.00041442217546070674   Iteration 23 of 100, tot loss = 5.302635659342227, l1: 0.00011537629989502223, l2: 0.00041488725907149035   Iteration 24 of 100, tot loss = 5.2771866619586945, l1: 0.00011477439087078285, l2: 0.00041294426955573726   Iteration 25 of 100, tot loss = 5.308505239486695, l1: 0.0001150837546447292, l2: 0.00041576676245313137   Iteration 26 of 100, tot loss = 5.272172533548796, l1: 0.00011387155693507969, l2: 0.00041334568959428   Iteration 27 of 100, tot loss = 5.217423032831262, l1: 0.0001138426784934752, l2: 0.00040789961834282923   Iteration 28 of 100, tot loss = 5.154811382293701, l1: 0.0001124900073981345, l2: 0.00040299112411698194   Iteration 29 of 100, tot loss = 5.07277773166525, l1: 0.00011036620244026538, l2: 0.0003969115641666576   Iteration 30 of 100, tot loss = 5.114308023452759, l1: 0.00011092899161061117, l2: 0.0004005018054158427   Iteration 31 of 100, tot loss = 5.058020584044918, l1: 0.0001100035920136443, l2: 0.00039579846094467587   Iteration 32 of 100, tot loss = 5.061188392341137, l1: 0.00010932035377209104, l2: 0.00039679847986917594   Iteration 33 of 100, tot loss = 5.107654477610732, l1: 0.00011010300052779105, l2: 0.00040066244202369654   Iteration 34 of 100, tot loss = 5.11349636666915, l1: 0.00011027274451728718, l2: 0.0004010768859777326   Iteration 35 of 100, tot loss = 5.060894673211234, l1: 0.0001090167544524385, l2: 0.0003970727070866685   Iteration 36 of 100, tot loss = 5.060633665985531, l1: 0.00010875720474561806, l2: 0.00039730615592108935   Iteration 37 of 100, tot loss = 5.061755882727133, l1: 0.00010787546675970048, l2: 0.0003983001165702385   Iteration 38 of 100, tot loss = 5.014170809795982, l1: 0.00010698196628685477, l2: 0.0003944351096594657   Iteration 39 of 100, tot loss = 5.097099169706687, l1: 0.0001082611875645876, l2: 0.0004014487259645158   Iteration 40 of 100, tot loss = 5.114624798297882, l1: 0.00010871267340917257, l2: 0.0004027498038340127   Iteration 41 of 100, tot loss = 5.104441956775944, l1: 0.00010881666582086259, l2: 0.00040162752755838104   Iteration 42 of 100, tot loss = 5.191364163444156, l1: 0.00011037324527153811, l2: 0.00040876316814406196   Iteration 43 of 100, tot loss = 5.239985288575638, l1: 0.00011143596295665321, l2: 0.00041256256253989195   Iteration 44 of 100, tot loss = 5.256626410917803, l1: 0.0001119069244272065, l2: 0.0004137557132873388   Iteration 45 of 100, tot loss = 5.262731340196398, l1: 0.00011201514854393382, l2: 0.00041425798262935133   Iteration 46 of 100, tot loss = 5.307258357172427, l1: 0.00011254008914756528, l2: 0.00041818574454341814   Iteration 47 of 100, tot loss = 5.25097379785903, l1: 0.00011169795277268388, l2: 0.00041339942501818244   Iteration 48 of 100, tot loss = 5.194353431463242, l1: 0.00011074946246480977, l2: 0.00040868587863466627   Iteration 49 of 100, tot loss = 5.171174088302924, l1: 0.00011031291536970198, l2: 0.0004068044918869641   Iteration 50 of 100, tot loss = 5.193507070541382, l1: 0.0001105713711149292, l2: 0.00040877933468436823   Iteration 51 of 100, tot loss = 5.1701119273316625, l1: 0.00011074398057911928, l2: 0.0004062672107430248   Iteration 52 of 100, tot loss = 5.183267758442805, l1: 0.00011109152301987006, l2: 0.0004072352511194857   Iteration 53 of 100, tot loss = 5.137973475006391, l1: 0.00011004726719583455, l2: 0.00040375007857911216   Iteration 54 of 100, tot loss = 5.1260506356204, l1: 0.0001101206650033688, l2: 0.0004024843971516627   Iteration 55 of 100, tot loss = 5.072117493369363, l1: 0.00010893459065383385, l2: 0.00039827715743079105   Iteration 56 of 100, tot loss = 5.035089837653296, l1: 0.00010791766434132504, l2: 0.0003955913182705574   Iteration 57 of 100, tot loss = 5.048726529405828, l1: 0.00010829151850837087, l2: 0.0003965811333169736   Iteration 58 of 100, tot loss = 5.018916216389886, l1: 0.00010772293968204457, l2: 0.00039416868069032526   Iteration 59 of 100, tot loss = 5.052446248167652, l1: 0.0001085244189583799, l2: 0.00039672020524819147   Iteration 60 of 100, tot loss = 5.069046533107757, l1: 0.00010868522579888425, l2: 0.00039821942724908393   Iteration 61 of 100, tot loss = 5.105122327804565, l1: 0.00010881510108268483, l2: 0.0004016971311028131   Iteration 62 of 100, tot loss = 5.092599949529094, l1: 0.00010906053044422034, l2: 0.00040019946426318417   Iteration 63 of 100, tot loss = 5.0947287234048995, l1: 0.00010930968935662393, l2: 0.00040016318246396997   Iteration 64 of 100, tot loss = 5.065390583127737, l1: 0.00010894243968095907, l2: 0.00039759661808602687   Iteration 65 of 100, tot loss = 5.062378461544331, l1: 0.00010886977727820452, l2: 0.0003973680684934012   Iteration 66 of 100, tot loss = 5.030863100832159, l1: 0.00010860252978366616, l2: 0.00039448377995568353   Iteration 67 of 100, tot loss = 5.070176711722986, l1: 0.00010939565759348516, l2: 0.0003976220132108889   Iteration 68 of 100, tot loss = 5.053710734142976, l1: 0.00010898054359345492, l2: 0.0003963905294179259   Iteration 69 of 100, tot loss = 5.040562166683916, l1: 0.0001086503123561683, l2: 0.0003954059038601438   Iteration 70 of 100, tot loss = 5.0314363070896695, l1: 0.00010868270822226935, l2: 0.0003944609222734081   Iteration 71 of 100, tot loss = 5.002149246108364, l1: 0.0001079973456872837, l2: 0.00039221757869596895   Iteration 72 of 100, tot loss = 4.9997104207674665, l1: 0.0001079948696214059, l2: 0.00039197617196704313   Iteration 73 of 100, tot loss = 4.973611795738952, l1: 0.00010755691509486546, l2: 0.0003898042638479949   Iteration 74 of 100, tot loss = 5.0142951623813525, l1: 0.00010830344487928685, l2: 0.0003931260702351259   Iteration 75 of 100, tot loss = 5.043489084243775, l1: 0.00010905131396915143, l2: 0.00039529759359235563   Iteration 76 of 100, tot loss = 5.009947933648762, l1: 0.00010835359841477934, l2: 0.0003926411939717486   Iteration 77 of 100, tot loss = 4.989334608053232, l1: 0.00010821120545128965, l2: 0.00039072225456936405   Iteration 78 of 100, tot loss = 4.983746773157364, l1: 0.00010809016254288741, l2: 0.00039028451413823624   Iteration 79 of 100, tot loss = 4.951991380015506, l1: 0.0001074322876026795, l2: 0.00038776684959884733   Iteration 80 of 100, tot loss = 4.9474577695131305, l1: 0.0001074241899914341, l2: 0.0003873215859130141   Iteration 81 of 100, tot loss = 4.937953934257413, l1: 0.00010743662718551257, l2: 0.00038635876496109745   Iteration 82 of 100, tot loss = 4.925179118063392, l1: 0.00010724848843670859, l2: 0.00038526942204576123   Iteration 83 of 100, tot loss = 4.891157558165401, l1: 0.00010673313367275903, l2: 0.0003823826209773837   Iteration 84 of 100, tot loss = 4.966318351881845, l1: 0.00010786453605673852, l2: 0.0003887672977552505   Iteration 85 of 100, tot loss = 4.945723121306475, l1: 0.00010757387507884928, l2: 0.0003869984358864124   Iteration 86 of 100, tot loss = 4.933473575946897, l1: 0.0001076154243661264, l2: 0.000385731931939602   Iteration 87 of 100, tot loss = 4.942838761998319, l1: 0.00010729550859682134, l2: 0.00038698836629166557   Iteration 88 of 100, tot loss = 4.93664180690592, l1: 0.00010753766946436372, l2: 0.0003861265099244933   Iteration 89 of 100, tot loss = 4.92036747396662, l1: 0.00010736925101360264, l2: 0.0003846674950550091   Iteration 90 of 100, tot loss = 4.923869448237949, l1: 0.00010775469329221071, l2: 0.00038463225048164735   Iteration 91 of 100, tot loss = 4.926229421909039, l1: 0.0001080475315345441, l2: 0.00038457540990298635   Iteration 92 of 100, tot loss = 4.940014069494993, l1: 0.00010817628334853393, l2: 0.0003858251225408268   Iteration 93 of 100, tot loss = 4.969964542696553, l1: 0.00010867200301138944, l2: 0.0003883244498865679   Iteration 94 of 100, tot loss = 4.996122717857361, l1: 0.00010911933752643737, l2: 0.000390492932792961   Iteration 95 of 100, tot loss = 4.99148507620159, l1: 0.00010874153292541833, l2: 0.00039040697355583116   Iteration 96 of 100, tot loss = 4.959514598051707, l1: 0.00010815496606634649, l2: 0.00038779649260807975   Iteration 97 of 100, tot loss = 4.9595076128379585, l1: 0.00010844395334979989, l2: 0.0003875068068737164   Iteration 98 of 100, tot loss = 4.972931088233481, l1: 0.00010882807227786469, l2: 0.00038846503519714446   Iteration 99 of 100, tot loss = 4.98238997507577, l1: 0.00010887765775539329, l2: 0.0003893613385217678   Iteration 100 of 100, tot loss = 4.956854264736176, l1: 0.00010827787449670723, l2: 0.00038740755058825016
   End of epoch 1280; saving model... 

Epoch 1281 of 2000
   Iteration 1 of 100, tot loss = 3.2793946266174316, l1: 0.00010962056694552302, l2: 0.00021831889171153307   Iteration 2 of 100, tot loss = 4.477680444717407, l1: 0.00010143965118913911, l2: 0.00034632839378900826   Iteration 3 of 100, tot loss = 3.6318214734395347, l1: 8.351019884382065e-05, l2: 0.00027967194910161197   Iteration 4 of 100, tot loss = 3.8742637634277344, l1: 8.770174827077426e-05, l2: 0.00029972462652949616   Iteration 5 of 100, tot loss = 4.067631626129151, l1: 8.931106713134795e-05, l2: 0.00031745209125801923   Iteration 6 of 100, tot loss = 4.451683918635051, l1: 9.720631108696882e-05, l2: 0.0003479620791040361   Iteration 7 of 100, tot loss = 4.436226163591657, l1: 9.744230503981401e-05, l2: 0.0003461803093419543   Iteration 8 of 100, tot loss = 4.6363943219184875, l1: 0.00010127441510121571, l2: 0.00036236501910025254   Iteration 9 of 100, tot loss = 4.3480984634823265, l1: 9.680496683965127e-05, l2: 0.00033800487936888303   Iteration 10 of 100, tot loss = 4.31347086429596, l1: 9.705773045425304e-05, l2: 0.00033428935566917064   Iteration 11 of 100, tot loss = 4.200283050537109, l1: 9.631812413731082e-05, l2: 0.0003237101799723777   Iteration 12 of 100, tot loss = 4.712284167607625, l1: 0.00010358700941045147, l2: 0.0003676414101695021   Iteration 13 of 100, tot loss = 4.80639571409959, l1: 0.00010546734175967195, l2: 0.0003751722307732472   Iteration 14 of 100, tot loss = 4.8383738313402445, l1: 0.00010536503395996988, l2: 0.00037847235216759145   Iteration 15 of 100, tot loss = 4.861551411946615, l1: 0.00010699278791435063, l2: 0.0003791623574215919   Iteration 16 of 100, tot loss = 4.790358543395996, l1: 0.00010578838282526704, l2: 0.0003732474742719205   Iteration 17 of 100, tot loss = 4.805828851812026, l1: 0.00010631333733566434, l2: 0.0003742695501868558   Iteration 18 of 100, tot loss = 4.757866435580784, l1: 0.00010598647309557741, l2: 0.0003698001722417151   Iteration 19 of 100, tot loss = 4.664215752952977, l1: 0.00010509272435324659, l2: 0.00036132885342292294   Iteration 20 of 100, tot loss = 4.58600937128067, l1: 0.00010381331776443403, l2: 0.0003547876229276881   Iteration 21 of 100, tot loss = 4.5834310735975, l1: 0.00010448540290651311, l2: 0.000353857707176801   Iteration 22 of 100, tot loss = 4.665305928750471, l1: 0.00010596495287080126, l2: 0.00036056564237118107   Iteration 23 of 100, tot loss = 4.633524531903475, l1: 0.000105105707125799, l2: 0.00035824674768778294   Iteration 24 of 100, tot loss = 4.580474664767583, l1: 0.00010335839882221383, l2: 0.0003546890693542082   Iteration 25 of 100, tot loss = 4.524872827529907, l1: 0.00010242129326798021, l2: 0.00035006599151529374   Iteration 26 of 100, tot loss = 4.468168900563167, l1: 0.00010165117223200818, l2: 0.00034516571926920174   Iteration 27 of 100, tot loss = 4.474126798135263, l1: 0.00010137290222768638, l2: 0.00034603977864780636   Iteration 28 of 100, tot loss = 4.467695372445243, l1: 0.00010184142541090426, l2: 0.0003449281118394408   Iteration 29 of 100, tot loss = 4.509881068920267, l1: 0.00010212940482045363, l2: 0.00034885870188410424   Iteration 30 of 100, tot loss = 4.580813995997111, l1: 0.00010306022813892924, l2: 0.0003550211724359542   Iteration 31 of 100, tot loss = 4.600988618789181, l1: 0.0001040026031513398, l2: 0.00035609625892022687   Iteration 32 of 100, tot loss = 4.648755460977554, l1: 0.00010439598577249853, l2: 0.0003604795592764276   Iteration 33 of 100, tot loss = 4.645736506490996, l1: 0.00010451526340506406, l2: 0.000360058385977578   Iteration 34 of 100, tot loss = 4.6676245577195115, l1: 0.00010431489392752101, l2: 0.00036244756055941037   Iteration 35 of 100, tot loss = 4.751509053366525, l1: 0.00010608211138917666, l2: 0.00036906879228938905   Iteration 36 of 100, tot loss = 4.674420817030801, l1: 0.0001042614266124373, l2: 0.00036318065354458266   Iteration 37 of 100, tot loss = 4.67820443011619, l1: 0.00010473642465217995, l2: 0.00036308401762948344   Iteration 38 of 100, tot loss = 4.69770549159301, l1: 0.00010517413197943058, l2: 0.00036459641744027307   Iteration 39 of 100, tot loss = 4.674119273821513, l1: 0.00010464836538094693, l2: 0.00036276356229045166   Iteration 40 of 100, tot loss = 4.67367899119854, l1: 0.00010500475691515021, l2: 0.00036236314226698594   Iteration 41 of 100, tot loss = 4.700484296170677, l1: 0.00010523088091825385, l2: 0.0003648175486709877   Iteration 42 of 100, tot loss = 4.721802515642984, l1: 0.0001058669624312426, l2: 0.0003663132883957587   Iteration 43 of 100, tot loss = 4.704872594323269, l1: 0.00010592819087959955, l2: 0.00036455906804401945   Iteration 44 of 100, tot loss = 4.758934397589076, l1: 0.0001062941338948969, l2: 0.0003695993049901021   Iteration 45 of 100, tot loss = 4.7628146251042685, l1: 0.00010689774628392317, l2: 0.0003693837151836811   Iteration 46 of 100, tot loss = 4.7514143897139505, l1: 0.00010677419049898162, l2: 0.00036836724694180504   Iteration 47 of 100, tot loss = 4.742889432196922, l1: 0.00010636942603556003, l2: 0.00036791951594963433   Iteration 48 of 100, tot loss = 4.725442809363206, l1: 0.00010663233494293915, l2: 0.0003659119444516061   Iteration 49 of 100, tot loss = 4.689134313135731, l1: 0.00010597779120351853, l2: 0.00036293563874894566   Iteration 50 of 100, tot loss = 4.680040776729584, l1: 0.00010545453042141162, l2: 0.00036254954582545904   Iteration 51 of 100, tot loss = 4.696019422774222, l1: 0.00010603420392441217, l2: 0.0003635677367485329   Iteration 52 of 100, tot loss = 4.669086311872189, l1: 0.00010557467290849533, l2: 0.00036133395713897277   Iteration 53 of 100, tot loss = 4.698540078019196, l1: 0.00010570728704377236, l2: 0.0003641467192210257   Iteration 54 of 100, tot loss = 4.701222086394274, l1: 0.00010563694498462912, l2: 0.00036448526198337613   Iteration 55 of 100, tot loss = 4.719288177923723, l1: 0.0001060193133360537, l2: 0.0003659095022488724   Iteration 56 of 100, tot loss = 4.74423095371042, l1: 0.00010641590350652612, l2: 0.00036800718953600153   Iteration 57 of 100, tot loss = 4.756955751201563, l1: 0.00010686244173771946, l2: 0.00036883313135292967   Iteration 58 of 100, tot loss = 4.786658437087618, l1: 0.00010727661356213503, l2: 0.00037138922850537144   Iteration 59 of 100, tot loss = 4.741490042815774, l1: 0.00010632668044447694, l2: 0.0003678223221282617   Iteration 60 of 100, tot loss = 4.750855924685796, l1: 0.00010628539839672158, l2: 0.00036880019300345644   Iteration 61 of 100, tot loss = 4.728605643647616, l1: 0.00010608408673328623, l2: 0.00036677647669242357   Iteration 62 of 100, tot loss = 4.765300779573379, l1: 0.00010680467200854178, l2: 0.0003697254055183411   Iteration 63 of 100, tot loss = 4.751038439690121, l1: 0.00010683606483831277, l2: 0.00036826777888188464   Iteration 64 of 100, tot loss = 4.759048560634255, l1: 0.00010665789619679344, l2: 0.00036924695928064466   Iteration 65 of 100, tot loss = 4.744088383821341, l1: 0.00010625141691819477, l2: 0.00036815742115812517   Iteration 66 of 100, tot loss = 4.788494845231374, l1: 0.00010665639103646038, l2: 0.0003721930929300883   Iteration 67 of 100, tot loss = 4.7613671341938755, l1: 0.00010621566328683993, l2: 0.00036992104949930044   Iteration 68 of 100, tot loss = 4.751545595772126, l1: 0.00010603902522715918, l2: 0.0003691155340193826   Iteration 69 of 100, tot loss = 4.741810389187025, l1: 0.0001057067228944304, l2: 0.00036847431539976293   Iteration 70 of 100, tot loss = 4.733956818921225, l1: 0.00010559958219736083, l2: 0.00036779609924581435   Iteration 71 of 100, tot loss = 4.726281265137901, l1: 0.0001055561963728675, l2: 0.00036707192923362094   Iteration 72 of 100, tot loss = 4.728228501147694, l1: 0.00010554765023900675, l2: 0.0003672751986919644   Iteration 73 of 100, tot loss = 4.741968048761969, l1: 0.00010592037057767189, l2: 0.00036827643315805354   Iteration 74 of 100, tot loss = 4.768009706123455, l1: 0.00010632943800324888, l2: 0.00037047153140569255   Iteration 75 of 100, tot loss = 4.78236677646637, l1: 0.00010647827773937025, l2: 0.0003717583988327533   Iteration 76 of 100, tot loss = 4.7759698237243455, l1: 0.00010660348719906516, l2: 0.0003709934945187629   Iteration 77 of 100, tot loss = 4.777927446674991, l1: 0.00010696483736097292, l2: 0.00037082790642033344   Iteration 78 of 100, tot loss = 4.812169232429603, l1: 0.00010707446334634323, l2: 0.00037414245889522135   Iteration 79 of 100, tot loss = 4.851063418991958, l1: 0.00010738196242873298, l2: 0.00037772437819880964   Iteration 80 of 100, tot loss = 4.865592394769192, l1: 0.00010750337200988724, l2: 0.0003790558661421528   Iteration 81 of 100, tot loss = 4.867533876572126, l1: 0.00010756155164994265, l2: 0.0003791918347871368   Iteration 82 of 100, tot loss = 4.8698761971985425, l1: 0.00010761298130446671, l2: 0.00037937463728268065   Iteration 83 of 100, tot loss = 4.8846640744841245, l1: 0.0001079029296591446, l2: 0.0003805634767624987   Iteration 84 of 100, tot loss = 4.878960948614847, l1: 0.0001080825285541886, l2: 0.0003798135651214536   Iteration 85 of 100, tot loss = 4.848382923182319, l1: 0.00010730511297451278, l2: 0.00037753317823551374   Iteration 86 of 100, tot loss = 4.852847605250602, l1: 0.00010726767196015423, l2: 0.0003780170875468811   Iteration 87 of 100, tot loss = 4.907042422513852, l1: 0.0001080412431951243, l2: 0.0003826629981631413   Iteration 88 of 100, tot loss = 4.877683627334508, l1: 0.00010746412776411903, l2: 0.0003803042340719416   Iteration 89 of 100, tot loss = 4.915216530306956, l1: 0.00010789056924816609, l2: 0.00038363108312674496   Iteration 90 of 100, tot loss = 4.91208666033215, l1: 0.00010776124363474083, l2: 0.0003834474219022215   Iteration 91 of 100, tot loss = 4.882388619276194, l1: 0.00010735422556236089, l2: 0.00038088463594789033   Iteration 92 of 100, tot loss = 4.8677546083927155, l1: 0.00010706676928985753, l2: 0.00037970869111038905   Iteration 93 of 100, tot loss = 4.849759931205421, l1: 0.00010686120880055442, l2: 0.0003781147837583777   Iteration 94 of 100, tot loss = 4.846939192173329, l1: 0.00010665980574985758, l2: 0.0003780341131904935   Iteration 95 of 100, tot loss = 4.872386519532455, l1: 0.0001072416106150118, l2: 0.00037999704118670994   Iteration 96 of 100, tot loss = 4.870026770979166, l1: 0.00010739490202619588, l2: 0.0003796077750545616   Iteration 97 of 100, tot loss = 4.892296504728573, l1: 0.00010784892456098364, l2: 0.00038138072584880533   Iteration 98 of 100, tot loss = 4.87973596246875, l1: 0.00010773224356834901, l2: 0.00038024135277254926   Iteration 99 of 100, tot loss = 4.890131166487029, l1: 0.00010794661556237679, l2: 0.0003810665009289303   Iteration 100 of 100, tot loss = 4.874113556146622, l1: 0.00010758551045000786, l2: 0.00037982584501150996
   End of epoch 1281; saving model... 

Epoch 1282 of 2000
   Iteration 1 of 100, tot loss = 4.357729434967041, l1: 9.804027649806812e-05, l2: 0.0003377326938789338   Iteration 2 of 100, tot loss = 5.26083517074585, l1: 0.00011818407438113354, l2: 0.00040789946797303855   Iteration 3 of 100, tot loss = 4.707180102666219, l1: 0.00011190434452146292, l2: 0.00035881368482174975   Iteration 4 of 100, tot loss = 4.981228530406952, l1: 0.0001156563448603265, l2: 0.0003824665182037279   Iteration 5 of 100, tot loss = 4.778809976577759, l1: 0.00010863846546271816, l2: 0.00036924254382029174   Iteration 6 of 100, tot loss = 4.80213995774587, l1: 0.00010845657258566159, l2: 0.0003717574339437609   Iteration 7 of 100, tot loss = 4.8150657585689, l1: 0.0001058309935615398, l2: 0.0003756755863183311   Iteration 8 of 100, tot loss = 4.918962806463242, l1: 0.0001091571675715386, l2: 0.0003827391155937221   Iteration 9 of 100, tot loss = 5.008617798487346, l1: 0.00010876689904964426, l2: 0.0003920948850767066   Iteration 10 of 100, tot loss = 5.113563895225525, l1: 0.00011034847047994845, l2: 0.0004010079224826768   Iteration 11 of 100, tot loss = 5.018184466795488, l1: 0.00010733488902174444, l2: 0.0003944835599130866   Iteration 12 of 100, tot loss = 5.0783788959185285, l1: 0.00010927773898098773, l2: 0.00039856015306819853   Iteration 13 of 100, tot loss = 4.950013380784255, l1: 0.00010762449976307555, l2: 0.0003873768414800557   Iteration 14 of 100, tot loss = 4.933778081621442, l1: 0.0001070789538581656, l2: 0.0003862988586271448   Iteration 15 of 100, tot loss = 4.979242610931396, l1: 0.00010910739705044155, l2: 0.0003888168663252145   Iteration 16 of 100, tot loss = 5.040155440568924, l1: 0.00011009873333023279, l2: 0.000393916812754469   Iteration 17 of 100, tot loss = 4.991569967830882, l1: 0.00010891079086029683, l2: 0.0003902462074834415   Iteration 18 of 100, tot loss = 4.916807572046916, l1: 0.00010770103957232398, l2: 0.00038397971851130325   Iteration 19 of 100, tot loss = 4.890219236675062, l1: 0.00010699487182520036, l2: 0.00038202705266150206   Iteration 20 of 100, tot loss = 4.892666459083557, l1: 0.00010831815125129651, l2: 0.0003809484973317012   Iteration 21 of 100, tot loss = 4.816775174367995, l1: 0.00010804437520813995, l2: 0.0003736331446340219   Iteration 22 of 100, tot loss = 4.835310968485746, l1: 0.00010892633971789937, l2: 0.00037460475736720997   Iteration 23 of 100, tot loss = 4.875340306240579, l1: 0.00010926829196725284, l2: 0.0003782657390136434   Iteration 24 of 100, tot loss = 4.809080272912979, l1: 0.00010807734543050174, l2: 0.00037283068256025825   Iteration 25 of 100, tot loss = 4.84290961265564, l1: 0.00010813795524882153, l2: 0.0003761530062183738   Iteration 26 of 100, tot loss = 4.829012953318083, l1: 0.0001076094374114361, l2: 0.0003752918587102053   Iteration 27 of 100, tot loss = 4.808225446277195, l1: 0.00010641073324088077, l2: 0.0003744118126902591   Iteration 28 of 100, tot loss = 4.756260250295911, l1: 0.00010558067021650328, l2: 0.00037004535572902696   Iteration 29 of 100, tot loss = 4.730448928372613, l1: 0.00010504681418312649, l2: 0.0003679980792843833   Iteration 30 of 100, tot loss = 4.691156888008118, l1: 0.00010460373280996767, l2: 0.0003645119567712148   Iteration 31 of 100, tot loss = 4.754654369046611, l1: 0.00010632916101096799, l2: 0.00036913627615919517   Iteration 32 of 100, tot loss = 4.7253468334674835, l1: 0.00010564828721726371, l2: 0.0003668863955681445   Iteration 33 of 100, tot loss = 4.798149152235552, l1: 0.00010696772171120921, l2: 0.0003728471929207444   Iteration 34 of 100, tot loss = 4.760979112456827, l1: 0.0001068672290249892, l2: 0.0003692306816125946   Iteration 35 of 100, tot loss = 4.719425412586757, l1: 0.00010629865157950138, l2: 0.0003656438892773752   Iteration 36 of 100, tot loss = 4.736095276143816, l1: 0.0001061806506186258, l2: 0.0003674288770222726   Iteration 37 of 100, tot loss = 4.706465134749541, l1: 0.00010570258392190963, l2: 0.00036494392977171655   Iteration 38 of 100, tot loss = 4.738388218377766, l1: 0.00010619816269976774, l2: 0.0003676406585758454   Iteration 39 of 100, tot loss = 4.716259889113597, l1: 0.00010587243666985813, l2: 0.0003657535514018188   Iteration 40 of 100, tot loss = 4.70757424235344, l1: 0.00010581107380858157, l2: 0.0003649463491456117   Iteration 41 of 100, tot loss = 4.6612134154249985, l1: 0.00010503786094287956, l2: 0.000361083479308546   Iteration 42 of 100, tot loss = 4.652287908962795, l1: 0.00010543643033348158, l2: 0.00035979235932851833   Iteration 43 of 100, tot loss = 4.616829999657565, l1: 0.00010511564069866137, l2: 0.0003565673578969138   Iteration 44 of 100, tot loss = 4.650823717767542, l1: 0.00010535880309444937, l2: 0.0003597235670233865   Iteration 45 of 100, tot loss = 4.63274450302124, l1: 0.00010472444102763094, l2: 0.0003585500076749466   Iteration 46 of 100, tot loss = 4.722693681716919, l1: 0.00010637877620984395, l2: 0.0003658905906891248   Iteration 47 of 100, tot loss = 4.764847451067985, l1: 0.00010735138261844306, l2: 0.0003691333611718716   Iteration 48 of 100, tot loss = 4.763568212588628, l1: 0.00010731102141411004, l2: 0.00036904579822779243   Iteration 49 of 100, tot loss = 4.7472710317494915, l1: 0.00010744809028716302, l2: 0.00036727901154948514   Iteration 50 of 100, tot loss = 4.7932536411285405, l1: 0.00010778445299365558, l2: 0.0003715409108554013   Iteration 51 of 100, tot loss = 4.773603373882818, l1: 0.00010731542216472364, l2: 0.0003700449149119759   Iteration 52 of 100, tot loss = 4.76543940947606, l1: 0.00010740121867386803, l2: 0.0003691427216556174   Iteration 53 of 100, tot loss = 4.796600116873687, l1: 0.00010778821243153323, l2: 0.00037187179808619096   Iteration 54 of 100, tot loss = 4.7683646458166615, l1: 0.00010753173652725916, l2: 0.00036930472668717375   Iteration 55 of 100, tot loss = 4.758179564909502, l1: 0.00010762886760692874, l2: 0.00036818908755032517   Iteration 56 of 100, tot loss = 4.781423521893365, l1: 0.00010803936973908484, l2: 0.00037010298157318696   Iteration 57 of 100, tot loss = 4.780971046079669, l1: 0.00010811700996240241, l2: 0.0003699800941639727   Iteration 58 of 100, tot loss = 4.75333741204492, l1: 0.00010781962166596525, l2: 0.000367514119214571   Iteration 59 of 100, tot loss = 4.7397393250869495, l1: 0.00010743603565122427, l2: 0.00036653789634405936   Iteration 60 of 100, tot loss = 4.714766414960225, l1: 0.00010687919772559933, l2: 0.0003645974429673515   Iteration 61 of 100, tot loss = 4.723934400277059, l1: 0.00010713699574822743, l2: 0.0003652564435052212   Iteration 62 of 100, tot loss = 4.7279986950658985, l1: 0.00010709005800890736, l2: 0.00036570981131761425   Iteration 63 of 100, tot loss = 4.698546625318981, l1: 0.00010658980075580378, l2: 0.0003632648618400304   Iteration 64 of 100, tot loss = 4.772798884660006, l1: 0.00010783394395730284, l2: 0.0003694459444432141   Iteration 65 of 100, tot loss = 4.748200856722319, l1: 0.00010757777669753592, l2: 0.0003672423085215716   Iteration 66 of 100, tot loss = 4.749510403835412, l1: 0.00010765602884489591, l2: 0.00036729501086731693   Iteration 67 of 100, tot loss = 4.753763718391532, l1: 0.0001075161494351159, l2: 0.0003678602213821888   Iteration 68 of 100, tot loss = 4.854100472786847, l1: 0.0001090219121739638, l2: 0.0003763881346351221   Iteration 69 of 100, tot loss = 4.847902325616366, l1: 0.00010907896248154694, l2: 0.00037571126987408525   Iteration 70 of 100, tot loss = 4.821297747748239, l1: 0.00010886049141325721, l2: 0.0003732692830713599   Iteration 71 of 100, tot loss = 4.796600476117201, l1: 0.00010831729444580048, l2: 0.00037134275279781766   Iteration 72 of 100, tot loss = 4.840248770183987, l1: 0.00010908752089259603, l2: 0.0003749373556476914   Iteration 73 of 100, tot loss = 4.826001320799736, l1: 0.00010845274734231069, l2: 0.0003741473842201729   Iteration 74 of 100, tot loss = 4.852469428165539, l1: 0.00010867905734198143, l2: 0.00037656788520728916   Iteration 75 of 100, tot loss = 4.830589688618978, l1: 0.00010816872779590388, l2: 0.0003748902406853934   Iteration 76 of 100, tot loss = 4.813529030272835, l1: 0.0001076869786246741, l2: 0.0003736659242636769   Iteration 77 of 100, tot loss = 4.826497297782402, l1: 0.0001075718566615062, l2: 0.000375077872636861   Iteration 78 of 100, tot loss = 4.842224955558777, l1: 0.0001079036976592854, l2: 0.000376318797069148   Iteration 79 of 100, tot loss = 4.851478513283066, l1: 0.00010810448247290959, l2: 0.0003770433680914767   Iteration 80 of 100, tot loss = 4.836228483915329, l1: 0.00010797282702696975, l2: 0.00037565002057817767   Iteration 81 of 100, tot loss = 4.834630624747571, l1: 0.00010814739841778107, l2: 0.0003753156630454562   Iteration 82 of 100, tot loss = 4.8288771001304065, l1: 0.00010798769059275823, l2: 0.0003749000181148692   Iteration 83 of 100, tot loss = 4.802411257502544, l1: 0.00010753052077026118, l2: 0.0003727106035600069   Iteration 84 of 100, tot loss = 4.801464268139431, l1: 0.0001075674683220097, l2: 0.00037257895729410286   Iteration 85 of 100, tot loss = 4.780947365480311, l1: 0.0001071194708963637, l2: 0.0003709752645055928   Iteration 86 of 100, tot loss = 4.778061705966328, l1: 0.0001072101541192455, l2: 0.00037059601518829073   Iteration 87 of 100, tot loss = 4.759613185093321, l1: 0.00010677890498498883, l2: 0.00036918241232927973   Iteration 88 of 100, tot loss = 4.785708589987322, l1: 0.00010708427800471492, l2: 0.00037148658008267046   Iteration 89 of 100, tot loss = 4.7758908700407225, l1: 0.00010697105469435894, l2: 0.00037061803129873126   Iteration 90 of 100, tot loss = 4.782253472010295, l1: 0.00010723448237210202, l2: 0.0003709908634644105   Iteration 91 of 100, tot loss = 4.781885278093946, l1: 0.00010725668467425199, l2: 0.00037093184217375675   Iteration 92 of 100, tot loss = 4.766141575315724, l1: 0.00010681574618185708, l2: 0.00036979841024731286   Iteration 93 of 100, tot loss = 4.745850222085112, l1: 0.00010658372259293233, l2: 0.00036800129859212543   Iteration 94 of 100, tot loss = 4.746497953191716, l1: 0.00010666950293717549, l2: 0.00036798029109473996   Iteration 95 of 100, tot loss = 4.737062722758243, l1: 0.00010624450767768155, l2: 0.000367461763303972   Iteration 96 of 100, tot loss = 4.770765222609043, l1: 0.0001065679661375422, l2: 0.00037050855477597605   Iteration 97 of 100, tot loss = 4.7609083603337865, l1: 0.00010662583004774468, l2: 0.0003694650045291223   Iteration 98 of 100, tot loss = 4.743775679140675, l1: 0.00010641337449873836, l2: 0.0003679641920417471   Iteration 99 of 100, tot loss = 4.756154098896065, l1: 0.00010656734487227626, l2: 0.0003690480638525891   Iteration 100 of 100, tot loss = 4.778332729339599, l1: 0.00010680636485631112, l2: 0.0003710269069415517
   End of epoch 1282; saving model... 

Epoch 1283 of 2000
   Iteration 1 of 100, tot loss = 5.054622650146484, l1: 0.00012898091517854482, l2: 0.00037648138822987676   Iteration 2 of 100, tot loss = 4.765668630599976, l1: 0.00011252412150497548, l2: 0.00036404276033863425   Iteration 3 of 100, tot loss = 4.357083002726237, l1: 0.00010022277031869938, l2: 0.0003354855386229853   Iteration 4 of 100, tot loss = 4.702421545982361, l1: 0.00010681427374947816, l2: 0.00036342789098853245   Iteration 5 of 100, tot loss = 4.408928155899048, l1: 9.598151445970871e-05, l2: 0.0003449113050010055   Iteration 6 of 100, tot loss = 4.38724950949351, l1: 9.606136821579032e-05, l2: 0.00034266358610087383   Iteration 7 of 100, tot loss = 4.4638988971710205, l1: 9.864224297676369e-05, l2: 0.0003477476545543011   Iteration 8 of 100, tot loss = 4.450625330209732, l1: 0.00010178238017033436, l2: 0.00034328015681239776   Iteration 9 of 100, tot loss = 4.2397735648685035, l1: 9.995365245332423e-05, l2: 0.0003240237080414469   Iteration 10 of 100, tot loss = 4.134145760536194, l1: 9.848397494351956e-05, l2: 0.0003149306037812494   Iteration 11 of 100, tot loss = 4.040646899830211, l1: 9.849209659891626e-05, l2: 0.00030557259742636234   Iteration 12 of 100, tot loss = 4.016640941301982, l1: 9.818442443550642e-05, l2: 0.0003034796742819405   Iteration 13 of 100, tot loss = 3.8732756284567027, l1: 9.387049906833384e-05, l2: 0.0002934570679476914   Iteration 14 of 100, tot loss = 3.929935233933585, l1: 9.485348398032199e-05, l2: 0.00029814004249471637   Iteration 15 of 100, tot loss = 4.134263753890991, l1: 9.93943385158976e-05, l2: 0.00031403203805287677   Iteration 16 of 100, tot loss = 4.418036058545113, l1: 0.00010186341114604147, l2: 0.0003399401975912042   Iteration 17 of 100, tot loss = 4.580420816645903, l1: 0.00010400524992934045, l2: 0.0003540368326117887   Iteration 18 of 100, tot loss = 4.566878411504957, l1: 0.00010307867705705576, l2: 0.0003536091632364939   Iteration 19 of 100, tot loss = 4.532131584067094, l1: 0.00010088091527761303, l2: 0.000352332242887075   Iteration 20 of 100, tot loss = 4.500677919387817, l1: 0.00010029766417574137, l2: 0.00034977012692252176   Iteration 21 of 100, tot loss = 4.525126865931919, l1: 9.991321989911652e-05, l2: 0.0003525994668182518   Iteration 22 of 100, tot loss = 4.606325799768621, l1: 0.00010212141536282037, l2: 0.0003585111632921987   Iteration 23 of 100, tot loss = 4.647734268851902, l1: 0.00010241179204414315, l2: 0.00036236163198381013   Iteration 24 of 100, tot loss = 4.6880342562993365, l1: 0.0001026831296258024, l2: 0.0003661202924073829   Iteration 25 of 100, tot loss = 4.77596378326416, l1: 0.0001044094716780819, l2: 0.00037318690330721436   Iteration 26 of 100, tot loss = 4.754149400270903, l1: 0.00010429480831729821, l2: 0.00037112012908507424   Iteration 27 of 100, tot loss = 4.771237744225396, l1: 0.00010406059893149952, l2: 0.0003730631713686442   Iteration 28 of 100, tot loss = 4.866611633981977, l1: 0.00010543516769497987, l2: 0.00038122599237665   Iteration 29 of 100, tot loss = 4.872373153423441, l1: 0.00010606561509434711, l2: 0.00038117169631356055   Iteration 30 of 100, tot loss = 4.834600734710693, l1: 0.00010560866673282968, l2: 0.0003778514025422434   Iteration 31 of 100, tot loss = 4.813972350089781, l1: 0.00010530253676998038, l2: 0.0003760946940840973   Iteration 32 of 100, tot loss = 4.85165998339653, l1: 0.00010481759863978368, l2: 0.00038034839599276893   Iteration 33 of 100, tot loss = 4.790375196572506, l1: 0.00010363083525920625, l2: 0.00037540668034291065   Iteration 34 of 100, tot loss = 4.77500508112066, l1: 0.00010346745094626814, l2: 0.0003740330528885619   Iteration 35 of 100, tot loss = 4.795422887802124, l1: 0.00010412188546199884, l2: 0.0003754204001909654   Iteration 36 of 100, tot loss = 4.744260330994924, l1: 0.00010261797888233559, l2: 0.0003718080507597834   Iteration 37 of 100, tot loss = 4.709496214583114, l1: 0.00010198452523704996, l2: 0.00036896509236366664   Iteration 38 of 100, tot loss = 4.79250054610403, l1: 0.00010335430076673602, l2: 0.0003758957498480174   Iteration 39 of 100, tot loss = 4.795471839415721, l1: 0.00010313032460264646, l2: 0.0003764168557096034   Iteration 40 of 100, tot loss = 4.798910963535309, l1: 0.00010377817043263348, l2: 0.00037611292245856023   Iteration 41 of 100, tot loss = 4.7899333791034975, l1: 0.00010376484493551799, l2: 0.0003752284892692753   Iteration 42 of 100, tot loss = 4.764529438245864, l1: 0.00010273451144236052, l2: 0.0003737184289050111   Iteration 43 of 100, tot loss = 4.750696187795595, l1: 0.00010255633123749651, l2: 0.0003725132843354977   Iteration 44 of 100, tot loss = 4.748962310227481, l1: 0.00010286081322357694, l2: 0.00037203541466045533   Iteration 45 of 100, tot loss = 4.832137526406182, l1: 0.00010376401041867211, l2: 0.0003794497386681744   Iteration 46 of 100, tot loss = 4.865811788517496, l1: 0.00010416651686840771, l2: 0.0003824146573922759   Iteration 47 of 100, tot loss = 4.898039812737323, l1: 0.00010492245742848064, l2: 0.0003848815193781233   Iteration 48 of 100, tot loss = 4.91387224694093, l1: 0.00010559245174590615, l2: 0.00038579476768063614   Iteration 49 of 100, tot loss = 4.922523201728354, l1: 0.00010624119512408933, l2: 0.0003860111194496442   Iteration 50 of 100, tot loss = 4.919605097770691, l1: 0.00010636127510224469, l2: 0.00038559922919375824   Iteration 51 of 100, tot loss = 4.877330228394153, l1: 0.00010539449780766267, l2: 0.0003823385195573792   Iteration 52 of 100, tot loss = 4.8367526439520026, l1: 0.00010485403862940775, l2: 0.0003788212203429654   Iteration 53 of 100, tot loss = 4.80075467757459, l1: 0.00010453228722601022, l2: 0.00037554317546068287   Iteration 54 of 100, tot loss = 4.872824796923885, l1: 0.00010594650646251264, l2: 0.00038133596886095973   Iteration 55 of 100, tot loss = 4.894830725409768, l1: 0.00010619630292322571, l2: 0.00038328676574482496   Iteration 56 of 100, tot loss = 4.884335207087653, l1: 0.00010649039833359504, l2: 0.0003819431188146284   Iteration 57 of 100, tot loss = 4.861768806189821, l1: 0.00010605280476738699, l2: 0.0003801240724115224   Iteration 58 of 100, tot loss = 4.8511396852032895, l1: 0.00010626947143213454, l2: 0.0003788444935645234   Iteration 59 of 100, tot loss = 4.854216131113343, l1: 0.00010606225413986544, l2: 0.0003793593552812018   Iteration 60 of 100, tot loss = 4.899219576517741, l1: 0.00010716908915734773, l2: 0.0003827528647282937   Iteration 61 of 100, tot loss = 4.864821062713373, l1: 0.00010650917250981753, l2: 0.0003799729299212455   Iteration 62 of 100, tot loss = 4.871778661204923, l1: 0.00010620217213844655, l2: 0.0003809756904487468   Iteration 63 of 100, tot loss = 4.841848104719132, l1: 0.00010565510820015334, l2: 0.0003785296989556786   Iteration 64 of 100, tot loss = 4.8421533815562725, l1: 0.00010531983133432732, l2: 0.00037889550344516465   Iteration 65 of 100, tot loss = 4.841981363296509, l1: 0.00010561401180054348, l2: 0.00037858412095094816   Iteration 66 of 100, tot loss = 4.861851768060164, l1: 0.00010549541090740328, l2: 0.00038068976220639536   Iteration 67 of 100, tot loss = 4.845207153861202, l1: 0.00010505458692987032, l2: 0.00037946612482939136   Iteration 68 of 100, tot loss = 4.90031901177238, l1: 0.00010600284595592016, l2: 0.00038402905128725036   Iteration 69 of 100, tot loss = 4.86739006249801, l1: 0.0001051270841001668, l2: 0.00038161191796593744   Iteration 70 of 100, tot loss = 4.848159088407244, l1: 0.00010513417322986893, l2: 0.000379681731283199   Iteration 71 of 100, tot loss = 4.8458073978692715, l1: 0.00010539211077012227, l2: 0.00037918862469435313   Iteration 72 of 100, tot loss = 4.842043585247463, l1: 0.00010555063868701432, l2: 0.00037865371531451173   Iteration 73 of 100, tot loss = 4.848053644781244, l1: 0.00010569698367848329, l2: 0.00037910837591593853   Iteration 74 of 100, tot loss = 4.827186365385313, l1: 0.00010527651770767202, l2: 0.00037744211412909617   Iteration 75 of 100, tot loss = 4.826077709197998, l1: 0.00010533836117247119, l2: 0.00037726940490150204   Iteration 76 of 100, tot loss = 4.831955552101135, l1: 0.00010564988281801466, l2: 0.000377545668015955   Iteration 77 of 100, tot loss = 4.81994915627814, l1: 0.0001054178502307842, l2: 0.0003765770610249541   Iteration 78 of 100, tot loss = 4.790766419508518, l1: 0.00010498790904351223, l2: 0.00037408872856012284   Iteration 79 of 100, tot loss = 4.7666210856618765, l1: 0.00010445989792521076, l2: 0.00037220220635586146   Iteration 80 of 100, tot loss = 4.751042360067368, l1: 0.00010423154890304431, l2: 0.0003708726828335784   Iteration 81 of 100, tot loss = 4.749039414488239, l1: 0.0001041607342532514, l2: 0.0003707432026405715   Iteration 82 of 100, tot loss = 4.732977829328397, l1: 0.00010392717128369684, l2: 0.000369370607275902   Iteration 83 of 100, tot loss = 4.726674375763858, l1: 0.00010412622687437695, l2: 0.00036854120616587886   Iteration 84 of 100, tot loss = 4.724340379238129, l1: 0.00010444428724960224, l2: 0.00036798974629380696   Iteration 85 of 100, tot loss = 4.733749942218556, l1: 0.00010478866332799525, l2: 0.0003685863264014616   Iteration 86 of 100, tot loss = 4.718554047651069, l1: 0.00010471234595887687, l2: 0.00036714305433845366   Iteration 87 of 100, tot loss = 4.725639063736488, l1: 0.00010467875627105006, l2: 0.00036788514538282723   Iteration 88 of 100, tot loss = 4.7385885607112535, l1: 0.00010495859483357476, l2: 0.00036890025677910836   Iteration 89 of 100, tot loss = 4.716905982306834, l1: 0.00010436982594317955, l2: 0.00036732076809171255   Iteration 90 of 100, tot loss = 4.728405568334791, l1: 0.00010443069493501551, l2: 0.0003684098578459169   Iteration 91 of 100, tot loss = 4.722959562972352, l1: 0.00010414807152151448, l2: 0.0003681478807306081   Iteration 92 of 100, tot loss = 4.73776399311812, l1: 0.00010450057110890908, l2: 0.0003692758243983996   Iteration 93 of 100, tot loss = 4.721602937226654, l1: 0.00010438324033623181, l2: 0.00036777704951017896   Iteration 94 of 100, tot loss = 4.709089591148052, l1: 0.00010435797950312336, l2: 0.00036655097566371626   Iteration 95 of 100, tot loss = 4.686133053428248, l1: 0.00010374039809807743, l2: 0.0003648729034466669   Iteration 96 of 100, tot loss = 4.659014756480853, l1: 0.00010338870970372227, l2: 0.0003625127622702469   Iteration 97 of 100, tot loss = 4.670521819714418, l1: 0.00010367503678354137, l2: 0.00036337714193427224   Iteration 98 of 100, tot loss = 4.6555991586373775, l1: 0.00010343157065312417, l2: 0.00036212834185084367   Iteration 99 of 100, tot loss = 4.655980464183923, l1: 0.00010354844790076687, l2: 0.000362049595386994   Iteration 100 of 100, tot loss = 4.677138521671295, l1: 0.00010393253971415106, l2: 0.0003637813095701858
   End of epoch 1283; saving model... 

Epoch 1284 of 2000
   Iteration 1 of 100, tot loss = 4.11697244644165, l1: 0.00012617999163921922, l2: 0.0002855172788258642   Iteration 2 of 100, tot loss = 4.107126712799072, l1: 9.439323548576795e-05, l2: 0.00031631944875698537   Iteration 3 of 100, tot loss = 4.264358679453532, l1: 0.00010468740219948813, l2: 0.00032174846273846924   Iteration 4 of 100, tot loss = 3.5603692531585693, l1: 8.776460163062438e-05, l2: 0.00026827232431969605   Iteration 5 of 100, tot loss = 3.761329746246338, l1: 9.159307519439608e-05, l2: 0.00028453989361878487   Iteration 6 of 100, tot loss = 3.7745129267374673, l1: 8.88441985201401e-05, l2: 0.00028860708941162255   Iteration 7 of 100, tot loss = 4.003580229622977, l1: 9.351869604350733e-05, l2: 0.0003068393167008513   Iteration 8 of 100, tot loss = 3.865645945072174, l1: 9.384635632159188e-05, l2: 0.000292718228593003   Iteration 9 of 100, tot loss = 3.8684962590535483, l1: 9.452037839865725e-05, l2: 0.0002923292405385938   Iteration 10 of 100, tot loss = 4.060591220855713, l1: 9.808409449760803e-05, l2: 0.0003079750167671591   Iteration 11 of 100, tot loss = 4.242458213459361, l1: 0.00010226006601111625, l2: 0.000321985746268183   Iteration 12 of 100, tot loss = 4.167144298553467, l1: 0.0001005982915861144, l2: 0.0003161161293974146   Iteration 13 of 100, tot loss = 4.062164526719314, l1: 9.902641557765982e-05, l2: 0.0003071900282520801   Iteration 14 of 100, tot loss = 4.006333436284747, l1: 9.764474244937966e-05, l2: 0.0003029885930508109   Iteration 15 of 100, tot loss = 4.191070063908895, l1: 0.00010185380475983645, l2: 0.0003172531937404225   Iteration 16 of 100, tot loss = 4.270627394318581, l1: 0.0001024026842060266, l2: 0.00032466004995512776   Iteration 17 of 100, tot loss = 4.411435982760261, l1: 0.00010401443304384455, l2: 0.00033712916089879237   Iteration 18 of 100, tot loss = 4.520302891731262, l1: 0.0001064917742041871, l2: 0.00034553851259665354   Iteration 19 of 100, tot loss = 4.537500318727996, l1: 0.00010746936880566768, l2: 0.0003462806589117176   Iteration 20 of 100, tot loss = 4.643458473682403, l1: 0.0001092503618565388, l2: 0.0003550954832462594   Iteration 21 of 100, tot loss = 4.607757329940796, l1: 0.00010814776829820836, l2: 0.0003526279622400623   Iteration 22 of 100, tot loss = 4.667825969782743, l1: 0.0001081326170771552, l2: 0.00035864997930316764   Iteration 23 of 100, tot loss = 4.717195832211038, l1: 0.00010945310856661071, l2: 0.000362266473846672   Iteration 24 of 100, tot loss = 4.685311675071716, l1: 0.0001098544771593879, l2: 0.0003586766894538111   Iteration 25 of 100, tot loss = 4.67455602645874, l1: 0.00010881736408919096, l2: 0.0003586382372304797   Iteration 26 of 100, tot loss = 4.651319485444289, l1: 0.00010735919400198887, l2: 0.0003577727542366259   Iteration 27 of 100, tot loss = 4.613482563583939, l1: 0.00010681816207512316, l2: 0.00035453009466571665   Iteration 28 of 100, tot loss = 4.576200834342411, l1: 0.00010481379311905974, l2: 0.000352806290071125   Iteration 29 of 100, tot loss = 4.52322088438889, l1: 0.00010378568499461459, l2: 0.0003485364030548853   Iteration 30 of 100, tot loss = 4.529926300048828, l1: 0.00010475985376009096, l2: 0.0003482327767414972   Iteration 31 of 100, tot loss = 4.584260002259286, l1: 0.00010501332379194096, l2: 0.0003534126754534701   Iteration 32 of 100, tot loss = 4.616956248879433, l1: 0.00010548256273068546, l2: 0.0003562130605132552   Iteration 33 of 100, tot loss = 4.588673078652584, l1: 0.00010473289479374547, l2: 0.00035413441152989185   Iteration 34 of 100, tot loss = 4.532250292160931, l1: 0.00010355513244148289, l2: 0.00034966989487121976   Iteration 35 of 100, tot loss = 4.5599078178405765, l1: 0.00010408356133016891, l2: 0.0003519072184904612   Iteration 36 of 100, tot loss = 4.572922772831387, l1: 0.00010474290259783932, l2: 0.0003525493727162636   Iteration 37 of 100, tot loss = 4.572073846249967, l1: 0.00010477009267118331, l2: 0.0003524372897683517   Iteration 38 of 100, tot loss = 4.571213784970735, l1: 0.00010449176701704825, l2: 0.000352629609360041   Iteration 39 of 100, tot loss = 4.594764354901436, l1: 0.00010447561661026273, l2: 0.0003550008177393092   Iteration 40 of 100, tot loss = 4.567168092727661, l1: 0.00010466539843037025, l2: 0.0003520514099363936   Iteration 41 of 100, tot loss = 4.661362624749905, l1: 0.0001063648678387924, l2: 0.00035977139380453834   Iteration 42 of 100, tot loss = 4.624273078782218, l1: 0.00010510325106830957, l2: 0.00035732405549857677   Iteration 43 of 100, tot loss = 4.631815139637437, l1: 0.0001052374605341208, l2: 0.00035794405300484234   Iteration 44 of 100, tot loss = 4.641451039097526, l1: 0.00010576320065328301, l2: 0.00035838190226720394   Iteration 45 of 100, tot loss = 4.652869992785984, l1: 0.00010552304350615789, l2: 0.00035976395496012024   Iteration 46 of 100, tot loss = 4.691060206164485, l1: 0.00010550329044117066, l2: 0.00036360272908654144   Iteration 47 of 100, tot loss = 4.697490301538021, l1: 0.0001060511062957069, l2: 0.00036369792360346764   Iteration 48 of 100, tot loss = 4.766409034530322, l1: 0.00010683847328133804, l2: 0.0003698024293044   Iteration 49 of 100, tot loss = 4.7137742772394295, l1: 0.00010598877641462664, l2: 0.0003653886506443235   Iteration 50 of 100, tot loss = 4.678927373886109, l1: 0.00010551276078331284, l2: 0.00036237997614080086   Iteration 51 of 100, tot loss = 4.62964885375079, l1: 0.00010426622397704598, l2: 0.00035869866077715127   Iteration 52 of 100, tot loss = 4.662880466534541, l1: 0.00010479285992914811, l2: 0.0003614951862031236   Iteration 53 of 100, tot loss = 4.71988607802481, l1: 0.00010558468660884448, l2: 0.0003664039206067276   Iteration 54 of 100, tot loss = 4.738872616379349, l1: 0.00010582119567617256, l2: 0.00036806606523761594   Iteration 55 of 100, tot loss = 4.774733170596036, l1: 0.00010673210821220312, l2: 0.0003707412079992619   Iteration 56 of 100, tot loss = 4.767220480101449, l1: 0.00010591241237177331, l2: 0.000370809634789891   Iteration 57 of 100, tot loss = 4.758847035859761, l1: 0.00010575792233188015, l2: 0.0003701267807883325   Iteration 58 of 100, tot loss = 4.735841623668013, l1: 0.00010531682264940925, l2: 0.00036826733939333595   Iteration 59 of 100, tot loss = 4.747073387695571, l1: 0.00010551565916966095, l2: 0.00036919167930244513   Iteration 60 of 100, tot loss = 4.738743062814077, l1: 0.00010534524280956248, l2: 0.0003685290634166449   Iteration 61 of 100, tot loss = 4.699662423524701, l1: 0.00010472560616555532, l2: 0.00036524063606983143   Iteration 62 of 100, tot loss = 4.720200550171636, l1: 0.0001048719529330077, l2: 0.0003671481017297464   Iteration 63 of 100, tot loss = 4.735172313357157, l1: 0.0001050354923848765, l2: 0.00036848173869426346   Iteration 64 of 100, tot loss = 4.725980009883642, l1: 0.00010530805474218141, l2: 0.0003672899463253998   Iteration 65 of 100, tot loss = 4.694495608256413, l1: 0.00010495643985520403, l2: 0.00036449312107064404   Iteration 66 of 100, tot loss = 4.697243484583768, l1: 0.00010491032734342187, l2: 0.00036481402154701453   Iteration 67 of 100, tot loss = 4.678644792357487, l1: 0.0001046673971712207, l2: 0.00036319708251811344   Iteration 68 of 100, tot loss = 4.661223671015571, l1: 0.00010438961881471401, l2: 0.00036173274865552014   Iteration 69 of 100, tot loss = 4.659974499025207, l1: 0.00010460097790042789, l2: 0.0003613964721992396   Iteration 70 of 100, tot loss = 4.640476812635149, l1: 0.00010402896768937353, l2: 0.0003600187138155369   Iteration 71 of 100, tot loss = 4.664911391029896, l1: 0.00010463712271917808, l2: 0.0003618540166070501   Iteration 72 of 100, tot loss = 4.6587012145254345, l1: 0.0001049280522238405, l2: 0.0003609420697709235   Iteration 73 of 100, tot loss = 4.690169484647986, l1: 0.00010523986890042928, l2: 0.0003637770798427295   Iteration 74 of 100, tot loss = 4.657743325104585, l1: 0.00010463630496815313, l2: 0.0003611380277623146   Iteration 75 of 100, tot loss = 4.6502721277872725, l1: 0.00010487923694502873, l2: 0.00036014797631651164   Iteration 76 of 100, tot loss = 4.630064923512308, l1: 0.00010474985629116418, l2: 0.0003582566364217966   Iteration 77 of 100, tot loss = 4.652800346349741, l1: 0.0001053939177635785, l2: 0.00035988611668838163   Iteration 78 of 100, tot loss = 4.66463029384613, l1: 0.00010536505042415601, l2: 0.0003610979782444282   Iteration 79 of 100, tot loss = 4.636764644067498, l1: 0.00010465556900996915, l2: 0.00035902089476172775   Iteration 80 of 100, tot loss = 4.6538154751062395, l1: 0.00010467274100847134, l2: 0.0003607088056014618   Iteration 81 of 100, tot loss = 4.651511713310525, l1: 0.00010482463210477655, l2: 0.00036032653857815875   Iteration 82 of 100, tot loss = 4.646224740074902, l1: 0.00010488480598369056, l2: 0.0003597376674339857   Iteration 83 of 100, tot loss = 4.657517757760473, l1: 0.00010502598244389687, l2: 0.00036072579292434614   Iteration 84 of 100, tot loss = 4.637506257920038, l1: 0.0001047472613728522, l2: 0.0003590033638396389   Iteration 85 of 100, tot loss = 4.655430962057674, l1: 0.00010501160444726018, l2: 0.00036053149104940105   Iteration 86 of 100, tot loss = 4.68667934107226, l1: 0.00010545887287366367, l2: 0.0003632090604717865   Iteration 87 of 100, tot loss = 4.66907252388439, l1: 0.00010520870929975973, l2: 0.0003616985424700054   Iteration 88 of 100, tot loss = 4.658529455011541, l1: 0.00010526379426003338, l2: 0.00036058915065950714   Iteration 89 of 100, tot loss = 4.649448925189757, l1: 0.00010530834292745731, l2: 0.00035963654897458263   Iteration 90 of 100, tot loss = 4.639646442731221, l1: 0.00010520534750766173, l2: 0.000358759296230144   Iteration 91 of 100, tot loss = 4.647521776157421, l1: 0.00010555521945646903, l2: 0.00035919695753593937   Iteration 92 of 100, tot loss = 4.630192080269689, l1: 0.00010511876861647065, l2: 0.0003579004386784104   Iteration 93 of 100, tot loss = 4.6316635403581845, l1: 0.00010511517299868677, l2: 0.00035805118042394837   Iteration 94 of 100, tot loss = 4.662751068460181, l1: 0.00010555209368287754, l2: 0.00036072301210221297   Iteration 95 of 100, tot loss = 4.683389460413079, l1: 0.00010590474990558027, l2: 0.0003624341953055639   Iteration 96 of 100, tot loss = 4.676515959203243, l1: 0.00010557900816365873, l2: 0.00036207258684347227   Iteration 97 of 100, tot loss = 4.701473248373602, l1: 0.00010594748978151402, l2: 0.00036419983401164875   Iteration 98 of 100, tot loss = 4.72000651700156, l1: 0.00010648334474743544, l2: 0.00036551730590872467   Iteration 99 of 100, tot loss = 4.729829728001296, l1: 0.00010662642061372871, l2: 0.0003663565510192491   Iteration 100 of 100, tot loss = 4.722374958992004, l1: 0.00010659387331543257, l2: 0.00036564362148055807
   End of epoch 1284; saving model... 

Epoch 1285 of 2000
   Iteration 1 of 100, tot loss = 5.364703178405762, l1: 0.00011081757111242041, l2: 0.00042565277544781566   Iteration 2 of 100, tot loss = 5.1952691078186035, l1: 0.00011179177454323508, l2: 0.00040773514774627984   Iteration 3 of 100, tot loss = 4.850972811381022, l1: 0.00010533999738981947, l2: 0.0003797572959835331   Iteration 4 of 100, tot loss = 5.6203858852386475, l1: 0.0001219657660840312, l2: 0.0004400728357722983   Iteration 5 of 100, tot loss = 5.6740796089172365, l1: 0.0001241602687514387, l2: 0.0004432476998772472   Iteration 6 of 100, tot loss = 5.882688760757446, l1: 0.00012825941788226677, l2: 0.00046000946410155546   Iteration 7 of 100, tot loss = 5.367230585643223, l1: 0.00011574952908891387, l2: 0.00042097353434655815   Iteration 8 of 100, tot loss = 5.066194146871567, l1: 0.00011208500654902309, l2: 0.0003945344105886761   Iteration 9 of 100, tot loss = 5.14173330201043, l1: 0.00011406366603397246, l2: 0.00040010966929710575   Iteration 10 of 100, tot loss = 5.284310650825501, l1: 0.00011469754172139801, l2: 0.00041373352869413795   Iteration 11 of 100, tot loss = 5.09948975389654, l1: 0.00011113393570254134, l2: 0.00039881504595872354   Iteration 12 of 100, tot loss = 5.0748836398124695, l1: 0.00010990825467160903, l2: 0.00039758011310671765   Iteration 13 of 100, tot loss = 5.037170061698327, l1: 0.00010942277843311716, l2: 0.0003942942309479874   Iteration 14 of 100, tot loss = 5.110597763742719, l1: 0.00010773001330172909, l2: 0.0004033297674530851   Iteration 15 of 100, tot loss = 5.250006786982218, l1: 0.00010946482458772759, l2: 0.000415535857124875   Iteration 16 of 100, tot loss = 5.121907025575638, l1: 0.00010773464646263164, l2: 0.00040445605918648653   Iteration 17 of 100, tot loss = 5.044592422597549, l1: 0.00010649490671793876, l2: 0.00039796433788176407   Iteration 18 of 100, tot loss = 4.9352668391333685, l1: 0.00010496652612346224, l2: 0.000388560158575678   Iteration 19 of 100, tot loss = 4.9091079611527295, l1: 0.00010593821896595488, l2: 0.0003849725762847811   Iteration 20 of 100, tot loss = 5.083576583862305, l1: 0.00010883010145334993, l2: 0.0003995275546913035   Iteration 21 of 100, tot loss = 5.187623568943569, l1: 0.00011113873326740715, l2: 0.00040762362186796963   Iteration 22 of 100, tot loss = 5.195790550925515, l1: 0.00011141228870573369, l2: 0.00040816676284355873   Iteration 23 of 100, tot loss = 5.230785286944846, l1: 0.00011135709412527554, l2: 0.0004117214323385902   Iteration 24 of 100, tot loss = 5.417591571807861, l1: 0.00011413503762014443, l2: 0.0004276241161278449   Iteration 25 of 100, tot loss = 5.368433094024658, l1: 0.00011329648579703644, l2: 0.0004235468211118132   Iteration 26 of 100, tot loss = 5.345558239863469, l1: 0.00011232309966894369, l2: 0.00042223272276504967   Iteration 27 of 100, tot loss = 5.37449582417806, l1: 0.00011290428611337793, l2: 0.00042454529601080275   Iteration 28 of 100, tot loss = 5.311952480248043, l1: 0.00011277637531748042, l2: 0.0004184188728686422   Iteration 29 of 100, tot loss = 5.293118978368825, l1: 0.00011293492069194929, l2: 0.0004163769762240479   Iteration 30 of 100, tot loss = 5.241886218388875, l1: 0.00011271375212042282, l2: 0.0004114748686940099   Iteration 31 of 100, tot loss = 5.1562797715587, l1: 0.00011143168728355499, l2: 0.00040419628907505784   Iteration 32 of 100, tot loss = 5.1736504808068275, l1: 0.00011201509209968208, l2: 0.00040534995605412405   Iteration 33 of 100, tot loss = 5.100746082537102, l1: 0.00011064524093826273, l2: 0.00039942936784813577   Iteration 34 of 100, tot loss = 5.108427454443539, l1: 0.00011015099003356333, l2: 0.0004006917562732911   Iteration 35 of 100, tot loss = 5.033853776114327, l1: 0.00010841889577152739, l2: 0.00039496648283342695   Iteration 36 of 100, tot loss = 5.049911035431756, l1: 0.00010920117973809183, l2: 0.0003957899247729478   Iteration 37 of 100, tot loss = 5.003134321522069, l1: 0.00010777034510062328, l2: 0.000392543087867592   Iteration 38 of 100, tot loss = 5.064333181632192, l1: 0.0001086346849325836, l2: 0.0003977986332182282   Iteration 39 of 100, tot loss = 4.9922743027026835, l1: 0.00010695412912844823, l2: 0.0003922733008855572   Iteration 40 of 100, tot loss = 4.9556903600692745, l1: 0.00010707166557040182, l2: 0.0003884973702952266   Iteration 41 of 100, tot loss = 4.992234509165694, l1: 0.00010768835201771453, l2: 0.00039153509750598813   Iteration 42 of 100, tot loss = 4.999229805810111, l1: 0.0001079576623375206, l2: 0.0003919653169023583   Iteration 43 of 100, tot loss = 4.952022901801175, l1: 0.00010715511809901855, l2: 0.00038804717024864063   Iteration 44 of 100, tot loss = 4.916372830217535, l1: 0.00010593079059617594, l2: 0.00038570649055393665   Iteration 45 of 100, tot loss = 4.979998768700494, l1: 0.00010654636652260605, l2: 0.00039145350876626457   Iteration 46 of 100, tot loss = 5.0053873995076055, l1: 0.00010724940986382653, l2: 0.00039328932768472913   Iteration 47 of 100, tot loss = 4.951476132616084, l1: 0.00010646392385093892, l2: 0.00038868368713512144   Iteration 48 of 100, tot loss = 5.039276575048764, l1: 0.0001079508018240934, l2: 0.00039597685251161846   Iteration 49 of 100, tot loss = 5.075758209033888, l1: 0.00010841040844121492, l2: 0.00039916540934390636   Iteration 50 of 100, tot loss = 5.0420679140090945, l1: 0.00010782233541249297, l2: 0.00039638445276068523   Iteration 51 of 100, tot loss = 5.0302774719163486, l1: 0.00010778397343386256, l2: 0.00039524377087432453   Iteration 52 of 100, tot loss = 4.972895691028008, l1: 0.00010679688379101348, l2: 0.0003904926821773048   Iteration 53 of 100, tot loss = 4.97567307724143, l1: 0.00010667461641066936, l2: 0.0003908926883013919   Iteration 54 of 100, tot loss = 5.0019954001462015, l1: 0.00010715650825187806, l2: 0.0003930430288568863   Iteration 55 of 100, tot loss = 4.9922315987673676, l1: 0.00010722676088334993, l2: 0.00039199639603876594   Iteration 56 of 100, tot loss = 4.969014521156039, l1: 0.00010641724955868475, l2: 0.000390484199670027   Iteration 57 of 100, tot loss = 4.938854828215482, l1: 0.00010577011562418193, l2: 0.00038811536439441207   Iteration 58 of 100, tot loss = 4.940398397116826, l1: 0.00010602599712879525, l2: 0.0003880138395069135   Iteration 59 of 100, tot loss = 4.94787470769074, l1: 0.00010633029937014079, l2: 0.0003884571687795885   Iteration 60 of 100, tot loss = 4.956642468770345, l1: 0.00010630976818598962, l2: 0.0003893544759193901   Iteration 61 of 100, tot loss = 4.946035822883982, l1: 0.00010623332634510961, l2: 0.0003883702535903631   Iteration 62 of 100, tot loss = 4.9348360338518695, l1: 0.0001061597364652346, l2: 0.00038732386418550665   Iteration 63 of 100, tot loss = 4.9372715344504705, l1: 0.00010644033029657005, l2: 0.0003872868202586052   Iteration 64 of 100, tot loss = 4.9136996157467365, l1: 0.00010595171295335604, l2: 0.0003854182457416755   Iteration 65 of 100, tot loss = 4.88517756828895, l1: 0.00010565226783420747, l2: 0.0003828654862725391   Iteration 66 of 100, tot loss = 4.895260623007109, l1: 0.00010576274452790985, l2: 0.00038376331565704083   Iteration 67 of 100, tot loss = 4.917262440297141, l1: 0.00010645233297550272, l2: 0.0003852739095263906   Iteration 68 of 100, tot loss = 4.900469793992884, l1: 0.00010626480359539582, l2: 0.0003837821744074223   Iteration 69 of 100, tot loss = 4.852689210919366, l1: 0.00010535114821665785, l2: 0.000379917771552109   Iteration 70 of 100, tot loss = 4.844513361794608, l1: 0.00010551818845020275, l2: 0.00037893314646290883   Iteration 71 of 100, tot loss = 4.835777168542567, l1: 0.0001054963811352344, l2: 0.0003780813344170562   Iteration 72 of 100, tot loss = 4.824355178409153, l1: 0.00010528601680764243, l2: 0.00037714949939982034   Iteration 73 of 100, tot loss = 4.84129557544238, l1: 0.0001055651630114681, l2: 0.00037856439283206957   Iteration 74 of 100, tot loss = 4.818304277755119, l1: 0.00010543350607976381, l2: 0.0003763969198681062   Iteration 75 of 100, tot loss = 4.823232933680217, l1: 0.0001058494785199097, l2: 0.0003764738130848855   Iteration 76 of 100, tot loss = 4.841777202330138, l1: 0.00010597817608389673, l2: 0.0003781995424345185   Iteration 77 of 100, tot loss = 4.891019341233489, l1: 0.000106808666395728, l2: 0.0003822932663137795   Iteration 78 of 100, tot loss = 4.889965298848274, l1: 0.00010680489070252336, l2: 0.0003821916377488285   Iteration 79 of 100, tot loss = 4.865265333199803, l1: 0.00010602851541045334, l2: 0.0003804980163444777   Iteration 80 of 100, tot loss = 4.8593514323234555, l1: 0.00010574420207376534, l2: 0.0003801909399044234   Iteration 81 of 100, tot loss = 4.859639362052635, l1: 0.00010595008791790826, l2: 0.00038001384732692884   Iteration 82 of 100, tot loss = 4.883546398907173, l1: 0.00010624523224023728, l2: 0.0003821094067506038   Iteration 83 of 100, tot loss = 4.871533609298338, l1: 0.00010588441044267502, l2: 0.0003812689494049872   Iteration 84 of 100, tot loss = 4.858485397838411, l1: 0.00010578703478088192, l2: 0.00038006150411785624   Iteration 85 of 100, tot loss = 4.856701918209301, l1: 0.00010572824116132003, l2: 0.0003799419500865042   Iteration 86 of 100, tot loss = 4.869659063427947, l1: 0.00010612777949190362, l2: 0.00038083812606326024   Iteration 87 of 100, tot loss = 4.8894442963874205, l1: 0.0001063882556844947, l2: 0.00038255617322250344   Iteration 88 of 100, tot loss = 4.8678550936959, l1: 0.00010616023837428656, l2: 0.00038062526999221353   Iteration 89 of 100, tot loss = 4.895447843530205, l1: 0.00010678062657400537, l2: 0.00038276415670112697   Iteration 90 of 100, tot loss = 4.9133442083994545, l1: 0.00010728073519001353, l2: 0.0003840536846029055   Iteration 91 of 100, tot loss = 4.960029733050001, l1: 0.00010809153224857125, l2: 0.00038791144014704604   Iteration 92 of 100, tot loss = 4.974145863367164, l1: 0.00010796596700896296, l2: 0.0003894486179215955   Iteration 93 of 100, tot loss = 4.961234210639872, l1: 0.00010770293507663664, l2: 0.000388420484859937   Iteration 94 of 100, tot loss = 4.962596091818302, l1: 0.00010787488602719794, l2: 0.0003883847223713677   Iteration 95 of 100, tot loss = 4.967662093513891, l1: 0.00010788572433643582, l2: 0.0003888804842350318   Iteration 96 of 100, tot loss = 4.965866481264432, l1: 0.00010801046873136026, l2: 0.000388576178617465   Iteration 97 of 100, tot loss = 4.944037823332954, l1: 0.00010746862612107826, l2: 0.000386935155436675   Iteration 98 of 100, tot loss = 4.972827544017714, l1: 0.00010770840623192679, l2: 0.0003895743475925196   Iteration 99 of 100, tot loss = 4.9589974976549245, l1: 0.00010762143320982543, l2: 0.00038827831590562266   Iteration 100 of 100, tot loss = 4.947597377300262, l1: 0.00010747839718533214, l2: 0.00038728133993572557
   End of epoch 1285; saving model... 

Epoch 1286 of 2000
   Iteration 1 of 100, tot loss = 7.356499195098877, l1: 0.00013748259516432881, l2: 0.0005981673020869493   Iteration 2 of 100, tot loss = 8.678097486495972, l1: 0.0001520809601061046, l2: 0.0007157287618611008   Iteration 3 of 100, tot loss = 7.264693419138591, l1: 0.00013496973891354477, l2: 0.0005914995854254812   Iteration 4 of 100, tot loss = 6.775436282157898, l1: 0.000133206716782297, l2: 0.0005443369009299204   Iteration 5 of 100, tot loss = 6.2734424591064455, l1: 0.00012462459126254543, l2: 0.0005027196428272873   Iteration 6 of 100, tot loss = 5.874985694885254, l1: 0.00012001635938455972, l2: 0.00046748219756409526   Iteration 7 of 100, tot loss = 5.695908001491001, l1: 0.00012027944363321044, l2: 0.0004493113499068256   Iteration 8 of 100, tot loss = 5.425183981657028, l1: 0.00011850662940560142, l2: 0.00042401176324347034   Iteration 9 of 100, tot loss = 5.628000603781806, l1: 0.00012410978524712846, l2: 0.0004386902696246074   Iteration 10 of 100, tot loss = 5.513366341590881, l1: 0.00012350725955911912, l2: 0.00042782936943694947   Iteration 11 of 100, tot loss = 5.486447832801125, l1: 0.00012396261379101568, l2: 0.0004246821627020836   Iteration 12 of 100, tot loss = 5.4108386635780334, l1: 0.00012196352569541584, l2: 0.0004191203333903104   Iteration 13 of 100, tot loss = 5.23053686435406, l1: 0.00011907927098666102, l2: 0.0004039744101016997   Iteration 14 of 100, tot loss = 5.2053346293313165, l1: 0.00011957780173231316, l2: 0.0004009556547056751   Iteration 15 of 100, tot loss = 5.228200626373291, l1: 0.00012012737036760275, l2: 0.0004026926850201562   Iteration 16 of 100, tot loss = 5.084861025214195, l1: 0.000117381637210201, l2: 0.0003911044586857315   Iteration 17 of 100, tot loss = 5.008319770588594, l1: 0.0001162211290861973, l2: 0.0003846108430640443   Iteration 18 of 100, tot loss = 5.069225549697876, l1: 0.00011776965725908263, l2: 0.0003891528936542778   Iteration 19 of 100, tot loss = 5.132870623939915, l1: 0.00011784159601359677, l2: 0.0003954454640686316   Iteration 20 of 100, tot loss = 5.201477122306824, l1: 0.00011770445307774935, l2: 0.0004024432579171844   Iteration 21 of 100, tot loss = 5.211004870278495, l1: 0.00011766169856335702, l2: 0.00040343878763018264   Iteration 22 of 100, tot loss = 5.167018261822787, l1: 0.00011651674999897791, l2: 0.00040018507570493966   Iteration 23 of 100, tot loss = 5.109118409778761, l1: 0.00011589024918224743, l2: 0.0003950215911533198   Iteration 24 of 100, tot loss = 5.087129483620326, l1: 0.00011535086347673011, l2: 0.00039336208525734645   Iteration 25 of 100, tot loss = 5.181634683609008, l1: 0.00011663167242659256, l2: 0.00040153179550543426   Iteration 26 of 100, tot loss = 5.143647404817434, l1: 0.00011465423589330525, l2: 0.00039971050528737786   Iteration 27 of 100, tot loss = 5.225154602969134, l1: 0.00011603710757939282, l2: 0.00040647835272398814   Iteration 28 of 100, tot loss = 5.2567033512251715, l1: 0.00011463659757282585, l2: 0.00041103373847103545   Iteration 29 of 100, tot loss = 5.245421976878725, l1: 0.00011393592661800634, l2: 0.00041060627072824743   Iteration 30 of 100, tot loss = 5.231709599494934, l1: 0.00011314704703787962, l2: 0.00041002391274863235   Iteration 31 of 100, tot loss = 5.152682065963745, l1: 0.00011122096956598633, l2: 0.00040404723756270664   Iteration 32 of 100, tot loss = 5.122063688933849, l1: 0.00011141732500163926, l2: 0.00040078904339679866   Iteration 33 of 100, tot loss = 5.171252893679069, l1: 0.00011258201916599788, l2: 0.00040454326936565906   Iteration 34 of 100, tot loss = 5.1710611862294815, l1: 0.00011273448730849744, l2: 0.00040437163047232284   Iteration 35 of 100, tot loss = 5.159157405580793, l1: 0.0001129768008208235, l2: 0.00040293893918195474   Iteration 36 of 100, tot loss = 5.130286951859792, l1: 0.00011193749368329818, l2: 0.0004010912012341174   Iteration 37 of 100, tot loss = 5.102508280728315, l1: 0.00011193019137764979, l2: 0.0003983206363438905   Iteration 38 of 100, tot loss = 5.075346187541359, l1: 0.0001109780351773799, l2: 0.00039655658283185114   Iteration 39 of 100, tot loss = 4.999085811468271, l1: 0.000109786471745256, l2: 0.0003901221084211451   Iteration 40 of 100, tot loss = 4.978426688909531, l1: 0.00010958852426483645, l2: 0.00038825414412713144   Iteration 41 of 100, tot loss = 4.97841634401461, l1: 0.00010987764365315187, l2: 0.00038796399042816696   Iteration 42 of 100, tot loss = 4.937234061104911, l1: 0.0001091956306654971, l2: 0.00038452777482147907   Iteration 43 of 100, tot loss = 4.980041171229163, l1: 0.00011006230632246601, l2: 0.00038794180998297104   Iteration 44 of 100, tot loss = 4.940305081280795, l1: 0.0001092960107019859, l2: 0.0003847344970298846   Iteration 45 of 100, tot loss = 5.035704697502984, l1: 0.00011096437430953503, l2: 0.0003926060945054309   Iteration 46 of 100, tot loss = 5.096514950627866, l1: 0.00011183859303697398, l2: 0.00039781290093821514   Iteration 47 of 100, tot loss = 5.086084822390942, l1: 0.00011161939224582462, l2: 0.0003969890890306139   Iteration 48 of 100, tot loss = 5.0306935260693235, l1: 0.00011029850247723516, l2: 0.0003927708494302351   Iteration 49 of 100, tot loss = 5.032106083266589, l1: 0.00011030384514609123, l2: 0.00039290676275933426   Iteration 50 of 100, tot loss = 5.045628237724304, l1: 0.00011053122580051422, l2: 0.0003940315975341946   Iteration 51 of 100, tot loss = 5.004997575984282, l1: 0.0001097349859443128, l2: 0.00039076477125990115   Iteration 52 of 100, tot loss = 5.010260531535516, l1: 0.00011007566060400747, l2: 0.00039095039196231833   Iteration 53 of 100, tot loss = 5.029353866037333, l1: 0.00011057991060573291, l2: 0.00039235547482572   Iteration 54 of 100, tot loss = 5.025809663313407, l1: 0.00011034476133149669, l2: 0.0003922362038456076   Iteration 55 of 100, tot loss = 4.984087055379694, l1: 0.00010955545810495758, l2: 0.0003888532463778657   Iteration 56 of 100, tot loss = 5.00826547401292, l1: 0.00010985400214329795, l2: 0.0003909725447946195   Iteration 57 of 100, tot loss = 4.999699805912218, l1: 0.00010969945160149221, l2: 0.00039027052856940907   Iteration 58 of 100, tot loss = 4.993307273963402, l1: 0.00010997464314327929, l2: 0.0003893560835798592   Iteration 59 of 100, tot loss = 5.083075535499443, l1: 0.00011096099688537356, l2: 0.00039734655544280186   Iteration 60 of 100, tot loss = 5.087091267108917, l1: 0.00011126078655555223, l2: 0.0003974483394510268   Iteration 61 of 100, tot loss = 5.058371453988747, l1: 0.00011076783195050189, l2: 0.0003950693126916733   Iteration 62 of 100, tot loss = 5.070975007549409, l1: 0.00011109308686554282, l2: 0.0003960044130273614   Iteration 63 of 100, tot loss = 5.104914532767402, l1: 0.00011146332782652554, l2: 0.00039902812452979446   Iteration 64 of 100, tot loss = 5.102317828685045, l1: 0.0001115577121026945, l2: 0.00039867407053861825   Iteration 65 of 100, tot loss = 5.119651710070096, l1: 0.0001116888817005719, l2: 0.000400276288900596   Iteration 66 of 100, tot loss = 5.1454497864752105, l1: 0.00011182762550147758, l2: 0.0004027173530299809   Iteration 67 of 100, tot loss = 5.125660017355164, l1: 0.00011187032116629156, l2: 0.00040069568025664107   Iteration 68 of 100, tot loss = 5.177514304133022, l1: 0.000112794963784194, l2: 0.0004049564661673845   Iteration 69 of 100, tot loss = 5.169994129650835, l1: 0.0001124958344271231, l2: 0.00040450357841294044   Iteration 70 of 100, tot loss = 5.1197442787034175, l1: 0.00011142115326947532, l2: 0.0004005532745005829   Iteration 71 of 100, tot loss = 5.120711593560769, l1: 0.00011136728969343583, l2: 0.000400703869314051   Iteration 72 of 100, tot loss = 5.127581263581912, l1: 0.00011121321030158369, l2: 0.0004015449158032425   Iteration 73 of 100, tot loss = 5.114560896403169, l1: 0.00011054614181263525, l2: 0.00040090994786611467   Iteration 74 of 100, tot loss = 5.086921587183669, l1: 0.00011014753535336413, l2: 0.00039854462335603565   Iteration 75 of 100, tot loss = 5.0794761832555135, l1: 0.00011016401646581168, l2: 0.00039778360165655614   Iteration 76 of 100, tot loss = 5.1027573519631435, l1: 0.0001102408630111696, l2: 0.00040003487160184274   Iteration 77 of 100, tot loss = 5.1076964139938354, l1: 0.00011033388094707428, l2: 0.0004004357596587196   Iteration 78 of 100, tot loss = 5.1312416684933195, l1: 0.0001105912900432789, l2: 0.0004025328764841199   Iteration 79 of 100, tot loss = 5.1376948703693435, l1: 0.00011054929872662348, l2: 0.0004032201880052733   Iteration 80 of 100, tot loss = 5.101007191836834, l1: 0.00010993002674695163, l2: 0.00040017069222813005   Iteration 81 of 100, tot loss = 5.118574164531849, l1: 0.00011009851638206143, l2: 0.00040175889965066294   Iteration 82 of 100, tot loss = 5.128247160737107, l1: 0.00011021550509634303, l2: 0.00040260921040742955   Iteration 83 of 100, tot loss = 5.116957513682814, l1: 0.00010979417405555485, l2: 0.0004019015770214778   Iteration 84 of 100, tot loss = 5.10582102338473, l1: 0.00010963933246308617, l2: 0.0004009427693312145   Iteration 85 of 100, tot loss = 5.094798316675074, l1: 0.00010946742468684747, l2: 0.0004000124062740189   Iteration 86 of 100, tot loss = 5.067762531513392, l1: 0.0001090568902602468, l2: 0.0003977193619628155   Iteration 87 of 100, tot loss = 5.053371504805554, l1: 0.00010885223899345749, l2: 0.000396484910542595   Iteration 88 of 100, tot loss = 5.051246972246603, l1: 0.0001090100113701324, l2: 0.0003961146850832103   Iteration 89 of 100, tot loss = 5.027743635552652, l1: 0.00010852547911985965, l2: 0.00039424888365849684   Iteration 90 of 100, tot loss = 5.036675888962216, l1: 0.00010873713217733894, l2: 0.00039493045575606325   Iteration 91 of 100, tot loss = 5.034411457868723, l1: 0.00010874422858270898, l2: 0.00039469691602369916   Iteration 92 of 100, tot loss = 5.024455569360567, l1: 0.00010855913731812389, l2: 0.0003938864187069197   Iteration 93 of 100, tot loss = 5.034406658141844, l1: 0.00010870953165752042, l2: 0.00039473113351030855   Iteration 94 of 100, tot loss = 5.031809762437293, l1: 0.00010827811387029447, l2: 0.0003949028617538948   Iteration 95 of 100, tot loss = 5.050150782183597, l1: 0.00010860255826724155, l2: 0.0003964125187945013   Iteration 96 of 100, tot loss = 5.034339749564727, l1: 0.00010861124690109136, l2: 0.0003948227269271835   Iteration 97 of 100, tot loss = 5.042259318312419, l1: 0.00010825037109676374, l2: 0.000395975559175773   Iteration 98 of 100, tot loss = 5.054170675423681, l1: 0.00010827793259374388, l2: 0.000397139133105045   Iteration 99 of 100, tot loss = 5.045611341794332, l1: 0.00010812404405148088, l2: 0.00039643708824043663   Iteration 100 of 100, tot loss = 5.040216239690781, l1: 0.00010792746110382722, l2: 0.00039609416111488824
   End of epoch 1286; saving model... 

Epoch 1287 of 2000
   Iteration 1 of 100, tot loss = 3.227537155151367, l1: 8.75397163326852e-05, l2: 0.000235213985433802   Iteration 2 of 100, tot loss = 3.693601608276367, l1: 9.115630018641241e-05, l2: 0.0002782038500299677   Iteration 3 of 100, tot loss = 4.797220706939697, l1: 0.00010849307727767155, l2: 0.00037122899084351957   Iteration 4 of 100, tot loss = 4.899436831474304, l1: 0.00010591718637442682, l2: 0.00038402648351620883   Iteration 5 of 100, tot loss = 5.205378723144531, l1: 0.00011118094698758796, l2: 0.0004093569121323526   Iteration 6 of 100, tot loss = 5.30003809928894, l1: 0.00011435882455164877, l2: 0.0004156449722358957   Iteration 7 of 100, tot loss = 5.547366755349295, l1: 0.00011828271817648783, l2: 0.0004364539511568312   Iteration 8 of 100, tot loss = 5.323278069496155, l1: 0.00011470907611510484, l2: 0.00041761872489587404   Iteration 9 of 100, tot loss = 5.329616705576579, l1: 0.00011308760844662579, l2: 0.00041987405469020206   Iteration 10 of 100, tot loss = 5.151154470443726, l1: 0.00011012106770067476, l2: 0.00040499437309335915   Iteration 11 of 100, tot loss = 5.087215900421143, l1: 0.00011138269606321542, l2: 0.00039733888909474695   Iteration 12 of 100, tot loss = 4.947855651378632, l1: 0.00010851033584913239, l2: 0.0003862752249309172   Iteration 13 of 100, tot loss = 5.141846711818989, l1: 0.00011215459156888895, l2: 0.0004020300783360234   Iteration 14 of 100, tot loss = 5.010592886379787, l1: 0.00010991800653365707, l2: 0.0003911412807480831   Iteration 15 of 100, tot loss = 5.027317730585734, l1: 0.0001103261936805211, l2: 0.00039240557816810906   Iteration 16 of 100, tot loss = 5.004994228482246, l1: 0.0001105898759306001, l2: 0.0003899095445376588   Iteration 17 of 100, tot loss = 5.0286929887883804, l1: 0.00011075636618496741, l2: 0.0003921129305929165   Iteration 18 of 100, tot loss = 4.856181628174252, l1: 0.00010784414047925061, l2: 0.00037777402030769736   Iteration 19 of 100, tot loss = 4.991316801623294, l1: 0.00010952536774168134, l2: 0.0003896063104835584   Iteration 20 of 100, tot loss = 5.149371629953384, l1: 0.00011125959081255132, l2: 0.0004036775717395358   Iteration 21 of 100, tot loss = 5.097261775107611, l1: 0.00011137258167894158, l2: 0.0003983535959074895   Iteration 22 of 100, tot loss = 5.122269917618144, l1: 0.00011306332205342848, l2: 0.0003991636701605537   Iteration 23 of 100, tot loss = 5.2033624493557475, l1: 0.00011458912685275604, l2: 0.00040574711949928945   Iteration 24 of 100, tot loss = 5.178165619572003, l1: 0.00011268537658300677, l2: 0.00040513118559222977   Iteration 25 of 100, tot loss = 5.147019410133362, l1: 0.00011285770844551735, l2: 0.0004018442321103066   Iteration 26 of 100, tot loss = 5.095166311814235, l1: 0.00011224383552661703, l2: 0.00039727279531339614   Iteration 27 of 100, tot loss = 5.194466922018263, l1: 0.00011298217564385764, l2: 0.00040646451654740505   Iteration 28 of 100, tot loss = 5.211996346712112, l1: 0.00011398337288223306, l2: 0.00040721626075017933   Iteration 29 of 100, tot loss = 5.377058584114601, l1: 0.0001159251183582518, l2: 0.0004217807386182891   Iteration 30 of 100, tot loss = 5.354932002226511, l1: 0.00011552214721935647, l2: 0.0004199710519363483   Iteration 31 of 100, tot loss = 5.38892682521574, l1: 0.00011630929809818494, l2: 0.00042258338409385853   Iteration 32 of 100, tot loss = 5.303880315274, l1: 0.00011526759578828205, l2: 0.0004151204357185634   Iteration 33 of 100, tot loss = 5.311136560006575, l1: 0.00011565279518518682, l2: 0.000415460860081525   Iteration 34 of 100, tot loss = 5.347480475902557, l1: 0.00011612262578102458, l2: 0.00041862542189739863   Iteration 35 of 100, tot loss = 5.3502822705677575, l1: 0.00011642318266759892, l2: 0.0004186050450828459   Iteration 36 of 100, tot loss = 5.3764637013276415, l1: 0.00011677371235742208, l2: 0.00042087265885331563   Iteration 37 of 100, tot loss = 5.33717033347568, l1: 0.00011615414474068234, l2: 0.00041756288909881905   Iteration 38 of 100, tot loss = 5.324042850419095, l1: 0.00011603401272671028, l2: 0.0004163702726241593   Iteration 39 of 100, tot loss = 5.383042962123186, l1: 0.00011687689971623536, l2: 0.00042142739626937185   Iteration 40 of 100, tot loss = 5.366202965378761, l1: 0.00011690925684888498, l2: 0.0004197110400127713   Iteration 41 of 100, tot loss = 5.392288780793911, l1: 0.00011727562298069754, l2: 0.0004219532548165994   Iteration 42 of 100, tot loss = 5.399656378087544, l1: 0.0001173029232588652, l2: 0.00042266271512268023   Iteration 43 of 100, tot loss = 5.385243352069411, l1: 0.00011689788118738486, l2: 0.00042162645407844073   Iteration 44 of 100, tot loss = 5.391330689191818, l1: 0.00011729883102485805, l2: 0.00042183423846092245   Iteration 45 of 100, tot loss = 5.401267319255405, l1: 0.00011730824666705707, l2: 0.00042281848663050265   Iteration 46 of 100, tot loss = 5.362516141456107, l1: 0.00011680365020330773, l2: 0.00041944796533283335   Iteration 47 of 100, tot loss = 5.330611596716211, l1: 0.00011649557865816764, l2: 0.00041656558243359656   Iteration 48 of 100, tot loss = 5.262059561908245, l1: 0.00011489948315102083, l2: 0.000411306474537317   Iteration 49 of 100, tot loss = 5.2695920443048285, l1: 0.00011525770397773678, l2: 0.00041170150210263626   Iteration 50 of 100, tot loss = 5.271236021518707, l1: 0.00011571292074222584, l2: 0.0004114106833003461   Iteration 51 of 100, tot loss = 5.279102019235199, l1: 0.00011609856436752678, l2: 0.0004118116385321699   Iteration 52 of 100, tot loss = 5.334007953221981, l1: 0.0001171037366265619, l2: 0.0004162970603479502   Iteration 53 of 100, tot loss = 5.3151357241396635, l1: 0.00011694242027072007, l2: 0.00041457115373802635   Iteration 54 of 100, tot loss = 5.3012965189086065, l1: 0.0001166739206113292, l2: 0.0004134557330635963   Iteration 55 of 100, tot loss = 5.318898619305004, l1: 0.00011677170253030702, l2: 0.00041511816141957586   Iteration 56 of 100, tot loss = 5.346137602414403, l1: 0.00011645798342994697, l2: 0.0004181557783990034   Iteration 57 of 100, tot loss = 5.357531583100035, l1: 0.00011690765173274517, l2: 0.0004188455084722751   Iteration 58 of 100, tot loss = 5.3508336893443404, l1: 0.00011677539046574785, l2: 0.0004183079797477493   Iteration 59 of 100, tot loss = 5.317451616465035, l1: 0.00011607961956326755, l2: 0.00041566554346453337   Iteration 60 of 100, tot loss = 5.305681039889653, l1: 0.0001158814018102324, l2: 0.000414686703394788   Iteration 61 of 100, tot loss = 5.285147668885403, l1: 0.00011579640137869484, l2: 0.00041271836666741454   Iteration 62 of 100, tot loss = 5.332969759741137, l1: 0.00011657810841091803, l2: 0.00041671886884291927   Iteration 63 of 100, tot loss = 5.3623982024571255, l1: 0.00011712932719669808, l2: 0.0004191104944477538   Iteration 64 of 100, tot loss = 5.335876038298011, l1: 0.0001168789338521492, l2: 0.00041670867130960687   Iteration 65 of 100, tot loss = 5.330926988675044, l1: 0.00011698892882962424, l2: 0.0004161037715116086   Iteration 66 of 100, tot loss = 5.3097767595088845, l1: 0.00011633442414732241, l2: 0.00041464325322797805   Iteration 67 of 100, tot loss = 5.294555299317659, l1: 0.00011592365338930986, l2: 0.00041353187798313907   Iteration 68 of 100, tot loss = 5.278947891557918, l1: 0.00011554309452656205, l2: 0.00041235169577275347   Iteration 69 of 100, tot loss = 5.292219063510066, l1: 0.00011548925170285182, l2: 0.0004137326563384546   Iteration 70 of 100, tot loss = 5.297795947960445, l1: 0.00011545677807589527, l2: 0.0004143228184797668   Iteration 71 of 100, tot loss = 5.273945652263265, l1: 0.00011491036639199592, l2: 0.0004124842005335844   Iteration 72 of 100, tot loss = 5.261719271540642, l1: 0.00011469044466139167, l2: 0.0004114814845605805   Iteration 73 of 100, tot loss = 5.277938449219482, l1: 0.00011446439354698021, l2: 0.0004133294533446993   Iteration 74 of 100, tot loss = 5.269491896436021, l1: 0.00011416911941007522, l2: 0.0004127800721845061   Iteration 75 of 100, tot loss = 5.274221742947896, l1: 0.00011441582517970043, l2: 0.00041300635047567387   Iteration 76 of 100, tot loss = 5.271168707232726, l1: 0.00011425888897065306, l2: 0.0004128579833042367   Iteration 77 of 100, tot loss = 5.2446652836613845, l1: 0.00011383629208029998, l2: 0.00041063023767397777   Iteration 78 of 100, tot loss = 5.240704403473781, l1: 0.00011365440164249427, l2: 0.0004104160403725333   Iteration 79 of 100, tot loss = 5.244020560119726, l1: 0.00011362478471710347, l2: 0.00041077727351714917   Iteration 80 of 100, tot loss = 5.250942541658878, l1: 0.00011343654418851656, l2: 0.00041165771253872665   Iteration 81 of 100, tot loss = 5.305003544430674, l1: 0.00011425911491833063, l2: 0.0004162412427112828   Iteration 82 of 100, tot loss = 5.282365846924666, l1: 0.00011366835239708208, l2: 0.00041456823521180124   Iteration 83 of 100, tot loss = 5.3084389689456986, l1: 0.00011405679741797194, l2: 0.00041678710209475047   Iteration 84 of 100, tot loss = 5.285583085957027, l1: 0.00011371011970324963, l2: 0.0004148481914723691   Iteration 85 of 100, tot loss = 5.254507641231313, l1: 0.00011328434574191788, l2: 0.0004121664209592649   Iteration 86 of 100, tot loss = 5.263684321281522, l1: 0.00011356686610629685, l2: 0.0004128015682718522   Iteration 87 of 100, tot loss = 5.258788293805615, l1: 0.0001133390828674718, l2: 0.0004125397489138547   Iteration 88 of 100, tot loss = 5.2286359979347745, l1: 0.00011284431444768084, l2: 0.0004100192876318364   Iteration 89 of 100, tot loss = 5.220181999581583, l1: 0.00011260967041605173, l2: 0.000409408531503574   Iteration 90 of 100, tot loss = 5.204846094714271, l1: 0.00011247031591968456, l2: 0.00040801429568091405   Iteration 91 of 100, tot loss = 5.228326094019544, l1: 0.00011264958955727486, l2: 0.000410183022153093   Iteration 92 of 100, tot loss = 5.199504293825315, l1: 0.0001122691560142456, l2: 0.0004076812754488935   Iteration 93 of 100, tot loss = 5.228997175411512, l1: 0.0001126658298548997, l2: 0.0004102338898128339   Iteration 94 of 100, tot loss = 5.273458872703796, l1: 0.00011331169836457194, l2: 0.00041403419086139925   Iteration 95 of 100, tot loss = 5.258904516069513, l1: 0.00011291405355318841, l2: 0.0004129764000505307   Iteration 96 of 100, tot loss = 5.284309674054384, l1: 0.00011308635131020613, l2: 0.00041534461782551563   Iteration 97 of 100, tot loss = 5.287939847130136, l1: 0.00011309531065617381, l2: 0.00041569867540395714   Iteration 98 of 100, tot loss = 5.284949520412757, l1: 0.00011307338566123745, l2: 0.0004154215678180644   Iteration 99 of 100, tot loss = 5.2830037625149044, l1: 0.00011298273885687295, l2: 0.000415317638808946   Iteration 100 of 100, tot loss = 5.282744220495224, l1: 0.0001131704152430757, l2: 0.00041510400813422166
   End of epoch 1287; saving model... 

Epoch 1288 of 2000
   Iteration 1 of 100, tot loss = 6.026132583618164, l1: 0.00010073883458971977, l2: 0.0005018744268454611   Iteration 2 of 100, tot loss = 5.831639051437378, l1: 0.00011012619870598428, l2: 0.00047303771134465933   Iteration 3 of 100, tot loss = 6.1229227383931475, l1: 0.00012339503397621834, l2: 0.0004888972422728936   Iteration 4 of 100, tot loss = 5.496629893779755, l1: 0.00011458436529210303, l2: 0.0004350786257418804   Iteration 5 of 100, tot loss = 5.302349996566773, l1: 0.0001123988869949244, l2: 0.00041783612105064096   Iteration 6 of 100, tot loss = 5.306634068489075, l1: 0.00011295671720290557, l2: 0.00041770669728672755   Iteration 7 of 100, tot loss = 5.052392618996756, l1: 0.0001041936972926903, l2: 0.0004010455699504486   Iteration 8 of 100, tot loss = 5.074497580528259, l1: 0.00010385542464064201, l2: 0.00040359433842240833   Iteration 9 of 100, tot loss = 5.075739065806071, l1: 0.00010279756760509271, l2: 0.00040477634239424433   Iteration 10 of 100, tot loss = 5.192384481430054, l1: 0.00010631555269355886, l2: 0.0004129228938836604   Iteration 11 of 100, tot loss = 5.195117777044123, l1: 0.00010776712455977263, l2: 0.0004117446501781656   Iteration 12 of 100, tot loss = 5.132612784703572, l1: 0.0001052471598086413, l2: 0.00040801411523716524   Iteration 13 of 100, tot loss = 5.014410440738384, l1: 0.000105089484490096, l2: 0.00039635155609665584   Iteration 14 of 100, tot loss = 4.986846634319851, l1: 0.0001044941641989031, l2: 0.0003941904973804152   Iteration 15 of 100, tot loss = 5.008131265640259, l1: 0.00010606969590298831, l2: 0.0003947434287207822   Iteration 16 of 100, tot loss = 5.097351685166359, l1: 0.00010881729031098075, l2: 0.00040091787741403095   Iteration 17 of 100, tot loss = 5.160941390430226, l1: 0.00011057728518019704, l2: 0.0004055168503648875   Iteration 18 of 100, tot loss = 5.280836780865987, l1: 0.00011176972556212504, l2: 0.0004163139496490152   Iteration 19 of 100, tot loss = 5.153575621153179, l1: 0.00011027993566944803, l2: 0.0004050776241408465   Iteration 20 of 100, tot loss = 5.183101534843445, l1: 0.00011098643590230494, l2: 0.0004073237148986664   Iteration 21 of 100, tot loss = 5.1196689038049605, l1: 0.00011067499629765128, l2: 0.00040129189160541587   Iteration 22 of 100, tot loss = 5.2008972059596665, l1: 0.00011225919661228545, l2: 0.00040783052166013726   Iteration 23 of 100, tot loss = 5.409361663072006, l1: 0.00011503092821879798, l2: 0.00042590523535972864   Iteration 24 of 100, tot loss = 5.519029051065445, l1: 0.00011654256407685655, l2: 0.00043536033930043533   Iteration 25 of 100, tot loss = 5.688472776412964, l1: 0.00011924638849450275, l2: 0.00044960088620427997   Iteration 26 of 100, tot loss = 5.645152027790363, l1: 0.00011834823173837952, l2: 0.0004461669668671675   Iteration 27 of 100, tot loss = 5.636579557701394, l1: 0.00011827424222482714, l2: 0.00044538371029948057   Iteration 28 of 100, tot loss = 5.605466714927128, l1: 0.00011777223699027672, l2: 0.00044277443053683134   Iteration 29 of 100, tot loss = 5.642742707811553, l1: 0.00011926306088471079, l2: 0.00044501120684234874   Iteration 30 of 100, tot loss = 5.636634929974874, l1: 0.00011962203861912713, l2: 0.00044404145107061294   Iteration 31 of 100, tot loss = 5.654744555873256, l1: 0.00011990702337390112, l2: 0.0004455674286638837   Iteration 32 of 100, tot loss = 5.596653498709202, l1: 0.00011877250312863907, l2: 0.000440892843016627   Iteration 33 of 100, tot loss = 5.547804933605772, l1: 0.00011780178002399308, l2: 0.0004369787096617405   Iteration 34 of 100, tot loss = 5.4557195971993835, l1: 0.00011572340554266702, l2: 0.0004298485509934835   Iteration 35 of 100, tot loss = 5.390109893253872, l1: 0.00011395242347914194, l2: 0.0004250585629571495   Iteration 36 of 100, tot loss = 5.330660283565521, l1: 0.00011306704153765977, l2: 0.00041999898449931707   Iteration 37 of 100, tot loss = 5.355674105721551, l1: 0.00011347469852488437, l2: 0.00042209270995462666   Iteration 38 of 100, tot loss = 5.496060503156562, l1: 0.00011534863791128278, l2: 0.0004342574117656209   Iteration 39 of 100, tot loss = 5.481100907692542, l1: 0.00011509601646162666, l2: 0.0004330140737016709   Iteration 40 of 100, tot loss = 5.477303856611252, l1: 0.00011434182106313528, l2: 0.0004333885641244706   Iteration 41 of 100, tot loss = 5.395614583317826, l1: 0.00011306271867730042, l2: 0.0004264987389721749   Iteration 42 of 100, tot loss = 5.327463467915853, l1: 0.00011162004643882115, l2: 0.0004211263000061514   Iteration 43 of 100, tot loss = 5.325088312459546, l1: 0.00011144337309646247, l2: 0.00042106545777980585   Iteration 44 of 100, tot loss = 5.317752675576643, l1: 0.00011181848830429689, l2: 0.00041995677913126366   Iteration 45 of 100, tot loss = 5.290271833207872, l1: 0.00011129430988350779, l2: 0.0004177328732718403   Iteration 46 of 100, tot loss = 5.2180337413497595, l1: 0.00011019717590977012, l2: 0.00041160619793631866   Iteration 47 of 100, tot loss = 5.209866110314715, l1: 0.00011032351880629071, l2: 0.00041066309185748166   Iteration 48 of 100, tot loss = 5.203573835392793, l1: 0.00010977662721719146, l2: 0.0004105807568824578   Iteration 49 of 100, tot loss = 5.16964675212393, l1: 0.00010959468005350981, l2: 0.0004073699959434987   Iteration 50 of 100, tot loss = 5.132624185085296, l1: 0.00010907908254011999, l2: 0.00040418333635898305   Iteration 51 of 100, tot loss = 5.120430429776509, l1: 0.00010866603784676295, l2: 0.0004033770052118081   Iteration 52 of 100, tot loss = 5.12645112321927, l1: 0.00010883717837154669, l2: 0.00040380793367288646   Iteration 53 of 100, tot loss = 5.129937151692948, l1: 0.00010907721620894318, l2: 0.0004039164986746948   Iteration 54 of 100, tot loss = 5.112986147403717, l1: 0.00010868530113873055, l2: 0.00040261331364659043   Iteration 55 of 100, tot loss = 5.1718270626935094, l1: 0.00010998683607216331, l2: 0.000407195870436474   Iteration 56 of 100, tot loss = 5.136257288711412, l1: 0.00010966078954360066, l2: 0.0004039649395833424   Iteration 57 of 100, tot loss = 5.123903238982485, l1: 0.00010914743561124054, l2: 0.0004032428885438365   Iteration 58 of 100, tot loss = 5.096903441281154, l1: 0.00010877276038407781, l2: 0.0004009175842431181   Iteration 59 of 100, tot loss = 5.102813359034264, l1: 0.00010931316026441074, l2: 0.0004009681764173211   Iteration 60 of 100, tot loss = 5.101770220200221, l1: 0.00010982588652647488, l2: 0.0004003511365832916   Iteration 61 of 100, tot loss = 5.080871068063329, l1: 0.00010958647306056189, l2: 0.0003985006352180432   Iteration 62 of 100, tot loss = 5.0491250541902355, l1: 0.00010934525662084906, l2: 0.0003955672504924869   Iteration 63 of 100, tot loss = 5.038662159253681, l1: 0.00010929722691163612, l2: 0.0003945689908476428   Iteration 64 of 100, tot loss = 5.037290265783668, l1: 0.00010862930861321729, l2: 0.00039509971907136787   Iteration 65 of 100, tot loss = 5.032866190030025, l1: 0.00010809744142954094, l2: 0.00039518917888367117   Iteration 66 of 100, tot loss = 5.003591636816661, l1: 0.00010755093055856626, l2: 0.00039280823451311636   Iteration 67 of 100, tot loss = 5.000238217524628, l1: 0.00010730791581775507, l2: 0.0003927159070996429   Iteration 68 of 100, tot loss = 5.002377473256168, l1: 0.00010765255977757453, l2: 0.00039258518860579524   Iteration 69 of 100, tot loss = 4.997770100400068, l1: 0.0001076377779679825, l2: 0.00039213923298740303   Iteration 70 of 100, tot loss = 4.992077103682926, l1: 0.00010774161798638358, l2: 0.00039146609321635747   Iteration 71 of 100, tot loss = 4.981728928189882, l1: 0.00010742491366740145, l2: 0.00039074798016218654   Iteration 72 of 100, tot loss = 4.972895514633921, l1: 0.00010750212828093531, l2: 0.00038978742425873253   Iteration 73 of 100, tot loss = 4.963666927324582, l1: 0.00010746772376515852, l2: 0.0003888989696661903   Iteration 74 of 100, tot loss = 4.964213221459775, l1: 0.00010781293634421229, l2: 0.0003886083864632088   Iteration 75 of 100, tot loss = 4.929807667732239, l1: 0.00010713590915353658, l2: 0.00038584485824685546   Iteration 76 of 100, tot loss = 4.909385833301042, l1: 0.00010700759321480291, l2: 0.00038393099078491916   Iteration 77 of 100, tot loss = 4.949302012270147, l1: 0.00010764568774052291, l2: 0.0003872845138955329   Iteration 78 of 100, tot loss = 4.94073161864892, l1: 0.0001075711001575632, l2: 0.0003865020624242532   Iteration 79 of 100, tot loss = 4.924758453912373, l1: 0.0001075430351837768, l2: 0.00038493281087390133   Iteration 80 of 100, tot loss = 4.920217259228229, l1: 0.00010755570669971349, l2: 0.00038446601975010707   Iteration 81 of 100, tot loss = 4.926351001233231, l1: 0.00010789499342704571, l2: 0.0003847401074628219   Iteration 82 of 100, tot loss = 4.918471680908668, l1: 0.00010786031621469969, l2: 0.0003839868522623963   Iteration 83 of 100, tot loss = 4.919241268950772, l1: 0.00010806247571810058, l2: 0.0003838616518685258   Iteration 84 of 100, tot loss = 4.895899836506162, l1: 0.00010759498317860388, l2: 0.00038199500098320033   Iteration 85 of 100, tot loss = 4.891333985328674, l1: 0.00010733759250690448, l2: 0.0003817958063925343   Iteration 86 of 100, tot loss = 4.862694187219753, l1: 0.00010660160364221705, l2: 0.0003796678153543495   Iteration 87 of 100, tot loss = 4.891033349366023, l1: 0.00010715641089529067, l2: 0.0003819469240060526   Iteration 88 of 100, tot loss = 4.880772521550005, l1: 0.00010728077131716418, l2: 0.0003807964805508329   Iteration 89 of 100, tot loss = 4.86317289947124, l1: 0.00010671951355781469, l2: 0.00037959777603574686   Iteration 90 of 100, tot loss = 4.862439249621497, l1: 0.00010656814451471695, l2: 0.0003796757797115586   Iteration 91 of 100, tot loss = 4.856693497070899, l1: 0.00010651378133743591, l2: 0.00037915556778578643   Iteration 92 of 100, tot loss = 4.84761936120365, l1: 0.00010621154931710487, l2: 0.0003785503861803116   Iteration 93 of 100, tot loss = 4.853039904307294, l1: 0.00010648697793986949, l2: 0.0003788170116197478   Iteration 94 of 100, tot loss = 4.848854439055666, l1: 0.00010649913491896711, l2: 0.000378386307931307   Iteration 95 of 100, tot loss = 4.857514204477009, l1: 0.0001065258719455941, l2: 0.00037922554765827953   Iteration 96 of 100, tot loss = 4.884210639943679, l1: 0.00010689165871250832, l2: 0.0003815294042700164   Iteration 97 of 100, tot loss = 4.870816180386494, l1: 0.00010655960593175758, l2: 0.00038052201103035014   Iteration 98 of 100, tot loss = 4.86281174907879, l1: 0.00010635092554849154, l2: 0.0003799302485647934   Iteration 99 of 100, tot loss = 4.852599044038792, l1: 0.00010621818952554027, l2: 0.00037904171422453167   Iteration 100 of 100, tot loss = 4.866849032640457, l1: 0.00010632623409037478, l2: 0.0003803586683352478
   End of epoch 1288; saving model... 

Epoch 1289 of 2000
   Iteration 1 of 100, tot loss = 6.801395416259766, l1: 0.00015170485130511224, l2: 0.0005284346407279372   Iteration 2 of 100, tot loss = 4.863155007362366, l1: 0.00011918025120394304, l2: 0.0003671352314995602   Iteration 3 of 100, tot loss = 4.750994920730591, l1: 0.00011927856394322589, l2: 0.00035582091853333014   Iteration 4 of 100, tot loss = 4.91422837972641, l1: 0.00011515549522300716, l2: 0.0003762673368328251   Iteration 5 of 100, tot loss = 4.867991495132446, l1: 0.00011332259891787544, l2: 0.0003734765399713069   Iteration 6 of 100, tot loss = 4.470075408617656, l1: 0.00010762872140427741, l2: 0.00033937880895488587   Iteration 7 of 100, tot loss = 4.569099528448922, l1: 0.00010552856110734865, l2: 0.00035138138628099114   Iteration 8 of 100, tot loss = 4.701937824487686, l1: 0.00010630044835124863, l2: 0.00036389333217812236   Iteration 9 of 100, tot loss = 4.578582631217109, l1: 0.00010148605845946197, l2: 0.00035637220167296217   Iteration 10 of 100, tot loss = 4.6041504621505736, l1: 0.00010062259025289677, l2: 0.00035979245294583964   Iteration 11 of 100, tot loss = 4.87205585566434, l1: 0.00010289400432322344, l2: 0.0003843115819935602   Iteration 12 of 100, tot loss = 4.62898991505305, l1: 9.909851784565642e-05, l2: 0.00036380047458806075   Iteration 13 of 100, tot loss = 4.486585763784555, l1: 9.769974265229673e-05, l2: 0.00035095883448285837   Iteration 14 of 100, tot loss = 4.779095717838833, l1: 9.889040977993448e-05, l2: 0.0003790191642890152   Iteration 15 of 100, tot loss = 4.717207781473795, l1: 9.866584732662887e-05, l2: 0.00037305493315216155   Iteration 16 of 100, tot loss = 4.765288233757019, l1: 0.00010088780618389137, l2: 0.0003756410196729121   Iteration 17 of 100, tot loss = 4.895451910355511, l1: 0.00010270869996234337, l2: 0.000386836490874617   Iteration 18 of 100, tot loss = 4.913249863518609, l1: 0.00010376519478288376, l2: 0.0003875597928223821   Iteration 19 of 100, tot loss = 4.791953689173648, l1: 0.00010144442517469685, l2: 0.0003777509455654868   Iteration 20 of 100, tot loss = 4.7644134044647215, l1: 0.00010111163683177438, l2: 0.0003753297067305539   Iteration 21 of 100, tot loss = 4.7290957768758135, l1: 0.00010079851441939051, l2: 0.0003721110662722605   Iteration 22 of 100, tot loss = 4.736847400665283, l1: 0.00010156690735708584, l2: 0.00037211783571613273   Iteration 23 of 100, tot loss = 4.676416283068449, l1: 0.00010083349448193431, l2: 0.00036680813734520876   Iteration 24 of 100, tot loss = 4.713478356599808, l1: 0.00010223955329517291, l2: 0.00036910828506127774   Iteration 25 of 100, tot loss = 4.69455340385437, l1: 0.00010224446596112103, l2: 0.0003672108781756833   Iteration 26 of 100, tot loss = 4.714145431151757, l1: 0.00010335372420600973, l2: 0.00036806082117926475   Iteration 27 of 100, tot loss = 4.838998573797721, l1: 0.00010594667724540664, l2: 0.0003779531813321497   Iteration 28 of 100, tot loss = 4.819983337606702, l1: 0.00010574245555160035, l2: 0.00037625587875871655   Iteration 29 of 100, tot loss = 4.807540934661339, l1: 0.00010622531734607664, l2: 0.0003745287776210121   Iteration 30 of 100, tot loss = 4.841906412442525, l1: 0.00010694875406140152, l2: 0.00037724188926707334   Iteration 31 of 100, tot loss = 4.834732078736828, l1: 0.0001066477249673147, l2: 0.0003768254851644498   Iteration 32 of 100, tot loss = 4.828197307884693, l1: 0.0001064943417077302, l2: 0.000376325392153376   Iteration 33 of 100, tot loss = 4.836695259267634, l1: 0.00010700681587243734, l2: 0.0003766627134719976   Iteration 34 of 100, tot loss = 4.810523713336272, l1: 0.00010619251000153942, l2: 0.0003748598644640023   Iteration 35 of 100, tot loss = 4.775340563910348, l1: 0.00010594381996530242, l2: 0.0003715902396444497   Iteration 36 of 100, tot loss = 4.854670239819421, l1: 0.00010684951060587385, l2: 0.00037861751681905135   Iteration 37 of 100, tot loss = 4.833684824608468, l1: 0.00010657942196278757, l2: 0.0003767890640930902   Iteration 38 of 100, tot loss = 4.832542588836269, l1: 0.00010698454704987326, l2: 0.0003762697149221295   Iteration 39 of 100, tot loss = 4.908069531122844, l1: 0.0001082703508869506, l2: 0.00038253660619748424   Iteration 40 of 100, tot loss = 4.870564091205597, l1: 0.00010757883937912993, l2: 0.00037947757336951324   Iteration 41 of 100, tot loss = 4.854638204341981, l1: 0.00010705250350860652, l2: 0.0003784113196604999   Iteration 42 of 100, tot loss = 4.86684893426441, l1: 0.00010734360988689808, l2: 0.0003793412861774587   Iteration 43 of 100, tot loss = 4.911400196164153, l1: 0.00010842572458407863, l2: 0.00038271429738690425   Iteration 44 of 100, tot loss = 4.899989171461626, l1: 0.00010797046326677611, l2: 0.0003820284569387282   Iteration 45 of 100, tot loss = 4.9232214397854275, l1: 0.00010835713684274298, l2: 0.00038396501040551814   Iteration 46 of 100, tot loss = 4.940432538156924, l1: 0.00010881025351092988, l2: 0.0003852330033702816   Iteration 47 of 100, tot loss = 4.938583394314381, l1: 0.00010918945980015547, l2: 0.00038466888245819334   Iteration 48 of 100, tot loss = 4.924504558245341, l1: 0.000109236663623354, l2: 0.0003832137945209979   Iteration 49 of 100, tot loss = 4.901865219583317, l1: 0.00010926363756880164, l2: 0.00038092288667364614   Iteration 50 of 100, tot loss = 4.8601949548721315, l1: 0.00010839577676961198, l2: 0.00037762372143333776   Iteration 51 of 100, tot loss = 4.847164037180882, l1: 0.0001083294198763849, l2: 0.00037638698710247364   Iteration 52 of 100, tot loss = 4.863275523369129, l1: 0.00010856404207096112, l2: 0.0003777635129853689   Iteration 53 of 100, tot loss = 4.8877745799298555, l1: 0.00010893887959372357, l2: 0.0003798385812430906   Iteration 54 of 100, tot loss = 4.874808430671692, l1: 0.00010895878949668258, l2: 0.00037852205638989325   Iteration 55 of 100, tot loss = 4.874911277944391, l1: 0.00010879982411014762, l2: 0.0003786913063694638   Iteration 56 of 100, tot loss = 4.847039299351828, l1: 0.00010859132050557361, l2: 0.0003761126120350257   Iteration 57 of 100, tot loss = 4.826158377162197, l1: 0.00010835526081153371, l2: 0.00037426057959939437   Iteration 58 of 100, tot loss = 4.801979915849094, l1: 0.0001080310057973136, l2: 0.00037216698823111326   Iteration 59 of 100, tot loss = 4.8340849189435025, l1: 0.00010838830287142073, l2: 0.0003750201914522607   Iteration 60 of 100, tot loss = 4.853654444217682, l1: 0.00010880076297326013, l2: 0.0003765646838777078   Iteration 61 of 100, tot loss = 4.88635892946212, l1: 0.00010923644361757964, l2: 0.0003793994513470069   Iteration 62 of 100, tot loss = 4.8802727691588865, l1: 0.00010912187339741766, l2: 0.00037890540542937217   Iteration 63 of 100, tot loss = 4.845388302727351, l1: 0.00010841745074324694, l2: 0.00037612138123857596   Iteration 64 of 100, tot loss = 4.825513027608395, l1: 0.00010792996317832149, l2: 0.0003746213410522614   Iteration 65 of 100, tot loss = 4.826389371431791, l1: 0.00010778348033244794, l2: 0.00037485545841403877   Iteration 66 of 100, tot loss = 4.825397159114028, l1: 0.00010759703427814229, l2: 0.0003749426837065116   Iteration 67 of 100, tot loss = 4.781637150849869, l1: 0.00010670405243950855, l2: 0.0003714596645458282   Iteration 68 of 100, tot loss = 4.81562226134188, l1: 0.00010668597122489968, l2: 0.0003748762561265133   Iteration 69 of 100, tot loss = 4.847983230715212, l1: 0.00010752696560803747, l2: 0.0003772713587723969   Iteration 70 of 100, tot loss = 4.866685446671077, l1: 0.00010784332470003782, l2: 0.00037882522135207963   Iteration 71 of 100, tot loss = 4.882858996659937, l1: 0.00010830349010959739, l2: 0.0003799824107644922   Iteration 72 of 100, tot loss = 4.906686108973291, l1: 0.00010858730072666529, l2: 0.0003820813110198489   Iteration 73 of 100, tot loss = 4.882674845930648, l1: 0.00010822533748642069, l2: 0.000380042148195845   Iteration 74 of 100, tot loss = 4.874763980105117, l1: 0.00010798868921434355, l2: 0.0003794877096928807   Iteration 75 of 100, tot loss = 4.858827652931214, l1: 0.00010775597533211112, l2: 0.00037812679074704646   Iteration 76 of 100, tot loss = 4.8329312315112665, l1: 0.00010711893312538068, l2: 0.000376174190597866   Iteration 77 of 100, tot loss = 4.818400855188246, l1: 0.00010657122454306643, l2: 0.00037526886161059965   Iteration 78 of 100, tot loss = 4.820323762221214, l1: 0.00010699944491184746, l2: 0.0003750329318184716   Iteration 79 of 100, tot loss = 4.813734454444692, l1: 0.00010700183495056899, l2: 0.0003743716108515458   Iteration 80 of 100, tot loss = 4.7837153151631355, l1: 0.00010639423426255235, l2: 0.0003719772976182867   Iteration 81 of 100, tot loss = 4.795852886305915, l1: 0.00010658712077808256, l2: 0.00037299816886158545   Iteration 82 of 100, tot loss = 4.774604112636752, l1: 0.00010620409552868251, l2: 0.0003712563168558451   Iteration 83 of 100, tot loss = 4.748803760631975, l1: 0.00010585431703815169, l2: 0.0003690260602021029   Iteration 84 of 100, tot loss = 4.743610468648729, l1: 0.00010568138367532465, l2: 0.00036867966472137984   Iteration 85 of 100, tot loss = 4.757921680282144, l1: 0.00010594961016014327, l2: 0.00036984255934572393   Iteration 86 of 100, tot loss = 4.750584426314332, l1: 0.00010568725972620467, l2: 0.0003693711843753104   Iteration 87 of 100, tot loss = 4.7276960940196595, l1: 0.0001050110562385365, l2: 0.00036775855459246097   Iteration 88 of 100, tot loss = 4.696668050505898, l1: 0.00010457948002857103, l2: 0.0003650873264136449   Iteration 89 of 100, tot loss = 4.676195747396919, l1: 0.00010425148343273669, l2: 0.00036336809283598546   Iteration 90 of 100, tot loss = 4.681755142741733, l1: 0.00010395370914516712, l2: 0.0003642218066185402   Iteration 91 of 100, tot loss = 4.690798966439216, l1: 0.00010432003562148037, l2: 0.0003647598628898837   Iteration 92 of 100, tot loss = 4.687952769839245, l1: 0.00010419783879826909, l2: 0.00036459744027841066   Iteration 93 of 100, tot loss = 4.714245362948346, l1: 0.00010459697515089115, l2: 0.00036682756332885873   Iteration 94 of 100, tot loss = 4.743421612901891, l1: 0.00010499456282636953, l2: 0.00036934760050536035   Iteration 95 of 100, tot loss = 4.72823499629372, l1: 0.0001047117907115822, l2: 0.0003681117109648001   Iteration 96 of 100, tot loss = 4.725166412691276, l1: 0.00010428170742216025, l2: 0.00036823493595269   Iteration 97 of 100, tot loss = 4.747769333652614, l1: 0.00010447400177561332, l2: 0.0003703029335118811   Iteration 98 of 100, tot loss = 4.7235013173550975, l1: 0.00010409063138560706, l2: 0.0003682595022243676   Iteration 99 of 100, tot loss = 4.725932583664402, l1: 0.00010415060163300363, l2: 0.0003684426584654967   Iteration 100 of 100, tot loss = 4.6981248962879185, l1: 0.00010348433646868215, l2: 0.00036632815492339434
   End of epoch 1289; saving model... 

Epoch 1290 of 2000
   Iteration 1 of 100, tot loss = 1.8891621828079224, l1: 4.9082904297392815e-05, l2: 0.00013983332610223442   Iteration 2 of 100, tot loss = 4.078241646289825, l1: 9.75143957475666e-05, l2: 0.0003103097769781016   Iteration 3 of 100, tot loss = 4.385764956474304, l1: 0.00010747434619891767, l2: 0.0003311021573608741   Iteration 4 of 100, tot loss = 4.621607691049576, l1: 0.00011677283509925473, l2: 0.0003453879362496082   Iteration 5 of 100, tot loss = 4.984303307533264, l1: 0.00011808358976850286, l2: 0.0003803467407124117   Iteration 6 of 100, tot loss = 4.622448066870372, l1: 0.000108264820179708, l2: 0.0003539799848416199   Iteration 7 of 100, tot loss = 4.487466386386326, l1: 0.0001061145267158281, l2: 0.0003426321116941316   Iteration 8 of 100, tot loss = 4.567706272006035, l1: 0.00010616234931148938, l2: 0.0003506082794046961   Iteration 9 of 100, tot loss = 4.491056905852424, l1: 0.00010349609874538146, l2: 0.0003456095906181468   Iteration 10 of 100, tot loss = 4.373053777217865, l1: 0.0001019296334561659, l2: 0.00033537574199726805   Iteration 11 of 100, tot loss = 4.367317015474493, l1: 0.00010243741847013801, l2: 0.00033429428136018527   Iteration 12 of 100, tot loss = 4.254770189523697, l1: 9.974785128482229e-05, l2: 0.00032572916582770023   Iteration 13 of 100, tot loss = 4.364869915522062, l1: 0.00010239017669496556, l2: 0.00033409681502514734   Iteration 14 of 100, tot loss = 4.622704940182822, l1: 0.00010564771400822792, l2: 0.0003566227782617456   Iteration 15 of 100, tot loss = 4.675232116381327, l1: 0.0001064824296918232, l2: 0.00036104078365800283   Iteration 16 of 100, tot loss = 4.678779385983944, l1: 0.00010429833059788507, l2: 0.0003635796110756928   Iteration 17 of 100, tot loss = 4.549662919605479, l1: 0.00010189543001401676, l2: 0.00035307086535099456   Iteration 18 of 100, tot loss = 4.502732137839, l1: 0.00010096391573218473, l2: 0.00034930930107495643   Iteration 19 of 100, tot loss = 4.54193295303144, l1: 0.00010076934019085264, l2: 0.0003534239589690084   Iteration 20 of 100, tot loss = 4.582758420705796, l1: 0.00010186796152993338, l2: 0.000356407886283705   Iteration 21 of 100, tot loss = 4.657827462468829, l1: 0.00010303598000540486, l2: 0.00036274677023313763   Iteration 22 of 100, tot loss = 4.608540746298703, l1: 0.00010234725232294295, l2: 0.00035850682616000995   Iteration 23 of 100, tot loss = 4.6348364197689556, l1: 0.00010295266601251964, l2: 0.0003605309801692467   Iteration 24 of 100, tot loss = 4.544925207893054, l1: 0.00010132811181999084, l2: 0.0003531644130513693   Iteration 25 of 100, tot loss = 4.608584990501404, l1: 0.00010242808421025983, l2: 0.0003584304184187204   Iteration 26 of 100, tot loss = 4.551518004674178, l1: 0.00010245805480315958, l2: 0.0003526937496364833   Iteration 27 of 100, tot loss = 4.528381528677763, l1: 0.00010140747501820981, l2: 0.0003514306822412268   Iteration 28 of 100, tot loss = 4.474145535911832, l1: 0.00010075505464815901, l2: 0.00034665950391042443   Iteration 29 of 100, tot loss = 4.537757104840772, l1: 0.00010179975438875081, l2: 0.00035197596097814626   Iteration 30 of 100, tot loss = 4.660909513632457, l1: 0.0001038620397594059, l2: 0.00036222891649231317   Iteration 31 of 100, tot loss = 4.624399881209096, l1: 0.00010345871440574317, l2: 0.0003589812784470738   Iteration 32 of 100, tot loss = 4.582642059773207, l1: 0.00010255904533096327, l2: 0.0003557051650204812   Iteration 33 of 100, tot loss = 4.581802985884926, l1: 0.00010292063355639887, l2: 0.0003552596696835914   Iteration 34 of 100, tot loss = 4.551063190488255, l1: 0.00010254018161504064, l2: 0.0003525661424432388   Iteration 35 of 100, tot loss = 4.540969838414873, l1: 0.0001029697730181007, l2: 0.00035112721546153936   Iteration 36 of 100, tot loss = 4.576571043994692, l1: 0.0001038000127664418, l2: 0.00035385709553439583   Iteration 37 of 100, tot loss = 4.572195236747329, l1: 0.00010423466153804691, l2: 0.00035298486614624997   Iteration 38 of 100, tot loss = 4.5217189318255375, l1: 0.00010329376422496832, l2: 0.0003488781325071805   Iteration 39 of 100, tot loss = 4.472397892903059, l1: 0.00010193644154172104, l2: 0.00034530335128343163   Iteration 40 of 100, tot loss = 4.494460180401802, l1: 0.00010277592737111263, l2: 0.000346670094950241   Iteration 41 of 100, tot loss = 4.546866338427474, l1: 0.00010307614972655911, l2: 0.00035161048879494845   Iteration 42 of 100, tot loss = 4.5172896810940335, l1: 0.00010217356750564206, l2: 0.0003495554053751264   Iteration 43 of 100, tot loss = 4.488056457319925, l1: 0.0001018474864648915, l2: 0.00034695816413109556   Iteration 44 of 100, tot loss = 4.497542790391228, l1: 0.00010240164556307718, l2: 0.0003473526382797652   Iteration 45 of 100, tot loss = 4.54696171813541, l1: 0.00010288715065043006, l2: 0.0003518090260008143   Iteration 46 of 100, tot loss = 4.541756160881208, l1: 0.00010283276930314732, l2: 0.00035134285217677444   Iteration 47 of 100, tot loss = 4.552332327720967, l1: 0.00010345162538331041, l2: 0.00035178161194013   Iteration 48 of 100, tot loss = 4.578025994201501, l1: 0.00010365597017880646, l2: 0.000354146633981145   Iteration 49 of 100, tot loss = 4.519393405135797, l1: 0.00010227678626970559, l2: 0.00034966255889787355   Iteration 50 of 100, tot loss = 4.516340293884277, l1: 0.0001024697264074348, l2: 0.00034916430769953876   Iteration 51 of 100, tot loss = 4.6153461979884725, l1: 0.00010396497193997835, l2: 0.00035756965243148017   Iteration 52 of 100, tot loss = 4.568479684682993, l1: 0.00010313468919775914, l2: 0.0003537132839623015   Iteration 53 of 100, tot loss = 4.578726075730234, l1: 0.0001028777739331829, l2: 0.0003549948382468121   Iteration 54 of 100, tot loss = 4.616991987934819, l1: 0.00010335256983698518, l2: 0.0003583466338265377   Iteration 55 of 100, tot loss = 4.61667013168335, l1: 0.0001032792736871422, l2: 0.0003583877445304428   Iteration 56 of 100, tot loss = 4.592779155288424, l1: 0.0001033197049764567, l2: 0.0003559582156802727   Iteration 57 of 100, tot loss = 4.607406084997612, l1: 0.00010385778678985509, l2: 0.0003568828259104569   Iteration 58 of 100, tot loss = 4.620717036313024, l1: 0.00010429728580016547, l2: 0.000357774421994189   Iteration 59 of 100, tot loss = 4.624724739688938, l1: 0.00010457069953899609, l2: 0.00035790177811947417   Iteration 60 of 100, tot loss = 4.687290680408478, l1: 0.00010578735370169549, l2: 0.00036294171780658265   Iteration 61 of 100, tot loss = 4.668666788789093, l1: 0.00010585418812163388, l2: 0.00036101249432130185   Iteration 62 of 100, tot loss = 4.680125724884771, l1: 0.00010582836940527833, l2: 0.00036218420662466555   Iteration 63 of 100, tot loss = 4.685959335357424, l1: 0.00010574335257323193, l2: 0.0003628525846002121   Iteration 64 of 100, tot loss = 4.68069026991725, l1: 0.0001058500413364527, l2: 0.0003622189892666938   Iteration 65 of 100, tot loss = 4.6631993513840895, l1: 0.00010543730716632966, l2: 0.0003608826315030456   Iteration 66 of 100, tot loss = 4.645018415017561, l1: 0.00010507149327719218, l2: 0.00035943035177434933   Iteration 67 of 100, tot loss = 4.641043082991643, l1: 0.00010502871552503097, l2: 0.0003590755965630176   Iteration 68 of 100, tot loss = 4.626572651021621, l1: 0.00010467703599287608, l2: 0.0003579802330076585   Iteration 69 of 100, tot loss = 4.6101299541583955, l1: 0.00010469946162471467, l2: 0.00035631353755994445   Iteration 70 of 100, tot loss = 4.5948025975908555, l1: 0.00010449227598395997, l2: 0.00035498798741692944   Iteration 71 of 100, tot loss = 4.596776028754006, l1: 0.00010474178875291484, l2: 0.0003549358180337126   Iteration 72 of 100, tot loss = 4.592167165544298, l1: 0.0001045533710768925, l2: 0.0003546633492482619   Iteration 73 of 100, tot loss = 4.592632489661648, l1: 0.00010456643850711647, l2: 0.00035469681431207653   Iteration 74 of 100, tot loss = 4.605062413860011, l1: 0.00010488022402372539, l2: 0.0003556260218485133   Iteration 75 of 100, tot loss = 4.615207379659017, l1: 0.00010472798981936649, l2: 0.0003567927528638393   Iteration 76 of 100, tot loss = 4.583813174774773, l1: 0.0001041499309442435, l2: 0.00035423139132418377   Iteration 77 of 100, tot loss = 4.576221949094302, l1: 0.00010386414932882316, l2: 0.0003537580504327069   Iteration 78 of 100, tot loss = 4.559376988655481, l1: 0.00010370275590386696, l2: 0.0003522349477115756   Iteration 79 of 100, tot loss = 4.556931402109846, l1: 0.00010347972668949043, l2: 0.0003522134183749816   Iteration 80 of 100, tot loss = 4.55717753469944, l1: 0.00010292723136444693, l2: 0.0003527905266309972   Iteration 81 of 100, tot loss = 4.541706432530909, l1: 0.00010267626077373639, l2: 0.0003514943867533778   Iteration 82 of 100, tot loss = 4.525150554936107, l1: 0.00010237453135210317, l2: 0.00035014052832039164   Iteration 83 of 100, tot loss = 4.505177512226335, l1: 0.00010193195576212897, l2: 0.0003485857994374202   Iteration 84 of 100, tot loss = 4.522953921840305, l1: 0.00010211417646219378, l2: 0.00035018121984432496   Iteration 85 of 100, tot loss = 4.492486837330986, l1: 0.00010149667723453603, l2: 0.0003477520105661824   Iteration 86 of 100, tot loss = 4.504133763701417, l1: 0.00010157037895412434, l2: 0.0003488430017970914   Iteration 87 of 100, tot loss = 4.511944415925563, l1: 0.00010141764317362183, l2: 0.0003497768027892593   Iteration 88 of 100, tot loss = 4.517135827378794, l1: 0.00010148117015970374, l2: 0.0003502324168287239   Iteration 89 of 100, tot loss = 4.5078387514928755, l1: 0.00010139443413740327, l2: 0.0003493894452393955   Iteration 90 of 100, tot loss = 4.4963770879639515, l1: 0.00010102630828138597, l2: 0.0003486114048023915   Iteration 91 of 100, tot loss = 4.482625884014172, l1: 0.00010075871086683237, l2: 0.0003475038816699513   Iteration 92 of 100, tot loss = 4.471817380708197, l1: 0.00010038993656527727, l2: 0.00034679180569820227   Iteration 93 of 100, tot loss = 4.503825588892865, l1: 0.00010075705746027281, l2: 0.0003496255055608188   Iteration 94 of 100, tot loss = 4.494206485596109, l1: 0.00010059721840816758, l2: 0.00034882343436734987   Iteration 95 of 100, tot loss = 4.480681721787704, l1: 0.00010052168030310177, l2: 0.0003475464958587269   Iteration 96 of 100, tot loss = 4.481895085424185, l1: 0.00010076912830451572, l2: 0.0003474203845144075   Iteration 97 of 100, tot loss = 4.458984690843169, l1: 0.00010023112884379404, l2: 0.0003456673445360082   Iteration 98 of 100, tot loss = 4.482930067850619, l1: 0.00010044156487292268, l2: 0.00034785144638664525   Iteration 99 of 100, tot loss = 4.533880523961, l1: 0.00010136995078570143, l2: 0.00035201810577277547   Iteration 100 of 100, tot loss = 4.5343178570270535, l1: 0.00010133398485777434, l2: 0.00035209780515288
   End of epoch 1290; saving model... 

Epoch 1291 of 2000
   Iteration 1 of 100, tot loss = 3.9286468029022217, l1: 9.49822278926149e-05, l2: 0.00029788247775286436   Iteration 2 of 100, tot loss = 3.6893750429153442, l1: 9.26863394852262e-05, l2: 0.0002762511867331341   Iteration 3 of 100, tot loss = 4.460068305333455, l1: 0.00010699359942615654, l2: 0.00033901325271775323   Iteration 4 of 100, tot loss = 4.85375052690506, l1: 0.0001132550878537586, l2: 0.00037211999006103724   Iteration 5 of 100, tot loss = 5.519502115249634, l1: 0.00012470198416849598, l2: 0.0004272482357919216   Iteration 6 of 100, tot loss = 5.010110855102539, l1: 0.00011535546097244757, l2: 0.0003856556334843238   Iteration 7 of 100, tot loss = 4.701518365315029, l1: 0.0001104616406207372, l2: 0.0003596902055765635   Iteration 8 of 100, tot loss = 4.567808657884598, l1: 0.00010854349602595903, l2: 0.00034823737769329455   Iteration 9 of 100, tot loss = 4.668787452909681, l1: 0.00010863210142512496, l2: 0.0003582466532760817   Iteration 10 of 100, tot loss = 4.774469542503357, l1: 0.00011145632524858228, l2: 0.0003659906375105493   Iteration 11 of 100, tot loss = 4.746886188333685, l1: 0.00010955929279920052, l2: 0.0003651293346510184   Iteration 12 of 100, tot loss = 4.716494659582774, l1: 0.00010927943912975024, l2: 0.0003623700334477083   Iteration 13 of 100, tot loss = 4.685771777079656, l1: 0.00010828882043894667, l2: 0.0003602883628515813   Iteration 14 of 100, tot loss = 4.758614148412432, l1: 0.00011103416493694697, l2: 0.000364827253049173   Iteration 15 of 100, tot loss = 4.762557776769002, l1: 0.0001094757792695115, l2: 0.0003667799998462821   Iteration 16 of 100, tot loss = 4.636886090040207, l1: 0.0001069957415893441, l2: 0.0003566928699001437   Iteration 17 of 100, tot loss = 4.592624159420238, l1: 0.00010746783236219712, l2: 0.0003517945862019106   Iteration 18 of 100, tot loss = 4.6068734592861595, l1: 0.00010704426959920157, l2: 0.00035364307890025276   Iteration 19 of 100, tot loss = 4.519432758030138, l1: 0.00010550462818508477, l2: 0.00034643864982124225   Iteration 20 of 100, tot loss = 4.455823302268982, l1: 0.00010475018716533668, l2: 0.00034083214413840326   Iteration 21 of 100, tot loss = 4.499823456718808, l1: 0.00010532422276723774, l2: 0.000344658126344993   Iteration 22 of 100, tot loss = 4.591270533475009, l1: 0.0001064439976206896, l2: 0.0003526830590668727   Iteration 23 of 100, tot loss = 4.556820983472078, l1: 0.00010648700687021989, l2: 0.00034919509451593393   Iteration 24 of 100, tot loss = 4.64792733391126, l1: 0.00010745324774082594, l2: 0.0003573394884976248   Iteration 25 of 100, tot loss = 4.560218296051025, l1: 0.00010627689160173759, l2: 0.0003497449413407594   Iteration 26 of 100, tot loss = 4.510059503408579, l1: 0.00010558749524464544, l2: 0.0003454184580522661   Iteration 27 of 100, tot loss = 4.487714529037476, l1: 0.00010486075377600544, l2: 0.0003439107019436994   Iteration 28 of 100, tot loss = 4.500624869550977, l1: 0.00010411638036852569, l2: 0.00034594610977884647   Iteration 29 of 100, tot loss = 4.506290591996292, l1: 0.00010389269074698075, l2: 0.00034673637128435075   Iteration 30 of 100, tot loss = 4.467549721399943, l1: 0.00010332799914370601, l2: 0.00034342697569324324   Iteration 31 of 100, tot loss = 4.45231770700024, l1: 0.00010269969413545163, l2: 0.00034253207887823304   Iteration 32 of 100, tot loss = 4.500816501677036, l1: 0.00010319373654965602, l2: 0.00034688791583903367   Iteration 33 of 100, tot loss = 4.4671586354573565, l1: 0.00010272235345892665, l2: 0.00034399351253258914   Iteration 34 of 100, tot loss = 4.47702937967637, l1: 0.00010240886973299305, l2: 0.00034529407104641637   Iteration 35 of 100, tot loss = 4.533218601771764, l1: 0.00010359255663518394, l2: 0.00034972930526626963   Iteration 36 of 100, tot loss = 4.550297776858012, l1: 0.00010377165300370607, l2: 0.00035125812589992874   Iteration 37 of 100, tot loss = 4.5812242353284685, l1: 0.0001039609161199297, l2: 0.0003541615080541453   Iteration 38 of 100, tot loss = 4.547003959354601, l1: 0.00010326441310768899, l2: 0.0003514359835351474   Iteration 39 of 100, tot loss = 4.5941135210868635, l1: 0.00010374707907874089, l2: 0.0003556642742552914   Iteration 40 of 100, tot loss = 4.5853138089179994, l1: 0.0001038931723087444, l2: 0.00035463820968288927   Iteration 41 of 100, tot loss = 4.6276751611290905, l1: 0.00010464893438904432, l2: 0.00035811858194902905   Iteration 42 of 100, tot loss = 4.658219110398066, l1: 0.00010504600106200779, l2: 0.00036077591080573343   Iteration 43 of 100, tot loss = 4.642807832984037, l1: 0.0001041734304149137, l2: 0.00036010735392094005   Iteration 44 of 100, tot loss = 4.624152942137285, l1: 0.00010426486617441036, l2: 0.0003581504292510958   Iteration 45 of 100, tot loss = 4.649752574496799, l1: 0.00010478220614863352, l2: 0.00036019305310522516   Iteration 46 of 100, tot loss = 4.637585629587588, l1: 0.00010397240728944662, l2: 0.0003597861581036578   Iteration 47 of 100, tot loss = 4.6391809849028895, l1: 0.00010391527633901012, l2: 0.0003600028241944915   Iteration 48 of 100, tot loss = 4.639773527781169, l1: 0.00010369517167418962, l2: 0.00036028218346473295   Iteration 49 of 100, tot loss = 4.643724509647915, l1: 0.00010409691473184039, l2: 0.0003602755385479529   Iteration 50 of 100, tot loss = 4.637630729675293, l1: 0.00010398271217127331, l2: 0.0003597803629236296   Iteration 51 of 100, tot loss = 4.604032063016705, l1: 0.00010352340034967033, l2: 0.00035687980804747593   Iteration 52 of 100, tot loss = 4.5709929787195644, l1: 0.00010252050399755647, l2: 0.0003545787957801412   Iteration 53 of 100, tot loss = 4.617425869096, l1: 0.00010349966190537912, l2: 0.0003582429270400613   Iteration 54 of 100, tot loss = 4.6179789393036454, l1: 0.00010346286466368906, l2: 0.00035833503082998233   Iteration 55 of 100, tot loss = 4.638898680426857, l1: 0.00010370206470146183, l2: 0.00036018780506723985   Iteration 56 of 100, tot loss = 4.632248703922544, l1: 0.0001029279077816422, l2: 0.0003602969648324818   Iteration 57 of 100, tot loss = 4.665665262623837, l1: 0.00010383620106598787, l2: 0.00036273032718893596   Iteration 58 of 100, tot loss = 4.649394281979265, l1: 0.0001035915630727059, l2: 0.0003613478670715643   Iteration 59 of 100, tot loss = 4.614986197423127, l1: 0.00010308556017461943, l2: 0.00035841306163416384   Iteration 60 of 100, tot loss = 4.63764789501826, l1: 0.00010367944135699265, l2: 0.0003600853507426412   Iteration 61 of 100, tot loss = 4.618746632435283, l1: 0.00010370310087264592, l2: 0.00035817156518145357   Iteration 62 of 100, tot loss = 4.602363694098688, l1: 0.0001036775138345547, l2: 0.00035655885843762885   Iteration 63 of 100, tot loss = 4.606651442391532, l1: 0.00010377427009636507, l2: 0.0003568908772043263   Iteration 64 of 100, tot loss = 4.668179050087929, l1: 0.00010487374083822942, l2: 0.0003619441674800328   Iteration 65 of 100, tot loss = 4.697872910132775, l1: 0.00010496384963446942, l2: 0.00036482344484494   Iteration 66 of 100, tot loss = 4.6934858307693945, l1: 0.00010466252559016345, l2: 0.00036468606060335526   Iteration 67 of 100, tot loss = 4.682454489949924, l1: 0.00010484726671668103, l2: 0.00036339818542552137   Iteration 68 of 100, tot loss = 4.706732837592854, l1: 0.0001050054145119194, l2: 0.0003656678730776013   Iteration 69 of 100, tot loss = 4.705447725627733, l1: 0.00010516878946741664, l2: 0.0003653759867766116   Iteration 70 of 100, tot loss = 4.721088024548122, l1: 0.0001057056813026845, l2: 0.00036640312485230553   Iteration 71 of 100, tot loss = 4.701518904994911, l1: 0.00010542628030673089, l2: 0.00036472561387968264   Iteration 72 of 100, tot loss = 4.676602217886183, l1: 0.0001050026712619001, l2: 0.00036265755402887915   Iteration 73 of 100, tot loss = 4.68700530431042, l1: 0.00010560355958535196, l2: 0.0003630969737583057   Iteration 74 of 100, tot loss = 4.698888752911542, l1: 0.00010568724846470522, l2: 0.0003642016300865499   Iteration 75 of 100, tot loss = 4.735363502502441, l1: 0.00010603301527832325, l2: 0.0003675033382993812   Iteration 76 of 100, tot loss = 4.717962864198182, l1: 0.00010593363492118537, l2: 0.0003658626548330128   Iteration 77 of 100, tot loss = 4.722132586813593, l1: 0.00010583473378172531, l2: 0.00036637852812796686   Iteration 78 of 100, tot loss = 4.725469696216094, l1: 0.00010598112212625356, l2: 0.0003665658506902102   Iteration 79 of 100, tot loss = 4.713940804517722, l1: 0.0001056072234192309, l2: 0.0003657868602445726   Iteration 80 of 100, tot loss = 4.682608208060264, l1: 0.00010499769127818581, l2: 0.0003632631325672264   Iteration 81 of 100, tot loss = 4.680206684418667, l1: 0.00010486327318861264, l2: 0.00036315739828156145   Iteration 82 of 100, tot loss = 4.700306819706428, l1: 0.00010493125454359404, l2: 0.00036509943044909116   Iteration 83 of 100, tot loss = 4.703930869159929, l1: 0.00010477549778486893, l2: 0.0003656175915721288   Iteration 84 of 100, tot loss = 4.67186716340837, l1: 0.00010430885166507713, l2: 0.00036287786722477035   Iteration 85 of 100, tot loss = 4.675657067579381, l1: 0.00010410120889633511, l2: 0.00036346449992646847   Iteration 86 of 100, tot loss = 4.672446320223254, l1: 0.00010375372420280338, l2: 0.0003634909099187204   Iteration 87 of 100, tot loss = 4.672071130796411, l1: 0.00010374825633961933, l2: 0.0003634588587186139   Iteration 88 of 100, tot loss = 4.67061694372784, l1: 0.00010394916142269557, l2: 0.00036311253494535447   Iteration 89 of 100, tot loss = 4.679281692826346, l1: 0.00010414106905914247, l2: 0.0003637871023181177   Iteration 90 of 100, tot loss = 4.692759320471022, l1: 0.00010409686070084313, l2: 0.0003651790733177525   Iteration 91 of 100, tot loss = 4.7102593301416755, l1: 0.0001043469267612666, l2: 0.00036667900816870054   Iteration 92 of 100, tot loss = 4.721797137156777, l1: 0.00010479696194796934, l2: 0.000367382753725194   Iteration 93 of 100, tot loss = 4.701183203727968, l1: 0.00010449835862081889, l2: 0.0003656199637247169   Iteration 94 of 100, tot loss = 4.68254957807825, l1: 0.0001041567114604751, l2: 0.0003640982481846685   Iteration 95 of 100, tot loss = 4.679245471954346, l1: 0.00010399982960447423, l2: 0.0003639247193381092   Iteration 96 of 100, tot loss = 4.66259949405988, l1: 0.00010400416920219868, l2: 0.00036225578196535935   Iteration 97 of 100, tot loss = 4.690836911348953, l1: 0.00010452647103127525, l2: 0.0003645572222987689   Iteration 98 of 100, tot loss = 4.688561444379846, l1: 0.0001044872167359263, l2: 0.0003643689300848304   Iteration 99 of 100, tot loss = 4.698115209136346, l1: 0.00010426363245862763, l2: 0.0003655478906537867   Iteration 100 of 100, tot loss = 4.695681309700012, l1: 0.00010424811760458397, l2: 0.00036532001584419047
   End of epoch 1291; saving model... 

Epoch 1292 of 2000
   Iteration 1 of 100, tot loss = 5.691256999969482, l1: 0.0001187932284665294, l2: 0.0004503324453253299   Iteration 2 of 100, tot loss = 6.267866373062134, l1: 0.00012182268619653769, l2: 0.0005049639294156805   Iteration 3 of 100, tot loss = 5.5423814455668134, l1: 0.00010914496670011431, l2: 0.0004450931737665087   Iteration 4 of 100, tot loss = 4.745080530643463, l1: 9.825481174630113e-05, l2: 0.0003762532360269688   Iteration 5 of 100, tot loss = 4.494168710708618, l1: 9.699933289084584e-05, l2: 0.0003524175321217626   Iteration 6 of 100, tot loss = 4.5199373960494995, l1: 9.476648968605635e-05, l2: 0.0003572272495754684   Iteration 7 of 100, tot loss = 4.665669202804565, l1: 9.762408028888916e-05, l2: 0.00036894284338424247   Iteration 8 of 100, tot loss = 4.657567411661148, l1: 9.552863321005134e-05, l2: 0.0003702281101141125   Iteration 9 of 100, tot loss = 4.716513660218981, l1: 9.70203400356695e-05, l2: 0.0003746310281308575   Iteration 10 of 100, tot loss = 4.69743583202362, l1: 9.841960927587934e-05, l2: 0.00037132397410459815   Iteration 11 of 100, tot loss = 4.696551257913763, l1: 9.777864232257178e-05, l2: 0.0003718764822803099   Iteration 12 of 100, tot loss = 4.572247842947642, l1: 9.832683099375572e-05, l2: 0.0003588979537501776   Iteration 13 of 100, tot loss = 4.6625772806314325, l1: 0.00010066773001409064, l2: 0.0003655899994415589   Iteration 14 of 100, tot loss = 4.668485999107361, l1: 0.00010070688793868092, l2: 0.00036614171299983615   Iteration 15 of 100, tot loss = 4.5921947956085205, l1: 0.00010014369375615691, l2: 0.0003590757871279493   Iteration 16 of 100, tot loss = 4.696676149964333, l1: 0.00010301698557668715, l2: 0.0003666506281660986   Iteration 17 of 100, tot loss = 4.770865370245541, l1: 0.00010337441028027302, l2: 0.000373712125378113   Iteration 18 of 100, tot loss = 4.781012442376879, l1: 0.00010473150460812677, l2: 0.00037336974006999907   Iteration 19 of 100, tot loss = 4.874039361351414, l1: 0.00010624630445626711, l2: 0.00038115763010481665   Iteration 20 of 100, tot loss = 4.951668322086334, l1: 0.00010758918615465518, l2: 0.0003875776463246439   Iteration 21 of 100, tot loss = 4.980425641650245, l1: 0.00010755123990488105, l2: 0.00039049132291914983   Iteration 22 of 100, tot loss = 4.966578689488498, l1: 0.00010873172637233934, l2: 0.00038792614213889465   Iteration 23 of 100, tot loss = 5.011240451232247, l1: 0.00010887925853476982, l2: 0.00039224478772983116   Iteration 24 of 100, tot loss = 5.008557866017024, l1: 0.00010805407115791847, l2: 0.00039280171586142387   Iteration 25 of 100, tot loss = 4.954242057800293, l1: 0.00010702297993702814, l2: 0.00038840122579131275   Iteration 26 of 100, tot loss = 4.960581614420964, l1: 0.00010755798995467977, l2: 0.00038850017153890803   Iteration 27 of 100, tot loss = 4.9271151401378495, l1: 0.00010684622786896027, l2: 0.00038586528616077785   Iteration 28 of 100, tot loss = 4.876631004469735, l1: 0.00010609906348690856, l2: 0.00038156403700538376   Iteration 29 of 100, tot loss = 4.9169144794858735, l1: 0.00010658026264442517, l2: 0.000385111185284492   Iteration 30 of 100, tot loss = 4.965575329462687, l1: 0.00010766010940036116, l2: 0.00038889742330259954   Iteration 31 of 100, tot loss = 4.890279762206539, l1: 0.00010670862097721246, l2: 0.0003823193546850234   Iteration 32 of 100, tot loss = 4.832688547670841, l1: 0.00010637890159159724, l2: 0.00037688995280404924   Iteration 33 of 100, tot loss = 4.9326945290421, l1: 0.00010750534706995964, l2: 0.0003857641039014032   Iteration 34 of 100, tot loss = 4.8802739732405716, l1: 0.00010706832814749385, l2: 0.0003809590673293261   Iteration 35 of 100, tot loss = 4.839854907989502, l1: 0.00010607628812847127, l2: 0.0003779092013636338   Iteration 36 of 100, tot loss = 4.880116052097744, l1: 0.00010689409241927529, l2: 0.00038111751119787287   Iteration 37 of 100, tot loss = 4.942618627805968, l1: 0.00010814113724139847, l2: 0.00038612072396947925   Iteration 38 of 100, tot loss = 4.927369193026894, l1: 0.00010775117889830941, l2: 0.0003849857388750503   Iteration 39 of 100, tot loss = 4.940655194796049, l1: 0.00010857749685573463, l2: 0.00038548802153971524   Iteration 40 of 100, tot loss = 4.914112281799317, l1: 0.00010795450580189936, l2: 0.00038345672146533616   Iteration 41 of 100, tot loss = 4.872667748753617, l1: 0.00010737757803304348, l2: 0.0003798891965544033   Iteration 42 of 100, tot loss = 4.85286093325842, l1: 0.0001074878827715847, l2: 0.0003777982107879195   Iteration 43 of 100, tot loss = 4.858695645665014, l1: 0.00010770349306471343, l2: 0.0003781660712394474   Iteration 44 of 100, tot loss = 4.783183433792808, l1: 0.00010599672226486622, l2: 0.00037232162090235346   Iteration 45 of 100, tot loss = 4.7660814815097385, l1: 0.0001055480434539883, l2: 0.00037106010406407424   Iteration 46 of 100, tot loss = 4.740575997725777, l1: 0.00010490862587175798, l2: 0.00036914897313893204   Iteration 47 of 100, tot loss = 4.7708566036630184, l1: 0.00010522447120549375, l2: 0.0003718611887381214   Iteration 48 of 100, tot loss = 4.76285723845164, l1: 0.00010496961059895209, l2: 0.00037131611270524445   Iteration 49 of 100, tot loss = 4.791411643125573, l1: 0.00010534254004002777, l2: 0.0003737986240148715   Iteration 50 of 100, tot loss = 4.8086100196838375, l1: 0.00010559277616266627, l2: 0.00037526822547079064   Iteration 51 of 100, tot loss = 4.789369059543984, l1: 0.00010569469197755953, l2: 0.0003732422136475661   Iteration 52 of 100, tot loss = 4.7742670224263115, l1: 0.00010544249527391422, l2: 0.00037198420683965267   Iteration 53 of 100, tot loss = 4.774928929670802, l1: 0.00010534992406116184, l2: 0.0003721429693198517   Iteration 54 of 100, tot loss = 4.761507670084636, l1: 0.00010497578915857486, l2: 0.0003711749785806104   Iteration 55 of 100, tot loss = 4.828825933283025, l1: 0.00010566525220796889, l2: 0.0003772173416853713   Iteration 56 of 100, tot loss = 4.8142805099487305, l1: 0.00010510759453349497, l2: 0.00037632045717016026   Iteration 57 of 100, tot loss = 4.8238213355081125, l1: 0.00010573905653967686, l2: 0.0003766430780064854   Iteration 58 of 100, tot loss = 4.776318772085782, l1: 0.00010468793779778182, l2: 0.0003729439404510074   Iteration 59 of 100, tot loss = 4.817701800394866, l1: 0.00010568113010761455, l2: 0.0003760890512070145   Iteration 60 of 100, tot loss = 4.791763178507487, l1: 0.00010538633960095467, l2: 0.00037378997961544277   Iteration 61 of 100, tot loss = 4.76123013652739, l1: 0.00010487483915745006, l2: 0.00037124817563530027   Iteration 62 of 100, tot loss = 4.739252613436792, l1: 0.00010419186882274752, l2: 0.0003697333934636141   Iteration 63 of 100, tot loss = 4.741517702738444, l1: 0.00010428634720035858, l2: 0.0003698654238380388   Iteration 64 of 100, tot loss = 4.756563745439053, l1: 0.00010468599913338039, l2: 0.0003709703761387573   Iteration 65 of 100, tot loss = 4.7638570345365086, l1: 0.00010475202339316288, l2: 0.00037163368061346073   Iteration 66 of 100, tot loss = 4.763800050273086, l1: 0.00010514521763428947, l2: 0.0003712347879076278   Iteration 67 of 100, tot loss = 4.7762635714972195, l1: 0.00010514551207445799, l2: 0.0003724808453055752   Iteration 68 of 100, tot loss = 4.736825609908385, l1: 0.00010416451010207632, l2: 0.00036951805122953374   Iteration 69 of 100, tot loss = 4.750705895216568, l1: 0.00010434379895003445, l2: 0.0003707267912788033   Iteration 70 of 100, tot loss = 4.760162888254438, l1: 0.00010440189305102519, l2: 0.0003716143963434401   Iteration 71 of 100, tot loss = 4.779982482883292, l1: 0.00010464412598209915, l2: 0.00037335412272229186   Iteration 72 of 100, tot loss = 4.775405794382095, l1: 0.00010444793088026927, l2: 0.00037309264906879334   Iteration 73 of 100, tot loss = 4.768345783834588, l1: 0.00010460347657001015, l2: 0.00037223110240581126   Iteration 74 of 100, tot loss = 4.781172188552651, l1: 0.00010495773484935116, l2: 0.00037315948498576907   Iteration 75 of 100, tot loss = 4.78324543317159, l1: 0.00010513253476043852, l2: 0.00037319200938024246   Iteration 76 of 100, tot loss = 4.764795068063234, l1: 0.0001052188223950093, l2: 0.0003712606852020958   Iteration 77 of 100, tot loss = 4.76923731085542, l1: 0.00010510912204165822, l2: 0.00037181460971009685   Iteration 78 of 100, tot loss = 4.76820293145302, l1: 0.00010481289693696174, l2: 0.0003720073969489955   Iteration 79 of 100, tot loss = 4.76365778416018, l1: 0.00010469250849417199, l2: 0.0003716732706702196   Iteration 80 of 100, tot loss = 4.762460985779763, l1: 0.00010493862223484029, l2: 0.00037130747723495005   Iteration 81 of 100, tot loss = 4.759308099746704, l1: 0.00010483082717846515, l2: 0.0003710999835338616   Iteration 82 of 100, tot loss = 4.753691053972012, l1: 0.00010469731875296352, l2: 0.0003706717876278761   Iteration 83 of 100, tot loss = 4.766240473253181, l1: 0.00010487299456495331, l2: 0.00037175105351334753   Iteration 84 of 100, tot loss = 4.743535371053786, l1: 0.00010434631409831734, l2: 0.0003700072237344492   Iteration 85 of 100, tot loss = 4.749095131369198, l1: 0.00010451723873676896, l2: 0.0003703922748065773   Iteration 86 of 100, tot loss = 4.732571665630784, l1: 0.00010438385944262547, l2: 0.0003688733076012076   Iteration 87 of 100, tot loss = 4.737261254211952, l1: 0.00010456266523112715, l2: 0.0003691634606341189   Iteration 88 of 100, tot loss = 4.729732792485844, l1: 0.0001045101295692968, l2: 0.00036846315007013885   Iteration 89 of 100, tot loss = 4.714076912804936, l1: 0.00010414104322540532, l2: 0.0003672666486197026   Iteration 90 of 100, tot loss = 4.741827495892843, l1: 0.00010479483991932486, l2: 0.00036938791041999747   Iteration 91 of 100, tot loss = 4.792700649617792, l1: 0.00010516015046940103, l2: 0.000374109915455747   Iteration 92 of 100, tot loss = 4.805264677690423, l1: 0.00010550255647894633, l2: 0.0003750239124517817   Iteration 93 of 100, tot loss = 4.786617494398548, l1: 0.00010514644248272112, l2: 0.0003735153081980834   Iteration 94 of 100, tot loss = 4.822827349317834, l1: 0.00010567925071766095, l2: 0.0003766034852231585   Iteration 95 of 100, tot loss = 4.821363790411699, l1: 0.00010576837410457972, l2: 0.0003763680057569505   Iteration 96 of 100, tot loss = 4.805205839375655, l1: 0.0001054255501079145, l2: 0.00037509503446623665   Iteration 97 of 100, tot loss = 4.814295245200088, l1: 0.00010568885566677524, l2: 0.00037574066956630746   Iteration 98 of 100, tot loss = 4.809801824238836, l1: 0.00010561993031861137, l2: 0.0003753602527746009   Iteration 99 of 100, tot loss = 4.803666073866565, l1: 0.00010551855295415581, l2: 0.0003748480553210788   Iteration 100 of 100, tot loss = 4.794164311885834, l1: 0.00010521077219891595, l2: 0.00037420565982756673
   End of epoch 1292; saving model... 

Epoch 1293 of 2000
   Iteration 1 of 100, tot loss = 3.1892693042755127, l1: 6.429915083572268e-05, l2: 0.0002546277828514576   Iteration 2 of 100, tot loss = 3.924777865409851, l1: 8.099248225335032e-05, l2: 0.0003114853170700371   Iteration 3 of 100, tot loss = 4.574352502822876, l1: 9.893117627749841e-05, l2: 0.000358504078273351   Iteration 4 of 100, tot loss = 4.705109179019928, l1: 0.00010261271745548584, l2: 0.0003678982102428563   Iteration 5 of 100, tot loss = 5.0823225498199465, l1: 0.00011138356931041926, l2: 0.00039684869698248805   Iteration 6 of 100, tot loss = 5.052354375521342, l1: 0.00011459068628028035, l2: 0.0003906447576203694   Iteration 7 of 100, tot loss = 4.906259025846209, l1: 0.0001107664346428854, l2: 0.0003798594739886799   Iteration 8 of 100, tot loss = 4.917220443487167, l1: 0.00011058470590796787, l2: 0.00038113734262879007   Iteration 9 of 100, tot loss = 4.729454278945923, l1: 0.00010403811271923284, l2: 0.00036890731684656604   Iteration 10 of 100, tot loss = 4.606435084342957, l1: 9.921493583533447e-05, l2: 0.0003614285757066682   Iteration 11 of 100, tot loss = 4.513445138931274, l1: 0.0001003765663137363, l2: 0.0003509679491716352   Iteration 12 of 100, tot loss = 4.328988929589589, l1: 9.675895095521507e-05, l2: 0.00033613994310144335   Iteration 13 of 100, tot loss = 4.293145326467661, l1: 9.594334411443784e-05, l2: 0.0003333711893691753   Iteration 14 of 100, tot loss = 4.198474049568176, l1: 9.36969666197131e-05, l2: 0.00032615043996234557   Iteration 15 of 100, tot loss = 4.124836746851603, l1: 9.12266698530099e-05, l2: 0.0003212570057561   Iteration 16 of 100, tot loss = 4.159004136919975, l1: 9.246623380931851e-05, l2: 0.0003234341820643749   Iteration 17 of 100, tot loss = 4.273275445489323, l1: 9.440236510058371e-05, l2: 0.0003329251810689183   Iteration 18 of 100, tot loss = 4.344116515583462, l1: 9.413900928241976e-05, l2: 0.00034027264498743333   Iteration 19 of 100, tot loss = 4.303944135967054, l1: 9.212462119696858e-05, l2: 0.00033826979434754893   Iteration 20 of 100, tot loss = 4.257584869861603, l1: 9.2335475346772e-05, l2: 0.00033342301394441166   Iteration 21 of 100, tot loss = 4.229637043816703, l1: 9.230160780827559e-05, l2: 0.0003306620991289882   Iteration 22 of 100, tot loss = 4.244113065979698, l1: 9.32626862777397e-05, l2: 0.0003311486236105504   Iteration 23 of 100, tot loss = 4.247829426889834, l1: 9.352033742723744e-05, l2: 0.00033126260804356605   Iteration 24 of 100, tot loss = 4.39546752969424, l1: 9.562965594038057e-05, l2: 0.000343917098386252   Iteration 25 of 100, tot loss = 4.34673849105835, l1: 9.531835909001529e-05, l2: 0.00033935549145098777   Iteration 26 of 100, tot loss = 4.408046520673311, l1: 9.686410507688728e-05, l2: 0.0003439405470719346   Iteration 27 of 100, tot loss = 4.38315847184923, l1: 9.661304701930257e-05, l2: 0.00034170280074855935   Iteration 28 of 100, tot loss = 4.371482985360282, l1: 9.678311289462727e-05, l2: 0.00034036518679515993   Iteration 29 of 100, tot loss = 4.379273842120993, l1: 9.651033379386405e-05, l2: 0.000341417051647019   Iteration 30 of 100, tot loss = 4.375233825047811, l1: 9.615265977724145e-05, l2: 0.0003413707236177288   Iteration 31 of 100, tot loss = 4.319561650676112, l1: 9.513525113899021e-05, l2: 0.0003368209149958866   Iteration 32 of 100, tot loss = 4.307594463229179, l1: 9.384345992202725e-05, l2: 0.00033691598764562514   Iteration 33 of 100, tot loss = 4.443690458933513, l1: 9.561643199765857e-05, l2: 0.0003487526132217185   Iteration 34 of 100, tot loss = 4.3876702294630165, l1: 9.495290075445904e-05, l2: 0.00034381412200780367   Iteration 35 of 100, tot loss = 4.421834489277431, l1: 9.522842311915675e-05, l2: 0.00034695502545218916   Iteration 36 of 100, tot loss = 4.384400679005517, l1: 9.480553560731803e-05, l2: 0.0003436345319237767   Iteration 37 of 100, tot loss = 4.412466944874944, l1: 9.56329186502026e-05, l2: 0.00034561377518961355   Iteration 38 of 100, tot loss = 4.35962532695971, l1: 9.467563225036046e-05, l2: 0.0003412868999906718   Iteration 39 of 100, tot loss = 4.316527103766417, l1: 9.417394866408684e-05, l2: 0.00033747876146569464   Iteration 40 of 100, tot loss = 4.339137238264084, l1: 9.452174854231998e-05, l2: 0.0003393919760128483   Iteration 41 of 100, tot loss = 4.34918475151062, l1: 9.528555014318355e-05, l2: 0.0003396329255604253   Iteration 42 of 100, tot loss = 4.340596533956981, l1: 9.575852337071584e-05, l2: 0.0003383011304928611   Iteration 43 of 100, tot loss = 4.382319433744564, l1: 9.669676026448521e-05, l2: 0.00034153518377976535   Iteration 44 of 100, tot loss = 4.30464725873687, l1: 9.512782079665075e-05, l2: 0.00033533690574668896   Iteration 45 of 100, tot loss = 4.2896105157004465, l1: 9.446458506216813e-05, l2: 0.0003344964670961619   Iteration 46 of 100, tot loss = 4.253294128438701, l1: 9.397681155267334e-05, l2: 0.000331352601674355   Iteration 47 of 100, tot loss = 4.2295067487879, l1: 9.370855773437491e-05, l2: 0.00032924211746889186   Iteration 48 of 100, tot loss = 4.276978674034278, l1: 9.495297050913602e-05, l2: 0.00033274489624091075   Iteration 49 of 100, tot loss = 4.281418939026034, l1: 9.487770207948527e-05, l2: 0.0003332641913866795   Iteration 50 of 100, tot loss = 4.287193720340729, l1: 9.469432174228132e-05, l2: 0.0003340250502515119   Iteration 51 of 100, tot loss = 4.310921400201087, l1: 9.484768830303688e-05, l2: 0.00033624445167529924   Iteration 52 of 100, tot loss = 4.29661976144864, l1: 9.464371578360442e-05, l2: 0.00033501826006753475   Iteration 53 of 100, tot loss = 4.310920821045929, l1: 9.523862014625679e-05, l2: 0.00033585346160557576   Iteration 54 of 100, tot loss = 4.331272096545608, l1: 9.583242663885121e-05, l2: 0.0003372947824202668   Iteration 55 of 100, tot loss = 4.301811515201222, l1: 9.526758962734179e-05, l2: 0.0003349135609989224   Iteration 56 of 100, tot loss = 4.39382766187191, l1: 9.647967557871848e-05, l2: 0.0003429030903134844   Iteration 57 of 100, tot loss = 4.401464849187617, l1: 9.636051560703076e-05, l2: 0.0003437859690676672   Iteration 58 of 100, tot loss = 4.403846471474089, l1: 9.628355361075655e-05, l2: 0.00034410109332509757   Iteration 59 of 100, tot loss = 4.385911963753781, l1: 9.633409033501836e-05, l2: 0.000342257105931837   Iteration 60 of 100, tot loss = 4.345358153184255, l1: 9.57167264156548e-05, l2: 0.0003388190888169144   Iteration 61 of 100, tot loss = 4.334985705672717, l1: 9.56643885788775e-05, l2: 0.00033783418215624226   Iteration 62 of 100, tot loss = 4.345986993082108, l1: 9.570379346945217e-05, l2: 0.0003388949059421242   Iteration 63 of 100, tot loss = 4.308473908711994, l1: 9.489380837684231e-05, l2: 0.00033595358261317456   Iteration 64 of 100, tot loss = 4.291793923825026, l1: 9.449941148886865e-05, l2: 0.00033467998116520903   Iteration 65 of 100, tot loss = 4.2954322044666, l1: 9.461325456728585e-05, l2: 0.0003349299662379333   Iteration 66 of 100, tot loss = 4.301347288218412, l1: 9.456377224661551e-05, l2: 0.00033557095737319975   Iteration 67 of 100, tot loss = 4.282449391350817, l1: 9.459655392226944e-05, l2: 0.0003336483857867225   Iteration 68 of 100, tot loss = 4.303980950046988, l1: 9.494040526025107e-05, l2: 0.00033545769035888095   Iteration 69 of 100, tot loss = 4.301624094230541, l1: 9.49260921924429e-05, l2: 0.0003352363175666972   Iteration 70 of 100, tot loss = 4.312883332797459, l1: 9.516641335462087e-05, l2: 0.00033612192086625975   Iteration 71 of 100, tot loss = 4.29081525265331, l1: 9.506477260696506e-05, l2: 0.00033401675380087575   Iteration 72 of 100, tot loss = 4.338400754663679, l1: 9.586922260496067e-05, l2: 0.000337970854262595   Iteration 73 of 100, tot loss = 4.327493278947595, l1: 9.55405722217545e-05, l2: 0.0003372087572543397   Iteration 74 of 100, tot loss = 4.328526880290057, l1: 9.573810736607836e-05, l2: 0.00033711458210647734   Iteration 75 of 100, tot loss = 4.33596376101176, l1: 9.587526738566037e-05, l2: 0.00033772110987532263   Iteration 76 of 100, tot loss = 4.340532770282344, l1: 9.583936183557135e-05, l2: 0.0003382139162376437   Iteration 77 of 100, tot loss = 4.365086329447759, l1: 9.600304758067351e-05, l2: 0.00034050558674430425   Iteration 78 of 100, tot loss = 4.363113767061478, l1: 9.588326802492165e-05, l2: 0.00034042811015611875   Iteration 79 of 100, tot loss = 4.3767544378208205, l1: 9.590072184874782e-05, l2: 0.00034177472309837164   Iteration 80 of 100, tot loss = 4.359104472398758, l1: 9.547225699861883e-05, l2: 0.00034043819150610945   Iteration 81 of 100, tot loss = 4.337046496662093, l1: 9.50825228363011e-05, l2: 0.0003386221282266046   Iteration 82 of 100, tot loss = 4.353367157098724, l1: 9.565513640309957e-05, l2: 0.0003396815805101097   Iteration 83 of 100, tot loss = 4.3502450305295275, l1: 9.562023242105202e-05, l2: 0.0003394042721182299   Iteration 84 of 100, tot loss = 4.358511280445826, l1: 9.558128104323433e-05, l2: 0.00034026984845002603   Iteration 85 of 100, tot loss = 4.354650820002837, l1: 9.576115998617538e-05, l2: 0.0003397039234508103   Iteration 86 of 100, tot loss = 4.348980207775915, l1: 9.575077452017319e-05, l2: 0.0003391472477402246   Iteration 87 of 100, tot loss = 4.358117511902733, l1: 9.589705227515607e-05, l2: 0.0003399147001267732   Iteration 88 of 100, tot loss = 4.357330650091171, l1: 9.585184249193513e-05, l2: 0.0003398812233916198   Iteration 89 of 100, tot loss = 4.351025353656726, l1: 9.579606042923719e-05, l2: 0.00033930647574441006   Iteration 90 of 100, tot loss = 4.349607390827603, l1: 9.602922501572822e-05, l2: 0.0003389315150141354   Iteration 91 of 100, tot loss = 4.364357206847641, l1: 9.642980836400926e-05, l2: 0.0003400059132408197   Iteration 92 of 100, tot loss = 4.36670147076897, l1: 9.649623627964225e-05, l2: 0.0003401739118682757   Iteration 93 of 100, tot loss = 4.383264933862994, l1: 9.649817984218219e-05, l2: 0.00034182831485097306   Iteration 94 of 100, tot loss = 4.364806997015121, l1: 9.623245522503721e-05, l2: 0.0003402482459236164   Iteration 95 of 100, tot loss = 4.358529698221307, l1: 9.610072187282831e-05, l2: 0.00033975224934310017   Iteration 96 of 100, tot loss = 4.3627546181281405, l1: 9.638170899961551e-05, l2: 0.000339893753865302   Iteration 97 of 100, tot loss = 4.367174881020772, l1: 9.6572351824377e-05, l2: 0.0003401451373140366   Iteration 98 of 100, tot loss = 4.36648120198931, l1: 9.653765657127891e-05, l2: 0.00034011046487389   Iteration 99 of 100, tot loss = 4.353947389005411, l1: 9.624770580264128e-05, l2: 0.00033914703442750385   Iteration 100 of 100, tot loss = 4.394466648101806, l1: 9.693261832580901e-05, l2: 0.00034251404744281897
   End of epoch 1293; saving model... 

Epoch 1294 of 2000
   Iteration 1 of 100, tot loss = 4.68659782409668, l1: 0.00012293257168494165, l2: 0.00034572722506709397   Iteration 2 of 100, tot loss = 3.8484548330307007, l1: 0.00010444464714964852, l2: 0.00028040084725944325   Iteration 3 of 100, tot loss = 3.790162722269694, l1: 9.956596962486704e-05, l2: 0.00027945031858204555   Iteration 4 of 100, tot loss = 3.694636106491089, l1: 9.486445196671411e-05, l2: 0.0002745991696428973   Iteration 5 of 100, tot loss = 3.634574127197266, l1: 9.260593651561067e-05, l2: 0.0002708514832193032   Iteration 6 of 100, tot loss = 3.9749366442362466, l1: 9.986241760392052e-05, l2: 0.00029763125591368106   Iteration 7 of 100, tot loss = 3.9450293268476213, l1: 9.626520165641393e-05, l2: 0.00029823773781702457   Iteration 8 of 100, tot loss = 4.343317925930023, l1: 0.00010308955643267836, l2: 0.0003312422431918094   Iteration 9 of 100, tot loss = 4.100282033284505, l1: 9.817084598277385e-05, l2: 0.0003118573642698013   Iteration 10 of 100, tot loss = 3.9937135219573974, l1: 9.676711270003579e-05, l2: 0.0003026042453711852   Iteration 11 of 100, tot loss = 4.1084349805658515, l1: 9.895142350367017e-05, l2: 0.00031189207749610597   Iteration 12 of 100, tot loss = 4.130191047986348, l1: 9.989260555206177e-05, l2: 0.0003131265038973652   Iteration 13 of 100, tot loss = 4.002179842728835, l1: 9.795781751073754e-05, l2: 0.00030226017070862535   Iteration 14 of 100, tot loss = 3.9458208765302385, l1: 9.709671420361181e-05, l2: 0.00029748537781415507   Iteration 15 of 100, tot loss = 3.942153724034627, l1: 9.782945659632484e-05, l2: 0.0002963859209557995   Iteration 16 of 100, tot loss = 4.019324526190758, l1: 9.898925191009766e-05, l2: 0.0003029432054972858   Iteration 17 of 100, tot loss = 3.9184368638431324, l1: 9.714456887997906e-05, l2: 0.00029469912303337715   Iteration 18 of 100, tot loss = 4.0329452355702715, l1: 9.948548621549789e-05, l2: 0.0003038090411185597   Iteration 19 of 100, tot loss = 4.043156749323795, l1: 9.989862691758102e-05, l2: 0.0003044170520488957   Iteration 20 of 100, tot loss = 4.144200706481934, l1: 0.00010153043258469552, l2: 0.0003128896409180015   Iteration 21 of 100, tot loss = 4.159395581200009, l1: 0.00010109732206079311, l2: 0.00031484223880051145   Iteration 22 of 100, tot loss = 4.148855231025002, l1: 9.999668525944634e-05, l2: 0.0003148888407105749   Iteration 23 of 100, tot loss = 4.20989755962206, l1: 0.00010065143831762607, l2: 0.00032033831888364386   Iteration 24 of 100, tot loss = 4.209834317366282, l1: 9.952964622546763e-05, l2: 0.00032145378524243523   Iteration 25 of 100, tot loss = 4.232191123962402, l1: 9.935716167092324e-05, l2: 0.00032386195147410035   Iteration 26 of 100, tot loss = 4.28843984237084, l1: 9.938117671901217e-05, l2: 0.0003294628091120663   Iteration 27 of 100, tot loss = 4.381887630180076, l1: 0.00010160668979450647, l2: 0.00033658207609766613   Iteration 28 of 100, tot loss = 4.387619120734079, l1: 0.00010217414325909755, l2: 0.0003365877710166387   Iteration 29 of 100, tot loss = 4.354310019262906, l1: 0.00010114702534588889, l2: 0.00033428397928846293   Iteration 30 of 100, tot loss = 4.359182548522949, l1: 0.00010132154663248609, l2: 0.0003345967105512197   Iteration 31 of 100, tot loss = 4.46962308883667, l1: 0.00010289405726061593, l2: 0.00034406825415639867   Iteration 32 of 100, tot loss = 4.442005284130573, l1: 0.00010154671338113985, l2: 0.0003426538169151172   Iteration 33 of 100, tot loss = 4.4697488654743545, l1: 0.00010188711839322367, l2: 0.0003450877715791152   Iteration 34 of 100, tot loss = 4.4817629912320305, l1: 0.00010204544688509915, l2: 0.00034613085581975826   Iteration 35 of 100, tot loss = 4.45285004888262, l1: 0.00010207978224830835, l2: 0.0003432052259865616   Iteration 36 of 100, tot loss = 4.430406557189094, l1: 0.00010174198248730843, l2: 0.0003412986762769934   Iteration 37 of 100, tot loss = 4.36872158179412, l1: 0.00010007457889072169, l2: 0.0003367975823002288   Iteration 38 of 100, tot loss = 4.345839331024571, l1: 9.918352349670481e-05, l2: 0.0003354004124718669   Iteration 39 of 100, tot loss = 4.307651403622749, l1: 9.85853752913849e-05, l2: 0.00033217976735427213   Iteration 40 of 100, tot loss = 4.339108806848526, l1: 9.924633113769232e-05, l2: 0.00033466455133748243   Iteration 41 of 100, tot loss = 4.385495075365392, l1: 9.968960077413812e-05, l2: 0.00033885990794770785   Iteration 42 of 100, tot loss = 4.368802161443801, l1: 0.00010002643024823296, l2: 0.0003368537873029709   Iteration 43 of 100, tot loss = 4.345939087313275, l1: 9.946892947313202e-05, l2: 0.00033512498051751143   Iteration 44 of 100, tot loss = 4.325006680055098, l1: 9.869326268843989e-05, l2: 0.000333807406687728   Iteration 45 of 100, tot loss = 4.385405370924208, l1: 9.960476252470269e-05, l2: 0.0003389357750873185   Iteration 46 of 100, tot loss = 4.456440593885339, l1: 0.00010055879236302724, l2: 0.00034508526732679456   Iteration 47 of 100, tot loss = 4.448206404422192, l1: 0.00010058232092365444, l2: 0.0003442383195432101   Iteration 48 of 100, tot loss = 4.4571128984292345, l1: 0.00010057501663141011, l2: 0.0003451362730023296   Iteration 49 of 100, tot loss = 4.4309646256115975, l1: 9.998349077602829e-05, l2: 0.0003431129717144507   Iteration 50 of 100, tot loss = 4.444654426574707, l1: 0.00010020217239798513, l2: 0.0003442632697988302   Iteration 51 of 100, tot loss = 4.4094643966824405, l1: 9.963895710968121e-05, l2: 0.00034130748199280716   Iteration 52 of 100, tot loss = 4.351501866028859, l1: 9.833583037386864e-05, l2: 0.0003368143558103699   Iteration 53 of 100, tot loss = 4.422857246308957, l1: 9.910928394056786e-05, l2: 0.0003431764404390464   Iteration 54 of 100, tot loss = 4.399886663313265, l1: 9.880738906320874e-05, l2: 0.00034118127654504926   Iteration 55 of 100, tot loss = 4.383232296596874, l1: 9.892467099813405e-05, l2: 0.00033939855771181594   Iteration 56 of 100, tot loss = 4.387960227472441, l1: 9.900616571810263e-05, l2: 0.00033978985577830045   Iteration 57 of 100, tot loss = 4.385753311608967, l1: 9.91326668187404e-05, l2: 0.0003394426631418531   Iteration 58 of 100, tot loss = 4.392954215921205, l1: 9.931740037402427e-05, l2: 0.00033997802002604344   Iteration 59 of 100, tot loss = 4.369438145120265, l1: 9.901866403613555e-05, l2: 0.00033792514937053736   Iteration 60 of 100, tot loss = 4.393445124228795, l1: 9.939088267856278e-05, l2: 0.0003399536282813642   Iteration 61 of 100, tot loss = 4.4103436997679415, l1: 9.995268335275841e-05, l2: 0.0003410816857243934   Iteration 62 of 100, tot loss = 4.405665903322158, l1: 9.968430540832361e-05, l2: 0.00034088228341950583   Iteration 63 of 100, tot loss = 4.366232327052525, l1: 9.897390029638902e-05, l2: 0.0003376493310382117   Iteration 64 of 100, tot loss = 4.350407898426056, l1: 9.910758973319389e-05, l2: 0.00033593319858482573   Iteration 65 of 100, tot loss = 4.3364536872276895, l1: 9.888721212449197e-05, l2: 0.00033475815524490406   Iteration 66 of 100, tot loss = 4.362507964625503, l1: 9.904454346917302e-05, l2: 0.00033720625188751995   Iteration 67 of 100, tot loss = 4.419069460968473, l1: 9.977805247719733e-05, l2: 0.0003421288926272528   Iteration 68 of 100, tot loss = 4.408937566420612, l1: 9.946535502582693e-05, l2: 0.0003414284005617339   Iteration 69 of 100, tot loss = 4.4161751857702285, l1: 9.926595811376119e-05, l2: 0.0003423515590839088   Iteration 70 of 100, tot loss = 4.411564302444458, l1: 9.905444130708929e-05, l2: 0.0003421019875012072   Iteration 71 of 100, tot loss = 4.387562661103799, l1: 9.85102327011729e-05, l2: 0.00034024603222168874   Iteration 72 of 100, tot loss = 4.362968428267373, l1: 9.780678854238229e-05, l2: 0.0003384900529555226   Iteration 73 of 100, tot loss = 4.373340355206842, l1: 9.802850245375326e-05, l2: 0.0003393055324535179   Iteration 74 of 100, tot loss = 4.359973427411672, l1: 9.791014378385405e-05, l2: 0.0003380871983952946   Iteration 75 of 100, tot loss = 4.3400467681884765, l1: 9.77700427271581e-05, l2: 0.0003362346337720131   Iteration 76 of 100, tot loss = 4.342714761432848, l1: 9.772045551041006e-05, l2: 0.00033655102034620203   Iteration 77 of 100, tot loss = 4.354168062086229, l1: 9.778861355001724e-05, l2: 0.0003376281924179888   Iteration 78 of 100, tot loss = 4.344079616742256, l1: 9.741879157678713e-05, l2: 0.0003369891699269199   Iteration 79 of 100, tot loss = 4.341483019575288, l1: 9.727217175053209e-05, l2: 0.0003368761300188902   Iteration 80 of 100, tot loss = 4.356396579742432, l1: 9.788872880562849e-05, l2: 0.0003377509287020075   Iteration 81 of 100, tot loss = 4.3594080253883645, l1: 9.78046212108938e-05, l2: 0.0003381361805813755   Iteration 82 of 100, tot loss = 4.3588885912081095, l1: 9.80806944869805e-05, l2: 0.0003378081638380181   Iteration 83 of 100, tot loss = 4.37869096664061, l1: 9.829999474178809e-05, l2: 0.0003395691008730623   Iteration 84 of 100, tot loss = 4.38091622647785, l1: 9.816198304352362e-05, l2: 0.0003399296386175722   Iteration 85 of 100, tot loss = 4.373832136041978, l1: 9.822857412954738e-05, l2: 0.00033915463838424974   Iteration 86 of 100, tot loss = 4.3718241702678595, l1: 9.813138007471977e-05, l2: 0.0003390510359760049   Iteration 87 of 100, tot loss = 4.345877987214888, l1: 9.757960584863014e-05, l2: 0.00033700819182124034   Iteration 88 of 100, tot loss = 4.3670636144551365, l1: 9.802389029738896e-05, l2: 0.00033868246993568556   Iteration 89 of 100, tot loss = 4.357925929380267, l1: 9.800459732748881e-05, l2: 0.00033778799442689487   Iteration 90 of 100, tot loss = 4.3874491426679825, l1: 9.86155404664007e-05, l2: 0.0003401293726508609   Iteration 91 of 100, tot loss = 4.376303418652042, l1: 9.864853783945584e-05, l2: 0.00033898180293732906   Iteration 92 of 100, tot loss = 4.366992242958235, l1: 9.823479123714724e-05, l2: 0.00033846443203282945   Iteration 93 of 100, tot loss = 4.378045438438334, l1: 9.867560670157815e-05, l2: 0.0003391289359061248   Iteration 94 of 100, tot loss = 4.367379028746423, l1: 9.822712947541422e-05, l2: 0.0003385107722467782   Iteration 95 of 100, tot loss = 4.366589940221686, l1: 9.826930399867707e-05, l2: 0.00033838968880253994   Iteration 96 of 100, tot loss = 4.3650717213749886, l1: 9.818320991901904e-05, l2: 0.00033832396108361235   Iteration 97 of 100, tot loss = 4.377034553547495, l1: 9.847145668469782e-05, l2: 0.00033923199765753846   Iteration 98 of 100, tot loss = 4.374082217411119, l1: 9.867681019757732e-05, l2: 0.00033873141038217295   Iteration 99 of 100, tot loss = 4.359110343335855, l1: 9.84535243957788e-05, l2: 0.00033745750878358996   Iteration 100 of 100, tot loss = 4.356178505420685, l1: 9.868390381598147e-05, l2: 0.00033693394580041057
   End of epoch 1294; saving model... 

Epoch 1295 of 2000
   Iteration 1 of 100, tot loss = 5.595012187957764, l1: 0.00012897193664684892, l2: 0.00043052926775999367   Iteration 2 of 100, tot loss = 5.263609170913696, l1: 0.00011475282735773362, l2: 0.000411608096328564   Iteration 3 of 100, tot loss = 5.619584560394287, l1: 0.00012575018869635338, l2: 0.0004362082787944625   Iteration 4 of 100, tot loss = 5.0923691391944885, l1: 0.00011774506310757715, l2: 0.0003914918561349623   Iteration 5 of 100, tot loss = 5.386623811721802, l1: 0.00012236377951921896, l2: 0.00041629860061220825   Iteration 6 of 100, tot loss = 5.8640291293462115, l1: 0.00012621478890650906, l2: 0.0004601881228154525   Iteration 7 of 100, tot loss = 5.537564992904663, l1: 0.00012319362472875843, l2: 0.0004305628743687911   Iteration 8 of 100, tot loss = 5.4561629593372345, l1: 0.00012088918811059557, l2: 0.0004247271062922664   Iteration 9 of 100, tot loss = 5.186074124442206, l1: 0.0001151148616271611, l2: 0.0004034925514133647   Iteration 10 of 100, tot loss = 5.404968667030334, l1: 0.00011603294769884087, l2: 0.00042446392035344616   Iteration 11 of 100, tot loss = 5.132925835522738, l1: 0.00011210494928739287, l2: 0.00040118763387330216   Iteration 12 of 100, tot loss = 5.06045530239741, l1: 0.00011025932205181259, l2: 0.0003957862051417275   Iteration 13 of 100, tot loss = 5.061627259621253, l1: 0.00010968504200438753, l2: 0.0003964776820234525   Iteration 14 of 100, tot loss = 5.256766540663583, l1: 0.00011167926069382312, l2: 0.0004139973908812473   Iteration 15 of 100, tot loss = 5.202747519810995, l1: 0.00011208499733281011, l2: 0.00040818975248839705   Iteration 16 of 100, tot loss = 5.17793034017086, l1: 0.00011349575152053148, l2: 0.0004042972805109457   Iteration 17 of 100, tot loss = 5.087046062245088, l1: 0.00011089324235560044, l2: 0.00039781136215691836   Iteration 18 of 100, tot loss = 5.1123915248447, l1: 0.00011057159362179745, l2: 0.00040066755981469114   Iteration 19 of 100, tot loss = 5.192067623138428, l1: 0.00011160851174377297, l2: 0.00040759825022082383   Iteration 20 of 100, tot loss = 5.062892591953277, l1: 0.0001083194772945717, l2: 0.0003979697823524475   Iteration 21 of 100, tot loss = 4.915578467505319, l1: 0.00010595354546066595, l2: 0.00038560430194982994   Iteration 22 of 100, tot loss = 4.914195158264854, l1: 0.00010643621780301063, l2: 0.0003849832990619523   Iteration 23 of 100, tot loss = 4.99813377338907, l1: 0.00010695113024153018, l2: 0.00039286224826988155   Iteration 24 of 100, tot loss = 5.115264981985092, l1: 0.00010820368288477766, l2: 0.0004033228172678112   Iteration 25 of 100, tot loss = 5.07495114326477, l1: 0.0001080316813022364, l2: 0.00039946343458723276   Iteration 26 of 100, tot loss = 5.111293178338271, l1: 0.00010885974370467011, l2: 0.00040226957431654085   Iteration 27 of 100, tot loss = 5.070663566942568, l1: 0.00010798153447534855, l2: 0.0003990848226412372   Iteration 28 of 100, tot loss = 5.011143258639744, l1: 0.00010670216072737406, l2: 0.0003944121651459552   Iteration 29 of 100, tot loss = 4.953401491559785, l1: 0.00010605244172219155, l2: 0.0003892877073918373   Iteration 30 of 100, tot loss = 4.985684720675151, l1: 0.00010693074570250853, l2: 0.0003916377264734668   Iteration 31 of 100, tot loss = 5.041526894415578, l1: 0.00010789136696633162, l2: 0.0003962613226312603   Iteration 32 of 100, tot loss = 5.003946170210838, l1: 0.00010730573546879896, l2: 0.0003930888819922984   Iteration 33 of 100, tot loss = 5.041372674884218, l1: 0.00010768986193756008, l2: 0.0003964474048133883   Iteration 34 of 100, tot loss = 5.1090436262242935, l1: 0.00010886012312445982, l2: 0.0004020442399504485   Iteration 35 of 100, tot loss = 5.0899471691676546, l1: 0.00010783346733660437, l2: 0.000401161249776903   Iteration 36 of 100, tot loss = 5.136806448300679, l1: 0.00010876503872875926, l2: 0.0004049156054356394   Iteration 37 of 100, tot loss = 5.108595010396597, l1: 0.000109256125142565, l2: 0.0004016033761561974   Iteration 38 of 100, tot loss = 5.091493242665341, l1: 0.00010900677242668615, l2: 0.0004001425515621361   Iteration 39 of 100, tot loss = 5.088248252868652, l1: 0.00010909347297573008, l2: 0.0003997313529581357   Iteration 40 of 100, tot loss = 5.150628745555878, l1: 0.00010970278299282654, l2: 0.00040536009291827215   Iteration 41 of 100, tot loss = 5.118989863046786, l1: 0.00010945953376571329, l2: 0.0004024394534431716   Iteration 42 of 100, tot loss = 5.0793127956844515, l1: 0.00010932603713410485, l2: 0.00039860524321695613   Iteration 43 of 100, tot loss = 5.061447393062503, l1: 0.00010881554729611137, l2: 0.0003973291926275462   Iteration 44 of 100, tot loss = 5.038391151211479, l1: 0.00010856017872191097, l2: 0.00039527893750346266   Iteration 45 of 100, tot loss = 5.022244045469496, l1: 0.0001078089722593884, l2: 0.0003944154334021732   Iteration 46 of 100, tot loss = 4.978839584018873, l1: 0.00010714027943638781, l2: 0.00039074367985291326   Iteration 47 of 100, tot loss = 4.9614822306531545, l1: 0.00010662073335434469, l2: 0.0003895274904665557   Iteration 48 of 100, tot loss = 4.932979042331378, l1: 0.00010571428439713297, l2: 0.00038758362037090893   Iteration 49 of 100, tot loss = 4.905607744139068, l1: 0.0001057337826843687, l2: 0.0003848269923913236   Iteration 50 of 100, tot loss = 4.8935243082046505, l1: 0.00010589075456664432, l2: 0.0003834616765379906   Iteration 51 of 100, tot loss = 4.840911313599231, l1: 0.00010466327840535371, l2: 0.00037942785317735634   Iteration 52 of 100, tot loss = 4.836390431110676, l1: 0.00010398222058518932, l2: 0.0003796568232176539   Iteration 53 of 100, tot loss = 4.8240166880049795, l1: 0.00010410259710624814, l2: 0.0003782990723540831   Iteration 54 of 100, tot loss = 4.815637120494136, l1: 0.00010409745354317474, l2: 0.0003774662591577335   Iteration 55 of 100, tot loss = 4.829036539251154, l1: 0.00010447456628422846, l2: 0.0003784290885298767   Iteration 56 of 100, tot loss = 4.83782616683415, l1: 0.0001041288549978552, l2: 0.00037965376343761036   Iteration 57 of 100, tot loss = 4.822612348355745, l1: 0.00010408894932448962, l2: 0.0003781722870802409   Iteration 58 of 100, tot loss = 4.826005948000941, l1: 0.00010390384215560485, l2: 0.00037869675373979685   Iteration 59 of 100, tot loss = 4.822101192959284, l1: 0.00010422353647053399, l2: 0.00037798658358643495   Iteration 60 of 100, tot loss = 4.791268010934194, l1: 0.00010395162559386032, l2: 0.00037517517606223315   Iteration 61 of 100, tot loss = 4.7772430904576035, l1: 0.00010387881894664625, l2: 0.00037384549080447647   Iteration 62 of 100, tot loss = 4.730887561075149, l1: 0.00010287156193601643, l2: 0.00037021719500414965   Iteration 63 of 100, tot loss = 4.722278089750381, l1: 0.0001027309904563322, l2: 0.00036949681932114004   Iteration 64 of 100, tot loss = 4.713279755786061, l1: 0.00010258602657131632, l2: 0.00036874195006930677   Iteration 65 of 100, tot loss = 4.727284704721891, l1: 0.00010273444758450541, l2: 0.0003699940246476147   Iteration 66 of 100, tot loss = 4.712057017918789, l1: 0.00010236964694878694, l2: 0.0003688360564851654   Iteration 67 of 100, tot loss = 4.718066519765712, l1: 0.0001027542409124791, l2: 0.00036905241288321175   Iteration 68 of 100, tot loss = 4.742551549392588, l1: 0.00010316106609560152, l2: 0.0003710940909513738   Iteration 69 of 100, tot loss = 4.743634028711181, l1: 0.0001035083586285127, l2: 0.00037085504605994976   Iteration 70 of 100, tot loss = 4.738056807858603, l1: 0.00010325012527963346, l2: 0.0003705555575184657   Iteration 71 of 100, tot loss = 4.720204422171687, l1: 0.00010307688952848302, l2: 0.0003689435545542002   Iteration 72 of 100, tot loss = 4.765564026104079, l1: 0.00010371304511459635, l2: 0.00037284335869319167   Iteration 73 of 100, tot loss = 4.778259814602055, l1: 0.00010405793423845738, l2: 0.0003737680486814846   Iteration 74 of 100, tot loss = 4.799748048589036, l1: 0.00010455584103781164, l2: 0.0003754189648945514   Iteration 75 of 100, tot loss = 4.80195861975352, l1: 0.00010445336525056822, l2: 0.00037574249727185817   Iteration 76 of 100, tot loss = 4.820389466850381, l1: 0.00010423729073456918, l2: 0.0003778016568838594   Iteration 77 of 100, tot loss = 4.828149747538876, l1: 0.00010431993104685279, l2: 0.00037849504465098166   Iteration 78 of 100, tot loss = 4.817885803870666, l1: 0.00010446153823860909, l2: 0.00037732704311985377   Iteration 79 of 100, tot loss = 4.801867039897774, l1: 0.00010419510763773659, l2: 0.00037599159710409853   Iteration 80 of 100, tot loss = 4.832443751394749, l1: 0.00010468166542523249, l2: 0.0003785627106481115   Iteration 81 of 100, tot loss = 4.828608161137428, l1: 0.00010412714564827857, l2: 0.0003787336713292372   Iteration 82 of 100, tot loss = 4.852877647411533, l1: 0.0001042195390476902, l2: 0.0003810682266215203   Iteration 83 of 100, tot loss = 4.8360155648495775, l1: 0.00010396983393493209, l2: 0.0003796317237667762   Iteration 84 of 100, tot loss = 4.811125464382625, l1: 0.00010357587217142628, l2: 0.00037753667527881804   Iteration 85 of 100, tot loss = 4.853650445096633, l1: 0.00010418139384402072, l2: 0.00038118365114582156   Iteration 86 of 100, tot loss = 4.836598073327264, l1: 0.00010363657912780899, l2: 0.00038002322879376237   Iteration 87 of 100, tot loss = 4.828828843160608, l1: 0.00010340900917071849, l2: 0.0003794738753556927   Iteration 88 of 100, tot loss = 4.845951544967565, l1: 0.00010353517361429773, l2: 0.0003810599815055453   Iteration 89 of 100, tot loss = 4.829018299499254, l1: 0.00010337909631917693, l2: 0.0003795227344249888   Iteration 90 of 100, tot loss = 4.845448675420549, l1: 0.00010378545670795979, l2: 0.00038075941184716715   Iteration 91 of 100, tot loss = 4.851406187801571, l1: 0.00010368985473817972, l2: 0.00038145076541695744   Iteration 92 of 100, tot loss = 4.8372822701931, l1: 0.00010351991841162089, l2: 0.0003802083100827472   Iteration 93 of 100, tot loss = 4.845023741004288, l1: 0.00010346604055181767, l2: 0.0003810363346948639   Iteration 94 of 100, tot loss = 4.845830664989796, l1: 0.00010366458035547267, l2: 0.0003809184874742331   Iteration 95 of 100, tot loss = 4.840956383002432, l1: 0.00010344867493442603, l2: 0.0003806469644922273   Iteration 96 of 100, tot loss = 4.840635017802318, l1: 0.00010328015491722908, l2: 0.00038078334803988884   Iteration 97 of 100, tot loss = 4.871251884194994, l1: 0.00010392804080728423, l2: 0.00038319714886457995   Iteration 98 of 100, tot loss = 4.8640932757027295, l1: 0.00010383324078414874, l2: 0.0003825760877403735   Iteration 99 of 100, tot loss = 4.851578978577045, l1: 0.00010359701161058808, l2: 0.0003815608872559317   Iteration 100 of 100, tot loss = 4.8532050001621245, l1: 0.00010392703195975628, l2: 0.0003813934691424947
   End of epoch 1295; saving model... 

Epoch 1296 of 2000
   Iteration 1 of 100, tot loss = 2.6172597408294678, l1: 4.766956772073172e-05, l2: 0.000214056417462416   Iteration 2 of 100, tot loss = 4.2100454568862915, l1: 7.800848652550485e-05, l2: 0.00034299605613341555   Iteration 3 of 100, tot loss = 4.461868047714233, l1: 9.136783167681035e-05, l2: 0.0003548189755141114   Iteration 4 of 100, tot loss = 4.378186047077179, l1: 9.1364209765743e-05, l2: 0.0003464543951849919   Iteration 5 of 100, tot loss = 3.9251959800720213, l1: 8.584585812059231e-05, l2: 0.0003066737437620759   Iteration 6 of 100, tot loss = 4.105848471323649, l1: 8.908947893360164e-05, l2: 0.0003214953709781791   Iteration 7 of 100, tot loss = 4.362918513161795, l1: 9.409111901602176e-05, l2: 0.0003422007347191019   Iteration 8 of 100, tot loss = 4.517335295677185, l1: 9.939600795405568e-05, l2: 0.0003523375235090498   Iteration 9 of 100, tot loss = 4.521593517727322, l1: 0.0001017001587671176, l2: 0.0003504591942247417   Iteration 10 of 100, tot loss = 4.414584064483643, l1: 9.969926068151836e-05, l2: 0.0003417591447941959   Iteration 11 of 100, tot loss = 4.379511269656095, l1: 9.9403727786425e-05, l2: 0.00033854739740490913   Iteration 12 of 100, tot loss = 4.475702961285909, l1: 0.00010136349343762656, l2: 0.0003462068028359984   Iteration 13 of 100, tot loss = 4.391308491046612, l1: 0.00010062486706794097, l2: 0.0003385059826541692   Iteration 14 of 100, tot loss = 4.723137719290597, l1: 0.00010546202637280138, l2: 0.00036685174356015135   Iteration 15 of 100, tot loss = 4.650379912058512, l1: 0.00010464156827462526, l2: 0.00036039642097118   Iteration 16 of 100, tot loss = 4.804983168840408, l1: 0.00010600375139802054, l2: 0.00037449456613103393   Iteration 17 of 100, tot loss = 4.881750219008502, l1: 0.00010766073319246061, l2: 0.00038051429232034613   Iteration 18 of 100, tot loss = 4.893466552098592, l1: 0.0001089718411498729, l2: 0.0003803748178244051   Iteration 19 of 100, tot loss = 4.957863456324527, l1: 0.00010968692157276612, l2: 0.00038609942605130767   Iteration 20 of 100, tot loss = 4.893736016750336, l1: 0.00010795338403113419, l2: 0.00038142021949170155   Iteration 21 of 100, tot loss = 4.844358171735491, l1: 0.00010710064972434858, l2: 0.00037733516849887866   Iteration 22 of 100, tot loss = 4.81882381439209, l1: 0.00010615087161237501, l2: 0.0003757315121633424   Iteration 23 of 100, tot loss = 4.8352998650592305, l1: 0.00010569833213357133, l2: 0.0003778316566477651   Iteration 24 of 100, tot loss = 4.865949829419454, l1: 0.00010611231315730645, l2: 0.0003804826737905387   Iteration 25 of 100, tot loss = 4.870946292877197, l1: 0.00010603929942590184, l2: 0.00038105533458292483   Iteration 26 of 100, tot loss = 4.873856232716487, l1: 0.00010488799853192177, l2: 0.00038249762795973   Iteration 27 of 100, tot loss = 4.830902938489561, l1: 0.00010382572010577726, l2: 0.0003792645767572577   Iteration 28 of 100, tot loss = 4.851534732750484, l1: 0.00010459252799981706, l2: 0.0003805609480228408   Iteration 29 of 100, tot loss = 4.784748019843266, l1: 0.00010264917301387398, l2: 0.0003758256322631998   Iteration 30 of 100, tot loss = 4.905361533164978, l1: 0.00010464970558435502, l2: 0.00038588645159810163   Iteration 31 of 100, tot loss = 4.977859797016267, l1: 0.00010545749413018535, l2: 0.0003923284891624785   Iteration 32 of 100, tot loss = 5.013206012547016, l1: 0.0001062693005451365, l2: 0.00039505130416728207   Iteration 33 of 100, tot loss = 5.062543558351921, l1: 0.00010760809759217823, l2: 0.00039864626270366097   Iteration 34 of 100, tot loss = 5.040859916630914, l1: 0.00010717304918644539, l2: 0.00039691294617060684   Iteration 35 of 100, tot loss = 5.036625392096383, l1: 0.00010728274469978974, l2: 0.0003963797981019265   Iteration 36 of 100, tot loss = 5.018956535392338, l1: 0.00010633416210718376, l2: 0.0003955614944667711   Iteration 37 of 100, tot loss = 4.958674366409714, l1: 0.00010551664594016541, l2: 0.00039035079385644784   Iteration 38 of 100, tot loss = 4.943560738312571, l1: 0.00010582737616603386, l2: 0.0003885287009516584   Iteration 39 of 100, tot loss = 4.943645513974703, l1: 0.00010622872119151558, l2: 0.0003881358339313943   Iteration 40 of 100, tot loss = 5.027841317653656, l1: 0.00010800937134263222, l2: 0.0003947747642087052   Iteration 41 of 100, tot loss = 5.006800511988198, l1: 0.00010785326532374999, l2: 0.0003928267897430398   Iteration 42 of 100, tot loss = 5.041599705105736, l1: 0.00010872423661653774, l2: 0.00039543573768155297   Iteration 43 of 100, tot loss = 5.079278280568677, l1: 0.00010952343502647764, l2: 0.0003984043970947739   Iteration 44 of 100, tot loss = 5.077592199498957, l1: 0.00010937644112180516, l2: 0.00039838278164759026   Iteration 45 of 100, tot loss = 5.05670607884725, l1: 0.00010952556096728787, l2: 0.00039614504946964895   Iteration 46 of 100, tot loss = 4.990165435749551, l1: 0.00010825586468854453, l2: 0.00039076068123508975   Iteration 47 of 100, tot loss = 5.007248061768552, l1: 0.00010885549512595196, l2: 0.0003918693131554198   Iteration 48 of 100, tot loss = 5.067817106842995, l1: 0.00010974060182888934, l2: 0.0003970411104698239   Iteration 49 of 100, tot loss = 5.0345556687335575, l1: 0.00010906798120324348, l2: 0.0003943875873323568   Iteration 50 of 100, tot loss = 5.0518952655792235, l1: 0.00010857186302018818, l2: 0.0003966176658286713   Iteration 51 of 100, tot loss = 5.0032184030495435, l1: 0.00010781869908290294, l2: 0.00039250314362374516   Iteration 52 of 100, tot loss = 5.013837644687066, l1: 0.0001080869031704004, l2: 0.00039329686310688534   Iteration 53 of 100, tot loss = 5.00827605319473, l1: 0.0001084177965822164, l2: 0.00039240981002979614   Iteration 54 of 100, tot loss = 4.9648946832727505, l1: 0.00010791047174684031, l2: 0.0003885789976468206   Iteration 55 of 100, tot loss = 4.970077289234508, l1: 0.00010832663002392193, l2: 0.0003886810998665169   Iteration 56 of 100, tot loss = 4.933312139340809, l1: 0.0001080482827221983, l2: 0.0003852829319969585   Iteration 57 of 100, tot loss = 4.923268012833177, l1: 0.00010815468838125908, l2: 0.00038417211346728565   Iteration 58 of 100, tot loss = 4.9779941747928484, l1: 0.00010827982742770912, l2: 0.0003895195904047357   Iteration 59 of 100, tot loss = 4.982796689211312, l1: 0.00010823943906007887, l2: 0.0003900402302013205   Iteration 60 of 100, tot loss = 4.985852730274201, l1: 0.00010819641496103334, l2: 0.00039038885855309974   Iteration 61 of 100, tot loss = 4.951521881291123, l1: 0.00010728431361265573, l2: 0.0003878678751968183   Iteration 62 of 100, tot loss = 4.909240987993056, l1: 0.00010651521217603718, l2: 0.00038440888719407904   Iteration 63 of 100, tot loss = 4.953587588809786, l1: 0.00010728645775954993, l2: 0.000388072301459957   Iteration 64 of 100, tot loss = 4.976086836308241, l1: 0.00010759316751318693, l2: 0.00039001551658657263   Iteration 65 of 100, tot loss = 4.959794341600858, l1: 0.00010748162200728145, l2: 0.0003884978126734495   Iteration 66 of 100, tot loss = 4.997499707973365, l1: 0.00010809713079872768, l2: 0.0003916528402249131   Iteration 67 of 100, tot loss = 5.0431424575065495, l1: 0.00010883778217683005, l2: 0.000395476463626125   Iteration 68 of 100, tot loss = 5.018951833248138, l1: 0.00010853817772894057, l2: 0.00039335700592560256   Iteration 69 of 100, tot loss = 5.014922560125157, l1: 0.00010863545247812526, l2: 0.00039285680395551026   Iteration 70 of 100, tot loss = 4.994107334954398, l1: 0.00010778562521279257, l2: 0.00039162510864636197   Iteration 71 of 100, tot loss = 4.990070363165627, l1: 0.0001080479061117881, l2: 0.00039095913050171563   Iteration 72 of 100, tot loss = 4.97520414325926, l1: 0.00010772434446456221, l2: 0.00038979607011747756   Iteration 73 of 100, tot loss = 4.971468585811249, l1: 0.00010748533462253974, l2: 0.00038966152431086113   Iteration 74 of 100, tot loss = 4.965080067918107, l1: 0.0001075552603209429, l2: 0.00038895274652482793   Iteration 75 of 100, tot loss = 4.9765155919392905, l1: 0.00010773445149728408, l2: 0.000389917108307903   Iteration 76 of 100, tot loss = 4.951904193351143, l1: 0.00010740656886666425, l2: 0.0003877838508311757   Iteration 77 of 100, tot loss = 4.985140432010997, l1: 0.00010818190110678023, l2: 0.0003903321422902601   Iteration 78 of 100, tot loss = 5.012774397165347, l1: 0.00010852139157344564, l2: 0.0003927560487332252   Iteration 79 of 100, tot loss = 4.991817893861215, l1: 0.00010820632727516084, l2: 0.00039097546272663566   Iteration 80 of 100, tot loss = 5.013010957837105, l1: 0.00010839334936463274, l2: 0.00039290774657274596   Iteration 81 of 100, tot loss = 5.0068800096158625, l1: 0.00010861794789103263, l2: 0.0003920700534011157   Iteration 82 of 100, tot loss = 4.978721063311507, l1: 0.00010800738354879374, l2: 0.000389864723263422   Iteration 83 of 100, tot loss = 4.966042777141893, l1: 0.00010779886759841444, l2: 0.00038880541049650906   Iteration 84 of 100, tot loss = 4.957608824684506, l1: 0.00010791980401422951, l2: 0.0003878410785719531   Iteration 85 of 100, tot loss = 4.953265689401066, l1: 0.00010768190443152836, l2: 0.00038764466435703285   Iteration 86 of 100, tot loss = 4.957708879958751, l1: 0.00010806103713294697, l2: 0.0003877098507621477   Iteration 87 of 100, tot loss = 4.9550721946804, l1: 0.00010791829648867902, l2: 0.0003875889229968233   Iteration 88 of 100, tot loss = 4.936216944997961, l1: 0.00010753314402543104, l2: 0.00038608855042680676   Iteration 89 of 100, tot loss = 4.9370654620481345, l1: 0.00010730280447522377, l2: 0.0003864037417780524   Iteration 90 of 100, tot loss = 4.918643641471863, l1: 0.0001072665368863252, l2: 0.00038459782727942284   Iteration 91 of 100, tot loss = 4.927809204374041, l1: 0.00010731880372207212, l2: 0.0003854621167852451   Iteration 92 of 100, tot loss = 4.927786617175393, l1: 0.00010731557548475077, l2: 0.00038546308623272523   Iteration 93 of 100, tot loss = 4.968539050830308, l1: 0.00010771845007829759, l2: 0.0003891354542531033   Iteration 94 of 100, tot loss = 4.971123043526995, l1: 0.00010803004427524433, l2: 0.0003890822591393829   Iteration 95 of 100, tot loss = 4.992843469820524, l1: 0.00010851138975244555, l2: 0.0003907729562151393   Iteration 96 of 100, tot loss = 4.977884980539481, l1: 0.0001082117353613891, l2: 0.0003895767619421046   Iteration 97 of 100, tot loss = 5.035568141445671, l1: 0.00010905336625676016, l2: 0.0003945034470759601   Iteration 98 of 100, tot loss = 5.026431125037524, l1: 0.00010896388649060249, l2: 0.00039367922504518027   Iteration 99 of 100, tot loss = 5.035364577264497, l1: 0.00010927788209319444, l2: 0.00039425857495657647   Iteration 100 of 100, tot loss = 5.031086781024933, l1: 0.00010915179318544687, l2: 0.0003939568842179142
   End of epoch 1296; saving model... 

Epoch 1297 of 2000
   Iteration 1 of 100, tot loss = 4.131776332855225, l1: 8.849742880556732e-05, l2: 0.0003246802370995283   Iteration 2 of 100, tot loss = 4.537240982055664, l1: 0.00010122369712917134, l2: 0.00035250042856205255   Iteration 3 of 100, tot loss = 5.726834297180176, l1: 0.00011452690038519601, l2: 0.00045815654448233545   Iteration 4 of 100, tot loss = 5.09925639629364, l1: 0.00010687756730476394, l2: 0.0004030480813526083   Iteration 5 of 100, tot loss = 4.867287635803223, l1: 0.00010099470819113776, l2: 0.0003857340634567663   Iteration 6 of 100, tot loss = 5.271223147710164, l1: 0.00010812587061082013, l2: 0.0004189964456600137   Iteration 7 of 100, tot loss = 5.377836840493338, l1: 0.00010878488995201354, l2: 0.00042899879917968065   Iteration 8 of 100, tot loss = 5.19685024023056, l1: 0.0001056680657711695, l2: 0.0004140169639867963   Iteration 9 of 100, tot loss = 5.032615052329169, l1: 0.00010388054377270034, l2: 0.00039938096597325057   Iteration 10 of 100, tot loss = 4.909603905677796, l1: 0.00010306973854312673, l2: 0.00038789065583841873   Iteration 11 of 100, tot loss = 5.138057600368153, l1: 0.00010859181268394671, l2: 0.0004052139503817836   Iteration 12 of 100, tot loss = 5.138787527879079, l1: 0.00011140649075969122, l2: 0.000402472264492341   Iteration 13 of 100, tot loss = 5.0554869174957275, l1: 0.00011228133091488137, l2: 0.0003932673638561167   Iteration 14 of 100, tot loss = 5.0255212272916525, l1: 0.00011168192525344369, l2: 0.00039087020247409655   Iteration 15 of 100, tot loss = 5.035071897506714, l1: 0.00011283637286396697, l2: 0.00039067082398105415   Iteration 16 of 100, tot loss = 5.094571486115456, l1: 0.00011354329717505607, l2: 0.00039591385666426504   Iteration 17 of 100, tot loss = 5.2059113418354706, l1: 0.00011415400684413994, l2: 0.0004064371331718148   Iteration 18 of 100, tot loss = 5.16500265068478, l1: 0.00011319138704695636, l2: 0.0004033088844153099   Iteration 19 of 100, tot loss = 5.163565096102263, l1: 0.00011374143344399177, l2: 0.0004026150831793386   Iteration 20 of 100, tot loss = 5.168567144870758, l1: 0.00011397991329431534, l2: 0.0004028768053103704   Iteration 21 of 100, tot loss = 5.107724677948725, l1: 0.0001135309856700977, l2: 0.0003972414865746119   Iteration 22 of 100, tot loss = 5.179908048022877, l1: 0.00011395970696784471, l2: 0.00040403110125969926   Iteration 23 of 100, tot loss = 5.21825673269189, l1: 0.00011432736146557347, l2: 0.0004074983165913221   Iteration 24 of 100, tot loss = 5.2630216379960375, l1: 0.00011528417568721731, l2: 0.0004110179913065319   Iteration 25 of 100, tot loss = 5.290935640335083, l1: 0.00011661113676382228, l2: 0.000412482432438992   Iteration 26 of 100, tot loss = 5.226947069168091, l1: 0.00011578935076473639, l2: 0.0004069053611601703   Iteration 27 of 100, tot loss = 5.251223316899052, l1: 0.0001154605068020626, l2: 0.0004096618283289933   Iteration 28 of 100, tot loss = 5.185066495622907, l1: 0.00011406707147086439, l2: 0.0004044395812213354   Iteration 29 of 100, tot loss = 5.128552379279301, l1: 0.00011274928843265335, l2: 0.0004001059524257314   Iteration 30 of 100, tot loss = 5.098158987363179, l1: 0.00011295659035871116, l2: 0.00039685931163451946   Iteration 31 of 100, tot loss = 5.1986658573150635, l1: 0.00011447386373953534, l2: 0.00040539272360488654   Iteration 32 of 100, tot loss = 5.171915911138058, l1: 0.00011417923269618768, l2: 0.0004030123604934488   Iteration 33 of 100, tot loss = 5.145670565691861, l1: 0.0001133685293483943, l2: 0.0004011985301710383   Iteration 34 of 100, tot loss = 5.1517084696713615, l1: 0.00011351142101749942, l2: 0.00040165942824815456   Iteration 35 of 100, tot loss = 5.072493021828787, l1: 0.00011230618422684659, l2: 0.00039494312035718134   Iteration 36 of 100, tot loss = 5.0427991019354925, l1: 0.00011191509858892662, l2: 0.000392364813402916   Iteration 37 of 100, tot loss = 5.007706423063536, l1: 0.00011093888787326177, l2: 0.0003898317557507874   Iteration 38 of 100, tot loss = 4.977977715040508, l1: 0.00011008972624519007, l2: 0.00038770804684119006   Iteration 39 of 100, tot loss = 5.021260762825991, l1: 0.00010945237889432181, l2: 0.0003926736983255698   Iteration 40 of 100, tot loss = 4.991605132818222, l1: 0.00010874405616050354, l2: 0.0003904164579580538   Iteration 41 of 100, tot loss = 5.024332878066272, l1: 0.00010942984226858243, l2: 0.00039300344679958936   Iteration 42 of 100, tot loss = 5.0492597704841975, l1: 0.00010917254448508, l2: 0.0003957534333624478   Iteration 43 of 100, tot loss = 4.998453156892643, l1: 0.00010771653961815308, l2: 0.0003921287770513012   Iteration 44 of 100, tot loss = 5.015761218287728, l1: 0.00010800088065562622, l2: 0.00039357524193209514   Iteration 45 of 100, tot loss = 4.971447743309868, l1: 0.00010734931007997754, l2: 0.0003897954651620239   Iteration 46 of 100, tot loss = 4.951201656590337, l1: 0.00010679144978222094, l2: 0.00038832871676118964   Iteration 47 of 100, tot loss = 4.924841997471262, l1: 0.00010631521775181643, l2: 0.0003861689827395009   Iteration 48 of 100, tot loss = 4.8997687349716825, l1: 0.00010633221071050987, l2: 0.00038364466369481914   Iteration 49 of 100, tot loss = 4.9028695602806245, l1: 0.00010680383208210161, l2: 0.0003834831242078953   Iteration 50 of 100, tot loss = 4.931461892127991, l1: 0.00010698265243263449, l2: 0.00038616353704128413   Iteration 51 of 100, tot loss = 4.931908107271381, l1: 0.00010691542004625422, l2: 0.00038627539042328647   Iteration 52 of 100, tot loss = 4.9173786502618055, l1: 0.00010656340971483866, l2: 0.00038517445500474423   Iteration 53 of 100, tot loss = 4.9121155513907375, l1: 0.00010650313149353507, l2: 0.0003847084233300093   Iteration 54 of 100, tot loss = 4.945451935132344, l1: 0.00010711794655233259, l2: 0.00038742724670252454   Iteration 55 of 100, tot loss = 4.907541223005815, l1: 0.00010672164902313273, l2: 0.00038403247308451684   Iteration 56 of 100, tot loss = 4.920580250876291, l1: 0.00010667811037429991, l2: 0.00038537991427542044   Iteration 57 of 100, tot loss = 4.888098185522514, l1: 0.00010630012069381529, l2: 0.0003825096975656618   Iteration 58 of 100, tot loss = 4.935108904180856, l1: 0.0001071040897558901, l2: 0.00038640680025403936   Iteration 59 of 100, tot loss = 4.928261389166622, l1: 0.00010712168400690859, l2: 0.0003857044545022013   Iteration 60 of 100, tot loss = 4.92538982629776, l1: 0.0001069564112185617, l2: 0.00038558257147087717   Iteration 61 of 100, tot loss = 4.93362726539862, l1: 0.00010659879870993299, l2: 0.0003867639277844125   Iteration 62 of 100, tot loss = 4.947251862095248, l1: 0.00010700344764993661, l2: 0.0003877217388963477   Iteration 63 of 100, tot loss = 4.950446586760264, l1: 0.0001070361924778101, l2: 0.00038800846636554017   Iteration 64 of 100, tot loss = 4.927328631281853, l1: 0.00010669951331010452, l2: 0.0003860333501961577   Iteration 65 of 100, tot loss = 4.93749031653771, l1: 0.0001067186202607655, l2: 0.0003870304120937362   Iteration 66 of 100, tot loss = 4.92460510225007, l1: 0.00010663308647494749, l2: 0.0003858274247966509   Iteration 67 of 100, tot loss = 4.931209144307606, l1: 0.00010662775180683081, l2: 0.0003864931630286902   Iteration 68 of 100, tot loss = 4.908573599422679, l1: 0.00010593913761755069, l2: 0.00038491822252741684   Iteration 69 of 100, tot loss = 4.91092724731003, l1: 0.00010611723500651483, l2: 0.00038497548940949196   Iteration 70 of 100, tot loss = 4.882015446254186, l1: 0.00010588625154923647, l2: 0.0003823152926218297   Iteration 71 of 100, tot loss = 4.876292712251905, l1: 0.00010534220364462899, l2: 0.00038228706664956687   Iteration 72 of 100, tot loss = 4.877360853883955, l1: 0.00010544982999110491, l2: 0.0003822862543327372   Iteration 73 of 100, tot loss = 4.860834562615173, l1: 0.00010558113777509265, l2: 0.0003805023172315312   Iteration 74 of 100, tot loss = 4.851774238251351, l1: 0.00010564712126349175, l2: 0.0003795303011822791   Iteration 75 of 100, tot loss = 4.847464431126912, l1: 0.00010585330184161043, l2: 0.0003788931400049478   Iteration 76 of 100, tot loss = 4.82695072889328, l1: 0.00010530311230563951, l2: 0.00037739195939043144   Iteration 77 of 100, tot loss = 4.826175767105895, l1: 0.00010522181523870073, l2: 0.0003773957599020962   Iteration 78 of 100, tot loss = 4.816736511695079, l1: 0.00010480388580337883, l2: 0.00037686976392228063   Iteration 79 of 100, tot loss = 4.819334805766238, l1: 0.0001047257946647963, l2: 0.0003772076842658033   Iteration 80 of 100, tot loss = 4.818676015734672, l1: 0.00010459434352014796, l2: 0.00037727325652667785   Iteration 81 of 100, tot loss = 4.793371712719953, l1: 0.00010396888898901159, l2: 0.00037536828079473593   Iteration 82 of 100, tot loss = 4.812282637851994, l1: 0.000104155514169933, l2: 0.0003770727480736152   Iteration 83 of 100, tot loss = 4.807298470692462, l1: 0.00010390408899556531, l2: 0.0003768257568492161   Iteration 84 of 100, tot loss = 4.814478709584191, l1: 0.00010433641606747794, l2: 0.00037711145411040413   Iteration 85 of 100, tot loss = 4.844964913760914, l1: 0.00010500156358295284, l2: 0.000379494926721022   Iteration 86 of 100, tot loss = 4.857700131660284, l1: 0.00010500496669962114, l2: 0.0003807650454989418   Iteration 87 of 100, tot loss = 4.8535334926912155, l1: 0.00010513133597198939, l2: 0.00038022201206540455   Iteration 88 of 100, tot loss = 4.857714252038435, l1: 0.00010550804613144878, l2: 0.0003802633775855859   Iteration 89 of 100, tot loss = 4.843924484895856, l1: 0.0001052485738378646, l2: 0.0003791438731864576   Iteration 90 of 100, tot loss = 4.843214665518866, l1: 0.000105263668633092, l2: 0.0003790577965749738   Iteration 91 of 100, tot loss = 4.85130211023184, l1: 0.00010511018283048563, l2: 0.000380020026868506   Iteration 92 of 100, tot loss = 4.8300363162289495, l1: 0.00010497332226330637, l2: 0.0003780303080060074   Iteration 93 of 100, tot loss = 4.827315953470046, l1: 0.0001045759245146176, l2: 0.00037815566932011433   Iteration 94 of 100, tot loss = 4.857939093670947, l1: 0.0001048710997720457, l2: 0.0003809228083094027   Iteration 95 of 100, tot loss = 4.8365547355852625, l1: 0.00010472542443925417, l2: 0.0003789300480428593   Iteration 96 of 100, tot loss = 4.824955937763055, l1: 0.00010469443562518184, l2: 0.0003778011573558615   Iteration 97 of 100, tot loss = 4.821429530369866, l1: 0.00010446546666477759, l2: 0.00037767748560242777   Iteration 98 of 100, tot loss = 4.8301979011418865, l1: 0.00010464061296613868, l2: 0.000378379176699851   Iteration 99 of 100, tot loss = 4.823960537862296, l1: 0.00010464171947490406, l2: 0.00037775433377122903   Iteration 100 of 100, tot loss = 4.823593466281891, l1: 0.00010465456543897744, l2: 0.0003777047806943301
   End of epoch 1297; saving model... 

Epoch 1298 of 2000
   Iteration 1 of 100, tot loss = 2.3663692474365234, l1: 6.157397729111835e-05, l2: 0.0001750629598973319   Iteration 2 of 100, tot loss = 3.9495182037353516, l1: 9.252270319848321e-05, l2: 0.0003024291290785186   Iteration 3 of 100, tot loss = 4.22438112894694, l1: 9.803746312779064e-05, l2: 0.0003244006511522457   Iteration 4 of 100, tot loss = 4.092851459980011, l1: 9.649851199355908e-05, l2: 0.00031278663300327025   Iteration 5 of 100, tot loss = 4.107314825057983, l1: 0.00010063312365673482, l2: 0.00031009835947770625   Iteration 6 of 100, tot loss = 4.087900479634603, l1: 9.338398194813635e-05, l2: 0.00031540606626852724   Iteration 7 of 100, tot loss = 4.402867657797677, l1: 9.923765121909258e-05, l2: 0.00034104911069984416   Iteration 8 of 100, tot loss = 4.367469131946564, l1: 9.941774669641745e-05, l2: 0.00033732916017470416   Iteration 9 of 100, tot loss = 4.217570649252997, l1: 9.72441707239745e-05, l2: 0.00032451288684064313   Iteration 10 of 100, tot loss = 4.256537365913391, l1: 9.800285442906898e-05, l2: 0.0003276508738053963   Iteration 11 of 100, tot loss = 4.394205245104703, l1: 0.00010145729406345212, l2: 0.00033796322002837604   Iteration 12 of 100, tot loss = 4.19649875164032, l1: 9.654167600577541e-05, l2: 0.00032310818884676945   Iteration 13 of 100, tot loss = 4.460447348081148, l1: 0.00010142804183907664, l2: 0.0003446166833432821   Iteration 14 of 100, tot loss = 4.504127400262015, l1: 0.0001028330173409943, l2: 0.0003475797129794955   Iteration 15 of 100, tot loss = 4.658421389261881, l1: 0.00010420355150320877, l2: 0.0003616385782758395   Iteration 16 of 100, tot loss = 4.612850666046143, l1: 0.00010409601827632287, l2: 0.0003571890392777277   Iteration 17 of 100, tot loss = 4.692005578209372, l1: 0.00010496673348825425, l2: 0.00036423381671364256   Iteration 18 of 100, tot loss = 4.695807774861653, l1: 0.00010516676168966417, l2: 0.00036441400879994035   Iteration 19 of 100, tot loss = 4.759562542563991, l1: 0.00010487558450366028, l2: 0.0003710806639374871   Iteration 20 of 100, tot loss = 4.8254001379013065, l1: 0.00010676674683054443, l2: 0.00037577326293103397   Iteration 21 of 100, tot loss = 4.769074871426537, l1: 0.00010572228555767132, l2: 0.0003711851978940623   Iteration 22 of 100, tot loss = 4.6601797775788745, l1: 0.00010410219064479779, l2: 0.00036191578295653346   Iteration 23 of 100, tot loss = 4.732310761576113, l1: 0.00010570946039439625, l2: 0.0003675216114219359   Iteration 24 of 100, tot loss = 4.749349067608516, l1: 0.00010576091093147018, l2: 0.00036917399180917226   Iteration 25 of 100, tot loss = 4.791260480880737, l1: 0.00010657316539436579, l2: 0.00037255287810694426   Iteration 26 of 100, tot loss = 4.854234484525827, l1: 0.00010679423016531822, l2: 0.00037862921229903947   Iteration 27 of 100, tot loss = 4.827087940993132, l1: 0.00010601113118119192, l2: 0.0003766976560992016   Iteration 28 of 100, tot loss = 4.762406349182129, l1: 0.0001054485850805317, l2: 0.0003707920436032249   Iteration 29 of 100, tot loss = 4.726743599464154, l1: 0.00010577133647969057, l2: 0.00036690301755603786   Iteration 30 of 100, tot loss = 4.765205144882202, l1: 0.00010588122847063156, l2: 0.00037063928029965607   Iteration 31 of 100, tot loss = 4.762000806869999, l1: 0.00010505340211356299, l2: 0.00037114667394498905   Iteration 32 of 100, tot loss = 4.780197322368622, l1: 0.00010557448513281997, l2: 0.0003724452426467906   Iteration 33 of 100, tot loss = 4.746878905729814, l1: 0.00010435870905039889, l2: 0.00037032917713407767   Iteration 34 of 100, tot loss = 4.691784935839036, l1: 0.000103839450115201, l2: 0.0003653390393832096   Iteration 35 of 100, tot loss = 4.719244418825422, l1: 0.00010451437140415822, l2: 0.0003674100664544052   Iteration 36 of 100, tot loss = 4.746670358710819, l1: 0.000105604969576234, l2: 0.0003690620623173244   Iteration 37 of 100, tot loss = 4.771988436982438, l1: 0.00010634547868756124, l2: 0.0003708533614691397   Iteration 38 of 100, tot loss = 4.740403771400452, l1: 0.00010611963099156703, l2: 0.000367920742734752   Iteration 39 of 100, tot loss = 4.684028387069702, l1: 0.00010535049044007722, l2: 0.00036305234467503254   Iteration 40 of 100, tot loss = 4.64064302444458, l1: 0.00010454123530507786, l2: 0.0003595230631617596   Iteration 41 of 100, tot loss = 4.639446851683826, l1: 0.00010449717406228912, l2: 0.0003594475072132787   Iteration 42 of 100, tot loss = 4.640746491295951, l1: 0.00010506372997215727, l2: 0.0003590109155906941   Iteration 43 of 100, tot loss = 4.650873372721118, l1: 0.00010514460577829849, l2: 0.0003599427274271283   Iteration 44 of 100, tot loss = 4.663992480798201, l1: 0.00010438848750544076, l2: 0.0003620107576848981   Iteration 45 of 100, tot loss = 4.734524324205187, l1: 0.00010595207519751663, l2: 0.000367500353927931   Iteration 46 of 100, tot loss = 4.788083885027015, l1: 0.00010662845607325637, l2: 0.00037217992971620885   Iteration 47 of 100, tot loss = 4.798747458356492, l1: 0.0001065839413910153, l2: 0.0003732908018641768   Iteration 48 of 100, tot loss = 4.836720069249471, l1: 0.00010758063460040528, l2: 0.00037609136992008035   Iteration 49 of 100, tot loss = 4.921023699702049, l1: 0.00010927710610879015, l2: 0.0003828252615212292   Iteration 50 of 100, tot loss = 4.906438150405884, l1: 0.00010923825015197508, l2: 0.00038140556222060696   Iteration 51 of 100, tot loss = 4.883629719416301, l1: 0.00010911940697434486, l2: 0.0003792435623016026   Iteration 52 of 100, tot loss = 4.88166100703753, l1: 0.00010926693119892242, l2: 0.00037889916683735256   Iteration 53 of 100, tot loss = 4.864580874173146, l1: 0.00010891738037451364, l2: 0.0003775407042341166   Iteration 54 of 100, tot loss = 4.855901532702976, l1: 0.00010870933194248075, l2: 0.0003768808186308619   Iteration 55 of 100, tot loss = 4.829442765495994, l1: 0.00010803395628251812, l2: 0.0003749103176894344   Iteration 56 of 100, tot loss = 4.86287848012788, l1: 0.00010831324808740257, l2: 0.00037797459741081445   Iteration 57 of 100, tot loss = 4.858264559193661, l1: 0.00010822380136232823, l2: 0.00037760265184803296   Iteration 58 of 100, tot loss = 4.827802053813277, l1: 0.00010750451374092493, l2: 0.0003752756890427353   Iteration 59 of 100, tot loss = 4.849295183763665, l1: 0.00010740706143147012, l2: 0.0003775224541965083   Iteration 60 of 100, tot loss = 4.79778617421786, l1: 0.00010656440505651214, l2: 0.00037321420980636805   Iteration 61 of 100, tot loss = 4.820322617155607, l1: 0.00010719647962403422, l2: 0.0003748357795287672   Iteration 62 of 100, tot loss = 4.797253764444782, l1: 0.0001067907890119973, l2: 0.00037293458479971083   Iteration 63 of 100, tot loss = 4.8024577432208595, l1: 0.00010724187663909683, l2: 0.0003730038949888983   Iteration 64 of 100, tot loss = 4.820190334692597, l1: 0.00010753230498039557, l2: 0.0003744867264003915   Iteration 65 of 100, tot loss = 4.824742949925936, l1: 0.00010739477645704308, l2: 0.00037507951682737956   Iteration 66 of 100, tot loss = 4.782417479789618, l1: 0.0001063988238456659, l2: 0.00037184292258726515   Iteration 67 of 100, tot loss = 4.791990011485655, l1: 0.00010654050455022076, l2: 0.0003726584946522976   Iteration 68 of 100, tot loss = 4.811444445567973, l1: 0.00010616258923084174, l2: 0.0003749818541323561   Iteration 69 of 100, tot loss = 4.783623151157213, l1: 0.00010576492755005702, l2: 0.00037259738662438736   Iteration 70 of 100, tot loss = 4.752745408671243, l1: 0.00010520308704664266, l2: 0.00037007145282197076   Iteration 71 of 100, tot loss = 4.7513578559311345, l1: 0.00010519533782531704, l2: 0.00036994044652166055   Iteration 72 of 100, tot loss = 4.727837461564276, l1: 0.00010501430956032386, l2: 0.0003677694353933071   Iteration 73 of 100, tot loss = 4.7459903269597925, l1: 0.00010533805510781909, l2: 0.00036926097617315587   Iteration 74 of 100, tot loss = 4.7482517329422205, l1: 0.00010516837642786118, l2: 0.00036965679513790794   Iteration 75 of 100, tot loss = 4.731453293164571, l1: 0.00010486427752766759, l2: 0.0003682810501777567   Iteration 76 of 100, tot loss = 4.739908270145717, l1: 0.00010510917088954317, l2: 0.0003688816546185224   Iteration 77 of 100, tot loss = 4.734461606322945, l1: 0.00010484656802448142, l2: 0.0003685995912014555   Iteration 78 of 100, tot loss = 4.738724667292375, l1: 0.00010492439254542264, l2: 0.00036894807281220355   Iteration 79 of 100, tot loss = 4.706858207907858, l1: 0.00010414226765337671, l2: 0.0003665435518236843   Iteration 80 of 100, tot loss = 4.726288641989231, l1: 0.00010450901554577285, l2: 0.00036811984709856916   Iteration 81 of 100, tot loss = 4.739609802210772, l1: 0.0001048902461066681, l2: 0.0003690707326089147   Iteration 82 of 100, tot loss = 4.7096867517727175, l1: 0.000104174493461004, l2: 0.00036679418012570795   Iteration 83 of 100, tot loss = 4.727328480008137, l1: 0.00010416277045915632, l2: 0.00036857007620140844   Iteration 84 of 100, tot loss = 4.733753529332933, l1: 0.00010445190418111105, l2: 0.0003689234471537583   Iteration 85 of 100, tot loss = 4.778739992310019, l1: 0.00010529370539349176, l2: 0.00037258029170676737   Iteration 86 of 100, tot loss = 4.743476679158765, l1: 0.00010455599134052877, l2: 0.00036979167457185363   Iteration 87 of 100, tot loss = 4.739386054291122, l1: 0.00010454196145192014, l2: 0.0003693966418618454   Iteration 88 of 100, tot loss = 4.746378188783472, l1: 0.00010457176652132397, l2: 0.00037006605014705036   Iteration 89 of 100, tot loss = 4.71940309545967, l1: 0.00010412974251869783, l2: 0.0003678105650384638   Iteration 90 of 100, tot loss = 4.765514225429959, l1: 0.0001047572965439435, l2: 0.00037179412441522194   Iteration 91 of 100, tot loss = 4.776105136661739, l1: 0.00010500257537517297, l2: 0.0003726079366470503   Iteration 92 of 100, tot loss = 4.774002204770627, l1: 0.00010506469844467179, l2: 0.00037233552061281995   Iteration 93 of 100, tot loss = 4.805532086280085, l1: 0.00010560576821037729, l2: 0.0003749474388470883   Iteration 94 of 100, tot loss = 4.835040173632033, l1: 0.00010584062232389461, l2: 0.00037766339347457533   Iteration 95 of 100, tot loss = 4.820214028107492, l1: 0.00010564549252433751, l2: 0.000376375908825522   Iteration 96 of 100, tot loss = 4.829371285935243, l1: 0.00010600459499225205, l2: 0.0003769325320869636   Iteration 97 of 100, tot loss = 4.863481356925571, l1: 0.00010660731712369412, l2: 0.00037974081733972123   Iteration 98 of 100, tot loss = 4.846874679837908, l1: 0.0001063596322340772, l2: 0.00037832783464739116   Iteration 99 of 100, tot loss = 4.855092405068754, l1: 0.00010639630698903482, l2: 0.00037911293269405515   Iteration 100 of 100, tot loss = 4.865694155693054, l1: 0.00010665884019545047, l2: 0.00037991057433828246
   End of epoch 1298; saving model... 

Epoch 1299 of 2000
   Iteration 1 of 100, tot loss = 7.604384422302246, l1: 0.00016630908066872507, l2: 0.0005941293784417212   Iteration 2 of 100, tot loss = 7.5407891273498535, l1: 0.0001598941526026465, l2: 0.0005941847630310804   Iteration 3 of 100, tot loss = 7.466708342234294, l1: 0.00014535317071325457, l2: 0.0006013176753185689   Iteration 4 of 100, tot loss = 7.166633248329163, l1: 0.00013682757889910135, l2: 0.0005798357597086579   Iteration 5 of 100, tot loss = 6.411231565475464, l1: 0.00012619300396181642, l2: 0.0005149301607161761   Iteration 6 of 100, tot loss = 6.175621867179871, l1: 0.00012487373654342568, l2: 0.0004926884624486169   Iteration 7 of 100, tot loss = 5.999782528196063, l1: 0.00012617059525967176, l2: 0.00047380767396784255   Iteration 8 of 100, tot loss = 5.965528637170792, l1: 0.00012428015179466456, l2: 0.0004722727280750405   Iteration 9 of 100, tot loss = 5.715872340732151, l1: 0.00012063044737765772, l2: 0.00045095680009884137   Iteration 10 of 100, tot loss = 5.42867271900177, l1: 0.00011456273350631818, l2: 0.00042830455204239114   Iteration 11 of 100, tot loss = 5.620358748869463, l1: 0.00011830735787622292, l2: 0.0004437285335205326   Iteration 12 of 100, tot loss = 5.704409221808116, l1: 0.00011997842496687856, l2: 0.00045046250913098146   Iteration 13 of 100, tot loss = 5.46348760678218, l1: 0.00011467707316534451, l2: 0.00043167169832696137   Iteration 14 of 100, tot loss = 5.344953741346087, l1: 0.00011351619001028926, l2: 0.000420979195041582   Iteration 15 of 100, tot loss = 5.45943390528361, l1: 0.00011526828408629323, l2: 0.00043067511481543385   Iteration 16 of 100, tot loss = 5.346585795283318, l1: 0.0001146032773249317, l2: 0.0004200553103146376   Iteration 17 of 100, tot loss = 5.173020236632404, l1: 0.00011121645366074517, l2: 0.00040608557773178774   Iteration 18 of 100, tot loss = 5.264984356032477, l1: 0.00011175232147151191, l2: 0.00041474612064323283   Iteration 19 of 100, tot loss = 5.056546393193696, l1: 0.00010767686265018328, l2: 0.0003979777826124949   Iteration 20 of 100, tot loss = 5.249068194627762, l1: 0.0001106457089917967, l2: 0.00041426111602049787   Iteration 21 of 100, tot loss = 5.334218053590684, l1: 0.00011235341350714277, l2: 0.00042106839724250937   Iteration 22 of 100, tot loss = 5.385627795349468, l1: 0.00011233632804677737, l2: 0.00042622645857839166   Iteration 23 of 100, tot loss = 5.30788645018702, l1: 0.00011197110712680075, l2: 0.00041881754504688814   Iteration 24 of 100, tot loss = 5.291302079955737, l1: 0.00011231614795785087, l2: 0.0004168140673452096   Iteration 25 of 100, tot loss = 5.245154929161072, l1: 0.00011074384499806911, l2: 0.00041377165616722776   Iteration 26 of 100, tot loss = 5.22476819386849, l1: 0.00011039929729752027, l2: 0.0004120775290805166   Iteration 27 of 100, tot loss = 5.254468948752792, l1: 0.00011161714010759843, l2: 0.00041382976174915073   Iteration 28 of 100, tot loss = 5.208621599844524, l1: 0.00011154312648743923, l2: 0.0004093190408249419   Iteration 29 of 100, tot loss = 5.280067982344792, l1: 0.00011245032736292944, l2: 0.00041555647827596563   Iteration 30 of 100, tot loss = 5.2428794503211975, l1: 0.00011192363987599189, l2: 0.0004123643120692577   Iteration 31 of 100, tot loss = 5.177272000620442, l1: 0.00011068877304956737, l2: 0.00040703843420018414   Iteration 32 of 100, tot loss = 5.1310597024858, l1: 0.00010996655782946618, l2: 0.0004031394194043969   Iteration 33 of 100, tot loss = 5.1068953420176655, l1: 0.00010983921057424706, l2: 0.0004008503308410361   Iteration 34 of 100, tot loss = 5.087784427053788, l1: 0.00011020251579740194, l2: 0.0003985759345489308   Iteration 35 of 100, tot loss = 5.032245966366359, l1: 0.00010962426853698812, l2: 0.0003936003355192952   Iteration 36 of 100, tot loss = 5.023538301388423, l1: 0.00010962810928402987, l2: 0.00039272572778120067   Iteration 37 of 100, tot loss = 5.090757308779536, l1: 0.00011073068811595943, l2: 0.0003983450503280462   Iteration 38 of 100, tot loss = 5.095665834451976, l1: 0.00011051273089067668, l2: 0.0003990538597894269   Iteration 39 of 100, tot loss = 5.094044553927886, l1: 0.00011023249606249663, l2: 0.0003991719667026654   Iteration 40 of 100, tot loss = 5.048764225840569, l1: 0.00010981172799802152, l2: 0.0003950647018427844   Iteration 41 of 100, tot loss = 5.017251889880111, l1: 0.00010872737198596729, l2: 0.0003929978246139022   Iteration 42 of 100, tot loss = 4.975572747843606, l1: 0.00010805179755109185, l2: 0.00038950548453916173   Iteration 43 of 100, tot loss = 4.983456836190334, l1: 0.00010843248750207686, l2: 0.00038991320311757845   Iteration 44 of 100, tot loss = 4.9636849517172035, l1: 0.00010823350021382794, l2: 0.00038813500164410056   Iteration 45 of 100, tot loss = 4.924842180146111, l1: 0.00010755599780370378, l2: 0.0003849282267361155   Iteration 46 of 100, tot loss = 4.887482184430827, l1: 0.00010634321623238857, l2: 0.0003824050080489225   Iteration 47 of 100, tot loss = 4.889775669321101, l1: 0.0001063293551696722, l2: 0.0003826482182163052   Iteration 48 of 100, tot loss = 4.902807749807835, l1: 0.00010678176370978083, l2: 0.00038349901797118946   Iteration 49 of 100, tot loss = 4.847906844956534, l1: 0.0001055646580240062, l2: 0.00037922603342496807   Iteration 50 of 100, tot loss = 4.819410345554352, l1: 0.00010550620005233214, l2: 0.0003764348411641549   Iteration 51 of 100, tot loss = 4.815448969018226, l1: 0.00010519922829653118, l2: 0.0003763456757844645   Iteration 52 of 100, tot loss = 4.836716388280575, l1: 0.00010528876821179158, l2: 0.00037838287789782044   Iteration 53 of 100, tot loss = 4.851980823390889, l1: 0.00010599107272870276, l2: 0.0003792070167277553   Iteration 54 of 100, tot loss = 4.856301009654999, l1: 0.00010607194673395026, l2: 0.0003795581615122501   Iteration 55 of 100, tot loss = 4.855541977015409, l1: 0.00010601802985713055, l2: 0.00037953617535425686   Iteration 56 of 100, tot loss = 4.823999747633934, l1: 0.00010548704449320212, l2: 0.000376912937878972   Iteration 57 of 100, tot loss = 4.895616265765407, l1: 0.0001068706414866539, l2: 0.0003826909928568312   Iteration 58 of 100, tot loss = 4.867319577726825, l1: 0.00010659725576888064, l2: 0.00038013470939717833   Iteration 59 of 100, tot loss = 4.890180201853736, l1: 0.00010678225472644445, l2: 0.0003822357725535551   Iteration 60 of 100, tot loss = 4.875672429800034, l1: 0.00010613567428663373, l2: 0.00038143157592761175   Iteration 61 of 100, tot loss = 4.931286899769892, l1: 0.00010635972595067512, l2: 0.0003867689707387071   Iteration 62 of 100, tot loss = 4.931314416470066, l1: 0.00010659101932223195, l2: 0.00038654042838325335   Iteration 63 of 100, tot loss = 4.9775960956301, l1: 0.00010723438858154363, l2: 0.0003905252269182783   Iteration 64 of 100, tot loss = 4.956865968182683, l1: 0.00010706886587286135, l2: 0.0003886177370304722   Iteration 65 of 100, tot loss = 4.947854036551256, l1: 0.00010648443869565828, l2: 0.0003883009710429738   Iteration 66 of 100, tot loss = 4.957206251043262, l1: 0.00010664852313559756, l2: 0.0003890721075577224   Iteration 67 of 100, tot loss = 4.950629414017521, l1: 0.0001064876036687212, l2: 0.00038857534304070077   Iteration 68 of 100, tot loss = 4.942289438317804, l1: 0.00010663304013124777, l2: 0.0003875959087856566   Iteration 69 of 100, tot loss = 4.9627179840336675, l1: 0.00010672935024848667, l2: 0.0003895424530998318   Iteration 70 of 100, tot loss = 4.991353390898023, l1: 0.00010724339205937992, l2: 0.00039189195179330585   Iteration 71 of 100, tot loss = 4.991911574148796, l1: 0.00010718918837111732, l2: 0.00039200197372824924   Iteration 72 of 100, tot loss = 4.975834159387483, l1: 0.00010690421459003119, l2: 0.0003906792061469686   Iteration 73 of 100, tot loss = 4.987332182387783, l1: 0.00010705702840063517, l2: 0.0003916761948135426   Iteration 74 of 100, tot loss = 5.008028702155964, l1: 0.00010716171734226943, l2: 0.00039364115739747805   Iteration 75 of 100, tot loss = 5.028284881909689, l1: 0.00010790236439788713, l2: 0.00039492612830751265   Iteration 76 of 100, tot loss = 5.0415591017196055, l1: 0.00010810536954714305, l2: 0.00039605054505380776   Iteration 77 of 100, tot loss = 5.009735093488322, l1: 0.00010774391671180023, l2: 0.00039322959690418264   Iteration 78 of 100, tot loss = 4.992223530243605, l1: 0.0001077975440495767, l2: 0.0003914248131294675   Iteration 79 of 100, tot loss = 4.998617907113667, l1: 0.00010781704447795107, l2: 0.0003920447508122218   Iteration 80 of 100, tot loss = 4.974757085740566, l1: 0.00010751460586106986, l2: 0.0003899611075212306   Iteration 81 of 100, tot loss = 4.972625648533857, l1: 0.00010757679195160529, l2: 0.00038968577751398296   Iteration 82 of 100, tot loss = 4.9992272199654, l1: 0.00010770676693404889, l2: 0.0003922159597029194   Iteration 83 of 100, tot loss = 5.000781189964478, l1: 0.00010789140256988558, l2: 0.00039218672130787073   Iteration 84 of 100, tot loss = 4.978696756419682, l1: 0.00010763220546193355, l2: 0.0003902374750655976   Iteration 85 of 100, tot loss = 4.97398340141072, l1: 0.00010746033290428493, l2: 0.00038993801221402145   Iteration 86 of 100, tot loss = 5.027836085751999, l1: 0.00010831678996192898, l2: 0.0003944668240491896   Iteration 87 of 100, tot loss = 5.0282552995901, l1: 0.00010846736103272194, l2: 0.00039435817414142685   Iteration 88 of 100, tot loss = 5.016640420664441, l1: 0.00010816102886482523, l2: 0.00039350301837905795   Iteration 89 of 100, tot loss = 4.9929461063963645, l1: 0.00010785924759955051, l2: 0.0003914353678891111   Iteration 90 of 100, tot loss = 4.996947628921933, l1: 0.00010795990286472564, l2: 0.00039173486511572263   Iteration 91 of 100, tot loss = 4.972980262158991, l1: 0.00010741805917759947, l2: 0.0003898799719876139   Iteration 92 of 100, tot loss = 4.953575653874355, l1: 0.00010700599222732505, l2: 0.0003883515780572729   Iteration 93 of 100, tot loss = 4.956433389776496, l1: 0.00010708222653584114, l2: 0.00038856111761991196   Iteration 94 of 100, tot loss = 4.960908388837855, l1: 0.00010716407160408438, l2: 0.0003889267727766097   Iteration 95 of 100, tot loss = 4.955443945683931, l1: 0.00010717491348163764, l2: 0.0003883694864558301   Iteration 96 of 100, tot loss = 4.955346173296372, l1: 0.00010732547510391062, l2: 0.0003882091477104647   Iteration 97 of 100, tot loss = 4.949896730098528, l1: 0.00010722936075552818, l2: 0.00038776031781417636   Iteration 98 of 100, tot loss = 4.967149288070445, l1: 0.00010749704463761395, l2: 0.0003892178895939094   Iteration 99 of 100, tot loss = 4.9575211170947915, l1: 0.00010734254266770387, l2: 0.0003884095745484552   Iteration 100 of 100, tot loss = 4.947675892114639, l1: 0.00010719936100940686, l2: 0.0003875682336365571
   End of epoch 1299; saving model... 

Epoch 1300 of 2000
   Iteration 1 of 100, tot loss = 3.120821475982666, l1: 5.234583295532502e-05, l2: 0.00025973632000386715   Iteration 2 of 100, tot loss = 3.4080055952072144, l1: 7.674836342630442e-05, l2: 0.00026405219978187233   Iteration 3 of 100, tot loss = 5.0379854043324785, l1: 0.00010607267298231211, l2: 0.0003977258747909218   Iteration 4 of 100, tot loss = 4.78757506608963, l1: 9.923437664838275e-05, l2: 0.0003795231314143166   Iteration 5 of 100, tot loss = 4.759315919876099, l1: 0.000104886604094645, l2: 0.00037104499060660603   Iteration 6 of 100, tot loss = 4.743763009707133, l1: 0.0001027458195797711, l2: 0.0003716304781846702   Iteration 7 of 100, tot loss = 5.012248277664185, l1: 0.00010708404079196043, l2: 0.0003941407859591501   Iteration 8 of 100, tot loss = 4.8120006918907166, l1: 0.00010373021041232278, l2: 0.00037746985617559403   Iteration 9 of 100, tot loss = 4.673126326666938, l1: 0.00010207705417997204, l2: 0.0003652355743623856   Iteration 10 of 100, tot loss = 4.629175281524658, l1: 0.00010043827533081639, l2: 0.0003624792501796037   Iteration 11 of 100, tot loss = 4.5625761638988145, l1: 0.00010039589522586374, l2: 0.00035586171857589346   Iteration 12 of 100, tot loss = 4.64301061630249, l1: 0.00010217168619419681, l2: 0.000362129372661002   Iteration 13 of 100, tot loss = 4.657655679262602, l1: 0.00010263099969821409, l2: 0.00036313456411545095   Iteration 14 of 100, tot loss = 4.619878019605364, l1: 0.00010242268529379674, l2: 0.0003595651110767254   Iteration 15 of 100, tot loss = 4.548891544342041, l1: 0.00010164623260304022, l2: 0.0003532429167535156   Iteration 16 of 100, tot loss = 4.563447415828705, l1: 0.00010213686141469225, l2: 0.00035420787571638357   Iteration 17 of 100, tot loss = 4.613817074719598, l1: 0.00010162078144597163, l2: 0.0003597609209143283   Iteration 18 of 100, tot loss = 4.5042910973231, l1: 9.920956997828196e-05, l2: 0.0003512195358376226   Iteration 19 of 100, tot loss = 4.437429967679475, l1: 9.838643037303547e-05, l2: 0.0003453565640436289   Iteration 20 of 100, tot loss = 4.407945668697357, l1: 9.941202024492668e-05, l2: 0.0003413825448660646   Iteration 21 of 100, tot loss = 4.330724659420195, l1: 9.819843693375809e-05, l2: 0.0003348740269540853   Iteration 22 of 100, tot loss = 4.454710407690569, l1: 0.00010033129778590096, l2: 0.00034513974176000124   Iteration 23 of 100, tot loss = 4.582825422286987, l1: 0.00010228608231915607, l2: 0.00035599645849762726   Iteration 24 of 100, tot loss = 4.525430103143056, l1: 0.00010115510106819177, l2: 0.00035138790796433267   Iteration 25 of 100, tot loss = 4.485589914321899, l1: 0.00010079496321850456, l2: 0.00034776402637362483   Iteration 26 of 100, tot loss = 4.434615731239319, l1: 9.975477125688206e-05, l2: 0.0003437068002173104   Iteration 27 of 100, tot loss = 4.526000861768369, l1: 0.00010112660203486923, l2: 0.0003514734810407929   Iteration 28 of 100, tot loss = 4.5236310703413825, l1: 0.00010070820657607069, l2: 0.0003516548968036659   Iteration 29 of 100, tot loss = 4.50772501682413, l1: 0.00010040068508683832, l2: 0.00035037181304029095   Iteration 30 of 100, tot loss = 4.5222950379053755, l1: 0.00010070534929885374, l2: 0.00035152415124078593   Iteration 31 of 100, tot loss = 4.483893640579716, l1: 0.00010052710410579288, l2: 0.00034786225625148585   Iteration 32 of 100, tot loss = 4.462083652615547, l1: 0.00010046402110219788, l2: 0.0003457443403931393   Iteration 33 of 100, tot loss = 4.385150880524606, l1: 9.888036595040114e-05, l2: 0.0003396347182746412   Iteration 34 of 100, tot loss = 4.352441843818216, l1: 9.872448240725241e-05, l2: 0.0003365196975851979   Iteration 35 of 100, tot loss = 4.362047481536865, l1: 9.867614062386565e-05, l2: 0.00033752860285208696   Iteration 36 of 100, tot loss = 4.355840179655287, l1: 9.84616634822386e-05, l2: 0.00033712234936602827   Iteration 37 of 100, tot loss = 4.362768727379876, l1: 9.881025323773281e-05, l2: 0.00033746661407545815   Iteration 38 of 100, tot loss = 4.341588465790999, l1: 9.847921922444489e-05, l2: 0.0003356796223670244   Iteration 39 of 100, tot loss = 4.297274613991762, l1: 9.79236196704173e-05, l2: 0.00033180383653439675   Iteration 40 of 100, tot loss = 4.313890385627746, l1: 9.855216894720798e-05, l2: 0.00033283686461800245   Iteration 41 of 100, tot loss = 4.312713902171065, l1: 9.900311364085278e-05, l2: 0.0003322682711835269   Iteration 42 of 100, tot loss = 4.308962015878587, l1: 9.855450384964657e-05, l2: 0.00033234169261829393   Iteration 43 of 100, tot loss = 4.422483654909356, l1: 0.00010052104191218373, l2: 0.0003417273181311916   Iteration 44 of 100, tot loss = 4.371393252502788, l1: 9.944160245306028e-05, l2: 0.0003376977173585064   Iteration 45 of 100, tot loss = 4.328193643358019, l1: 9.875475516309961e-05, l2: 0.00033406460360209974   Iteration 46 of 100, tot loss = 4.342241390891697, l1: 9.89417891579923e-05, l2: 0.0003352823449582185   Iteration 47 of 100, tot loss = 4.333455364754859, l1: 9.939225980060849e-05, l2: 0.00033395327231351365   Iteration 48 of 100, tot loss = 4.350006813804309, l1: 9.945270069996089e-05, l2: 0.0003355479766469216   Iteration 49 of 100, tot loss = 4.341236936802766, l1: 9.961450209351713e-05, l2: 0.0003345091874730222   Iteration 50 of 100, tot loss = 4.333905363082886, l1: 0.00010007510063587688, l2: 0.00033331543207168576   Iteration 51 of 100, tot loss = 4.31271975180682, l1: 9.933409013433894e-05, l2: 0.0003319378815563943   Iteration 52 of 100, tot loss = 4.278196839185862, l1: 9.8839624255421e-05, l2: 0.00032898005604063376   Iteration 53 of 100, tot loss = 4.305281000317268, l1: 9.941740523263777e-05, l2: 0.0003311106916473089   Iteration 54 of 100, tot loss = 4.318128629967019, l1: 9.955970731594048e-05, l2: 0.00033225315241401806   Iteration 55 of 100, tot loss = 4.330284248698842, l1: 9.995700423711572e-05, l2: 0.0003330714175138961   Iteration 56 of 100, tot loss = 4.358271930898939, l1: 0.00010064264266215364, l2: 0.00033518454750135006   Iteration 57 of 100, tot loss = 4.36877645526016, l1: 0.00010076555267196933, l2: 0.00033611208945921245   Iteration 58 of 100, tot loss = 4.369120030567564, l1: 0.00010066356543543877, l2: 0.00033624843415274314   Iteration 59 of 100, tot loss = 4.367082418021509, l1: 0.00010077878551644478, l2: 0.00033592945310066186   Iteration 60 of 100, tot loss = 4.323530077934265, l1: 0.00010001299863991638, l2: 0.00033234000620723235   Iteration 61 of 100, tot loss = 4.310841822233356, l1: 9.975228427630682e-05, l2: 0.000331331895006874   Iteration 62 of 100, tot loss = 4.329778052145435, l1: 0.00010003286168343507, l2: 0.0003329449404367141   Iteration 63 of 100, tot loss = 4.422379671581208, l1: 0.00010154763315637995, l2: 0.00034069033126653514   Iteration 64 of 100, tot loss = 4.453300032764673, l1: 0.0001024317101609995, l2: 0.00034289828988676163   Iteration 65 of 100, tot loss = 4.435760230284471, l1: 0.00010183451724095413, l2: 0.0003417415028357377   Iteration 66 of 100, tot loss = 4.435420227773262, l1: 0.00010182112038654105, l2: 0.0003417208992521575   Iteration 67 of 100, tot loss = 4.44006281112557, l1: 0.00010156528835285649, l2: 0.0003424409898336796   Iteration 68 of 100, tot loss = 4.454802418456358, l1: 0.00010157940906698487, l2: 0.00034390083034368754   Iteration 69 of 100, tot loss = 4.439511423525603, l1: 0.00010093925442745693, l2: 0.0003430118856646215   Iteration 70 of 100, tot loss = 4.455751255580357, l1: 0.0001012133745202196, l2: 0.00034436174890808095   Iteration 71 of 100, tot loss = 4.428456454209878, l1: 0.00010074589810986728, l2: 0.000342099745261883   Iteration 72 of 100, tot loss = 4.393805243902737, l1: 0.00010008649299682777, l2: 0.00033929402939571044   Iteration 73 of 100, tot loss = 4.404269373580201, l1: 0.00010052376600382658, l2: 0.00033990316934586975   Iteration 74 of 100, tot loss = 4.41075470479759, l1: 0.00010042338097362615, l2: 0.0003406520872148748   Iteration 75 of 100, tot loss = 4.398087414105733, l1: 0.00010007516815676354, l2: 0.0003397335712603914   Iteration 76 of 100, tot loss = 4.391339811839555, l1: 0.00010003275484019756, l2: 0.00033910122443845276   Iteration 77 of 100, tot loss = 4.390055243071023, l1: 0.00010021641256488775, l2: 0.00033878910953599305   Iteration 78 of 100, tot loss = 4.3901434143384295, l1: 0.00010032752820384652, l2: 0.00033868681137578393   Iteration 79 of 100, tot loss = 4.393853273572801, l1: 0.00010022659349672296, l2: 0.0003391587321132221   Iteration 80 of 100, tot loss = 4.395329381525516, l1: 0.00010028553610936796, l2: 0.0003392474000975199   Iteration 81 of 100, tot loss = 4.3901601706022095, l1: 0.00010003996695884803, l2: 0.0003389760482313261   Iteration 82 of 100, tot loss = 4.454522364023255, l1: 0.00010105250995122066, l2: 0.00034439972438210644   Iteration 83 of 100, tot loss = 4.478138634957463, l1: 0.00010170989805034429, l2: 0.00034610396354523747   Iteration 84 of 100, tot loss = 4.465714864787602, l1: 0.00010141806285987731, l2: 0.00034515342148162223   Iteration 85 of 100, tot loss = 4.464725389200098, l1: 0.00010133376494837541, l2: 0.00034513877189430573   Iteration 86 of 100, tot loss = 4.459519925505616, l1: 0.00010139375000679053, l2: 0.00034455824052836466   Iteration 87 of 100, tot loss = 4.450791398684184, l1: 0.00010117115438668239, l2: 0.0003439079836812459   Iteration 88 of 100, tot loss = 4.434502481059595, l1: 0.00010085529559298514, l2: 0.00034259495069735834   Iteration 89 of 100, tot loss = 4.441022137577614, l1: 0.00010109208212750754, l2: 0.0003430101296339927   Iteration 90 of 100, tot loss = 4.467689904901716, l1: 0.00010144320561569961, l2: 0.0003453257829177245   Iteration 91 of 100, tot loss = 4.4511541940353725, l1: 0.00010092774335554658, l2: 0.000344187674092789   Iteration 92 of 100, tot loss = 4.4578102295813355, l1: 0.00010120363221295005, l2: 0.00034457738879807397   Iteration 93 of 100, tot loss = 4.447944755195289, l1: 0.00010131737324490553, l2: 0.00034347710051631393   Iteration 94 of 100, tot loss = 4.429295226614526, l1: 0.00010075732159825844, l2: 0.0003421721992205501   Iteration 95 of 100, tot loss = 4.44968929416255, l1: 0.00010117677819308531, l2: 0.0003437921492915944   Iteration 96 of 100, tot loss = 4.447840731590986, l1: 0.0001010841843177938, l2: 0.0003436998873288151   Iteration 97 of 100, tot loss = 4.470985340088913, l1: 0.00010140109558970206, l2: 0.0003456974367715524   Iteration 98 of 100, tot loss = 4.455916269701355, l1: 0.00010106296138808236, l2: 0.00034452866377102266   Iteration 99 of 100, tot loss = 4.450515678434661, l1: 0.0001012148917138205, l2: 0.00034383667441557697   Iteration 100 of 100, tot loss = 4.455143393278122, l1: 0.0001013347620391869, l2: 0.0003441795754042687
   End of epoch 1300; saving model... 

Epoch 1301 of 2000
   Iteration 1 of 100, tot loss = 5.302216053009033, l1: 0.0001166657530120574, l2: 0.0004135558847337961   Iteration 2 of 100, tot loss = 4.148543953895569, l1: 9.208879782818258e-05, l2: 0.00032276561250910163   Iteration 3 of 100, tot loss = 4.268276929855347, l1: 8.661117802451675e-05, l2: 0.00034021653118543327   Iteration 4 of 100, tot loss = 4.422145307064056, l1: 8.977806828625035e-05, l2: 0.00035243646561866626   Iteration 5 of 100, tot loss = 4.3267412185668945, l1: 9.427454642718658e-05, l2: 0.0003383995790500194   Iteration 6 of 100, tot loss = 4.464361826578776, l1: 9.892901410542738e-05, l2: 0.0003475071668314437   Iteration 7 of 100, tot loss = 4.423986639295306, l1: 9.629383254962574e-05, l2: 0.00034610483063650984   Iteration 8 of 100, tot loss = 4.85488098859787, l1: 0.00010570500762696611, l2: 0.00037978309410391375   Iteration 9 of 100, tot loss = 4.740096251169841, l1: 0.00010292373998608027, l2: 0.0003710858873091638   Iteration 10 of 100, tot loss = 4.812443733215332, l1: 0.00010524714016355575, l2: 0.00037599723727907983   Iteration 11 of 100, tot loss = 4.891353260387074, l1: 0.00010830274814824489, l2: 0.00038083257756873286   Iteration 12 of 100, tot loss = 4.68287839492162, l1: 0.00010439052736425462, l2: 0.00036389731273326714   Iteration 13 of 100, tot loss = 4.747029836361225, l1: 0.00010538856832471747, l2: 0.0003693144166251071   Iteration 14 of 100, tot loss = 4.715366618973868, l1: 0.00010541360669386839, l2: 0.0003661230574445134   Iteration 15 of 100, tot loss = 4.9008190631866455, l1: 0.00010822539091653501, l2: 0.00038185651743939767   Iteration 16 of 100, tot loss = 4.8129706382751465, l1: 0.00010740717061707983, l2: 0.0003738898949450231   Iteration 17 of 100, tot loss = 4.736743001376881, l1: 0.00010715123404652866, l2: 0.00036652306666570334   Iteration 18 of 100, tot loss = 4.697711865107219, l1: 0.00010620472514549167, l2: 0.0003635664624097343   Iteration 19 of 100, tot loss = 4.908999719117817, l1: 0.00010947185058428563, l2: 0.0003814281226368621   Iteration 20 of 100, tot loss = 4.8361103415489195, l1: 0.00010828384183696471, l2: 0.00037532719361479396   Iteration 21 of 100, tot loss = 4.720370542435419, l1: 0.00010572761658252039, l2: 0.00036630943872123246   Iteration 22 of 100, tot loss = 4.6132224364714185, l1: 0.00010290664282696194, l2: 0.0003584156021877954   Iteration 23 of 100, tot loss = 4.635758845702462, l1: 0.00010194918302105457, l2: 0.00036162670332250065   Iteration 24 of 100, tot loss = 4.674112627903621, l1: 0.00010254682016845133, l2: 0.0003648644451459404   Iteration 25 of 100, tot loss = 4.647371301651001, l1: 0.00010307936885510571, l2: 0.00036165776313282547   Iteration 26 of 100, tot loss = 4.568637224344107, l1: 0.00010206433174831685, l2: 0.0003547993921809113   Iteration 27 of 100, tot loss = 4.614238456443504, l1: 0.00010346149752472734, l2: 0.000357962349813466   Iteration 28 of 100, tot loss = 4.554217619555337, l1: 0.00010167297218036506, l2: 0.00035374879163490344   Iteration 29 of 100, tot loss = 4.645085638967053, l1: 0.000102652297284984, l2: 0.00036185626783568795   Iteration 30 of 100, tot loss = 4.706738877296448, l1: 0.00010325189626504046, l2: 0.00036742199251117805   Iteration 31 of 100, tot loss = 4.702138293174006, l1: 0.00010404444272412107, l2: 0.00036616938849610666   Iteration 32 of 100, tot loss = 4.74418356269598, l1: 0.0001041151481331326, l2: 0.0003703032089106273   Iteration 33 of 100, tot loss = 4.751900882431955, l1: 0.00010390885202496341, l2: 0.0003712812383927292   Iteration 34 of 100, tot loss = 4.735102898934308, l1: 0.0001043599793988773, l2: 0.0003691503130967784   Iteration 35 of 100, tot loss = 4.718488264083862, l1: 0.00010421573221849809, l2: 0.0003676330970068063   Iteration 36 of 100, tot loss = 4.721372995111677, l1: 0.0001041016728575212, l2: 0.00036803563044587564   Iteration 37 of 100, tot loss = 4.77400210741404, l1: 0.00010454367824226014, l2: 0.00037285653682350104   Iteration 38 of 100, tot loss = 4.68067049666455, l1: 0.00010266528921707312, l2: 0.00036540176485064685   Iteration 39 of 100, tot loss = 4.725825881346678, l1: 0.00010271780896194589, l2: 0.00036986478423783317   Iteration 40 of 100, tot loss = 4.744227609038353, l1: 0.00010299852719981572, l2: 0.000371424238619511   Iteration 41 of 100, tot loss = 4.736579601357623, l1: 0.00010277532108863474, l2: 0.0003708826442584168   Iteration 42 of 100, tot loss = 4.7999246546200345, l1: 0.00010372496719702169, l2: 0.00037626750383337607   Iteration 43 of 100, tot loss = 4.778489110081694, l1: 0.00010385454828961926, l2: 0.00037399436811494185   Iteration 44 of 100, tot loss = 4.833993594754826, l1: 0.00010478213690393287, l2: 0.000378617228059077   Iteration 45 of 100, tot loss = 4.866881484455533, l1: 0.00010580527604260068, l2: 0.0003808828778952981   Iteration 46 of 100, tot loss = 4.860931642677473, l1: 0.00010569326482452553, l2: 0.0003803999048693146   Iteration 47 of 100, tot loss = 4.859739311197971, l1: 0.00010552549599409797, l2: 0.0003804484400229449   Iteration 48 of 100, tot loss = 4.856574820975463, l1: 0.00010537797887385143, l2: 0.00038027950813557254   Iteration 49 of 100, tot loss = 4.886311139379229, l1: 0.00010597594214510648, l2: 0.00038265517636021715   Iteration 50 of 100, tot loss = 4.853744866847992, l1: 0.00010602723843476269, l2: 0.00037934725289233027   Iteration 51 of 100, tot loss = 4.857421218180189, l1: 0.00010617436447657882, l2: 0.0003795677609969953   Iteration 52 of 100, tot loss = 4.830885481375915, l1: 0.00010585030486254254, l2: 0.00037723824677344126   Iteration 53 of 100, tot loss = 4.787899613380432, l1: 0.00010513824339075243, l2: 0.00037365172108863744   Iteration 54 of 100, tot loss = 4.793453907525098, l1: 0.00010536098522640748, l2: 0.0003739844089270466   Iteration 55 of 100, tot loss = 4.801218355785717, l1: 0.00010545878209417093, l2: 0.0003746630571020598   Iteration 56 of 100, tot loss = 4.806235009006092, l1: 0.00010587729015501932, l2: 0.00037474621473977877   Iteration 57 of 100, tot loss = 4.78062763757873, l1: 0.00010548851749167385, l2: 0.00037257425005041194   Iteration 58 of 100, tot loss = 4.805663415070238, l1: 0.00010592657311243438, l2: 0.0003746397721035213   Iteration 59 of 100, tot loss = 4.777885762311644, l1: 0.00010527832026867653, l2: 0.00037251025984868786   Iteration 60 of 100, tot loss = 4.770370723803838, l1: 0.00010503341066699552, l2: 0.0003720036654461486   Iteration 61 of 100, tot loss = 4.7810281945056605, l1: 0.00010509679594208852, l2: 0.00037300602779113   Iteration 62 of 100, tot loss = 4.8038781169922125, l1: 0.0001056131577863726, l2: 0.00037477465840281857   Iteration 63 of 100, tot loss = 4.801698141627842, l1: 0.00010562316201108005, l2: 0.00037454665658832897   Iteration 64 of 100, tot loss = 4.787025244906545, l1: 0.0001055515854773148, l2: 0.00037315094323275844   Iteration 65 of 100, tot loss = 4.79897559422713, l1: 0.00010550105143920518, l2: 0.00037439651205204425   Iteration 66 of 100, tot loss = 4.820912391850443, l1: 0.00010551460433238059, l2: 0.00037657663845245474   Iteration 67 of 100, tot loss = 4.814075058965541, l1: 0.00010552472271391802, l2: 0.00037588278629205454   Iteration 68 of 100, tot loss = 4.802367092931972, l1: 0.00010552555501336054, l2: 0.0003747111575862886   Iteration 69 of 100, tot loss = 4.7958725518074585, l1: 0.00010576840624866156, l2: 0.000373818852138989   Iteration 70 of 100, tot loss = 4.812685539041246, l1: 0.00010598328720204466, l2: 0.00037528527027461676   Iteration 71 of 100, tot loss = 4.863344026283479, l1: 0.00010697920797482907, l2: 0.0003793551975985805   Iteration 72 of 100, tot loss = 4.84913665552934, l1: 0.00010664842582425788, l2: 0.00037826524284254346   Iteration 73 of 100, tot loss = 4.8373972471446205, l1: 0.00010671417286885864, l2: 0.0003770255549070872   Iteration 74 of 100, tot loss = 4.84015114726247, l1: 0.00010666880007192958, l2: 0.00037734631710132033   Iteration 75 of 100, tot loss = 4.8653644863764445, l1: 0.00010711200617758247, l2: 0.0003794244443997741   Iteration 76 of 100, tot loss = 4.8750281004529255, l1: 0.00010745745715355856, l2: 0.0003800453552302267   Iteration 77 of 100, tot loss = 4.859036462647574, l1: 0.00010736856087792798, l2: 0.0003785350879309578   Iteration 78 of 100, tot loss = 4.827343651881585, l1: 0.00010710834020499486, l2: 0.0003756260273990054   Iteration 79 of 100, tot loss = 4.83279252203205, l1: 0.00010722772327150053, l2: 0.0003760515312852833   Iteration 80 of 100, tot loss = 4.837876842916012, l1: 0.00010739976000877505, l2: 0.0003763879263715353   Iteration 81 of 100, tot loss = 4.8811622416531595, l1: 0.00010815466588677804, l2: 0.0003799615596007142   Iteration 82 of 100, tot loss = 4.864521574683305, l1: 0.00010786678871138464, l2: 0.0003785853701495997   Iteration 83 of 100, tot loss = 4.844482354370944, l1: 0.00010753766019409243, l2: 0.00037691057663192097   Iteration 84 of 100, tot loss = 4.821115126212438, l1: 0.00010694367847463582, l2: 0.0003751678355218333   Iteration 85 of 100, tot loss = 4.837688509155722, l1: 0.00010707296485848287, l2: 0.00037669588705552194   Iteration 86 of 100, tot loss = 4.832078896289648, l1: 0.00010696378198782589, l2: 0.00037624410893037093   Iteration 87 of 100, tot loss = 4.822816926857521, l1: 0.00010682602575913221, l2: 0.00037545566805022843   Iteration 88 of 100, tot loss = 4.832489399747415, l1: 0.00010714949538933897, l2: 0.00037609944559914186   Iteration 89 of 100, tot loss = 4.818765196907386, l1: 0.00010661266137861511, l2: 0.0003752638594040731   Iteration 90 of 100, tot loss = 4.835272726747725, l1: 0.00010694282100303099, l2: 0.00037658445281623347   Iteration 91 of 100, tot loss = 4.850142082015236, l1: 0.00010710370934448092, l2: 0.00037791050032195684   Iteration 92 of 100, tot loss = 4.849267468504284, l1: 0.00010718347031165057, l2: 0.0003777432779632205   Iteration 93 of 100, tot loss = 4.857294027523328, l1: 0.00010749612306182083, l2: 0.00037823328094291553   Iteration 94 of 100, tot loss = 4.891588065218418, l1: 0.00010802922901967442, l2: 0.0003811295787949967   Iteration 95 of 100, tot loss = 4.883370837412382, l1: 0.00010790522394113635, l2: 0.0003804318608385266   Iteration 96 of 100, tot loss = 4.880939077585936, l1: 0.0001077592971038636, l2: 0.0003803346118426513   Iteration 97 of 100, tot loss = 4.882619195377703, l1: 0.00010775018955362462, l2: 0.0003805117309273499   Iteration 98 of 100, tot loss = 4.873626257692065, l1: 0.00010775479848011948, l2: 0.0003796078281641025   Iteration 99 of 100, tot loss = 4.879579978759843, l1: 0.00010790972869885577, l2: 0.0003800482700538887   Iteration 100 of 100, tot loss = 4.89661318898201, l1: 0.000108025989975431, l2: 0.00038163533012266273
   End of epoch 1301; saving model... 

Epoch 1302 of 2000
   Iteration 1 of 100, tot loss = 3.4843597412109375, l1: 8.685755165060982e-05, l2: 0.00026157841784879565   Iteration 2 of 100, tot loss = 4.3481831550598145, l1: 0.0001030078528856393, l2: 0.00033181045728269964   Iteration 3 of 100, tot loss = 3.6691585381825766, l1: 8.974790883560975e-05, l2: 0.0002771679379899676   Iteration 4 of 100, tot loss = 3.4848073720932007, l1: 8.021033318073023e-05, l2: 0.0002682704034668859   Iteration 5 of 100, tot loss = 3.4914074897766114, l1: 7.45288489270024e-05, l2: 0.0002746118960203603   Iteration 6 of 100, tot loss = 3.7198102474212646, l1: 8.287538488123876e-05, l2: 0.00028910563317670796   Iteration 7 of 100, tot loss = 3.592019489833287, l1: 8.191635216852384e-05, l2: 0.00027728559091753723   Iteration 8 of 100, tot loss = 3.7137644290924072, l1: 8.258937032223912e-05, l2: 0.000288787070530816   Iteration 9 of 100, tot loss = 3.6828120019700794, l1: 8.135815248048554e-05, l2: 0.00028692304719394696   Iteration 10 of 100, tot loss = 3.82457275390625, l1: 8.283887655125e-05, l2: 0.0002996183975483291   Iteration 11 of 100, tot loss = 3.7163479978388008, l1: 8.274478419810872e-05, l2: 0.00028889001458248293   Iteration 12 of 100, tot loss = 3.7071507374445596, l1: 8.073114167927997e-05, l2: 0.00028998393221021007   Iteration 13 of 100, tot loss = 3.8183106642503004, l1: 8.298008273973559e-05, l2: 0.0002988509827096445   Iteration 14 of 100, tot loss = 3.762231639453343, l1: 8.260896226732126e-05, l2: 0.00029361420144726125   Iteration 15 of 100, tot loss = 3.7595870971679686, l1: 8.141871973445328e-05, l2: 0.00029453999013639987   Iteration 16 of 100, tot loss = 3.775956481695175, l1: 8.283648435281066e-05, l2: 0.0002947591656266013   Iteration 17 of 100, tot loss = 3.799189343171961, l1: 8.482110677184264e-05, l2: 0.0002950978301027242   Iteration 18 of 100, tot loss = 3.7864727311664157, l1: 8.651929824231451e-05, l2: 0.0002921279762328292   Iteration 19 of 100, tot loss = 3.7566376234355725, l1: 8.520411957808966e-05, l2: 0.0002904596438288297   Iteration 20 of 100, tot loss = 3.802681541442871, l1: 8.602851903560804e-05, l2: 0.00029423963569570334   Iteration 21 of 100, tot loss = 3.809209710075742, l1: 8.648732095718428e-05, l2: 0.0002944336502834977   Iteration 22 of 100, tot loss = 3.7644492496143687, l1: 8.628255537504711e-05, l2: 0.0002901623698893342   Iteration 23 of 100, tot loss = 4.062188728995945, l1: 9.066150564847149e-05, l2: 0.00031555736369109184   Iteration 24 of 100, tot loss = 4.0271958609422045, l1: 8.917818513509701e-05, l2: 0.0003135413977967498   Iteration 25 of 100, tot loss = 4.0474997234344485, l1: 8.900708591681905e-05, l2: 0.00031574288324918596   Iteration 26 of 100, tot loss = 4.093403256856478, l1: 9.050780066060655e-05, l2: 0.0003188325214208677   Iteration 27 of 100, tot loss = 4.0776234556127475, l1: 8.945624520290746e-05, l2: 0.0003183060967044353   Iteration 28 of 100, tot loss = 4.131190078599112, l1: 8.995255445500203e-05, l2: 0.0003231664507308908   Iteration 29 of 100, tot loss = 4.217106621840904, l1: 8.992817900685498e-05, l2: 0.0003317824804729878   Iteration 30 of 100, tot loss = 4.265253114700317, l1: 9.070103854658858e-05, l2: 0.00033582426985958593   Iteration 31 of 100, tot loss = 4.371446424914945, l1: 9.220481520206968e-05, l2: 0.0003449398242200034   Iteration 32 of 100, tot loss = 4.389857724308968, l1: 9.262742639748467e-05, l2: 0.000346358343904285   Iteration 33 of 100, tot loss = 4.432010072650331, l1: 9.332655430295166e-05, l2: 0.0003498744499114709   Iteration 34 of 100, tot loss = 4.4150977906058815, l1: 9.337752817373257e-05, l2: 0.0003481322479167241   Iteration 35 of 100, tot loss = 4.469567033222743, l1: 9.402001274533437e-05, l2: 0.0003529366874967569   Iteration 36 of 100, tot loss = 4.618237528536055, l1: 9.64881101026549e-05, l2: 0.00036533563858635415   Iteration 37 of 100, tot loss = 4.5724898157893, l1: 9.548440809610199e-05, l2: 0.00036176456912496204   Iteration 38 of 100, tot loss = 4.578326789956344, l1: 9.620342337400775e-05, l2: 0.0003616292509066529   Iteration 39 of 100, tot loss = 4.660642367142898, l1: 9.733951045722605e-05, l2: 0.0003687247211555353   Iteration 40 of 100, tot loss = 4.713162362575531, l1: 9.787430908545503e-05, l2: 0.0003734419216925744   Iteration 41 of 100, tot loss = 4.663190946346376, l1: 9.72868263311173e-05, l2: 0.00036903226282447577   Iteration 42 of 100, tot loss = 4.669019948868525, l1: 9.742097427078988e-05, l2: 0.00036948101478628814   Iteration 43 of 100, tot loss = 4.6394202542859455, l1: 9.716011481941678e-05, l2: 0.0003667819045678994   Iteration 44 of 100, tot loss = 4.610796738754619, l1: 9.694171165557981e-05, l2: 0.00036413795632225544   Iteration 45 of 100, tot loss = 4.608729357189603, l1: 9.712146848970507e-05, l2: 0.00036375146139309636   Iteration 46 of 100, tot loss = 4.619093744651131, l1: 9.735808105134588e-05, l2: 0.00036455128777205295   Iteration 47 of 100, tot loss = 4.605709704946964, l1: 9.731380208556421e-05, l2: 0.00036325716250080694   Iteration 48 of 100, tot loss = 4.603965689738591, l1: 9.788233099546535e-05, l2: 0.0003625142326200148   Iteration 49 of 100, tot loss = 4.59552677310243, l1: 9.774914115598445e-05, l2: 0.0003618035306298763   Iteration 50 of 100, tot loss = 4.6278000164031985, l1: 9.878215198114049e-05, l2: 0.00036399784381501376   Iteration 51 of 100, tot loss = 4.647133799160228, l1: 9.928523001030507e-05, l2: 0.0003654281431854721   Iteration 52 of 100, tot loss = 4.63108863738867, l1: 9.8743111965842e-05, l2: 0.0003643657451343293   Iteration 53 of 100, tot loss = 4.645822250618125, l1: 9.925047437855885e-05, l2: 0.00036533174427876356   Iteration 54 of 100, tot loss = 4.63435063097212, l1: 9.911480723261505e-05, l2: 0.0003643202491932445   Iteration 55 of 100, tot loss = 4.680368211052635, l1: 9.996952305135148e-05, l2: 0.0003680672919885679   Iteration 56 of 100, tot loss = 4.686130911111832, l1: 0.00010029030655849576, l2: 0.0003683227784806929   Iteration 57 of 100, tot loss = 4.675183819051375, l1: 0.00010033290782941326, l2: 0.00036718546854037985   Iteration 58 of 100, tot loss = 4.65333643041808, l1: 0.00010036828380493559, l2: 0.0003649653537712734   Iteration 59 of 100, tot loss = 4.681108842461796, l1: 0.00010044644610172974, l2: 0.000367664433108106   Iteration 60 of 100, tot loss = 4.671358088652293, l1: 9.989902810048079e-05, l2: 0.00036723677621921527   Iteration 61 of 100, tot loss = 4.639917764507357, l1: 9.935911770835596e-05, l2: 0.00036463265395036244   Iteration 62 of 100, tot loss = 4.622298859780835, l1: 9.914288139336878e-05, l2: 0.0003630869999347675   Iteration 63 of 100, tot loss = 4.604771318889799, l1: 9.903366639812861e-05, l2: 0.0003614434607452639   Iteration 64 of 100, tot loss = 4.629716821014881, l1: 9.948273628879178e-05, l2: 0.0003634889412751363   Iteration 65 of 100, tot loss = 4.65105499120859, l1: 0.00010016011146944948, l2: 0.0003649453829544095   Iteration 66 of 100, tot loss = 4.648817344145342, l1: 0.00010022914179761287, l2: 0.00036465258789107656   Iteration 67 of 100, tot loss = 4.660883056583689, l1: 0.00010048131939108873, l2: 0.0003656069821949159   Iteration 68 of 100, tot loss = 4.624880099997801, l1: 9.995782629095415e-05, l2: 0.0003625301797001157   Iteration 69 of 100, tot loss = 4.651333411534627, l1: 0.00010014845310766047, l2: 0.000364984883597809   Iteration 70 of 100, tot loss = 4.650846593720573, l1: 0.00010023197709025615, l2: 0.000364852677739691   Iteration 71 of 100, tot loss = 4.652803572130875, l1: 0.00010056717573988094, l2: 0.00036471317670616306   Iteration 72 of 100, tot loss = 4.646838542487886, l1: 0.00010048436469231901, l2: 0.0003641994846677537   Iteration 73 of 100, tot loss = 4.626999841977472, l1: 0.00010026439037437118, l2: 0.0003624355892111764   Iteration 74 of 100, tot loss = 4.604276186711079, l1: 9.989613735760449e-05, l2: 0.0003605314767982408   Iteration 75 of 100, tot loss = 4.591370042165121, l1: 9.945697427610867e-05, l2: 0.00035968002513982357   Iteration 76 of 100, tot loss = 4.5687939587392306, l1: 9.91410683318942e-05, l2: 0.0003577383228586864   Iteration 77 of 100, tot loss = 4.589847685454727, l1: 9.960006496698185e-05, l2: 0.0003593846988885409   Iteration 78 of 100, tot loss = 4.568701634040246, l1: 9.902615723452375e-05, l2: 0.00035784400171952514   Iteration 79 of 100, tot loss = 4.571189427677589, l1: 9.931608798944603e-05, l2: 0.000357802850275324   Iteration 80 of 100, tot loss = 4.618515211343765, l1: 9.987073140109715e-05, l2: 0.00036198078578308923   Iteration 81 of 100, tot loss = 4.61999883769471, l1: 9.973497731994872e-05, l2: 0.0003622649027319211   Iteration 82 of 100, tot loss = 4.606183150919472, l1: 9.97589182455757e-05, l2: 0.0003608593931237077   Iteration 83 of 100, tot loss = 4.604449438761516, l1: 9.972681020525374e-05, l2: 0.00036071812967539775   Iteration 84 of 100, tot loss = 4.61601045585814, l1: 0.0001001313476947308, l2: 0.00036146969432593323   Iteration 85 of 100, tot loss = 4.626202846975888, l1: 0.00010001294058918788, l2: 0.00036260734075981683   Iteration 86 of 100, tot loss = 4.623417394105778, l1: 0.00010000590982388023, l2: 0.0003623358262662213   Iteration 87 of 100, tot loss = 4.605539628829079, l1: 9.948383681723666e-05, l2: 0.0003610701230287702   Iteration 88 of 100, tot loss = 4.626075457442891, l1: 9.999415806305478e-05, l2: 0.0003626133850709514   Iteration 89 of 100, tot loss = 4.614214607838834, l1: 9.974825714768454e-05, l2: 0.00036167320118447914   Iteration 90 of 100, tot loss = 4.638917711046007, l1: 9.996599259870386e-05, l2: 0.00036392577603692187   Iteration 91 of 100, tot loss = 4.634760835668543, l1: 9.984826261483409e-05, l2: 0.00036362781855252133   Iteration 92 of 100, tot loss = 4.625669163206349, l1: 9.968403320153416e-05, l2: 0.0003628828807964253   Iteration 93 of 100, tot loss = 4.638340575720674, l1: 9.969379939858661e-05, l2: 0.0003641402561365256   Iteration 94 of 100, tot loss = 4.6357035941266, l1: 9.952568890436484e-05, l2: 0.00036404466865202806   Iteration 95 of 100, tot loss = 4.624667918054681, l1: 9.944301774866242e-05, l2: 0.00036302377197180725   Iteration 96 of 100, tot loss = 4.638889280458291, l1: 9.988308052773694e-05, l2: 0.00036400584531293134   Iteration 97 of 100, tot loss = 4.656300783157349, l1: 0.00010002527598828106, l2: 0.00036560480027416357   Iteration 98 of 100, tot loss = 4.651169307377874, l1: 9.999765539500263e-05, l2: 0.00036511927316255146   Iteration 99 of 100, tot loss = 4.652550121750495, l1: 0.00010000071437609608, l2: 0.0003652542956218105   Iteration 100 of 100, tot loss = 4.66178610086441, l1: 0.0001002319778126548, l2: 0.0003659466300450731
   End of epoch 1302; saving model... 

Epoch 1303 of 2000
   Iteration 1 of 100, tot loss = 5.744855880737305, l1: 0.00014186356565915048, l2: 0.00043262200779281557   Iteration 2 of 100, tot loss = 4.526387810707092, l1: 0.00011711307888617739, l2: 0.000335525706759654   Iteration 3 of 100, tot loss = 4.688892126083374, l1: 0.00011916445995060106, l2: 0.00034972475259564817   Iteration 4 of 100, tot loss = 5.294786870479584, l1: 0.00012814751244150102, l2: 0.0004013311772723682   Iteration 5 of 100, tot loss = 5.228768396377563, l1: 0.00012774920905940236, l2: 0.000395127636147663   Iteration 6 of 100, tot loss = 5.3654987414677935, l1: 0.00012580183829413727, l2: 0.0004107480393334602   Iteration 7 of 100, tot loss = 5.269849130085537, l1: 0.00012409998453222215, l2: 0.00040288492787762413   Iteration 8 of 100, tot loss = 5.225800544023514, l1: 0.00012111087562516332, l2: 0.00040146918036043644   Iteration 9 of 100, tot loss = 5.1516908804575605, l1: 0.00011992477811872959, l2: 0.0003952443108169569   Iteration 10 of 100, tot loss = 5.0217962741851805, l1: 0.00011725222284439952, l2: 0.0003849274042295292   Iteration 11 of 100, tot loss = 4.8872035199945625, l1: 0.00011457277154973285, l2: 0.00037414758116938174   Iteration 12 of 100, tot loss = 4.916130542755127, l1: 0.00011454993424801312, l2: 0.00037706312044368434   Iteration 13 of 100, tot loss = 4.892470579880935, l1: 0.00011386798723833635, l2: 0.0003753790716963032   Iteration 14 of 100, tot loss = 4.927718332835606, l1: 0.0001155981065364488, l2: 0.0003771737267795418   Iteration 15 of 100, tot loss = 4.8738606135050455, l1: 0.00011526999442139641, l2: 0.000372116065894564   Iteration 16 of 100, tot loss = 4.788231983780861, l1: 0.00011447644601503271, l2: 0.00036434675166674424   Iteration 17 of 100, tot loss = 4.949919041465311, l1: 0.00011477357004289789, l2: 0.00038021833204445154   Iteration 18 of 100, tot loss = 5.0439702537324695, l1: 0.00011466089644171816, l2: 0.0003897361279491128   Iteration 19 of 100, tot loss = 4.979825233158312, l1: 0.000112357562772415, l2: 0.0003856249585576159   Iteration 20 of 100, tot loss = 4.910463166236878, l1: 0.00011032809779862874, l2: 0.0003807182161835954   Iteration 21 of 100, tot loss = 4.948413417452858, l1: 0.00011005493676445137, l2: 0.00038478640185314275   Iteration 22 of 100, tot loss = 4.8842516270550815, l1: 0.00010883336521642791, l2: 0.00037959179396487093   Iteration 23 of 100, tot loss = 4.777722255043361, l1: 0.00010699552892575689, l2: 0.00037077669390861917   Iteration 24 of 100, tot loss = 4.75689971446991, l1: 0.00010705862435618958, l2: 0.00036863134361434885   Iteration 25 of 100, tot loss = 4.730336399078369, l1: 0.00010677666403353215, l2: 0.00036625697219278664   Iteration 26 of 100, tot loss = 4.671067366233239, l1: 0.00010488921865404476, l2: 0.00036221751417122927   Iteration 27 of 100, tot loss = 4.646242106402362, l1: 0.00010488635908350934, l2: 0.0003597378483930327   Iteration 28 of 100, tot loss = 4.73245768887656, l1: 0.00010654804711001426, l2: 0.0003666977193331279   Iteration 29 of 100, tot loss = 4.719523150345375, l1: 0.00010594087143545039, l2: 0.00036601144228339325   Iteration 30 of 100, tot loss = 4.647560477256775, l1: 0.00010467800905947419, l2: 0.00036007803719257937   Iteration 31 of 100, tot loss = 4.622893279598605, l1: 0.00010458237199101507, l2: 0.00035770695455627697   Iteration 32 of 100, tot loss = 4.632948212325573, l1: 0.00010553893400810921, l2: 0.00035775588594333385   Iteration 33 of 100, tot loss = 4.67246836604494, l1: 0.00010680819380495726, l2: 0.00036043864207441044   Iteration 34 of 100, tot loss = 4.6498459577560425, l1: 0.00010689604342213202, l2: 0.0003580885519006509   Iteration 35 of 100, tot loss = 4.683345692498343, l1: 0.00010734702092512245, l2: 0.000360987547069921   Iteration 36 of 100, tot loss = 4.737874315844642, l1: 0.00010732377520778553, l2: 0.000366463654548473   Iteration 37 of 100, tot loss = 4.710989597681406, l1: 0.0001065185028277778, l2: 0.00036458045478620743   Iteration 38 of 100, tot loss = 4.716325326969749, l1: 0.00010634920478256198, l2: 0.0003652833268126032   Iteration 39 of 100, tot loss = 4.717286177170583, l1: 0.00010678070089526068, l2: 0.00036494791670403897   Iteration 40 of 100, tot loss = 4.778676158189773, l1: 0.00010689128130252357, l2: 0.0003709763335791649   Iteration 41 of 100, tot loss = 4.735170986594223, l1: 0.0001061830442312185, l2: 0.0003673340528976263   Iteration 42 of 100, tot loss = 4.682780668849037, l1: 0.00010517940998917246, l2: 0.00036309865598533567   Iteration 43 of 100, tot loss = 4.673437756161357, l1: 0.00010524334094614375, l2: 0.00036210043397786224   Iteration 44 of 100, tot loss = 4.6523450721393935, l1: 0.00010506168184127256, l2: 0.0003601728249949784   Iteration 45 of 100, tot loss = 4.702590804629856, l1: 0.00010594997576036904, l2: 0.00036430910404305905   Iteration 46 of 100, tot loss = 4.699297158614449, l1: 0.00010570724067371845, l2: 0.0003642224742772827   Iteration 47 of 100, tot loss = 4.670576927509714, l1: 0.00010521201445644305, l2: 0.00036184567781016625   Iteration 48 of 100, tot loss = 4.643037736415863, l1: 0.00010475390558895015, l2: 0.0003595498677289773   Iteration 49 of 100, tot loss = 4.613738230296543, l1: 0.00010422001153584665, l2: 0.00035715381150390495   Iteration 50 of 100, tot loss = 4.642373509407044, l1: 0.00010494566995475907, l2: 0.0003592916807974689   Iteration 51 of 100, tot loss = 4.639399505129047, l1: 0.00010526957945936981, l2: 0.00035867037156861567   Iteration 52 of 100, tot loss = 4.596841408656194, l1: 0.00010450011646193721, l2: 0.00035518402504609327   Iteration 53 of 100, tot loss = 4.545647679634814, l1: 0.0001036801854151802, l2: 0.00035088458307140137   Iteration 54 of 100, tot loss = 4.510039316283332, l1: 0.00010298937242488912, l2: 0.00034801455977786746   Iteration 55 of 100, tot loss = 4.515878274224021, l1: 0.0001034380979862445, l2: 0.00034814973001961005   Iteration 56 of 100, tot loss = 4.492553438459124, l1: 0.00010287113173035323, l2: 0.000346384212337268   Iteration 57 of 100, tot loss = 4.4907024283158155, l1: 0.00010334460807427062, l2: 0.0003457256350788827   Iteration 58 of 100, tot loss = 4.4890622434944945, l1: 0.0001032830881238875, l2: 0.0003456231362210458   Iteration 59 of 100, tot loss = 4.491547657271563, l1: 0.00010335249427984216, l2: 0.0003458022715137103   Iteration 60 of 100, tot loss = 4.501788981755575, l1: 0.00010372224357221663, l2: 0.00034645665533995876   Iteration 61 of 100, tot loss = 4.492569794420336, l1: 0.0001033308966964155, l2: 0.0003459260834297013   Iteration 62 of 100, tot loss = 4.511724245163702, l1: 0.0001038552545064557, l2: 0.0003473171710802783   Iteration 63 of 100, tot loss = 4.48680982135591, l1: 0.00010305080667465172, l2: 0.00034563017626559097   Iteration 64 of 100, tot loss = 4.482237309217453, l1: 0.00010293170720387934, l2: 0.000345292024576338   Iteration 65 of 100, tot loss = 4.474786901473999, l1: 0.00010262969036165697, l2: 0.00034484900086401745   Iteration 66 of 100, tot loss = 4.472656939968918, l1: 0.00010261322208724309, l2: 0.00034465247308341503   Iteration 67 of 100, tot loss = 4.468479558603088, l1: 0.00010261884153127296, l2: 0.0003442291158195863   Iteration 68 of 100, tot loss = 4.4663934321964485, l1: 0.00010258944910353475, l2: 0.0003440498959903112   Iteration 69 of 100, tot loss = 4.486975776976434, l1: 0.00010248494531286111, l2: 0.00034621263435785323   Iteration 70 of 100, tot loss = 4.506863025256566, l1: 0.00010302688341263482, l2: 0.0003476594208872744   Iteration 71 of 100, tot loss = 4.539054947839657, l1: 0.00010367347470453521, l2: 0.0003502320212928552   Iteration 72 of 100, tot loss = 4.521503292851978, l1: 0.00010337525615492875, l2: 0.0003487750743968516   Iteration 73 of 100, tot loss = 4.511968534286708, l1: 0.00010325464011171565, l2: 0.00034794221474304286   Iteration 74 of 100, tot loss = 4.510706456931862, l1: 0.00010344841993591635, l2: 0.0003476222273830727   Iteration 75 of 100, tot loss = 4.518240222930908, l1: 0.00010369666303934839, l2: 0.00034812736053330205   Iteration 76 of 100, tot loss = 4.5182188435604695, l1: 0.00010362996052112736, l2: 0.0003481919254562327   Iteration 77 of 100, tot loss = 4.574996428056196, l1: 0.00010456367704432228, l2: 0.0003529359679517953   Iteration 78 of 100, tot loss = 4.579651563595503, l1: 0.00010465880819911268, l2: 0.0003533063501937506   Iteration 79 of 100, tot loss = 4.585181332841704, l1: 0.00010506902032795574, l2: 0.0003534491148871617   Iteration 80 of 100, tot loss = 4.561016345024109, l1: 0.00010454742546244234, l2: 0.0003515542110108072   Iteration 81 of 100, tot loss = 4.531540517453794, l1: 0.00010385293384904709, l2: 0.0003493011197546658   Iteration 82 of 100, tot loss = 4.513260024349864, l1: 0.00010340658166281106, l2: 0.0003479194222940927   Iteration 83 of 100, tot loss = 4.51213440550379, l1: 0.00010340810134159738, l2: 0.0003478053407757599   Iteration 84 of 100, tot loss = 4.534933683418092, l1: 0.00010350516484842436, l2: 0.0003499882047998697   Iteration 85 of 100, tot loss = 4.543525732264799, l1: 0.00010358965921434847, l2: 0.0003507629151606713   Iteration 86 of 100, tot loss = 4.563767896142116, l1: 0.00010386070840298956, l2: 0.0003525160818542137   Iteration 87 of 100, tot loss = 4.57011603486949, l1: 0.00010370787398035948, l2: 0.00035330373007702457   Iteration 88 of 100, tot loss = 4.580228450623426, l1: 0.00010380775322780458, l2: 0.00035421509214343547   Iteration 89 of 100, tot loss = 4.6303273667110485, l1: 0.00010445042785709682, l2: 0.00035858230833408914   Iteration 90 of 100, tot loss = 4.652005177074009, l1: 0.0001050400824978068, l2: 0.00036016043434048897   Iteration 91 of 100, tot loss = 4.6536468752137905, l1: 0.0001046902125581055, l2: 0.00036067447409540373   Iteration 92 of 100, tot loss = 4.649155842221302, l1: 0.00010452623285780884, l2: 0.0003603893503326538   Iteration 93 of 100, tot loss = 4.633980617728285, l1: 0.00010416452943157124, l2: 0.0003592335312033913   Iteration 94 of 100, tot loss = 4.615266586871857, l1: 0.00010390615553912172, l2: 0.00035762050197747083   Iteration 95 of 100, tot loss = 4.6123325096933465, l1: 0.00010389621717254877, l2: 0.00035733703254280905   Iteration 96 of 100, tot loss = 4.590107863148053, l1: 0.00010356102423732712, l2: 0.0003554497608699118   Iteration 97 of 100, tot loss = 4.599773274254553, l1: 0.00010383568152655684, l2: 0.0003561416447952332   Iteration 98 of 100, tot loss = 4.59983939054061, l1: 0.00010357059972546044, l2: 0.000356413338336932   Iteration 99 of 100, tot loss = 4.5718604771777835, l1: 0.0001029826680035448, l2: 0.0003542033786417427   Iteration 100 of 100, tot loss = 4.556135737895966, l1: 0.00010274686803313671, l2: 0.0003528667046339251
   End of epoch 1303; saving model... 

Epoch 1304 of 2000
   Iteration 1 of 100, tot loss = 4.585598468780518, l1: 0.00010769289656309411, l2: 0.0003508669324219227   Iteration 2 of 100, tot loss = 4.422838926315308, l1: 0.00010353961260989308, l2: 0.00033874428481794894   Iteration 3 of 100, tot loss = 5.88449239730835, l1: 0.00013163172116037458, l2: 0.0004568175257494052   Iteration 4 of 100, tot loss = 5.049008786678314, l1: 0.00011123259446321754, l2: 0.00039366829150822014   Iteration 5 of 100, tot loss = 4.876508951187134, l1: 0.00011027321961591952, l2: 0.0003773776872549206   Iteration 6 of 100, tot loss = 5.034703850746155, l1: 0.00010787814668825983, l2: 0.0003955922487269466   Iteration 7 of 100, tot loss = 5.234238386154175, l1: 0.00010694976195476816, l2: 0.0004164740800791021   Iteration 8 of 100, tot loss = 4.892924547195435, l1: 0.00010081292975883116, l2: 0.0003884795278281672   Iteration 9 of 100, tot loss = 4.816382726033528, l1: 0.00010216726473623162, l2: 0.00037947100968772755   Iteration 10 of 100, tot loss = 4.774807071685791, l1: 0.00010174597373406869, l2: 0.00037573473673546687   Iteration 11 of 100, tot loss = 4.489689165895635, l1: 9.611120946250263e-05, l2: 0.0003528577111534436   Iteration 12 of 100, tot loss = 4.462303866942723, l1: 9.562700051901629e-05, l2: 0.0003506033875358601   Iteration 13 of 100, tot loss = 4.805371495393606, l1: 0.00010091968719140053, l2: 0.00037961746476447355   Iteration 14 of 100, tot loss = 4.677473247051239, l1: 9.949825601194919e-05, l2: 0.00036824907043150494   Iteration 15 of 100, tot loss = 4.952892025311788, l1: 0.00010553088432061486, l2: 0.0003897583189730843   Iteration 16 of 100, tot loss = 4.8097523376345634, l1: 0.00010263618241879158, l2: 0.0003783390520766261   Iteration 17 of 100, tot loss = 4.732845117064083, l1: 0.00010234682507443187, l2: 0.00037093768803649307   Iteration 18 of 100, tot loss = 4.755709999137455, l1: 0.00010210963632238822, l2: 0.00037346136539579474   Iteration 19 of 100, tot loss = 4.725308800998487, l1: 0.00010152797384714512, l2: 0.0003710029084590803   Iteration 20 of 100, tot loss = 4.849601703882217, l1: 0.00010418438723718282, l2: 0.0003807757842878345   Iteration 21 of 100, tot loss = 4.862349697521755, l1: 0.0001044216277521281, l2: 0.00038181334232268414   Iteration 22 of 100, tot loss = 4.936901119622317, l1: 0.00010597940854495391, l2: 0.00038771070367974147   Iteration 23 of 100, tot loss = 4.8930909374485845, l1: 0.00010544758792409836, l2: 0.000383861505289805   Iteration 24 of 100, tot loss = 4.870405361056328, l1: 0.0001058200705301715, l2: 0.0003812204649875639   Iteration 25 of 100, tot loss = 4.817266488075257, l1: 0.00010454318107804284, l2: 0.00037718346749898046   Iteration 26 of 100, tot loss = 4.819862590386317, l1: 0.00010459299972773387, l2: 0.00037739325993103336   Iteration 27 of 100, tot loss = 4.974933134184943, l1: 0.00010688546252721507, l2: 0.0003906078501906315   Iteration 28 of 100, tot loss = 5.0041174079690665, l1: 0.00010765979225522772, l2: 0.00039275194753177596   Iteration 29 of 100, tot loss = 4.982353592741078, l1: 0.00010751666107760935, l2: 0.0003907186965706983   Iteration 30 of 100, tot loss = 4.983219214280447, l1: 0.00010831871283395837, l2: 0.00039000320781876023   Iteration 31 of 100, tot loss = 4.924126598142808, l1: 0.0001073669688682252, l2: 0.00038504569073988785   Iteration 32 of 100, tot loss = 4.862314153462648, l1: 0.00010568676805178256, l2: 0.00038054464721426484   Iteration 33 of 100, tot loss = 4.839330098845742, l1: 0.00010552164751447201, l2: 0.0003784113624450666   Iteration 34 of 100, tot loss = 4.867249618558323, l1: 0.0001060685810764827, l2: 0.00038065638021661844   Iteration 35 of 100, tot loss = 4.815646726744515, l1: 0.00010492160436115228, l2: 0.000376643067491906   Iteration 36 of 100, tot loss = 4.732950184080336, l1: 0.00010337478407665103, l2: 0.00036992023362674646   Iteration 37 of 100, tot loss = 4.739212435645026, l1: 0.000104257209042427, l2: 0.0003696640338308203   Iteration 38 of 100, tot loss = 4.793872582285028, l1: 0.0001056324684836191, l2: 0.0003737547889805252   Iteration 39 of 100, tot loss = 4.798070198450333, l1: 0.00010557796933706134, l2: 0.0003742290495070987   Iteration 40 of 100, tot loss = 4.7802161693573, l1: 0.00010536731115280417, l2: 0.00037265430510160515   Iteration 41 of 100, tot loss = 4.7952086053243494, l1: 0.00010583435966797946, l2: 0.00037368649983119854   Iteration 42 of 100, tot loss = 4.80890028817313, l1: 0.0001057051707729919, l2: 0.0003751848569317233   Iteration 43 of 100, tot loss = 4.839006457217904, l1: 0.00010665971278158818, l2: 0.00037724093187513737   Iteration 44 of 100, tot loss = 4.847112969918684, l1: 0.00010667671135293362, l2: 0.0003780345855788751   Iteration 45 of 100, tot loss = 4.964412318335639, l1: 0.00010825284326630128, l2: 0.00038818838721555143   Iteration 46 of 100, tot loss = 5.032868188360463, l1: 0.00010939386882692698, l2: 0.0003938929484043594   Iteration 47 of 100, tot loss = 4.99261114952412, l1: 0.00010833868346959749, l2: 0.00039092242962958173   Iteration 48 of 100, tot loss = 5.010389412442843, l1: 0.00010896576380522068, l2: 0.0003920731748318455   Iteration 49 of 100, tot loss = 5.084946131219669, l1: 0.00011053006491405243, l2: 0.0003979645458012059   Iteration 50 of 100, tot loss = 5.080751070976257, l1: 0.0001100870268419385, l2: 0.00039798807760234924   Iteration 51 of 100, tot loss = 5.075810437108956, l1: 0.00010984217635054579, l2: 0.00039773886537124566   Iteration 52 of 100, tot loss = 5.052926884247706, l1: 0.0001092694814220662, l2: 0.0003960232050023758   Iteration 53 of 100, tot loss = 5.04920354429281, l1: 0.00010951337520415315, l2: 0.0003954069776776827   Iteration 54 of 100, tot loss = 4.996536749380606, l1: 0.00010848134809714759, l2: 0.0003911723254498577   Iteration 55 of 100, tot loss = 5.031600111181086, l1: 0.00010876732697149485, l2: 0.0003943926829379052   Iteration 56 of 100, tot loss = 5.089263720171792, l1: 0.00010946412417719589, l2: 0.0003994622462154699   Iteration 57 of 100, tot loss = 5.064593984369646, l1: 0.00010934599240978347, l2: 0.000397113404592107   Iteration 58 of 100, tot loss = 5.140820470349542, l1: 0.00011025321215003777, l2: 0.00040382883413132793   Iteration 59 of 100, tot loss = 5.078566971471754, l1: 0.00010898201657317074, l2: 0.00039887467980822866   Iteration 60 of 100, tot loss = 5.045307151476542, l1: 0.00010809198981102479, l2: 0.00039643872478336564   Iteration 61 of 100, tot loss = 5.053583997194885, l1: 0.0001082835779463315, l2: 0.0003970748215072147   Iteration 62 of 100, tot loss = 5.008811723801397, l1: 0.00010760491503817556, l2: 0.00039327625692690604   Iteration 63 of 100, tot loss = 4.990887581355988, l1: 0.00010749491469589033, l2: 0.00039159384288636615   Iteration 64 of 100, tot loss = 5.010403648018837, l1: 0.00010771710805101975, l2: 0.00039332325616214803   Iteration 65 of 100, tot loss = 4.9968231494610125, l1: 0.00010743117449097693, l2: 0.0003922511394753551   Iteration 66 of 100, tot loss = 4.961038676175204, l1: 0.00010694722054309254, l2: 0.00038915664646827446   Iteration 67 of 100, tot loss = 4.95821957801705, l1: 0.00010707552629260611, l2: 0.00038874643077681177   Iteration 68 of 100, tot loss = 4.946750171044293, l1: 0.0001064201536899097, l2: 0.00038825486252094647   Iteration 69 of 100, tot loss = 4.940522622371065, l1: 0.00010626134699914078, l2: 0.00038779091435413727   Iteration 70 of 100, tot loss = 4.978264284133911, l1: 0.0001067461745801016, l2: 0.00039108025323782516   Iteration 71 of 100, tot loss = 4.989908238531838, l1: 0.00010696840440751133, l2: 0.000392022418759232   Iteration 72 of 100, tot loss = 5.003048340479533, l1: 0.00010730643169962504, l2: 0.00039299840166980476   Iteration 73 of 100, tot loss = 5.0796650664447105, l1: 0.00010840991363033001, l2: 0.00039955659253978847   Iteration 74 of 100, tot loss = 5.11171284881798, l1: 0.00010885717175197724, l2: 0.00040231411283555744   Iteration 75 of 100, tot loss = 5.088271150588989, l1: 0.00010848089717910625, l2: 0.0004003462175023742   Iteration 76 of 100, tot loss = 5.089733591205196, l1: 0.00010857568203822379, l2: 0.000400397676793019   Iteration 77 of 100, tot loss = 5.051330009064117, l1: 0.00010802974056432858, l2: 0.0003971032600422003   Iteration 78 of 100, tot loss = 5.054316428991465, l1: 0.0001084644620091123, l2: 0.0003969671801874378   Iteration 79 of 100, tot loss = 5.05506974835939, l1: 0.00010815342521490942, l2: 0.0003973535492115866   Iteration 80 of 100, tot loss = 5.044448876380921, l1: 0.00010798364687616412, l2: 0.00039646124032515216   Iteration 81 of 100, tot loss = 5.046950422687295, l1: 0.00010822594799439101, l2: 0.0003964690942307187   Iteration 82 of 100, tot loss = 5.034555289803482, l1: 0.00010803688879915593, l2: 0.00039541863997128975   Iteration 83 of 100, tot loss = 5.034065315522343, l1: 0.00010804237820823812, l2: 0.00039536415268624965   Iteration 84 of 100, tot loss = 5.094626120158604, l1: 0.00010908936482813996, l2: 0.00040037324591442786   Iteration 85 of 100, tot loss = 5.102662305270925, l1: 0.00010937492697848938, l2: 0.00040089130197884515   Iteration 86 of 100, tot loss = 5.112686406734378, l1: 0.0001096078297471882, l2: 0.00040166080954043347   Iteration 87 of 100, tot loss = 5.133937369817975, l1: 0.00011000984920230266, l2: 0.0004033838866590308   Iteration 88 of 100, tot loss = 5.133933289484545, l1: 0.00011011884912063992, l2: 0.0004032744781480605   Iteration 89 of 100, tot loss = 5.172812981551953, l1: 0.00011076665465593118, l2: 0.00040651464174577465   Iteration 90 of 100, tot loss = 5.156952529483371, l1: 0.00011076995009110155, l2: 0.0004049253010533802   Iteration 91 of 100, tot loss = 5.165523560492547, l1: 0.00011078522584284656, l2: 0.00040576712858087925   Iteration 92 of 100, tot loss = 5.181710906650709, l1: 0.00011083032614292081, l2: 0.00040734076308423107   Iteration 93 of 100, tot loss = 5.166046288705641, l1: 0.00011056276586411211, l2: 0.00040604186162004565   Iteration 94 of 100, tot loss = 5.1502144894701365, l1: 0.00011025188557160869, l2: 0.00040476956193278386   Iteration 95 of 100, tot loss = 5.14295929858559, l1: 0.00011024572936309452, l2: 0.0004040501991096933   Iteration 96 of 100, tot loss = 5.120905290047328, l1: 0.00010993021089689137, l2: 0.00040216031667720625   Iteration 97 of 100, tot loss = 5.106695723287838, l1: 0.00010974208192603384, l2: 0.000400927489141564   Iteration 98 of 100, tot loss = 5.1011257293273, l1: 0.0001094609669962665, l2: 0.0004006516048358041   Iteration 99 of 100, tot loss = 5.111041986581051, l1: 0.00010960455333450668, l2: 0.00040149964391303274   Iteration 100 of 100, tot loss = 5.106680762767792, l1: 0.00010939507239527302, l2: 0.00040127300228050446
   End of epoch 1304; saving model... 

Epoch 1305 of 2000
   Iteration 1 of 100, tot loss = 5.8624138832092285, l1: 0.00013053842121735215, l2: 0.0004557029460556805   Iteration 2 of 100, tot loss = 6.251142501831055, l1: 0.00013343764294404536, l2: 0.0004916766192764044   Iteration 3 of 100, tot loss = 6.071787198384603, l1: 0.0001221676890660698, l2: 0.0004850110368958364   Iteration 4 of 100, tot loss = 5.75326669216156, l1: 0.00012298688670853153, l2: 0.00045233979471959174   Iteration 5 of 100, tot loss = 5.5602153778076175, l1: 0.00012123739725211635, l2: 0.0004347841488197446   Iteration 6 of 100, tot loss = 5.6242438952128095, l1: 0.00012397330404686122, l2: 0.0004384510878783961   Iteration 7 of 100, tot loss = 5.761723858969552, l1: 0.00012061922669610274, l2: 0.0004555531611133899   Iteration 8 of 100, tot loss = 5.459365785121918, l1: 0.00011851733870571479, l2: 0.0004274192415323341   Iteration 9 of 100, tot loss = 5.348095099131267, l1: 0.00011742403067829501, l2: 0.0004173854815437355   Iteration 10 of 100, tot loss = 5.0269413709640505, l1: 0.0001115045859478414, l2: 0.00039118955319281665   Iteration 11 of 100, tot loss = 5.0429534695365215, l1: 0.00011289101993580434, l2: 0.0003914043296721171   Iteration 12 of 100, tot loss = 4.785575906435649, l1: 0.0001079203560342042, l2: 0.00037063723721075803   Iteration 13 of 100, tot loss = 4.792923377110408, l1: 0.00010733035882227481, l2: 0.00037196198200735333   Iteration 14 of 100, tot loss = 4.756880487714495, l1: 0.00010640795647174985, l2: 0.00036928009441388506   Iteration 15 of 100, tot loss = 4.826183160146077, l1: 0.00010844420006227059, l2: 0.00037417411998224757   Iteration 16 of 100, tot loss = 5.012990742921829, l1: 0.0001088566975795402, l2: 0.0003924423781427322   Iteration 17 of 100, tot loss = 4.999848141389735, l1: 0.0001086277849989368, l2: 0.000391357032212374   Iteration 18 of 100, tot loss = 4.998745997746785, l1: 0.0001093708647709314, l2: 0.00039050373662677076   Iteration 19 of 100, tot loss = 4.9522460134405835, l1: 0.00010736276051734182, l2: 0.00038786184067200676   Iteration 20 of 100, tot loss = 4.996451425552368, l1: 0.00010812762102432316, l2: 0.00039151752134785055   Iteration 21 of 100, tot loss = 5.02991197222755, l1: 0.00010782602493279791, l2: 0.0003951651742681861   Iteration 22 of 100, tot loss = 4.923258553851735, l1: 0.00010553424032299186, l2: 0.00038679161662003025   Iteration 23 of 100, tot loss = 4.901276495145715, l1: 0.00010510749321522029, l2: 0.0003850201566187341   Iteration 24 of 100, tot loss = 4.976203054189682, l1: 0.00010561490429002636, l2: 0.0003920054011056588   Iteration 25 of 100, tot loss = 5.063281774520874, l1: 0.0001081400632392615, l2: 0.00039818811288569125   Iteration 26 of 100, tot loss = 5.0251026061865, l1: 0.00010624278953540712, l2: 0.0003962674689738868   Iteration 27 of 100, tot loss = 4.9876188437143965, l1: 0.00010589471316456588, l2: 0.0003928671686918716   Iteration 28 of 100, tot loss = 4.93595894745418, l1: 0.00010396471225249115, l2: 0.00038963117913226597   Iteration 29 of 100, tot loss = 4.949473430370462, l1: 0.00010439283380100247, l2: 0.0003905545058058061   Iteration 30 of 100, tot loss = 4.936365938186645, l1: 0.00010457293222619531, l2: 0.00038906365856140234   Iteration 31 of 100, tot loss = 4.904723713474889, l1: 0.00010467505043995897, l2: 0.00038579731828518093   Iteration 32 of 100, tot loss = 4.849579684436321, l1: 0.00010397519940852362, l2: 0.00038098276581877144   Iteration 33 of 100, tot loss = 4.827802275166367, l1: 0.00010264417340460403, l2: 0.0003801360514692285   Iteration 34 of 100, tot loss = 4.784628307118135, l1: 0.00010191511565233882, l2: 0.0003765477124355076   Iteration 35 of 100, tot loss = 4.774903978620257, l1: 0.00010153508701478131, l2: 0.00037595530822207886   Iteration 36 of 100, tot loss = 4.837690856721666, l1: 0.0001025914598358213, l2: 0.00038117762354280177   Iteration 37 of 100, tot loss = 4.850279305432294, l1: 0.00010295499954293985, l2: 0.0003820729286195962   Iteration 38 of 100, tot loss = 4.8614417879205005, l1: 0.00010400461168932165, l2: 0.0003821395634747061   Iteration 39 of 100, tot loss = 4.840058730198787, l1: 0.00010364946287247735, l2: 0.0003803564072885097   Iteration 40 of 100, tot loss = 4.796930903196335, l1: 0.00010228155842924025, l2: 0.0003774115291889757   Iteration 41 of 100, tot loss = 4.8127771412454, l1: 0.00010271213729383151, l2: 0.0003785655731562434   Iteration 42 of 100, tot loss = 4.795948068300883, l1: 0.00010259104757486577, l2: 0.0003770037562519844   Iteration 43 of 100, tot loss = 4.805204130882441, l1: 0.00010321313108528725, l2: 0.00037730727845607977   Iteration 44 of 100, tot loss = 4.793836642395366, l1: 0.00010291533495050813, l2: 0.0003764683252401565   Iteration 45 of 100, tot loss = 4.765452485614353, l1: 0.00010243615494497741, l2: 0.0003741090897367232   Iteration 46 of 100, tot loss = 4.859163362046947, l1: 0.0001037952086177108, l2: 0.0003821211246996308   Iteration 47 of 100, tot loss = 4.861071814881995, l1: 0.00010296666825640986, l2: 0.0003831405112113645   Iteration 48 of 100, tot loss = 4.863877430558205, l1: 0.00010355064311321864, l2: 0.0003828370978832633   Iteration 49 of 100, tot loss = 4.835639880628002, l1: 0.00010316015285919706, l2: 0.0003804038332214541   Iteration 50 of 100, tot loss = 4.82265166759491, l1: 0.00010286449396517128, l2: 0.00037940067064482717   Iteration 51 of 100, tot loss = 4.798399144527959, l1: 0.00010293818509215307, l2: 0.0003769017274345399   Iteration 52 of 100, tot loss = 4.765992453465095, l1: 0.00010267632443663807, l2: 0.00037392291890752787   Iteration 53 of 100, tot loss = 4.726687845194115, l1: 0.00010180911656034197, l2: 0.000370859665980668   Iteration 54 of 100, tot loss = 4.712634015966345, l1: 0.00010217099662878793, l2: 0.00036909240319531547   Iteration 55 of 100, tot loss = 4.710332332957875, l1: 0.00010215468990447168, l2: 0.0003688785411544483   Iteration 56 of 100, tot loss = 4.6976194977760315, l1: 0.00010235998213050022, l2: 0.00036740196550713984   Iteration 57 of 100, tot loss = 4.704828245597973, l1: 0.00010264093049172835, l2: 0.00036784189165030653   Iteration 58 of 100, tot loss = 4.763410354482716, l1: 0.00010404392579120809, l2: 0.0003722971064391835   Iteration 59 of 100, tot loss = 4.763123100086794, l1: 0.00010408964491512437, l2: 0.00037222266220882284   Iteration 60 of 100, tot loss = 4.713728034496308, l1: 0.00010307950221128218, l2: 0.00036829329837928526   Iteration 61 of 100, tot loss = 4.675723967004995, l1: 0.0001025433843782683, l2: 0.0003650290094751132   Iteration 62 of 100, tot loss = 4.67230466873415, l1: 0.0001021071619165483, l2: 0.00036512330153791776   Iteration 63 of 100, tot loss = 4.675907293955485, l1: 0.0001020768969961893, l2: 0.00036551382961786457   Iteration 64 of 100, tot loss = 4.661796968430281, l1: 0.0001018675814634662, l2: 0.0003643121126515325   Iteration 65 of 100, tot loss = 4.62522061421321, l1: 0.00010135050940264661, l2: 0.0003611715492577507   Iteration 66 of 100, tot loss = 4.668199488610933, l1: 0.0001021016737605819, l2: 0.0003647182725906146   Iteration 67 of 100, tot loss = 4.712997870658761, l1: 0.00010257702996935209, l2: 0.0003687227544472066   Iteration 68 of 100, tot loss = 4.728373597649967, l1: 0.00010266910393350853, l2: 0.00037016825337880566   Iteration 69 of 100, tot loss = 4.701996602873871, l1: 0.00010227735085367306, l2: 0.0003679223068436419   Iteration 70 of 100, tot loss = 4.690965717179434, l1: 0.00010202608053597423, l2: 0.0003670704887814022   Iteration 71 of 100, tot loss = 4.681609251129795, l1: 0.0001022381432666804, l2: 0.0003659227796548217   Iteration 72 of 100, tot loss = 4.677250931660335, l1: 0.00010251914282990684, l2: 0.00036520594787564024   Iteration 73 of 100, tot loss = 4.658389209067985, l1: 0.00010211086396027471, l2: 0.00036372805463966645   Iteration 74 of 100, tot loss = 4.655291318893433, l1: 0.0001021554628980969, l2: 0.000363373666816089   Iteration 75 of 100, tot loss = 4.67527930577596, l1: 0.00010269800584258823, l2: 0.0003648299223277718   Iteration 76 of 100, tot loss = 4.652117032753794, l1: 0.00010236938353620262, l2: 0.00036284231732440134   Iteration 77 of 100, tot loss = 4.679057790087415, l1: 0.00010281939651210816, l2: 0.00036508638023205296   Iteration 78 of 100, tot loss = 4.6675257957898655, l1: 0.00010264958216747544, l2: 0.00036410299519327685   Iteration 79 of 100, tot loss = 4.688104873971094, l1: 0.00010294059089412971, l2: 0.0003658698947622759   Iteration 80 of 100, tot loss = 4.676352941989899, l1: 0.00010276743710164737, l2: 0.0003648678553872742   Iteration 81 of 100, tot loss = 4.642877587565669, l1: 0.00010199117897161753, l2: 0.00036229657811934793   Iteration 82 of 100, tot loss = 4.681165523645355, l1: 0.00010292909366132623, l2: 0.00036518745675893135   Iteration 83 of 100, tot loss = 4.7140567360154115, l1: 0.00010376576453809212, l2: 0.000367639906997568   Iteration 84 of 100, tot loss = 4.711205922421955, l1: 0.00010368365699486319, l2: 0.00036743693341295946   Iteration 85 of 100, tot loss = 4.743035521226771, l1: 0.00010408094488358235, l2: 0.0003702226052101811   Iteration 86 of 100, tot loss = 4.726269921591116, l1: 0.00010408520858280014, l2: 0.0003685417817228712   Iteration 87 of 100, tot loss = 4.7340909694803175, l1: 0.00010423098121108701, l2: 0.0003691781141618588   Iteration 88 of 100, tot loss = 4.7114499075846235, l1: 0.00010413192219196141, l2: 0.00036701306684831667   Iteration 89 of 100, tot loss = 4.697544529196922, l1: 0.00010357919704853139, l2: 0.00036617525424180404   Iteration 90 of 100, tot loss = 4.683188976181878, l1: 0.00010306967629326714, l2: 0.0003652492198549832   Iteration 91 of 100, tot loss = 4.687857358010261, l1: 0.00010321010056061793, l2: 0.0003655756338228277   Iteration 92 of 100, tot loss = 4.685138222963913, l1: 0.00010305976462879461, l2: 0.00036545405600918457   Iteration 93 of 100, tot loss = 4.674602498290359, l1: 0.00010289833036011024, l2: 0.0003645619175957656   Iteration 94 of 100, tot loss = 4.691548692419174, l1: 0.00010332281012682521, l2: 0.0003658320573331947   Iteration 95 of 100, tot loss = 4.674072024696752, l1: 0.00010317952676958062, l2: 0.00036422767382311195   Iteration 96 of 100, tot loss = 4.682451123992602, l1: 0.0001034862550568505, l2: 0.0003647588552363838   Iteration 97 of 100, tot loss = 4.697174676914805, l1: 0.0001036766580469215, l2: 0.00036604080730857154   Iteration 98 of 100, tot loss = 4.673070141247341, l1: 0.00010317537590727086, l2: 0.0003641316358639136   Iteration 99 of 100, tot loss = 4.688716698174525, l1: 0.00010332494896673127, l2: 0.0003655467188571382   Iteration 100 of 100, tot loss = 4.684866144657135, l1: 0.00010353948458941886, l2: 0.00036494712767307644
   End of epoch 1305; saving model... 

Epoch 1306 of 2000
   Iteration 1 of 100, tot loss = 3.930386543273926, l1: 8.621290908195078e-05, l2: 0.00030682573560625315   Iteration 2 of 100, tot loss = 5.074047088623047, l1: 9.496436177869327e-05, l2: 0.0004124403349123895   Iteration 3 of 100, tot loss = 5.260582288106282, l1: 9.866762411547825e-05, l2: 0.0004273906136707713   Iteration 4 of 100, tot loss = 4.760430693626404, l1: 9.217921979143284e-05, l2: 0.0003838638585875742   Iteration 5 of 100, tot loss = 4.61552095413208, l1: 9.365959267597646e-05, l2: 0.00036789251025766135   Iteration 6 of 100, tot loss = 4.9159111976623535, l1: 0.00010004295715286086, l2: 0.00039154816962157685   Iteration 7 of 100, tot loss = 4.937713895525251, l1: 0.00010361112903670541, l2: 0.0003901602592252727   Iteration 8 of 100, tot loss = 4.799155503511429, l1: 0.00010417192788736429, l2: 0.0003757436206797138   Iteration 9 of 100, tot loss = 4.831305106480916, l1: 0.00010735910068938715, l2: 0.0003757714067534026   Iteration 10 of 100, tot loss = 4.770278525352478, l1: 0.00010842705669347196, l2: 0.0003686007927171886   Iteration 11 of 100, tot loss = 4.69908881187439, l1: 0.00010821320095353506, l2: 0.00036169567663984543   Iteration 12 of 100, tot loss = 4.6318075855573015, l1: 0.00010730801902051705, l2: 0.0003558727354781392   Iteration 13 of 100, tot loss = 4.587760466795701, l1: 0.00010615624118005284, l2: 0.0003526198024001832   Iteration 14 of 100, tot loss = 4.626099978174482, l1: 0.0001054111500187511, l2: 0.00035719884492989095   Iteration 15 of 100, tot loss = 4.499694601694743, l1: 0.00010204999901664754, l2: 0.00034791945848458756   Iteration 16 of 100, tot loss = 4.3926414251327515, l1: 0.0001009867532957287, l2: 0.0003382773866178468   Iteration 17 of 100, tot loss = 4.482161325566909, l1: 0.00010388811685569475, l2: 0.0003443280139061458   Iteration 18 of 100, tot loss = 4.413683454195659, l1: 0.00010235744371635115, l2: 0.0003390109001581247   Iteration 19 of 100, tot loss = 4.439958584936042, l1: 0.00010263643803467091, l2: 0.00034135941869432206   Iteration 20 of 100, tot loss = 4.436556279659271, l1: 0.00010186739345954265, l2: 0.0003417882326175459   Iteration 21 of 100, tot loss = 4.529572952361334, l1: 0.00010197108349530026, l2: 0.0003509862123922046   Iteration 22 of 100, tot loss = 4.424748875878074, l1: 9.943525517221794e-05, l2: 0.00034303963316646826   Iteration 23 of 100, tot loss = 4.3298749923706055, l1: 9.80503078291962e-05, l2: 0.0003349371918770687   Iteration 24 of 100, tot loss = 4.493419210116069, l1: 0.0001006210638176223, l2: 0.0003487208578007994   Iteration 25 of 100, tot loss = 4.521572647094726, l1: 9.967408928787335e-05, l2: 0.0003524831758113578   Iteration 26 of 100, tot loss = 4.559069028267493, l1: 0.00010063329766405962, l2: 0.0003552736043974829   Iteration 27 of 100, tot loss = 4.56828550056175, l1: 0.00010168059439079284, l2: 0.00035514795551231754   Iteration 28 of 100, tot loss = 4.590041943958828, l1: 0.00010153559326551788, l2: 0.0003574686015781481   Iteration 29 of 100, tot loss = 4.623209410700305, l1: 0.0001027374450570968, l2: 0.00035958349602377237   Iteration 30 of 100, tot loss = 4.569825053215027, l1: 0.00010268762295406001, l2: 0.0003542948824663957   Iteration 31 of 100, tot loss = 4.504287773563016, l1: 0.0001017105950590884, l2: 0.0003487181825368034   Iteration 32 of 100, tot loss = 4.421763449907303, l1: 0.00010016767726028775, l2: 0.0003420086677579093   Iteration 33 of 100, tot loss = 4.445611144557144, l1: 0.00010017327936524948, l2: 0.00034438783503984183   Iteration 34 of 100, tot loss = 4.484773187076344, l1: 0.00010074094386843225, l2: 0.00034773637547486406   Iteration 35 of 100, tot loss = 4.464472307477679, l1: 0.0001007868410462314, l2: 0.0003456603901992951   Iteration 36 of 100, tot loss = 4.477493312623766, l1: 0.00010068044108872225, l2: 0.00034706889144015603   Iteration 37 of 100, tot loss = 4.468895757520521, l1: 0.00010092322161260715, l2: 0.0003459663555101567   Iteration 38 of 100, tot loss = 4.4986580673017, l1: 0.00010106871703005469, l2: 0.00034879709097654805   Iteration 39 of 100, tot loss = 4.498769491146772, l1: 0.00010110971598829239, l2: 0.00034876723433486547   Iteration 40 of 100, tot loss = 4.52015653848648, l1: 0.00010115441737070796, l2: 0.00035086123752989804   Iteration 41 of 100, tot loss = 4.481025032880829, l1: 0.00010074696618220315, l2: 0.000347355538595854   Iteration 42 of 100, tot loss = 4.465656110218593, l1: 0.00010002117839273221, l2: 0.0003465444339478078   Iteration 43 of 100, tot loss = 4.477298392805943, l1: 0.00010018251563197673, l2: 0.0003475473248156134   Iteration 44 of 100, tot loss = 4.443614000623876, l1: 9.964076467771189e-05, l2: 0.0003447206370103893   Iteration 45 of 100, tot loss = 4.388691176308526, l1: 9.8551468949558e-05, l2: 0.00034031765018072396   Iteration 46 of 100, tot loss = 4.480295901713164, l1: 0.0001001690122749373, l2: 0.00034786057886262626   Iteration 47 of 100, tot loss = 4.467805963881473, l1: 0.00010052618358229743, l2: 0.00034625441370967854   Iteration 48 of 100, tot loss = 4.425555303692818, l1: 9.968042233291878e-05, l2: 0.0003428751087994897   Iteration 49 of 100, tot loss = 4.463208748369801, l1: 0.00010053380500888737, l2: 0.0003457870705609153   Iteration 50 of 100, tot loss = 4.430361309051514, l1: 0.00010019796092819889, l2: 0.0003428381704725325   Iteration 51 of 100, tot loss = 4.453445920757219, l1: 0.00010022143432448217, l2: 0.0003451231579460642   Iteration 52 of 100, tot loss = 4.547631612190833, l1: 0.00010191845222624789, l2: 0.0003528447078469281   Iteration 53 of 100, tot loss = 4.531472017180245, l1: 0.00010171929671690482, l2: 0.0003514279038118163   Iteration 54 of 100, tot loss = 4.533760900850649, l1: 0.00010204363914251897, l2: 0.0003513324498069576   Iteration 55 of 100, tot loss = 4.557290259274569, l1: 0.00010202524327641269, l2: 0.0003537037817295641   Iteration 56 of 100, tot loss = 4.559562487261636, l1: 0.0001020946953238828, l2: 0.00035386155286687426   Iteration 57 of 100, tot loss = 4.5623348721286705, l1: 0.00010146813822262757, l2: 0.0003547653481323403   Iteration 58 of 100, tot loss = 4.593956848670697, l1: 0.0001014949813969875, l2: 0.0003579007025109604   Iteration 59 of 100, tot loss = 4.584606954606913, l1: 0.00010136440126235218, l2: 0.00035709629310847466   Iteration 60 of 100, tot loss = 4.58402685324351, l1: 0.00010135118360873699, l2: 0.0003570515007595532   Iteration 61 of 100, tot loss = 4.5719877813683185, l1: 0.0001013292437805756, l2: 0.00035586953353991763   Iteration 62 of 100, tot loss = 4.527115256555619, l1: 0.00010047640071500228, l2: 0.000352235124146025   Iteration 63 of 100, tot loss = 4.534576147321671, l1: 0.000100280430723658, l2: 0.00035317718385837024   Iteration 64 of 100, tot loss = 4.5270612724125385, l1: 0.00010008781629267105, l2: 0.00035261831089883344   Iteration 65 of 100, tot loss = 4.511432038820707, l1: 9.958769845140454e-05, l2: 0.0003515555057674646   Iteration 66 of 100, tot loss = 4.516687147545092, l1: 9.988807015002804e-05, l2: 0.0003517806449742762   Iteration 67 of 100, tot loss = 4.509531540657157, l1: 9.956696152612359e-05, l2: 0.00035138619297406454   Iteration 68 of 100, tot loss = 4.529995974372415, l1: 9.992322012455107e-05, l2: 0.00035307637795888106   Iteration 69 of 100, tot loss = 4.537354918493741, l1: 0.00010045503559935689, l2: 0.0003532804569830119   Iteration 70 of 100, tot loss = 4.53981830733163, l1: 0.00010042062967841048, l2: 0.00035356120206415654   Iteration 71 of 100, tot loss = 4.522354031952334, l1: 0.00010018455259982353, l2: 0.00035205085165011633   Iteration 72 of 100, tot loss = 4.49787242213885, l1: 9.947054488495471e-05, l2: 0.00035031669823284674   Iteration 73 of 100, tot loss = 4.476570171852634, l1: 9.93153526370369e-05, l2: 0.00034834166541408544   Iteration 74 of 100, tot loss = 4.509173106502843, l1: 9.99712251418591e-05, l2: 0.00035094608677821735   Iteration 75 of 100, tot loss = 4.498332703908284, l1: 9.988599888553533e-05, l2: 0.00034994727252827336   Iteration 76 of 100, tot loss = 4.4776697127442615, l1: 9.9576352591899e-05, l2: 0.0003481906198047861   Iteration 77 of 100, tot loss = 4.488056693758283, l1: 9.919083440054946e-05, l2: 0.00034961483593237913   Iteration 78 of 100, tot loss = 4.4713588036023655, l1: 9.892808404215313e-05, l2: 0.00034820779714769183   Iteration 79 of 100, tot loss = 4.448184653173519, l1: 9.848049113672828e-05, l2: 0.00034633797502787664   Iteration 80 of 100, tot loss = 4.433043187856674, l1: 9.790010290089413e-05, l2: 0.00034540421675046675   Iteration 81 of 100, tot loss = 4.422231073732729, l1: 9.802562191099342e-05, l2: 0.00034419748623838167   Iteration 82 of 100, tot loss = 4.401006759666815, l1: 9.758657182740025e-05, l2: 0.00034251410504178395   Iteration 83 of 100, tot loss = 4.40333362659776, l1: 9.788429837586942e-05, l2: 0.0003424490653315043   Iteration 84 of 100, tot loss = 4.411227529957181, l1: 9.798362493928012e-05, l2: 0.00034313912883157   Iteration 85 of 100, tot loss = 4.438443153044757, l1: 9.839843696681782e-05, l2: 0.00034544587902286475   Iteration 86 of 100, tot loss = 4.4594231899394545, l1: 9.881157496047943e-05, l2: 0.0003471307444310379   Iteration 87 of 100, tot loss = 4.466883974513788, l1: 9.89403280104382e-05, l2: 0.00034774806960646446   Iteration 88 of 100, tot loss = 4.4511094987392426, l1: 9.878927672120467e-05, l2: 0.0003463216731192502   Iteration 89 of 100, tot loss = 4.449403390455782, l1: 9.87954613009698e-05, l2: 0.000346144877787512   Iteration 90 of 100, tot loss = 4.457361949814691, l1: 9.889278565727485e-05, l2: 0.00034684340935200455   Iteration 91 of 100, tot loss = 4.462187670089386, l1: 9.913165870654796e-05, l2: 0.0003470871079416311   Iteration 92 of 100, tot loss = 4.484423209791598, l1: 9.919776541368185e-05, l2: 0.00034924455521041125   Iteration 93 of 100, tot loss = 4.5348221640433035, l1: 9.98513416014111e-05, l2: 0.00035363087438107017   Iteration 94 of 100, tot loss = 4.507663889134184, l1: 9.925197635776057e-05, l2: 0.00035151441214962843   Iteration 95 of 100, tot loss = 4.494885055642379, l1: 9.915527131708682e-05, l2: 0.0003503332338847318   Iteration 96 of 100, tot loss = 4.481241708000501, l1: 9.870801666996461e-05, l2: 0.00034941615391896147   Iteration 97 of 100, tot loss = 4.5000584445048855, l1: 9.913465298277796e-05, l2: 0.00035087119133245275   Iteration 98 of 100, tot loss = 4.5208593339336165, l1: 9.946878359484372e-05, l2: 0.0003526171496225882   Iteration 99 of 100, tot loss = 4.533764516464387, l1: 9.982165169956238e-05, l2: 0.0003535547997800144   Iteration 100 of 100, tot loss = 4.555684857368469, l1: 0.00010049690878076945, l2: 0.00035507157663232646
   End of epoch 1306; saving model... 

Epoch 1307 of 2000
   Iteration 1 of 100, tot loss = 7.907864093780518, l1: 0.00014287850353866816, l2: 0.000647907902020961   Iteration 2 of 100, tot loss = 7.1881208419799805, l1: 0.00013788206706522033, l2: 0.0005809300055261701   Iteration 3 of 100, tot loss = 6.383944511413574, l1: 0.00012509470252553, l2: 0.0005132997369704148   Iteration 4 of 100, tot loss = 6.6354900598526, l1: 0.00013617299009638373, l2: 0.0005273760034469888   Iteration 5 of 100, tot loss = 6.26918249130249, l1: 0.00012874939275206999, l2: 0.0004981688456609845   Iteration 6 of 100, tot loss = 6.213157097498576, l1: 0.00012989565463309796, l2: 0.0004914200447577363   Iteration 7 of 100, tot loss = 6.4193869318280905, l1: 0.00012942405737703666, l2: 0.0005125146251105304   Iteration 8 of 100, tot loss = 6.114770829677582, l1: 0.00012757391414197627, l2: 0.00048390316078439355   Iteration 9 of 100, tot loss = 5.973960929446751, l1: 0.00012355850534125543, l2: 0.0004738375816183786   Iteration 10 of 100, tot loss = 5.939206933975219, l1: 0.00012380193584249355, l2: 0.00047011875722091645   Iteration 11 of 100, tot loss = 5.681953365152532, l1: 0.00011839631770271808, l2: 0.0004497990197375078   Iteration 12 of 100, tot loss = 5.515238344669342, l1: 0.00011371779874025378, l2: 0.00043780603785611066   Iteration 13 of 100, tot loss = 5.243422278991113, l1: 0.00010811237678773559, l2: 0.00041622985288715706   Iteration 14 of 100, tot loss = 5.175603176866259, l1: 0.0001072159424698579, l2: 0.00041034437474861207   Iteration 15 of 100, tot loss = 5.294356656074524, l1: 0.00011053953930968418, l2: 0.0004188961252414932   Iteration 16 of 100, tot loss = 5.0374940559268, l1: 0.00010604503791000752, l2: 0.0003977043661507196   Iteration 17 of 100, tot loss = 5.017657216857462, l1: 0.00010505254420636715, l2: 0.00039671317533836425   Iteration 18 of 100, tot loss = 4.990468389458126, l1: 0.00010533953233486197, l2: 0.00039370730438450765   Iteration 19 of 100, tot loss = 4.911187705240752, l1: 0.00010545616475466981, l2: 0.0003856626043623117   Iteration 20 of 100, tot loss = 5.0481432497501375, l1: 0.00010805193433043314, l2: 0.00039676238884567283   Iteration 21 of 100, tot loss = 5.057642783437457, l1: 0.0001075249011115548, l2: 0.00039823937515306865   Iteration 22 of 100, tot loss = 5.077249705791473, l1: 0.00010807166224615436, l2: 0.00039965330572820017   Iteration 23 of 100, tot loss = 5.050131098083828, l1: 0.00010694109068540654, l2: 0.00039807201613692325   Iteration 24 of 100, tot loss = 5.017768546938896, l1: 0.00010611620731045453, l2: 0.0003956606445475093   Iteration 25 of 100, tot loss = 4.991293187141419, l1: 0.0001057990406116005, l2: 0.000393330276128836   Iteration 26 of 100, tot loss = 4.910387309697958, l1: 0.00010359210290726669, l2: 0.00038744662574922235   Iteration 27 of 100, tot loss = 4.9898732988922685, l1: 0.00010475620432838764, l2: 0.00039423112240102555   Iteration 28 of 100, tot loss = 5.0030240799699515, l1: 0.00010528440920357493, l2: 0.0003950179951581439   Iteration 29 of 100, tot loss = 5.062765413317187, l1: 0.00010616186685591198, l2: 0.00040011467103011393   Iteration 30 of 100, tot loss = 5.129892758528391, l1: 0.00010724127265954545, l2: 0.000405747999320738   Iteration 31 of 100, tot loss = 5.163337780583289, l1: 0.0001068127193936943, l2: 0.00040952105454409557   Iteration 32 of 100, tot loss = 5.252296607941389, l1: 0.00010741459925611707, l2: 0.00041781505842664046   Iteration 33 of 100, tot loss = 5.218206127484639, l1: 0.00010672388022860766, l2: 0.00041509672971160125   Iteration 34 of 100, tot loss = 5.253925432177151, l1: 0.00010747918574505842, l2: 0.000417913354741519   Iteration 35 of 100, tot loss = 5.234688155991691, l1: 0.0001072239081882539, l2: 0.0004162449048765536   Iteration 36 of 100, tot loss = 5.209717230664359, l1: 0.00010669301193249541, l2: 0.0004142787082754593   Iteration 37 of 100, tot loss = 5.165094514150877, l1: 0.00010684755211145416, l2: 0.0004096618960485668   Iteration 38 of 100, tot loss = 5.219089567661285, l1: 0.00010766149400871272, l2: 0.00041424745914379234   Iteration 39 of 100, tot loss = 5.188621261180976, l1: 0.00010719632001172823, l2: 0.00041166580288121715   Iteration 40 of 100, tot loss = 5.153535488247871, l1: 0.00010710933647715138, l2: 0.0004082442093931604   Iteration 41 of 100, tot loss = 5.182072014343448, l1: 0.00010813796715654728, l2: 0.00041006923093823945   Iteration 42 of 100, tot loss = 5.22561179172425, l1: 0.00010915239380708608, l2: 0.00041340878219454593   Iteration 43 of 100, tot loss = 5.206721885259761, l1: 0.00010907496834988706, l2: 0.0004115972172490559   Iteration 44 of 100, tot loss = 5.24049246582118, l1: 0.00010949380893262887, l2: 0.0004145554346244105   Iteration 45 of 100, tot loss = 5.207599457105001, l1: 0.00010913447776046167, l2: 0.00041162546517120466   Iteration 46 of 100, tot loss = 5.2377549850422405, l1: 0.00010952982254662936, l2: 0.0004142456733540672   Iteration 47 of 100, tot loss = 5.242679573119955, l1: 0.00011011691303599249, l2: 0.00041415104072818414   Iteration 48 of 100, tot loss = 5.250830320020516, l1: 0.00011003469042710397, l2: 0.00041504833825456444   Iteration 49 of 100, tot loss = 5.219064919316039, l1: 0.000109590121740309, l2: 0.0004123163668495812   Iteration 50 of 100, tot loss = 5.230247271060944, l1: 0.00010913323720160406, l2: 0.0004138914868235588   Iteration 51 of 100, tot loss = 5.251732809870851, l1: 0.0001099151535634198, l2: 0.00041525812440679644   Iteration 52 of 100, tot loss = 5.2331686684718495, l1: 0.00010975336779385823, l2: 0.00041356349650483864   Iteration 53 of 100, tot loss = 5.215606358816039, l1: 0.00010977632330152874, l2: 0.0004117843098630953   Iteration 54 of 100, tot loss = 5.250568877767633, l1: 0.00011021634478917501, l2: 0.00041484054020623226   Iteration 55 of 100, tot loss = 5.310124962980097, l1: 0.00011147108141978441, l2: 0.00041954141290096396   Iteration 56 of 100, tot loss = 5.289084743176188, l1: 0.00011103809648765102, l2: 0.0004178703758433195   Iteration 57 of 100, tot loss = 5.291310847851268, l1: 0.00011109449603776611, l2: 0.0004180365869072838   Iteration 58 of 100, tot loss = 5.273181826903902, l1: 0.00011077901700477453, l2: 0.00041653916384254034   Iteration 59 of 100, tot loss = 5.243913098917169, l1: 0.00011021200321087913, l2: 0.0004141793045673077   Iteration 60 of 100, tot loss = 5.240275603532791, l1: 0.00011026962765754433, l2: 0.0004137579303157205   Iteration 61 of 100, tot loss = 5.192605817904238, l1: 0.00010931421585140567, l2: 0.0004099463635089914   Iteration 62 of 100, tot loss = 5.21689578986937, l1: 0.00010946651309495792, l2: 0.0004122230633173228   Iteration 63 of 100, tot loss = 5.231008221232702, l1: 0.00010958380141620716, l2: 0.0004135170184160834   Iteration 64 of 100, tot loss = 5.2421142924577, l1: 0.00010975147029057553, l2: 0.0004144599568007834   Iteration 65 of 100, tot loss = 5.187586826544542, l1: 0.0001087473583398745, l2: 0.0004100113222823263   Iteration 66 of 100, tot loss = 5.195454256101088, l1: 0.00010838173153443319, l2: 0.00041116369208215144   Iteration 67 of 100, tot loss = 5.238664347734025, l1: 0.00010897369912839426, l2: 0.0004148927342177911   Iteration 68 of 100, tot loss = 5.212693784166785, l1: 0.00010879152047410196, l2: 0.0004124778568118071   Iteration 69 of 100, tot loss = 5.180261136828989, l1: 0.00010861989890606534, l2: 0.0004094062135571722   Iteration 70 of 100, tot loss = 5.1791554263659885, l1: 0.00010864846512309409, l2: 0.0004092670758836903   Iteration 71 of 100, tot loss = 5.164368503530261, l1: 0.00010870520782191306, l2: 0.0004077316411446229   Iteration 72 of 100, tot loss = 5.14136425488525, l1: 0.00010817853166776735, l2: 0.00040595789222505927   Iteration 73 of 100, tot loss = 5.109422061541309, l1: 0.00010784143723120071, l2: 0.00040310076740889315   Iteration 74 of 100, tot loss = 5.092387616634369, l1: 0.00010774020906742285, l2: 0.00040149855084490736   Iteration 75 of 100, tot loss = 5.085186015764872, l1: 0.00010761939008564998, l2: 0.0004008992097806186   Iteration 76 of 100, tot loss = 5.089624171194277, l1: 0.0001076018975015215, l2: 0.00040136051776291996   Iteration 77 of 100, tot loss = 5.060561423177843, l1: 0.00010731345612945809, l2: 0.0003987426843869125   Iteration 78 of 100, tot loss = 5.06184195402341, l1: 0.00010711111453696131, l2: 0.000399073079162731   Iteration 79 of 100, tot loss = 5.0439407025711445, l1: 0.00010656208187471367, l2: 0.0003978319866199612   Iteration 80 of 100, tot loss = 5.076430900394916, l1: 0.0001070203134077019, l2: 0.00040062277512333824   Iteration 81 of 100, tot loss = 5.069515023702457, l1: 0.00010706210750755912, l2: 0.0003998893935428817   Iteration 82 of 100, tot loss = 5.103803764029247, l1: 0.00010764200239308837, l2: 0.00040273837214222224   Iteration 83 of 100, tot loss = 5.077508077564009, l1: 0.00010719524604599385, l2: 0.0004005555597523856   Iteration 84 of 100, tot loss = 5.0780116731212255, l1: 0.00010730474570577609, l2: 0.0004004964199945486   Iteration 85 of 100, tot loss = 5.090196790414698, l1: 0.00010740825459566515, l2: 0.0004016114232774057   Iteration 86 of 100, tot loss = 5.07351534172546, l1: 0.00010711611803556077, l2: 0.0004002354150099225   Iteration 87 of 100, tot loss = 5.086907796476079, l1: 0.00010754372780719751, l2: 0.00040114705029998144   Iteration 88 of 100, tot loss = 5.06054567613385, l1: 0.00010719507901574781, l2: 0.00039885948701174294   Iteration 89 of 100, tot loss = 5.050503777653984, l1: 0.00010702602455413409, l2: 0.0003980243520672548   Iteration 90 of 100, tot loss = 5.061750643783146, l1: 0.00010732364785831629, l2: 0.00039885141547226036   Iteration 91 of 100, tot loss = 5.0681709706128295, l1: 0.00010721422770255051, l2: 0.00039960286860830143   Iteration 92 of 100, tot loss = 5.03307344213776, l1: 0.00010649950850758772, l2: 0.00039680783495145   Iteration 93 of 100, tot loss = 5.0158802014525214, l1: 0.0001062034497926918, l2: 0.000395384569582279   Iteration 94 of 100, tot loss = 4.987727049817431, l1: 0.00010564530462130627, l2: 0.0003931273994509111   Iteration 95 of 100, tot loss = 4.975847125053406, l1: 0.0001056022940534412, l2: 0.0003919824175107734   Iteration 96 of 100, tot loss = 4.961197218547265, l1: 0.00010536241037092016, l2: 0.00039075731046978035   Iteration 97 of 100, tot loss = 4.963701854047087, l1: 0.00010532538164118977, l2: 0.0003910448026216887   Iteration 98 of 100, tot loss = 4.973651254663662, l1: 0.00010540116770188229, l2: 0.00039196395649626967   Iteration 99 of 100, tot loss = 4.964021881421407, l1: 0.0001053120332389054, l2: 0.00039109015338136953   Iteration 100 of 100, tot loss = 4.956563297510147, l1: 0.00010521690805035178, l2: 0.0003904394201526884
   End of epoch 1307; saving model... 

Epoch 1308 of 2000
   Iteration 1 of 100, tot loss = 7.376684665679932, l1: 0.00013976926857139915, l2: 0.0005978991976007819   Iteration 2 of 100, tot loss = 6.263926029205322, l1: 0.00013087298430036753, l2: 0.0004955196054652333   Iteration 3 of 100, tot loss = 6.0116879145304365, l1: 0.0001257773304435735, l2: 0.00047539146423029405   Iteration 4 of 100, tot loss = 5.754111170768738, l1: 0.00012237186274433043, l2: 0.00045303925435291603   Iteration 5 of 100, tot loss = 5.478714656829834, l1: 0.00012080830783816055, l2: 0.00042706315871328117   Iteration 6 of 100, tot loss = 5.564043839772542, l1: 0.00012575647148575322, l2: 0.0004306479192261274   Iteration 7 of 100, tot loss = 5.4137749671936035, l1: 0.00012497612000775656, l2: 0.0004164013828683112   Iteration 8 of 100, tot loss = 5.280923008918762, l1: 0.00011868918954860419, l2: 0.0004094031173735857   Iteration 9 of 100, tot loss = 5.368287510342068, l1: 0.00012073815742041916, l2: 0.00041609059553593397   Iteration 10 of 100, tot loss = 5.312676429748535, l1: 0.00012081467211828567, l2: 0.0004104529740288854   Iteration 11 of 100, tot loss = 5.32930495522239, l1: 0.00012188029028369452, l2: 0.00041105020632544023   Iteration 12 of 100, tot loss = 5.275654633839925, l1: 0.00012126718502258882, l2: 0.0004062982795100349   Iteration 13 of 100, tot loss = 5.255124495579646, l1: 0.00012024984831581466, l2: 0.0004052626004872414   Iteration 14 of 100, tot loss = 5.1282176460538595, l1: 0.00011858456491609104, l2: 0.00039423719863407314   Iteration 15 of 100, tot loss = 5.119617382685344, l1: 0.00011945282603846863, l2: 0.000392508910348018   Iteration 16 of 100, tot loss = 5.00284318625927, l1: 0.00011700419736371259, l2: 0.0003832801194221247   Iteration 17 of 100, tot loss = 4.987723027958589, l1: 0.0001175092368023744, l2: 0.0003812630636864068   Iteration 18 of 100, tot loss = 4.900664567947388, l1: 0.00011365363186794436, l2: 0.00037641282283907966   Iteration 19 of 100, tot loss = 4.9708227609333235, l1: 0.00011432234743168872, l2: 0.0003827599269386969   Iteration 20 of 100, tot loss = 4.904189932346344, l1: 0.00011423656396800653, l2: 0.0003761824278626591   Iteration 21 of 100, tot loss = 4.776887201127552, l1: 0.00011199746603822513, l2: 0.0003656912525738811   Iteration 22 of 100, tot loss = 4.796559951522133, l1: 0.00011206491961440241, l2: 0.00036759107288989156   Iteration 23 of 100, tot loss = 4.8882396635801895, l1: 0.00011392672351338779, l2: 0.0003748972414304381   Iteration 24 of 100, tot loss = 4.851724396149318, l1: 0.00011392879150662338, l2: 0.0003712436458348141   Iteration 25 of 100, tot loss = 4.748724908828735, l1: 0.00011099295617896132, l2: 0.00036387953266967085   Iteration 26 of 100, tot loss = 4.685268759727478, l1: 0.00010985299391345157, l2: 0.00035867387990807544   Iteration 27 of 100, tot loss = 4.6532003084818525, l1: 0.0001095158180827706, l2: 0.0003558042102398489   Iteration 28 of 100, tot loss = 4.701883554458618, l1: 0.00010945601973487231, l2: 0.00036073233318997415   Iteration 29 of 100, tot loss = 4.812796905122954, l1: 0.00011087984967161901, l2: 0.0003703998382693028   Iteration 30 of 100, tot loss = 4.822710037231445, l1: 0.00011066580397406748, l2: 0.00037160519714234396   Iteration 31 of 100, tot loss = 4.820477577947801, l1: 0.00011113010388667575, l2: 0.0003709176507395422   Iteration 32 of 100, tot loss = 4.819250270724297, l1: 0.00011179190653365367, l2: 0.00037013311748523847   Iteration 33 of 100, tot loss = 4.843650355483547, l1: 0.00011269745840708669, l2: 0.00037166757454432434   Iteration 34 of 100, tot loss = 4.779663548750036, l1: 0.00011101926675645005, l2: 0.00036694708507960006   Iteration 35 of 100, tot loss = 4.77163714000157, l1: 0.00011074330177507363, l2: 0.00036642040899356027   Iteration 36 of 100, tot loss = 4.762874788708157, l1: 0.00011041956865180207, l2: 0.00036586790722342103   Iteration 37 of 100, tot loss = 4.787225890803981, l1: 0.00011002612970785137, l2: 0.00036869645692924087   Iteration 38 of 100, tot loss = 4.825332127119365, l1: 0.00011118754101564838, l2: 0.0003713456700145463   Iteration 39 of 100, tot loss = 4.805536698072385, l1: 0.00011070079880226284, l2: 0.00036985286883114336   Iteration 40 of 100, tot loss = 4.853601396083832, l1: 0.00011122181012979127, l2: 0.00037413832651509436   Iteration 41 of 100, tot loss = 4.877862220857201, l1: 0.00011096202273975218, l2: 0.0003768241969249552   Iteration 42 of 100, tot loss = 4.951078017552693, l1: 0.00011260545952176298, l2: 0.00038250233982190754   Iteration 43 of 100, tot loss = 4.992563059163648, l1: 0.00011328573541398983, l2: 0.0003859705686282237   Iteration 44 of 100, tot loss = 4.956017575480721, l1: 0.00011260119084578368, l2: 0.00038300056522447534   Iteration 45 of 100, tot loss = 5.032811286714342, l1: 0.00011342016942863766, l2: 0.0003898609581584525   Iteration 46 of 100, tot loss = 5.021100692127062, l1: 0.00011298537917711023, l2: 0.0003891246891251522   Iteration 47 of 100, tot loss = 5.018704815113798, l1: 0.00011294207433820702, l2: 0.00038892840638390166   Iteration 48 of 100, tot loss = 5.007335767149925, l1: 0.00011305692798183979, l2: 0.00038767664743015   Iteration 49 of 100, tot loss = 5.014933036298168, l1: 0.000113021834870463, l2: 0.00038847146818309796   Iteration 50 of 100, tot loss = 5.0171594190597535, l1: 0.00011270258481090423, l2: 0.0003890133564709686   Iteration 51 of 100, tot loss = 4.989748286265953, l1: 0.00011158817363263784, l2: 0.00038738665427766084   Iteration 52 of 100, tot loss = 4.9807803860077495, l1: 0.00011145725917371098, l2: 0.00038662077853447187   Iteration 53 of 100, tot loss = 4.958904387815943, l1: 0.0001110279057306712, l2: 0.00038486253223074426   Iteration 54 of 100, tot loss = 4.92073107207263, l1: 0.00011033166918894965, l2: 0.000381741436959365   Iteration 55 of 100, tot loss = 4.9164370840246026, l1: 0.00011025563647060401, l2: 0.00038138807100370864   Iteration 56 of 100, tot loss = 4.90220736180033, l1: 0.00010963723473521116, l2: 0.0003805835001133216   Iteration 57 of 100, tot loss = 4.953373034795125, l1: 0.00011052756599383429, l2: 0.0003848097370345178   Iteration 58 of 100, tot loss = 4.952456149561652, l1: 0.00011059243755378953, l2: 0.00038465317705011897   Iteration 59 of 100, tot loss = 4.929780790361307, l1: 0.00011046395488616111, l2: 0.00038251412368450565   Iteration 60 of 100, tot loss = 4.932206137975057, l1: 0.00011069632734385474, l2: 0.0003825242856692057   Iteration 61 of 100, tot loss = 4.9265023372212395, l1: 0.00011054505035853716, l2: 0.00038210518269792015   Iteration 62 of 100, tot loss = 4.9024544992754535, l1: 0.00010962144610417571, l2: 0.0003806240032821323   Iteration 63 of 100, tot loss = 4.908843206980872, l1: 0.00010969326490700792, l2: 0.0003811910557689973   Iteration 64 of 100, tot loss = 4.921682886779308, l1: 0.00011020522839544356, l2: 0.00038196306036297756   Iteration 65 of 100, tot loss = 4.894652880155123, l1: 0.00010977995099472956, l2: 0.0003796853369119792   Iteration 66 of 100, tot loss = 4.885115283908266, l1: 0.00010985241296470066, l2: 0.0003786591151958529   Iteration 67 of 100, tot loss = 4.875202698494071, l1: 0.00011009125369331633, l2: 0.00037742901568252373   Iteration 68 of 100, tot loss = 4.888116450870738, l1: 0.00011044595098877941, l2: 0.0003783656936909264   Iteration 69 of 100, tot loss = 4.854541453762331, l1: 0.00010989706793950155, l2: 0.00037555707693673617   Iteration 70 of 100, tot loss = 4.87915814944676, l1: 0.00011012834755612338, l2: 0.0003777874669010219   Iteration 71 of 100, tot loss = 4.847495975628705, l1: 0.00010980596546777262, l2: 0.00037494363159951296   Iteration 72 of 100, tot loss = 4.832560376988517, l1: 0.0001094211578573676, l2: 0.0003738348795094579   Iteration 73 of 100, tot loss = 4.8334819747977065, l1: 0.00010969919537884591, l2: 0.000373649001855097   Iteration 74 of 100, tot loss = 4.834106893152804, l1: 0.00010965777437377255, l2: 0.0003737529143502281   Iteration 75 of 100, tot loss = 4.813463045756023, l1: 0.00010922041251130092, l2: 0.00037212589173577724   Iteration 76 of 100, tot loss = 4.803642894092359, l1: 0.00010883586262360423, l2: 0.0003715284267520091   Iteration 77 of 100, tot loss = 4.790485360405662, l1: 0.00010851569289104203, l2: 0.0003705328430938111   Iteration 78 of 100, tot loss = 4.8039675278541365, l1: 0.00010880970097414087, l2: 0.00037158705153836845   Iteration 79 of 100, tot loss = 4.789189365845692, l1: 0.0001082454722494777, l2: 0.00037067346392741685   Iteration 80 of 100, tot loss = 4.790806546807289, l1: 0.0001083287606434169, l2: 0.00037075189356983174   Iteration 81 of 100, tot loss = 4.808824047630216, l1: 0.00010836417232546668, l2: 0.00037251823160647885   Iteration 82 of 100, tot loss = 4.830596688317089, l1: 0.00010838463361035725, l2: 0.000374675034502771   Iteration 83 of 100, tot loss = 4.8176752343235245, l1: 0.00010826836533161408, l2: 0.00037349915729431685   Iteration 84 of 100, tot loss = 4.788220212573097, l1: 0.00010774808630404074, l2: 0.0003710739342967004   Iteration 85 of 100, tot loss = 4.812611551845775, l1: 0.00010825504248393426, l2: 0.0003730061125037644   Iteration 86 of 100, tot loss = 4.795980095863342, l1: 0.00010807870814967955, l2: 0.00037151930121390886   Iteration 87 of 100, tot loss = 4.7715029113594145, l1: 0.00010763357078821142, l2: 0.0003695167200010934   Iteration 88 of 100, tot loss = 4.739645346999168, l1: 0.00010684439629668222, l2: 0.0003671201380215776   Iteration 89 of 100, tot loss = 4.731516903705812, l1: 0.000106890037151582, l2: 0.00036626165288662686   Iteration 90 of 100, tot loss = 4.714421392811669, l1: 0.00010650616297627695, l2: 0.0003649359761362171   Iteration 91 of 100, tot loss = 4.715049543223539, l1: 0.00010671254562771787, l2: 0.00036479240832824753   Iteration 92 of 100, tot loss = 4.740103725505912, l1: 0.00010688347316792478, l2: 0.000367126898788189   Iteration 93 of 100, tot loss = 4.721479863248845, l1: 0.00010665196419057626, l2: 0.0003654960216203284   Iteration 94 of 100, tot loss = 4.7119650371531225, l1: 0.00010621109601182051, l2: 0.00036498540708983435   Iteration 95 of 100, tot loss = 4.716604246591267, l1: 0.00010606486114403723, l2: 0.0003655955630452617   Iteration 96 of 100, tot loss = 4.721145400156577, l1: 0.00010627053870848613, l2: 0.00036584400064991013   Iteration 97 of 100, tot loss = 4.722054293475201, l1: 0.00010641625674006999, l2: 0.00036578917168992925   Iteration 98 of 100, tot loss = 4.70945560567233, l1: 0.00010632219568740728, l2: 0.00036462336393757437   Iteration 99 of 100, tot loss = 4.710552286620092, l1: 0.00010646595146314174, l2: 0.00036458927588866593   Iteration 100 of 100, tot loss = 4.706461714506149, l1: 0.00010607620286464226, l2: 0.0003645699674962088
   End of epoch 1308; saving model... 

Epoch 1309 of 2000
   Iteration 1 of 100, tot loss = 3.0455188751220703, l1: 8.862309186952189e-05, l2: 0.0002159287832910195   Iteration 2 of 100, tot loss = 2.720656394958496, l1: 7.243463187478483e-05, l2: 0.00019963100203312933   Iteration 3 of 100, tot loss = 3.982869307200114, l1: 8.861537450381245e-05, l2: 0.00030967154695341986   Iteration 4 of 100, tot loss = 3.836232542991638, l1: 8.513325701642316e-05, l2: 0.0002984899838338606   Iteration 5 of 100, tot loss = 4.231104183197021, l1: 9.259655635105447e-05, l2: 0.00033051385544240474   Iteration 6 of 100, tot loss = 4.193330446879069, l1: 8.85972391794591e-05, l2: 0.00033073580319372314   Iteration 7 of 100, tot loss = 4.3444951602390836, l1: 9.029697373210053e-05, l2: 0.00034415254153178206   Iteration 8 of 100, tot loss = 4.523338139057159, l1: 9.765941558725899e-05, l2: 0.0003546743973856792   Iteration 9 of 100, tot loss = 4.4722885555691185, l1: 9.869145125978523e-05, l2: 0.0003485374036245048   Iteration 10 of 100, tot loss = 4.461471176147461, l1: 9.646837643231265e-05, l2: 0.0003496787394396961   Iteration 11 of 100, tot loss = 4.525721593336626, l1: 9.717806229177354e-05, l2: 0.0003553940914571285   Iteration 12 of 100, tot loss = 4.576455434163411, l1: 9.907363831492451e-05, l2: 0.000358571899899592   Iteration 13 of 100, tot loss = 4.368859859613272, l1: 9.473288306393303e-05, l2: 0.00034215309782526817   Iteration 14 of 100, tot loss = 4.2853032520839145, l1: 9.479165133338288e-05, l2: 0.0003337386702436821   Iteration 15 of 100, tot loss = 4.531627209981282, l1: 9.955199348041788e-05, l2: 0.00035361072417193403   Iteration 16 of 100, tot loss = 4.486043319106102, l1: 9.900693703457364e-05, l2: 0.0003495973933240748   Iteration 17 of 100, tot loss = 4.361663285423727, l1: 9.706213270820787e-05, l2: 0.0003391041947693071   Iteration 18 of 100, tot loss = 4.4462253252665205, l1: 9.917010538629256e-05, l2: 0.000345452425083042   Iteration 19 of 100, tot loss = 4.373202248623497, l1: 9.805461549579999e-05, l2: 0.00033926560794689546   Iteration 20 of 100, tot loss = 4.477574896812439, l1: 9.901932753564324e-05, l2: 0.0003487381603918038   Iteration 21 of 100, tot loss = 4.407364198139736, l1: 9.892290004894935e-05, l2: 0.0003418135186491002   Iteration 22 of 100, tot loss = 4.411329562013799, l1: 9.873271483229473e-05, l2: 0.00034240024119340393   Iteration 23 of 100, tot loss = 4.420954071957132, l1: 9.900344973531269e-05, l2: 0.00034309195736726826   Iteration 24 of 100, tot loss = 4.411819686492284, l1: 9.869471887213876e-05, l2: 0.0003424872502364451   Iteration 25 of 100, tot loss = 4.431251974105835, l1: 9.941233525751159e-05, l2: 0.0003437128633959219   Iteration 26 of 100, tot loss = 4.459139484625596, l1: 9.96028496816647e-05, l2: 0.0003463111010765155   Iteration 27 of 100, tot loss = 4.516681432723999, l1: 0.00010033710363872901, l2: 0.0003513310420438992   Iteration 28 of 100, tot loss = 4.493643036910465, l1: 0.00010011820527974383, l2: 0.0003492461000860203   Iteration 29 of 100, tot loss = 4.496404080555357, l1: 0.00010035384184448048, l2: 0.0003492865678332425   Iteration 30 of 100, tot loss = 4.597867592175802, l1: 0.00010234481330068471, l2: 0.0003574419470775562   Iteration 31 of 100, tot loss = 4.581311848855788, l1: 0.0001028420639716299, l2: 0.0003552891219207536   Iteration 32 of 100, tot loss = 4.531332515180111, l1: 0.00010212608162873948, l2: 0.0003510071715027152   Iteration 33 of 100, tot loss = 4.5235982157967305, l1: 0.0001019285528772426, l2: 0.00035043127056930893   Iteration 34 of 100, tot loss = 4.582569059203653, l1: 0.00010248843190150664, l2: 0.00035576847585616633   Iteration 35 of 100, tot loss = 4.644112689154489, l1: 0.00010411306866444647, l2: 0.000360298202888641   Iteration 36 of 100, tot loss = 4.677140851815541, l1: 0.00010521966073105836, l2: 0.00036249442645283934   Iteration 37 of 100, tot loss = 4.6654855302862215, l1: 0.00010487973289658643, l2: 0.00036166882263559446   Iteration 38 of 100, tot loss = 4.6448444692712085, l1: 0.00010454954348173679, l2: 0.0003599349059329956   Iteration 39 of 100, tot loss = 4.613460736396985, l1: 0.00010420026489718554, l2: 0.00035714581108783395   Iteration 40 of 100, tot loss = 4.6093486785888675, l1: 0.00010377887538197683, l2: 0.00035715599478862713   Iteration 41 of 100, tot loss = 4.5542228687100295, l1: 0.00010279904541321007, l2: 0.00035262324388718187   Iteration 42 of 100, tot loss = 4.603499372800191, l1: 0.00010312616116910552, l2: 0.00035722377894368645   Iteration 43 of 100, tot loss = 4.616807898809744, l1: 0.00010360534699514595, l2: 0.0003580754459700222   Iteration 44 of 100, tot loss = 4.598064623095772, l1: 0.00010324449348528023, l2: 0.00035656197161932306   Iteration 45 of 100, tot loss = 4.672477144665188, l1: 0.00010424888751003891, l2: 0.0003629988295465915   Iteration 46 of 100, tot loss = 4.629818242529164, l1: 0.0001035593364656012, l2: 0.00035942249024114773   Iteration 47 of 100, tot loss = 4.645190867971866, l1: 0.00010350888732454165, l2: 0.0003610102028184709   Iteration 48 of 100, tot loss = 4.650303175052007, l1: 0.00010374749369172302, l2: 0.00036128282681602286   Iteration 49 of 100, tot loss = 4.620070223905603, l1: 0.00010346070826481267, l2: 0.00035854631664269433   Iteration 50 of 100, tot loss = 4.628190860748291, l1: 0.00010369837385951542, l2: 0.0003591207147110254   Iteration 51 of 100, tot loss = 4.6518859769783765, l1: 0.00010412778119287253, l2: 0.0003610608189631546   Iteration 52 of 100, tot loss = 4.616950612801772, l1: 0.00010345348402132423, l2: 0.00035824157972596225   Iteration 53 of 100, tot loss = 4.616335490964493, l1: 0.00010393207721447327, l2: 0.00035770147416151987   Iteration 54 of 100, tot loss = 4.612282708839134, l1: 0.00010404890970137246, l2: 0.0003571793632646505   Iteration 55 of 100, tot loss = 4.63225562355735, l1: 0.00010442275511608882, l2: 0.0003588028090730817   Iteration 56 of 100, tot loss = 4.602876237460545, l1: 0.00010432223384019121, l2: 0.0003559653915934697   Iteration 57 of 100, tot loss = 4.582373020941751, l1: 0.00010371728077236759, l2: 0.0003545200230100339   Iteration 58 of 100, tot loss = 4.561033544869258, l1: 0.00010340874573087385, l2: 0.00035269461057333535   Iteration 59 of 100, tot loss = 4.575469041274766, l1: 0.00010364788995811052, l2: 0.0003538990165876477   Iteration 60 of 100, tot loss = 4.58700266679128, l1: 0.00010384778627970566, l2: 0.00035485248251158435   Iteration 61 of 100, tot loss = 4.579524102758189, l1: 0.00010369961664317268, l2: 0.00035425279548078714   Iteration 62 of 100, tot loss = 4.59109458615703, l1: 0.00010403194896606428, l2: 0.00035507751149012735   Iteration 63 of 100, tot loss = 4.59955988989936, l1: 0.00010391561365741793, l2: 0.00035604037735278586   Iteration 64 of 100, tot loss = 4.598447695374489, l1: 0.00010376949478541064, l2: 0.0003560752772955311   Iteration 65 of 100, tot loss = 4.607119237459623, l1: 0.00010373661755763281, l2: 0.0003569753086104846   Iteration 66 of 100, tot loss = 4.606784271471428, l1: 0.00010366850752192973, l2: 0.0003570099217926547   Iteration 67 of 100, tot loss = 4.6032715057259175, l1: 0.000103390763023757, l2: 0.0003569363900929439   Iteration 68 of 100, tot loss = 4.590820003958309, l1: 0.00010326064704499645, l2: 0.0003558213560181149   Iteration 69 of 100, tot loss = 4.607732883398084, l1: 0.00010341639943468128, l2: 0.00035735689195554596   Iteration 70 of 100, tot loss = 4.577493725504194, l1: 0.0001029374314904479, l2: 0.0003548119441672627   Iteration 71 of 100, tot loss = 4.6050990030799115, l1: 0.0001033281232722261, l2: 0.0003571817798959747   Iteration 72 of 100, tot loss = 4.601065950261222, l1: 0.00010313799238146102, l2: 0.00035696860545107885   Iteration 73 of 100, tot loss = 4.580269228922178, l1: 0.00010274756930637722, l2: 0.0003552793565824985   Iteration 74 of 100, tot loss = 4.5899240745080485, l1: 0.00010302816244311052, l2: 0.0003559642480812161   Iteration 75 of 100, tot loss = 4.60825452486674, l1: 0.00010338755799845482, l2: 0.0003574378970855226   Iteration 76 of 100, tot loss = 4.605720234544654, l1: 0.00010316382682312426, l2: 0.0003574081990671785   Iteration 77 of 100, tot loss = 4.6299426555633545, l1: 0.00010353834130166268, l2: 0.0003594559271118374   Iteration 78 of 100, tot loss = 4.653246949880551, l1: 0.0001038283726069121, l2: 0.0003614963251703347   Iteration 79 of 100, tot loss = 4.655017439323135, l1: 0.00010373833522206479, l2: 0.0003617634113341617   Iteration 80 of 100, tot loss = 4.642307251691818, l1: 0.00010362917710153852, l2: 0.00036060155034647325   Iteration 81 of 100, tot loss = 4.632938161308383, l1: 0.0001032410816858626, l2: 0.0003600527365680462   Iteration 82 of 100, tot loss = 4.645363400622112, l1: 0.00010345172676152731, l2: 0.00036108461542270803   Iteration 83 of 100, tot loss = 4.647635833326593, l1: 0.00010364020842194827, l2: 0.00036112337719637855   Iteration 84 of 100, tot loss = 4.636314116773152, l1: 0.00010369800752197902, l2: 0.00035993340653866265   Iteration 85 of 100, tot loss = 4.652134275436401, l1: 0.00010391116991420002, l2: 0.0003613022601177149   Iteration 86 of 100, tot loss = 4.653923991114595, l1: 0.00010387808863523005, l2: 0.0003615143132899606   Iteration 87 of 100, tot loss = 4.654273759359601, l1: 0.00010379653980342629, l2: 0.00036163083926624693   Iteration 88 of 100, tot loss = 4.623656462539326, l1: 0.00010308297413013696, l2: 0.00035928267524551217   Iteration 89 of 100, tot loss = 4.624959967109595, l1: 0.00010297615047553397, l2: 0.00035951984931814244   Iteration 90 of 100, tot loss = 4.615300501717462, l1: 0.0001030540767614083, l2: 0.0003584759764231017   Iteration 91 of 100, tot loss = 4.621050572657323, l1: 0.00010323739959378624, l2: 0.0003588676609669474   Iteration 92 of 100, tot loss = 4.620831069738968, l1: 0.00010350721122845046, l2: 0.0003585758988818903   Iteration 93 of 100, tot loss = 4.603339843852545, l1: 0.00010324312629514894, l2: 0.0003570908612023378   Iteration 94 of 100, tot loss = 4.589718935337473, l1: 0.00010310501497866774, l2: 0.00035586688148001445   Iteration 95 of 100, tot loss = 4.597220420837402, l1: 0.00010317589480301218, l2: 0.00035654614988935035   Iteration 96 of 100, tot loss = 4.6320818811655045, l1: 0.00010370086037407115, l2: 0.00035950733051019296   Iteration 97 of 100, tot loss = 4.644878480852265, l1: 0.00010407842258705528, l2: 0.0003604094280930927   Iteration 98 of 100, tot loss = 4.640366213662284, l1: 0.00010406149399665194, l2: 0.0003599751301463788   Iteration 99 of 100, tot loss = 4.660373379485776, l1: 0.000104237037878119, l2: 0.00036180030270550174   Iteration 100 of 100, tot loss = 4.662605118751526, l1: 0.00010440067166200606, l2: 0.00036185984252369964
   End of epoch 1309; saving model... 

Epoch 1310 of 2000
   Iteration 1 of 100, tot loss = 1.6490010023117065, l1: 3.758590537472628e-05, l2: 0.0001273141970159486   Iteration 2 of 100, tot loss = 2.9211944937705994, l1: 6.783200842619408e-05, l2: 0.00022428745432989672   Iteration 3 of 100, tot loss = 3.7977529764175415, l1: 8.306560266646557e-05, l2: 0.0002967097146514182   Iteration 4 of 100, tot loss = 3.7703119814395905, l1: 8.34565189506975e-05, l2: 0.00029357469247770496   Iteration 5 of 100, tot loss = 3.884574055671692, l1: 8.668716254760511e-05, l2: 0.00030177024600561706   Iteration 6 of 100, tot loss = 3.855868875980377, l1: 8.903341161688634e-05, l2: 0.00029655347801356885   Iteration 7 of 100, tot loss = 4.456771765436445, l1: 0.00010011143448147257, l2: 0.0003455657507791849   Iteration 8 of 100, tot loss = 4.484653398394585, l1: 0.00010163251545236562, l2: 0.0003468328341114102   Iteration 9 of 100, tot loss = 4.5580999453862505, l1: 0.00010108000545490843, l2: 0.0003547299986368873   Iteration 10 of 100, tot loss = 4.61740597486496, l1: 0.00010425213804410305, l2: 0.0003574884656700306   Iteration 11 of 100, tot loss = 4.708473194729198, l1: 0.00010592705505339175, l2: 0.00036492026860783386   Iteration 12 of 100, tot loss = 4.678526868422826, l1: 0.00010696810416751153, l2: 0.00036088458606779267   Iteration 13 of 100, tot loss = 4.784567713737488, l1: 0.00010999361956447292, l2: 0.00036846315644932195   Iteration 14 of 100, tot loss = 5.060574761458805, l1: 0.000112985682530312, l2: 0.00039307180122705176   Iteration 15 of 100, tot loss = 5.017330447832744, l1: 0.00011311250539923398, l2: 0.00038862054643686863   Iteration 16 of 100, tot loss = 5.245710991322994, l1: 0.000115344585310595, l2: 0.00040922651987784775   Iteration 17 of 100, tot loss = 5.322276080355925, l1: 0.00011571332909036701, l2: 0.00041651428653173805   Iteration 18 of 100, tot loss = 5.221635083357493, l1: 0.00011340769217187901, l2: 0.00040875582312259614   Iteration 19 of 100, tot loss = 5.11349019878789, l1: 0.00011093013534647118, l2: 0.00040041889128348743   Iteration 20 of 100, tot loss = 5.057336431741715, l1: 0.00011018541390512837, l2: 0.0003955482352466788   Iteration 21 of 100, tot loss = 5.083601480438595, l1: 0.00010998226051534792, l2: 0.0003983778941411791   Iteration 22 of 100, tot loss = 5.022956138307398, l1: 0.00010930953414406923, l2: 0.0003929860864776525   Iteration 23 of 100, tot loss = 5.049611241921134, l1: 0.00011041066935159388, l2: 0.0003945504627747299   Iteration 24 of 100, tot loss = 4.910536929965019, l1: 0.00010774889202972797, l2: 0.000383304808565299   Iteration 25 of 100, tot loss = 4.878238453865051, l1: 0.00010817303176736459, l2: 0.00037965082039590923   Iteration 26 of 100, tot loss = 4.839438516360063, l1: 0.00010865196852067199, l2: 0.0003752918894931029   Iteration 27 of 100, tot loss = 4.808146799052203, l1: 0.00010821725488683485, l2: 0.00037259743085424243   Iteration 28 of 100, tot loss = 4.784026099102838, l1: 0.00010795292447645417, l2: 0.00037044969056816105   Iteration 29 of 100, tot loss = 4.847480547839198, l1: 0.00010843659853482426, l2: 0.00037631146250501405   Iteration 30 of 100, tot loss = 4.861204818884532, l1: 0.00010877505652994539, l2: 0.0003773454329348169   Iteration 31 of 100, tot loss = 4.940846862331513, l1: 0.00011014145509820551, l2: 0.00038394323905043665   Iteration 32 of 100, tot loss = 4.921461630612612, l1: 0.00011009409172402229, l2: 0.00038205207920327666   Iteration 33 of 100, tot loss = 4.964381669506882, l1: 0.00011133653187602194, l2: 0.000385101642512547   Iteration 34 of 100, tot loss = 4.9052696543581344, l1: 0.00011058687446433502, l2: 0.00037994009771329514   Iteration 35 of 100, tot loss = 4.878257673127311, l1: 0.00011010027748333024, l2: 0.00037772549632271484   Iteration 36 of 100, tot loss = 4.839284741216236, l1: 0.00010947968540777866, l2: 0.00037444879530489235   Iteration 37 of 100, tot loss = 4.8348884485863355, l1: 0.00010918582238200297, l2: 0.00037430302944739123   Iteration 38 of 100, tot loss = 4.84931071494755, l1: 0.00010945123990256291, l2: 0.0003754798381123692   Iteration 39 of 100, tot loss = 4.812821079523135, l1: 0.00010911995233096278, l2: 0.00037216216188449506   Iteration 40 of 100, tot loss = 4.934507092833519, l1: 0.00011089239669672679, l2: 0.00038255831896094605   Iteration 41 of 100, tot loss = 4.925477234328666, l1: 0.00011043774003799005, l2: 0.0003821099888272129   Iteration 42 of 100, tot loss = 4.946580248219626, l1: 0.00011027593202362325, l2: 0.0003843820976769729   Iteration 43 of 100, tot loss = 5.00299246089403, l1: 0.0001113460641855179, l2: 0.000388953187042674   Iteration 44 of 100, tot loss = 4.951357724991712, l1: 0.00010999167261616094, l2: 0.00038514410533190875   Iteration 45 of 100, tot loss = 5.007845860057407, l1: 0.00011085407337911117, l2: 0.00038993051825349944   Iteration 46 of 100, tot loss = 4.980458666449008, l1: 0.00011007537495575177, l2: 0.0003879704970092027   Iteration 47 of 100, tot loss = 5.003785115607242, l1: 0.00011059311718588318, l2: 0.0003897853990139915   Iteration 48 of 100, tot loss = 5.001165084540844, l1: 0.00011107295866471152, l2: 0.00038904355490861536   Iteration 49 of 100, tot loss = 5.015014397854707, l1: 0.00011143140957811942, l2: 0.0003900700352720118   Iteration 50 of 100, tot loss = 5.00053893327713, l1: 0.00011131517392641399, l2: 0.0003887387239956297   Iteration 51 of 100, tot loss = 4.985324142025966, l1: 0.00011070776328743508, l2: 0.0003878246555127678   Iteration 52 of 100, tot loss = 4.972026011118522, l1: 0.0001104789819762048, l2: 0.0003867236236356593   Iteration 53 of 100, tot loss = 5.010880823405284, l1: 0.00011115968739449152, l2: 0.000389928399872931   Iteration 54 of 100, tot loss = 5.018658260504405, l1: 0.00011130817768355848, l2: 0.00039055765335905125   Iteration 55 of 100, tot loss = 4.991309623284773, l1: 0.00011085251879359764, l2: 0.000388278448372148   Iteration 56 of 100, tot loss = 4.974793755582401, l1: 0.00010990022786115463, l2: 0.00038757915301955236   Iteration 57 of 100, tot loss = 4.924131717598229, l1: 0.00010918957687273066, l2: 0.000383223600230147   Iteration 58 of 100, tot loss = 4.927826476508174, l1: 0.0001093738076100447, l2: 0.0003834088452878922   Iteration 59 of 100, tot loss = 4.920404399855662, l1: 0.00010881522276746583, l2: 0.0003832252230001929   Iteration 60 of 100, tot loss = 4.895450490713119, l1: 0.00010863415785327864, l2: 0.00038091089688047455   Iteration 61 of 100, tot loss = 4.881226525932062, l1: 0.00010820782705702529, l2: 0.00037991483080330627   Iteration 62 of 100, tot loss = 4.924205528151605, l1: 0.00010899473307935912, l2: 0.00038342582481545244   Iteration 63 of 100, tot loss = 4.922986115728106, l1: 0.00010900752039604067, l2: 0.00038329109578164267   Iteration 64 of 100, tot loss = 4.907770102843642, l1: 0.00010854736763121764, l2: 0.0003822296473572351   Iteration 65 of 100, tot loss = 4.889618633343623, l1: 0.00010793600130665044, l2: 0.0003810258665068362   Iteration 66 of 100, tot loss = 4.921978901733052, l1: 0.0001088114554339413, l2: 0.0003833864389117948   Iteration 67 of 100, tot loss = 4.928651626430341, l1: 0.00010902348698599876, l2: 0.0003838416794054349   Iteration 68 of 100, tot loss = 4.938174733344247, l1: 0.00010851584835001952, l2: 0.0003853016282579012   Iteration 69 of 100, tot loss = 4.934260418449623, l1: 0.0001087786134820013, l2: 0.00038464743103258365   Iteration 70 of 100, tot loss = 4.92381923028401, l1: 0.00010869468325316639, l2: 0.0003836872424082165   Iteration 71 of 100, tot loss = 4.895165468605471, l1: 0.00010858338666853534, l2: 0.0003809331626054162   Iteration 72 of 100, tot loss = 4.891746413376596, l1: 0.0001083674211865097, l2: 0.0003808072223263379   Iteration 73 of 100, tot loss = 4.913713113902366, l1: 0.00010879223814237326, l2: 0.0003825790752746063   Iteration 74 of 100, tot loss = 4.945666234235506, l1: 0.00010887804219728914, l2: 0.00038568858321554755   Iteration 75 of 100, tot loss = 4.928980978329976, l1: 0.00010860608580211798, l2: 0.0003842920138655851   Iteration 76 of 100, tot loss = 4.9155012886775165, l1: 0.00010855926723850548, l2: 0.0003829908635357003   Iteration 77 of 100, tot loss = 4.901804120509656, l1: 0.00010842808294367021, l2: 0.00038175233080378406   Iteration 78 of 100, tot loss = 4.88726608416973, l1: 0.00010813220750349454, l2: 0.0003805944023695655   Iteration 79 of 100, tot loss = 4.899701249750355, l1: 0.00010854042690730642, l2: 0.0003814296992171461   Iteration 80 of 100, tot loss = 4.898564217984676, l1: 0.00010848044312297133, l2: 0.0003813759798504179   Iteration 81 of 100, tot loss = 4.905836845621651, l1: 0.00010859796083546646, l2: 0.0003819857249007687   Iteration 82 of 100, tot loss = 4.914552553397853, l1: 0.00010877025426186553, l2: 0.00038268500207866565   Iteration 83 of 100, tot loss = 4.9089029596512574, l1: 0.00010883787447721304, l2: 0.00038205242241699116   Iteration 84 of 100, tot loss = 4.901177876052403, l1: 0.0001087054669845938, l2: 0.0003814123213357691   Iteration 85 of 100, tot loss = 4.875883692853591, l1: 0.00010828144406653283, l2: 0.00037930692607701265   Iteration 86 of 100, tot loss = 4.868734682715217, l1: 0.00010772482304208436, l2: 0.0003791486459979138   Iteration 87 of 100, tot loss = 4.860093375732159, l1: 0.00010767103849920905, l2: 0.00037833829992450774   Iteration 88 of 100, tot loss = 4.86869942735542, l1: 0.00010747406593674731, l2: 0.00037939587723072196   Iteration 89 of 100, tot loss = 4.866744632131598, l1: 0.00010721413830682385, l2: 0.00037946032551169564   Iteration 90 of 100, tot loss = 4.8305813299285045, l1: 0.00010653542271433657, l2: 0.00037652271098017486   Iteration 91 of 100, tot loss = 4.842544837312384, l1: 0.00010669855216222529, l2: 0.0003775559318735479   Iteration 92 of 100, tot loss = 4.84223809838295, l1: 0.00010674818863860665, l2: 0.0003774756217393113   Iteration 93 of 100, tot loss = 4.820719504869112, l1: 0.00010666572024904302, l2: 0.0003754062306133389   Iteration 94 of 100, tot loss = 4.842280992802153, l1: 0.0001067976876235681, l2: 0.00037743041197090666   Iteration 95 of 100, tot loss = 4.8401684823789095, l1: 0.00010673937362490075, l2: 0.00037727747495457727   Iteration 96 of 100, tot loss = 4.837532700349887, l1: 0.00010653620207297838, l2: 0.0003772170680349518   Iteration 97 of 100, tot loss = 4.842960707920113, l1: 0.00010647566557157373, l2: 0.0003778204052125766   Iteration 98 of 100, tot loss = 4.856170806349541, l1: 0.00010674507512351052, l2: 0.00037887200522497864   Iteration 99 of 100, tot loss = 4.824682434399922, l1: 0.00010602793774909735, l2: 0.00037644030538039525   Iteration 100 of 100, tot loss = 4.822227650880814, l1: 0.0001056716951643466, l2: 0.0003765510694938712
   End of epoch 1310; saving model... 

Epoch 1311 of 2000
   Iteration 1 of 100, tot loss = 3.0024731159210205, l1: 8.601149602327496e-05, l2: 0.0002142358134733513   Iteration 2 of 100, tot loss = 3.906831383705139, l1: 9.59720964601729e-05, l2: 0.0002947110406239517   Iteration 3 of 100, tot loss = 3.80833371480306, l1: 8.72532570307764e-05, l2: 0.00029358011185346794   Iteration 4 of 100, tot loss = 5.374441146850586, l1: 0.00011143230040033814, l2: 0.0004260118257661816   Iteration 5 of 100, tot loss = 4.882320928573608, l1: 0.00010675264347810298, l2: 0.0003814794559730217   Iteration 6 of 100, tot loss = 5.003152012825012, l1: 0.0001072652072859152, l2: 0.0003930499976074013   Iteration 7 of 100, tot loss = 4.7583534717559814, l1: 0.00010245637843451862, l2: 0.00037337897187431475   Iteration 8 of 100, tot loss = 4.602284610271454, l1: 0.00010170133100473322, l2: 0.00035852713153872173   Iteration 9 of 100, tot loss = 4.538074122534858, l1: 9.968649909650494e-05, l2: 0.00035412091367308877   Iteration 10 of 100, tot loss = 4.401790380477905, l1: 9.767825758899562e-05, l2: 0.00034250078024342654   Iteration 11 of 100, tot loss = 4.377207105809992, l1: 9.933904816121371e-05, l2: 0.0003383816616736691   Iteration 12 of 100, tot loss = 4.252812763055165, l1: 9.708431086134321e-05, l2: 0.00032819696571095847   Iteration 13 of 100, tot loss = 4.359822805111225, l1: 0.00010080744742971056, l2: 0.00033517483545718   Iteration 14 of 100, tot loss = 4.386347242764065, l1: 0.00010110070745992874, l2: 0.0003375340196985884   Iteration 15 of 100, tot loss = 4.2996686140696205, l1: 9.762455495850493e-05, l2: 0.0003323423084414875   Iteration 16 of 100, tot loss = 4.299035921692848, l1: 9.815087832976133e-05, l2: 0.0003317527161925682   Iteration 17 of 100, tot loss = 4.432854610330918, l1: 0.00010147766442969441, l2: 0.0003418077976675704   Iteration 18 of 100, tot loss = 4.646336886617872, l1: 0.00010472693813628414, l2: 0.0003599067511256888   Iteration 19 of 100, tot loss = 4.699820104398225, l1: 0.00010554054729015518, l2: 0.00036444146366863464   Iteration 20 of 100, tot loss = 4.748877894878388, l1: 0.00010535731598793063, l2: 0.000369530471652979   Iteration 21 of 100, tot loss = 4.763552608944121, l1: 0.00010509750808566986, l2: 0.00037125774889275255   Iteration 22 of 100, tot loss = 4.788070516152815, l1: 0.00010578830816402015, l2: 0.0003730187408192168   Iteration 23 of 100, tot loss = 4.741536897161732, l1: 0.00010596337676321602, l2: 0.00036819030988819736   Iteration 24 of 100, tot loss = 4.724321275949478, l1: 0.00010503643746536302, l2: 0.0003673956871352857   Iteration 25 of 100, tot loss = 4.722358980178833, l1: 0.00010515657690120862, l2: 0.0003670793172204867   Iteration 26 of 100, tot loss = 4.7089373423503, l1: 0.00010499271845149521, l2: 0.0003659010124885334   Iteration 27 of 100, tot loss = 4.760083278020223, l1: 0.00010591040551878029, l2: 0.00037009791844024286   Iteration 28 of 100, tot loss = 4.718918297971998, l1: 0.00010564651113652092, l2: 0.0003662453148405932   Iteration 29 of 100, tot loss = 4.777767041633869, l1: 0.00010551078117589049, l2: 0.00037226592068126873   Iteration 30 of 100, tot loss = 4.738085254033407, l1: 0.000104778154733746, l2: 0.00036903036865017685   Iteration 31 of 100, tot loss = 4.686585426330566, l1: 0.00010392554747428926, l2: 0.0003647329928841622   Iteration 32 of 100, tot loss = 4.716367289423943, l1: 0.00010479354500603222, l2: 0.0003668431822916318   Iteration 33 of 100, tot loss = 4.6928244215069395, l1: 0.00010474397078558633, l2: 0.00036453846956702006   Iteration 34 of 100, tot loss = 4.7002357314614684, l1: 0.00010477767628959983, l2: 0.00036524589514181786   Iteration 35 of 100, tot loss = 4.667483976909092, l1: 0.00010430488577445171, l2: 0.0003624435102600338   Iteration 36 of 100, tot loss = 4.625524719556172, l1: 0.00010316434721365415, l2: 0.0003593881228880491   Iteration 37 of 100, tot loss = 4.684703942891714, l1: 0.0001039631203417918, l2: 0.000364507272811229   Iteration 38 of 100, tot loss = 4.623150204357348, l1: 0.00010301671021519916, l2: 0.00035929830942462245   Iteration 39 of 100, tot loss = 4.695095801964785, l1: 0.00010317746208061297, l2: 0.00036633211726812314   Iteration 40 of 100, tot loss = 4.687849253416061, l1: 0.00010316141524526756, l2: 0.00036562350978783795   Iteration 41 of 100, tot loss = 4.6630305139029895, l1: 0.00010257784849339415, l2: 0.0003637252024107999   Iteration 42 of 100, tot loss = 4.685829997062683, l1: 0.00010331522484886504, l2: 0.00036526777417055287   Iteration 43 of 100, tot loss = 4.683213683061822, l1: 0.00010301720765202717, l2: 0.0003653041599453752   Iteration 44 of 100, tot loss = 4.7642685337500135, l1: 0.00010413721082097089, l2: 0.00037228964191110043   Iteration 45 of 100, tot loss = 4.7909554110633, l1: 0.00010471734017806335, l2: 0.00037437820024529677   Iteration 46 of 100, tot loss = 4.783004869585452, l1: 0.00010424416857700714, l2: 0.0003740563175676431   Iteration 47 of 100, tot loss = 4.784504570859544, l1: 0.00010455740653498574, l2: 0.00037389304990473977   Iteration 48 of 100, tot loss = 4.779477382699649, l1: 0.00010431748281310622, l2: 0.0003736302548228802   Iteration 49 of 100, tot loss = 4.762107260373174, l1: 0.00010375486008528316, l2: 0.00037245586548922396   Iteration 50 of 100, tot loss = 4.735591106414795, l1: 0.00010367821654654108, l2: 0.00036988089355872945   Iteration 51 of 100, tot loss = 4.7476563921161725, l1: 0.00010407863052986453, l2: 0.0003706870087554824   Iteration 52 of 100, tot loss = 4.73767458475553, l1: 0.0001041439132117935, l2: 0.00036962354547890404   Iteration 53 of 100, tot loss = 4.750684729162252, l1: 0.00010405811258780612, l2: 0.00037101036094717754   Iteration 54 of 100, tot loss = 4.7771543926662865, l1: 0.00010493410122257451, l2: 0.00037278133822837844   Iteration 55 of 100, tot loss = 4.76058196587996, l1: 0.00010435942994345995, l2: 0.00037169876661871307   Iteration 56 of 100, tot loss = 4.7448044163840155, l1: 0.00010410806200005547, l2: 0.00037037237962067593   Iteration 57 of 100, tot loss = 4.751998039714077, l1: 0.00010383986904216407, l2: 0.00037135993484856985   Iteration 58 of 100, tot loss = 4.735800048400616, l1: 0.0001040466492574116, l2: 0.00036953335526551857   Iteration 59 of 100, tot loss = 4.690397141343456, l1: 0.00010295416211454845, l2: 0.000366085551785684   Iteration 60 of 100, tot loss = 4.700609223047892, l1: 0.00010358443851146148, l2: 0.0003664764837594703   Iteration 61 of 100, tot loss = 4.688182029567781, l1: 0.00010352209371421105, l2: 0.0003652961091634619   Iteration 62 of 100, tot loss = 4.728185596004609, l1: 0.00010401670859268521, l2: 0.00036880185108466616   Iteration 63 of 100, tot loss = 4.722344379576426, l1: 0.00010343300706904663, l2: 0.00036880143110581216   Iteration 64 of 100, tot loss = 4.7476749531924725, l1: 0.00010399000660754609, l2: 0.0003707774890244764   Iteration 65 of 100, tot loss = 4.724204254150391, l1: 0.00010372301952044766, l2: 0.000368697406240524   Iteration 66 of 100, tot loss = 4.702309908288898, l1: 0.00010344482137897461, l2: 0.0003667861700788225   Iteration 67 of 100, tot loss = 4.7114760021665205, l1: 0.00010374580689579068, l2: 0.00036740179391709895   Iteration 68 of 100, tot loss = 4.712307386538562, l1: 0.00010413956704938114, l2: 0.00036709117236622497   Iteration 69 of 100, tot loss = 4.67926828066508, l1: 0.00010355305947587772, l2: 0.00036437376937997675   Iteration 70 of 100, tot loss = 4.692131297928946, l1: 0.0001037371521046485, l2: 0.0003654759778458226   Iteration 71 of 100, tot loss = 4.699483277092518, l1: 0.00010359674061092705, l2: 0.00036635158749946564   Iteration 72 of 100, tot loss = 4.684520125389099, l1: 0.0001032142879214209, l2: 0.0003652377250141904   Iteration 73 of 100, tot loss = 4.68385686613109, l1: 0.00010321706706659601, l2: 0.0003651686198929682   Iteration 74 of 100, tot loss = 4.686330331338419, l1: 0.00010299983453453908, l2: 0.0003656331987381359   Iteration 75 of 100, tot loss = 4.709535535176595, l1: 0.00010330424493683191, l2: 0.0003676493091431136   Iteration 76 of 100, tot loss = 4.691286930912419, l1: 0.00010264837888634677, l2: 0.00036648031508727737   Iteration 77 of 100, tot loss = 4.706871871824388, l1: 0.00010274112317748912, l2: 0.00036794606459405663   Iteration 78 of 100, tot loss = 4.700476845105489, l1: 0.00010264392567011945, l2: 0.0003674037596703364   Iteration 79 of 100, tot loss = 4.724120722541326, l1: 0.00010297770905374738, l2: 0.0003694343644774908   Iteration 80 of 100, tot loss = 4.7473216444253925, l1: 0.00010348655309826427, l2: 0.0003712456129505881   Iteration 81 of 100, tot loss = 4.747355987996231, l1: 0.00010353273480884834, l2: 0.0003712028658440061   Iteration 82 of 100, tot loss = 4.771752732556041, l1: 0.00010387058975494455, l2: 0.0003733046850822752   Iteration 83 of 100, tot loss = 4.750679412520076, l1: 0.00010320333222555648, l2: 0.00037186461061110206   Iteration 84 of 100, tot loss = 4.720541902950832, l1: 0.00010267747490745802, l2: 0.00036937671687891927   Iteration 85 of 100, tot loss = 4.729149330363554, l1: 0.00010279016539296957, l2: 0.00037012476874652374   Iteration 86 of 100, tot loss = 4.711215698441793, l1: 0.00010239110976494144, l2: 0.00036873046100968063   Iteration 87 of 100, tot loss = 4.6957788138554015, l1: 0.00010215003362384453, l2: 0.0003674278487705615   Iteration 88 of 100, tot loss = 4.70884676954963, l1: 0.00010233653975691678, l2: 0.00036854813796790865   Iteration 89 of 100, tot loss = 4.72496705108814, l1: 0.00010272439661941994, l2: 0.0003697723093026663   Iteration 90 of 100, tot loss = 4.744890265994602, l1: 0.00010305989429374071, l2: 0.00037142913351999595   Iteration 91 of 100, tot loss = 4.744117024180653, l1: 0.0001029573304749447, l2: 0.0003714543733052439   Iteration 92 of 100, tot loss = 4.76528379191523, l1: 0.00010355821007618468, l2: 0.0003729701700491284   Iteration 93 of 100, tot loss = 4.7624055698353756, l1: 0.00010352388218512148, l2: 0.00037271667585422793   Iteration 94 of 100, tot loss = 4.783006541272427, l1: 0.0001040768730742089, l2: 0.00037422378196385985   Iteration 95 of 100, tot loss = 4.777491408900211, l1: 0.00010395667046204357, l2: 0.00037379247108842   Iteration 96 of 100, tot loss = 4.799243311087291, l1: 0.00010427860157354492, l2: 0.0003756457304007199   Iteration 97 of 100, tot loss = 4.792757516054763, l1: 0.0001041692871460571, l2: 0.0003751064652214595   Iteration 98 of 100, tot loss = 4.772123563046358, l1: 0.00010380209415434736, l2: 0.00037341026294672366   Iteration 99 of 100, tot loss = 4.789276327749695, l1: 0.00010396037833568772, l2: 0.00037496725527682537   Iteration 100 of 100, tot loss = 4.794362437725067, l1: 0.00010387157952209236, l2: 0.0003755646650097333
   End of epoch 1311; saving model... 

Epoch 1312 of 2000
   Iteration 1 of 100, tot loss = 4.063878059387207, l1: 9.665294055594131e-05, l2: 0.0003097348380833864   Iteration 2 of 100, tot loss = 4.745357513427734, l1: 0.0001086523407138884, l2: 0.0003658833884401247   Iteration 3 of 100, tot loss = 4.884685675303142, l1: 0.00010397627678078909, l2: 0.00038449228547203046   Iteration 4 of 100, tot loss = 5.3378986120224, l1: 0.0001123808542615734, l2: 0.00042140900768572465   Iteration 5 of 100, tot loss = 5.073126411437988, l1: 0.00010835884313564748, l2: 0.00039895379450172186   Iteration 6 of 100, tot loss = 5.318323850631714, l1: 0.0001127009066597869, l2: 0.00041913148015737534   Iteration 7 of 100, tot loss = 5.29917710168021, l1: 0.00011404528465521122, l2: 0.0004158724290651402   Iteration 8 of 100, tot loss = 4.979529410600662, l1: 0.00010838806610991014, l2: 0.0003895648769685067   Iteration 9 of 100, tot loss = 4.9689319133758545, l1: 0.00010720278644132325, l2: 0.00038969040744834475   Iteration 10 of 100, tot loss = 5.036590504646301, l1: 0.00010586778953438624, l2: 0.00039779126236680893   Iteration 11 of 100, tot loss = 4.998173215172508, l1: 0.00010513138377890837, l2: 0.00039468593736687166   Iteration 12 of 100, tot loss = 4.989128530025482, l1: 0.0001035180236309922, l2: 0.0003953948277436818   Iteration 13 of 100, tot loss = 4.776475337835459, l1: 9.960847707519021e-05, l2: 0.0003780390554591297   Iteration 14 of 100, tot loss = 4.99020915372031, l1: 0.0001033691925320974, l2: 0.00039565172275095914   Iteration 15 of 100, tot loss = 5.0502912680308025, l1: 0.00010414227678362901, l2: 0.00040088684666746605   Iteration 16 of 100, tot loss = 4.911868304014206, l1: 0.00010170528594244388, l2: 0.00038948154087847797   Iteration 17 of 100, tot loss = 4.884994815377628, l1: 0.00010299874317270758, l2: 0.0003855007329120246   Iteration 18 of 100, tot loss = 5.109498527314928, l1: 0.00010600478748933205, l2: 0.00040494505972472124   Iteration 19 of 100, tot loss = 5.1549884896529345, l1: 0.00010744906237385677, l2: 0.0004080497823643351   Iteration 20 of 100, tot loss = 5.149795818328857, l1: 0.00010680399464035872, l2: 0.000408175584016135   Iteration 21 of 100, tot loss = 5.150343577067058, l1: 0.00010746025925202827, l2: 0.00040757409615686075   Iteration 22 of 100, tot loss = 5.093119241974571, l1: 0.00010669638769203712, l2: 0.0004026155346020294   Iteration 23 of 100, tot loss = 5.068093932193259, l1: 0.00010763368355464596, l2: 0.0003991757080454946   Iteration 24 of 100, tot loss = 5.148309876521428, l1: 0.00010908907067156785, l2: 0.0004057419167414385   Iteration 25 of 100, tot loss = 5.092852697372437, l1: 0.0001084512984380126, l2: 0.00040083397121634337   Iteration 26 of 100, tot loss = 5.020848787747896, l1: 0.00010737359564070805, l2: 0.00039471128324154194   Iteration 27 of 100, tot loss = 5.0252908247488515, l1: 0.00010624691532467734, l2: 0.00039628216742399944   Iteration 28 of 100, tot loss = 5.051505310194833, l1: 0.00010665204562038915, l2: 0.0003984984858626766   Iteration 29 of 100, tot loss = 5.053998782716948, l1: 0.00010767335183557188, l2: 0.0003977265271181562   Iteration 30 of 100, tot loss = 5.075043312708536, l1: 0.0001082344463308497, l2: 0.00039926988635367405   Iteration 31 of 100, tot loss = 5.08280449528848, l1: 0.00010948449597626384, l2: 0.0003987959550408226   Iteration 32 of 100, tot loss = 5.023265980184078, l1: 0.0001085640385554143, l2: 0.0003937625606340589   Iteration 33 of 100, tot loss = 4.953439430757002, l1: 0.00010751442746506214, l2: 0.0003878295172012947   Iteration 34 of 100, tot loss = 4.973301866475274, l1: 0.00010763974098474992, l2: 0.00038969044801299735   Iteration 35 of 100, tot loss = 4.968168497085571, l1: 0.00010801955379845043, l2: 0.0003887972973253844   Iteration 36 of 100, tot loss = 5.019424219926198, l1: 0.00010889357367381713, l2: 0.0003930488484103181   Iteration 37 of 100, tot loss = 4.964689493179321, l1: 0.00010822464871641903, l2: 0.0003882443002972601   Iteration 38 of 100, tot loss = 5.085967170564752, l1: 0.00011041544138080146, l2: 0.0003981812758291581   Iteration 39 of 100, tot loss = 5.054234504699707, l1: 0.0001098520731783281, l2: 0.0003955713775492488   Iteration 40 of 100, tot loss = 5.02744374871254, l1: 0.00010956251553579932, l2: 0.0003931818599085091   Iteration 41 of 100, tot loss = 5.0232453753308555, l1: 0.00010976452425139298, l2: 0.00039256001338533057   Iteration 42 of 100, tot loss = 5.026987160955157, l1: 0.00011025116535685291, l2: 0.0003924475503957919   Iteration 43 of 100, tot loss = 5.0279344237127965, l1: 0.00010999463390975888, l2: 0.00039279880848182584   Iteration 44 of 100, tot loss = 4.970555614341389, l1: 0.00010847958004557189, l2: 0.00038857598139491694   Iteration 45 of 100, tot loss = 4.95040258301629, l1: 0.00010830492069039287, l2: 0.0003867353374112604   Iteration 46 of 100, tot loss = 4.965317016062529, l1: 0.00010813876408223143, l2: 0.00038839293721020624   Iteration 47 of 100, tot loss = 4.976323112528375, l1: 0.00010783526073631019, l2: 0.0003897970504538612   Iteration 48 of 100, tot loss = 4.940625901023547, l1: 0.00010671281719017618, l2: 0.0003873497726090136   Iteration 49 of 100, tot loss = 4.890844778138764, l1: 0.00010608610734509836, l2: 0.0003829983699168744   Iteration 50 of 100, tot loss = 4.874023909568787, l1: 0.00010582048649666831, l2: 0.0003815819034934975   Iteration 51 of 100, tot loss = 4.880317271924486, l1: 0.00010605076956790963, l2: 0.0003819809567443479   Iteration 52 of 100, tot loss = 4.896422344904679, l1: 0.00010654288696689316, l2: 0.00038309934657613316   Iteration 53 of 100, tot loss = 4.8775370975710315, l1: 0.00010630160085263856, l2: 0.0003814521080370606   Iteration 54 of 100, tot loss = 4.830302304691738, l1: 0.00010524620618660399, l2: 0.0003777840234020173   Iteration 55 of 100, tot loss = 4.841175092350353, l1: 0.00010563447503955103, l2: 0.00037848303274420854   Iteration 56 of 100, tot loss = 4.82059918982642, l1: 0.00010511631113071676, l2: 0.0003769436064300992   Iteration 57 of 100, tot loss = 4.835822573879309, l1: 0.00010561991260609622, l2: 0.0003779623437909769   Iteration 58 of 100, tot loss = 4.831303686931215, l1: 0.0001057503433975367, l2: 0.00037738002419214825   Iteration 59 of 100, tot loss = 4.854659452276715, l1: 0.00010600601493378991, l2: 0.0003794599294867682   Iteration 60 of 100, tot loss = 4.853151893615722, l1: 0.0001061091770073593, l2: 0.00037920601171208544   Iteration 61 of 100, tot loss = 4.882022380828857, l1: 0.00010677879385423052, l2: 0.00038142344336117024   Iteration 62 of 100, tot loss = 4.87152644126646, l1: 0.00010687005165128804, l2: 0.0003802825921147521   Iteration 63 of 100, tot loss = 4.893646384042407, l1: 0.0001071598950754075, l2: 0.0003822047428767537   Iteration 64 of 100, tot loss = 4.872484892606735, l1: 0.00010710046313988641, l2: 0.0003801480256697687   Iteration 65 of 100, tot loss = 4.878071154080905, l1: 0.00010702058291197038, l2: 0.0003807865320525777   Iteration 66 of 100, tot loss = 4.886214682550142, l1: 0.00010708619541246938, l2: 0.00038153527279835987   Iteration 67 of 100, tot loss = 4.908646967873644, l1: 0.00010742428237980635, l2: 0.000383440414686868   Iteration 68 of 100, tot loss = 4.908327663646025, l1: 0.00010758674103699912, l2: 0.00038324602578273593   Iteration 69 of 100, tot loss = 4.928372535152712, l1: 0.00010802187735216516, l2: 0.0003848153772170021   Iteration 70 of 100, tot loss = 4.906618796076093, l1: 0.00010766145647461859, l2: 0.00038300042423153563   Iteration 71 of 100, tot loss = 4.896395572474305, l1: 0.00010742621883072786, l2: 0.00038221333926858405   Iteration 72 of 100, tot loss = 4.858762035767238, l1: 0.00010668608415976955, l2: 0.0003791901203334823   Iteration 73 of 100, tot loss = 4.880286272257974, l1: 0.0001073600041097164, l2: 0.00038066862469175446   Iteration 74 of 100, tot loss = 4.85068179465629, l1: 0.00010655913728122548, l2: 0.00037850904364195476   Iteration 75 of 100, tot loss = 4.85477798461914, l1: 0.00010674010855533803, l2: 0.00037873769159584   Iteration 76 of 100, tot loss = 4.8309314250946045, l1: 0.0001063517400708454, l2: 0.00037674140414884806   Iteration 77 of 100, tot loss = 4.827233320706851, l1: 0.0001060333975678455, l2: 0.00037668993607369044   Iteration 78 of 100, tot loss = 4.816938076263819, l1: 0.00010626191625562616, l2: 0.000375431892909826   Iteration 79 of 100, tot loss = 4.8243794561941415, l1: 0.00010667910966639042, l2: 0.0003757588373780062   Iteration 80 of 100, tot loss = 4.803616264462471, l1: 0.00010636482775225887, l2: 0.00037399680004455147   Iteration 81 of 100, tot loss = 4.817256948094309, l1: 0.00010621345795642148, l2: 0.0003755122385804116   Iteration 82 of 100, tot loss = 4.800430085600876, l1: 0.0001061659703093649, l2: 0.00037387703994726305   Iteration 83 of 100, tot loss = 4.838114586221167, l1: 0.00010663719605149545, l2: 0.0003771742643488302   Iteration 84 of 100, tot loss = 4.840571644760313, l1: 0.00010676796323206785, l2: 0.0003772892033295994   Iteration 85 of 100, tot loss = 4.822408886516795, l1: 0.0001063443672405008, l2: 0.00037589652364498335   Iteration 86 of 100, tot loss = 4.819834368173466, l1: 0.00010644323104548507, l2: 0.0003755402080863606   Iteration 87 of 100, tot loss = 4.791557232538859, l1: 0.00010566486459026038, l2: 0.00037349086092924166   Iteration 88 of 100, tot loss = 4.805108170617711, l1: 0.00010589621206236188, l2: 0.0003746146070294675   Iteration 89 of 100, tot loss = 4.814821079875646, l1: 0.00010594458953154356, l2: 0.0003755375205962971   Iteration 90 of 100, tot loss = 4.831042530801561, l1: 0.00010600964948631978, l2: 0.00037709460561422426   Iteration 91 of 100, tot loss = 4.831061198161199, l1: 0.0001059950199915265, l2: 0.0003771111020526675   Iteration 92 of 100, tot loss = 4.8548851194589036, l1: 0.00010627078806820741, l2: 0.0003792177262569211   Iteration 93 of 100, tot loss = 4.8455218909889135, l1: 0.00010629374606964199, l2: 0.0003782584452726227   Iteration 94 of 100, tot loss = 4.828442730802171, l1: 0.0001060167610279329, l2: 0.00037682751420280955   Iteration 95 of 100, tot loss = 4.827245842783075, l1: 0.0001062994466337841, l2: 0.0003764251397162872   Iteration 96 of 100, tot loss = 4.83046367764473, l1: 0.00010640805411791614, l2: 0.00037663831547736965   Iteration 97 of 100, tot loss = 4.827457133027696, l1: 0.0001060218207394709, l2: 0.0003767238942469403   Iteration 98 of 100, tot loss = 4.845493744830696, l1: 0.00010632578767176803, l2: 0.00037822358836467394   Iteration 99 of 100, tot loss = 4.847841802269522, l1: 0.00010641959306667559, l2: 0.0003783645886208185   Iteration 100 of 100, tot loss = 4.818333340883255, l1: 0.0001059938225444057, l2: 0.0003758395129989367
   End of epoch 1312; saving model... 

Epoch 1313 of 2000
   Iteration 1 of 100, tot loss = 5.411106586456299, l1: 0.00013889375259168446, l2: 0.00040221685776486993   Iteration 2 of 100, tot loss = 4.114231705665588, l1: 0.00011253504635533318, l2: 0.0002988881024066359   Iteration 3 of 100, tot loss = 4.875343243281047, l1: 0.00011524443592255314, l2: 0.0003722898739700516   Iteration 4 of 100, tot loss = 4.282203137874603, l1: 0.00010441855920362286, l2: 0.0003238017379771918   Iteration 5 of 100, tot loss = 3.943550539016724, l1: 0.00010085531539516523, l2: 0.0002934997290140018   Iteration 6 of 100, tot loss = 4.142883817354838, l1: 0.00010592750792663234, l2: 0.0003083608632247585   Iteration 7 of 100, tot loss = 4.1854797431400845, l1: 0.00010740961235049846, l2: 0.0003111383515975571   Iteration 8 of 100, tot loss = 4.363834112882614, l1: 0.00011017359338438837, l2: 0.0003262098089180654   Iteration 9 of 100, tot loss = 4.437436925040351, l1: 0.00011226344455887254, l2: 0.0003314802419885786   Iteration 10 of 100, tot loss = 4.751029419898987, l1: 0.00011605203835642896, l2: 0.0003590508989873342   Iteration 11 of 100, tot loss = 4.676815856586803, l1: 0.00011250682886880399, l2: 0.00035517475308469414   Iteration 12 of 100, tot loss = 4.487080752849579, l1: 0.00010743923454962594, l2: 0.0003412688383832574   Iteration 13 of 100, tot loss = 4.358785629272461, l1: 0.00010421405666140624, l2: 0.0003316645038117153   Iteration 14 of 100, tot loss = 4.509086915424892, l1: 0.00010676271501454591, l2: 0.00034414597757859156   Iteration 15 of 100, tot loss = 4.438386090596517, l1: 0.00010321424730742971, l2: 0.0003406243621914958   Iteration 16 of 100, tot loss = 4.286702930927277, l1: 9.967359687834687e-05, l2: 0.0003289966971351532   Iteration 17 of 100, tot loss = 4.432564398821662, l1: 0.00010128546594333944, l2: 0.0003419709751409862   Iteration 18 of 100, tot loss = 4.473280323876275, l1: 0.00010052573694944537, l2: 0.00034680229631097365   Iteration 19 of 100, tot loss = 4.428693683523881, l1: 9.945054300356117e-05, l2: 0.0003434188256505877   Iteration 20 of 100, tot loss = 4.4470282912254335, l1: 9.967470268748002e-05, l2: 0.00034502812777645884   Iteration 21 of 100, tot loss = 4.674779562723069, l1: 0.00010370439589938282, l2: 0.00036377356099408296   Iteration 22 of 100, tot loss = 4.602412462234497, l1: 0.00010146783725676042, l2: 0.000358773409971036   Iteration 23 of 100, tot loss = 4.64901992549067, l1: 0.00010279675505286002, l2: 0.0003621052386259417   Iteration 24 of 100, tot loss = 4.695659359296163, l1: 0.00010433746865601279, l2: 0.0003652284697939952   Iteration 25 of 100, tot loss = 4.735936965942383, l1: 0.00010427834058646113, l2: 0.00036931535811163485   Iteration 26 of 100, tot loss = 4.757633227568406, l1: 0.00010410248628781679, l2: 0.00037166083795734897   Iteration 27 of 100, tot loss = 4.749202357398139, l1: 0.00010364202791135069, l2: 0.00037127820865458087   Iteration 28 of 100, tot loss = 4.756416440010071, l1: 0.00010340623703086749, l2: 0.0003722354075372485   Iteration 29 of 100, tot loss = 4.771960998403615, l1: 0.00010313464460115687, l2: 0.00037406145590999776   Iteration 30 of 100, tot loss = 4.805242617925008, l1: 0.0001033315587847028, l2: 0.00037719270427866527   Iteration 31 of 100, tot loss = 4.780709020553097, l1: 0.00010361315399941597, l2: 0.00037445774914005833   Iteration 32 of 100, tot loss = 4.741956330835819, l1: 0.00010283258438903431, l2: 0.0003713630503625609   Iteration 33 of 100, tot loss = 4.733118469064886, l1: 0.0001025839830452407, l2: 0.00037072786521589893   Iteration 34 of 100, tot loss = 4.720112555167255, l1: 0.0001023211437260376, l2: 0.0003696901129681946   Iteration 35 of 100, tot loss = 4.690457303183419, l1: 0.00010269560215030132, l2: 0.00036635012947954236   Iteration 36 of 100, tot loss = 4.676766037940979, l1: 0.0001027016694731881, l2: 0.00036497493581717007   Iteration 37 of 100, tot loss = 4.722767249957935, l1: 0.0001038823025442685, l2: 0.000368394424062776   Iteration 38 of 100, tot loss = 4.673050503981741, l1: 0.00010305284510985458, l2: 0.0003642522068204064   Iteration 39 of 100, tot loss = 4.725352654090295, l1: 0.00010402564154131911, l2: 0.0003685096266846626   Iteration 40 of 100, tot loss = 4.738633549213409, l1: 0.00010405290213384432, l2: 0.00036981045486754737   Iteration 41 of 100, tot loss = 4.738588856487739, l1: 0.00010429033633355597, l2: 0.00036956855129995726   Iteration 42 of 100, tot loss = 4.758310317993164, l1: 0.00010418760489084802, l2: 0.00037164342938922346   Iteration 43 of 100, tot loss = 4.730751065320747, l1: 0.00010438265971335864, l2: 0.000368692449207396   Iteration 44 of 100, tot loss = 4.6770376129583875, l1: 0.00010361187725555448, l2: 0.0003640918866122692   Iteration 45 of 100, tot loss = 4.686832804150051, l1: 0.00010366712020994681, l2: 0.0003650161622014518   Iteration 46 of 100, tot loss = 4.714354136715764, l1: 0.0001044820251674695, l2: 0.00036695339108321247   Iteration 47 of 100, tot loss = 4.763175695500475, l1: 0.00010484331141924486, l2: 0.000371474260897772   Iteration 48 of 100, tot loss = 4.784120197097461, l1: 0.00010494316878369621, l2: 0.00037346885346778436   Iteration 49 of 100, tot loss = 4.824410404477801, l1: 0.00010504450649617011, l2: 0.0003773965364992048   Iteration 50 of 100, tot loss = 4.809418082237244, l1: 0.00010450046698679216, l2: 0.00037644134397851303   Iteration 51 of 100, tot loss = 4.787955031675451, l1: 0.00010405612784761972, l2: 0.00037473937760189394   Iteration 52 of 100, tot loss = 4.745781219922579, l1: 0.00010326878075899843, l2: 0.00037130934344112087   Iteration 53 of 100, tot loss = 4.741802854358025, l1: 0.00010339305505490388, l2: 0.0003707872328616433   Iteration 54 of 100, tot loss = 4.713167442215814, l1: 0.00010328609857458139, l2: 0.0003680306477623095   Iteration 55 of 100, tot loss = 4.6626869440078735, l1: 0.0001024072912431703, l2: 0.00036386140511074865   Iteration 56 of 100, tot loss = 4.651122559394155, l1: 0.00010184304658521017, l2: 0.00036326921092820285   Iteration 57 of 100, tot loss = 4.613102630565041, l1: 0.00010114015809417571, l2: 0.000360170106624106   Iteration 58 of 100, tot loss = 4.606317867492807, l1: 0.00010047407873759672, l2: 0.00036015771008257206   Iteration 59 of 100, tot loss = 4.591229675179821, l1: 9.972930058858642e-05, l2: 0.00035939366858594627   Iteration 60 of 100, tot loss = 4.5741150359312694, l1: 9.960248735296773e-05, l2: 0.0003578090181690641   Iteration 61 of 100, tot loss = 4.558244288944807, l1: 9.89921587222584e-05, l2: 0.0003568322720296192   Iteration 62 of 100, tot loss = 4.546825683886005, l1: 9.905614012583805e-05, l2: 0.0003556264296827477   Iteration 63 of 100, tot loss = 4.583934903144836, l1: 9.978280222784388e-05, l2: 0.00035861068976200406   Iteration 64 of 100, tot loss = 4.596894575282931, l1: 9.99664039795789e-05, l2: 0.0003597230552259134   Iteration 65 of 100, tot loss = 4.573915340350225, l1: 9.96774096655337e-05, l2: 0.00035771412568954896   Iteration 66 of 100, tot loss = 4.585895608771931, l1: 9.986136804703467e-05, l2: 0.0003587281940836516   Iteration 67 of 100, tot loss = 4.612393704812918, l1: 0.00010065745577301289, l2: 0.0003605819156970273   Iteration 68 of 100, tot loss = 4.617031852988636, l1: 0.00010076926184196696, l2: 0.000360933924693039   Iteration 69 of 100, tot loss = 4.606934583705405, l1: 0.000100789710774299, l2: 0.0003599037489671584   Iteration 70 of 100, tot loss = 4.612022721767426, l1: 0.00010073772464238572, l2: 0.0003604645491577685   Iteration 71 of 100, tot loss = 4.624672347391155, l1: 0.00010107857953801318, l2: 0.0003613886566058746   Iteration 72 of 100, tot loss = 4.63661854300234, l1: 0.00010115592143241277, l2: 0.0003625059341882459   Iteration 73 of 100, tot loss = 4.611807811750125, l1: 0.00010084014198358476, l2: 0.0003603406404767007   Iteration 74 of 100, tot loss = 4.621288732902424, l1: 0.00010094847483914409, l2: 0.000361180399596452   Iteration 75 of 100, tot loss = 4.607803168296814, l1: 0.00010063343957881443, l2: 0.0003601468783260013   Iteration 76 of 100, tot loss = 4.599386539898421, l1: 0.00010076469310373987, l2: 0.0003591739619686881   Iteration 77 of 100, tot loss = 4.606549211910793, l1: 0.00010089299053992387, l2: 0.00035976193177773825   Iteration 78 of 100, tot loss = 4.647737122499025, l1: 0.0001017135274219655, l2: 0.00036306018610514747   Iteration 79 of 100, tot loss = 4.660341363918932, l1: 0.00010216629011608083, l2: 0.0003638678472385966   Iteration 80 of 100, tot loss = 4.688379214704037, l1: 0.00010246077049487212, l2: 0.00036637715184042465   Iteration 81 of 100, tot loss = 4.684230025903679, l1: 0.00010218548445271064, l2: 0.00036623751881883056   Iteration 82 of 100, tot loss = 4.695264367068686, l1: 0.00010237401888511291, l2: 0.0003671524189086063   Iteration 83 of 100, tot loss = 4.693915158869272, l1: 0.00010248574217695866, l2: 0.00036690577479634795   Iteration 84 of 100, tot loss = 4.674308006252561, l1: 0.00010187584870263457, l2: 0.00036555495292608003   Iteration 85 of 100, tot loss = 4.6892013732124775, l1: 0.0001019370043222272, l2: 0.00036698313419679726   Iteration 86 of 100, tot loss = 4.666871493638948, l1: 0.00010158185500057338, l2: 0.0003651052955023641   Iteration 87 of 100, tot loss = 4.676368988793472, l1: 0.00010184199659149805, l2: 0.00036579490359604956   Iteration 88 of 100, tot loss = 4.694111435250803, l1: 0.00010232303615835421, l2: 0.0003670881087600719   Iteration 89 of 100, tot loss = 4.7036715156576605, l1: 0.0001025215616424207, l2: 0.0003678455910896569   Iteration 90 of 100, tot loss = 4.683089008596208, l1: 0.0001022183501643465, l2: 0.00036609055176894695   Iteration 91 of 100, tot loss = 4.6717311301074185, l1: 0.0001020715356459662, l2: 0.00036510157847910056   Iteration 92 of 100, tot loss = 4.6776818075905675, l1: 0.00010213374792763953, l2: 0.00036563443395805183   Iteration 93 of 100, tot loss = 4.69211640281062, l1: 0.00010253483083278131, l2: 0.0003666768106506757   Iteration 94 of 100, tot loss = 4.6941519836162, l1: 0.00010266248899186278, l2: 0.0003667527102577956   Iteration 95 of 100, tot loss = 4.723415258056239, l1: 0.00010317531847058011, l2: 0.00036916620784292097   Iteration 96 of 100, tot loss = 4.727033291012049, l1: 0.00010331232772387011, l2: 0.0003693910017924888   Iteration 97 of 100, tot loss = 4.715976497561662, l1: 0.00010310469333114841, l2: 0.0003684929568445329   Iteration 98 of 100, tot loss = 4.703268889261752, l1: 0.00010265242443503147, l2: 0.00036767446489443964   Iteration 99 of 100, tot loss = 4.710313990862683, l1: 0.000102662040694438, l2: 0.0003683693587046229   Iteration 100 of 100, tot loss = 4.708348838090896, l1: 0.00010273186679114587, l2: 0.00036810301709920166
   End of epoch 1313; saving model... 

Epoch 1314 of 2000
   Iteration 1 of 100, tot loss = 4.254481792449951, l1: 0.00010147416469408199, l2: 0.0003239740035496652   Iteration 2 of 100, tot loss = 5.490641117095947, l1: 0.00011371686923666857, l2: 0.0004353472322691232   Iteration 3 of 100, tot loss = 6.506368954976399, l1: 0.0001287774575757794, l2: 0.0005218594257409374   Iteration 4 of 100, tot loss = 5.988662481307983, l1: 0.00012331127800280228, l2: 0.00047555495984852314   Iteration 5 of 100, tot loss = 5.59214038848877, l1: 0.00011284685751888901, l2: 0.00044636717066168783   Iteration 6 of 100, tot loss = 5.53579060236613, l1: 0.0001174781840139379, l2: 0.0004361008711081619   Iteration 7 of 100, tot loss = 5.917246546064105, l1: 0.0001238842212062861, l2: 0.00046784042829780707   Iteration 8 of 100, tot loss = 5.797881782054901, l1: 0.0001224713614647044, l2: 0.0004573168116621673   Iteration 9 of 100, tot loss = 5.642635769314236, l1: 0.00012126366507598302, l2: 0.0004429999066309796   Iteration 10 of 100, tot loss = 5.584955072402954, l1: 0.00012114109849790112, l2: 0.00043735440413001926   Iteration 11 of 100, tot loss = 5.3680574026974766, l1: 0.00011823960415891965, l2: 0.0004185661316362464   Iteration 12 of 100, tot loss = 5.216289182504018, l1: 0.0001177422139638414, l2: 0.00040388669973860186   Iteration 13 of 100, tot loss = 5.049116171323336, l1: 0.00011373942442542802, l2: 0.0003911721891759393   Iteration 14 of 100, tot loss = 4.996610743658883, l1: 0.00011236184946028516, l2: 0.00038729922380298376   Iteration 15 of 100, tot loss = 4.914125521977742, l1: 0.00011159680628528198, l2: 0.000379815746176367   Iteration 16 of 100, tot loss = 4.901854917407036, l1: 0.00011084022844443098, l2: 0.00037934526335448027   Iteration 17 of 100, tot loss = 4.906344175338745, l1: 0.00011007925187347129, l2: 0.00038055516608223757   Iteration 18 of 100, tot loss = 4.970414916674296, l1: 0.00011001751651444162, l2: 0.00038702397756019817   Iteration 19 of 100, tot loss = 4.997901251441554, l1: 0.00011144408582415628, l2: 0.00038834604109931544   Iteration 20 of 100, tot loss = 5.0710675597190855, l1: 0.00011110776395071298, l2: 0.00039599899464519693   Iteration 21 of 100, tot loss = 5.0715178194500155, l1: 0.00011156318942084908, l2: 0.0003955885961962243   Iteration 22 of 100, tot loss = 5.084113760427996, l1: 0.00011243294614938681, l2: 0.00039597843318584967   Iteration 23 of 100, tot loss = 5.054852599683016, l1: 0.00011150391630666411, l2: 0.00039398134559514404   Iteration 24 of 100, tot loss = 5.0621795157591505, l1: 0.00011089157427098446, l2: 0.00039532638038508594   Iteration 25 of 100, tot loss = 4.986131868362427, l1: 0.00010927923518465832, l2: 0.0003893339540809393   Iteration 26 of 100, tot loss = 4.930091518622178, l1: 0.00010810773248802155, l2: 0.00038490142189682677   Iteration 27 of 100, tot loss = 4.905671976230763, l1: 0.0001073960439599533, l2: 0.00038317115589355427   Iteration 28 of 100, tot loss = 4.832740119525364, l1: 0.00010696017488953657, l2: 0.00037631383903707113   Iteration 29 of 100, tot loss = 4.821281909942627, l1: 0.00010770098767885617, l2: 0.00037442720625629843   Iteration 30 of 100, tot loss = 4.767615040143331, l1: 0.00010694966986193321, l2: 0.00036981183705696216   Iteration 31 of 100, tot loss = 4.730956123721215, l1: 0.00010606030381524996, l2: 0.0003670353110083529   Iteration 32 of 100, tot loss = 4.732914283871651, l1: 0.00010587925680738408, l2: 0.00036741217354574474   Iteration 33 of 100, tot loss = 4.681180621638442, l1: 0.00010553363928479622, l2: 0.0003625844251611411   Iteration 34 of 100, tot loss = 4.703833860509536, l1: 0.0001058981557862888, l2: 0.00036448523229303057   Iteration 35 of 100, tot loss = 4.7568510736737935, l1: 0.00010646280258827443, l2: 0.0003692223070954372   Iteration 36 of 100, tot loss = 4.702197730541229, l1: 0.00010596012058764852, l2: 0.00036425965461401374   Iteration 37 of 100, tot loss = 4.68820968189755, l1: 0.00010614365247248143, l2: 0.000362677318342911   Iteration 38 of 100, tot loss = 4.673591293786702, l1: 0.00010549682050814706, l2: 0.0003618623117003345   Iteration 39 of 100, tot loss = 4.657865554858477, l1: 0.00010441564331696823, l2: 0.0003613709148992665   Iteration 40 of 100, tot loss = 4.662590569257736, l1: 0.00010444640756759326, l2: 0.000361812652045046   Iteration 41 of 100, tot loss = 4.65578917177712, l1: 0.00010478443188575755, l2: 0.00036079448785507914   Iteration 42 of 100, tot loss = 4.651171860240755, l1: 0.00010519753569886754, l2: 0.00035991965359287513   Iteration 43 of 100, tot loss = 4.603353456009266, l1: 0.00010455366011524876, l2: 0.00035578168882598535   Iteration 44 of 100, tot loss = 4.6801581599495625, l1: 0.00010577429070095108, l2: 0.0003622415289018218   Iteration 45 of 100, tot loss = 4.648762824800279, l1: 0.00010545356441677237, l2: 0.00035942272127916414   Iteration 46 of 100, tot loss = 4.651465047960696, l1: 0.00010572912161359969, l2: 0.0003594173859987084   Iteration 47 of 100, tot loss = 4.626305757684911, l1: 0.00010517244586384201, l2: 0.00035745813263619836   Iteration 48 of 100, tot loss = 4.6264596829811735, l1: 0.00010544331871642498, l2: 0.00035720265198809403   Iteration 49 of 100, tot loss = 4.630496302429511, l1: 0.00010599143599986801, l2: 0.0003570581968084966   Iteration 50 of 100, tot loss = 4.652401967048645, l1: 0.00010640249965945258, l2: 0.00035883769975043835   Iteration 51 of 100, tot loss = 4.671911319096883, l1: 0.00010717578612350146, l2: 0.0003600153483121711   Iteration 52 of 100, tot loss = 4.632860252490411, l1: 0.00010647621219015072, l2: 0.00035680981566726515   Iteration 53 of 100, tot loss = 4.667360787121755, l1: 0.00010661555567804619, l2: 0.00036012052579229383   Iteration 54 of 100, tot loss = 4.641911237328141, l1: 0.00010637219087651897, l2: 0.0003578189355687721   Iteration 55 of 100, tot loss = 4.663618768345226, l1: 0.00010672690748999065, l2: 0.00035963497172236783   Iteration 56 of 100, tot loss = 4.655368374926703, l1: 0.00010667238581975522, l2: 0.0003588644537688067   Iteration 57 of 100, tot loss = 4.645378217362521, l1: 0.00010631437831570541, l2: 0.000358223445609359   Iteration 58 of 100, tot loss = 4.663311000528006, l1: 0.00010664987461621359, l2: 0.0003596812279409215   Iteration 59 of 100, tot loss = 4.667144803677575, l1: 0.00010703695763861445, l2: 0.0003596775253970271   Iteration 60 of 100, tot loss = 4.637851933638255, l1: 0.00010639650584683598, l2: 0.0003573886899782034   Iteration 61 of 100, tot loss = 4.690748390604238, l1: 0.00010703621824848329, l2: 0.00036203862333929806   Iteration 62 of 100, tot loss = 4.710168065563325, l1: 0.00010714306183452268, l2: 0.0003638737473539227   Iteration 63 of 100, tot loss = 4.700631592008802, l1: 0.00010627228449078439, l2: 0.0003637908773112392   Iteration 64 of 100, tot loss = 4.698019314557314, l1: 0.00010615945751624167, l2: 0.0003636424767137214   Iteration 65 of 100, tot loss = 4.72208159153278, l1: 0.00010645662522829997, l2: 0.00036575153711825036   Iteration 66 of 100, tot loss = 4.706834471586979, l1: 0.00010645145167390265, l2: 0.0003642319982949023   Iteration 67 of 100, tot loss = 4.728424944094757, l1: 0.00010677952241845804, l2: 0.00036606297492911455   Iteration 68 of 100, tot loss = 4.701246279127457, l1: 0.000106463726832076, l2: 0.00036366090417312295   Iteration 69 of 100, tot loss = 4.726548198340596, l1: 0.00010663969373706715, l2: 0.00036601512924855763   Iteration 70 of 100, tot loss = 4.70049912588937, l1: 0.00010624006988239542, l2: 0.0003638098459175256   Iteration 71 of 100, tot loss = 4.680192161613787, l1: 0.00010581386539368549, l2: 0.0003622053537442362   Iteration 72 of 100, tot loss = 4.6975360247823925, l1: 0.00010613531104455533, l2: 0.00036361829410856846   Iteration 73 of 100, tot loss = 4.6976185824773085, l1: 0.0001060393531164888, l2: 0.0003637225075691862   Iteration 74 of 100, tot loss = 4.717243220355059, l1: 0.00010666626971018363, l2: 0.00036505805513337   Iteration 75 of 100, tot loss = 4.719944801330566, l1: 0.00010655182535022808, l2: 0.0003654426571059351   Iteration 76 of 100, tot loss = 4.706102135934327, l1: 0.00010627324641781773, l2: 0.00036433696923746827   Iteration 77 of 100, tot loss = 4.719243523362395, l1: 0.00010632670639640675, l2: 0.00036559764741067233   Iteration 78 of 100, tot loss = 4.736145279346368, l1: 0.00010657743764451693, l2: 0.0003670370913310669   Iteration 79 of 100, tot loss = 4.785771650604055, l1: 0.0001073171892752311, l2: 0.00037125997592347287   Iteration 80 of 100, tot loss = 4.796920648217201, l1: 0.00010741277214947332, l2: 0.00037227929333312205   Iteration 81 of 100, tot loss = 4.787593985781258, l1: 0.00010736248055135222, l2: 0.00037139691904698854   Iteration 82 of 100, tot loss = 4.792669124719573, l1: 0.00010757242928709119, l2: 0.0003716944840739527   Iteration 83 of 100, tot loss = 4.818264705589018, l1: 0.0001080257335347432, l2: 0.00037380073767250785   Iteration 84 of 100, tot loss = 4.79875586997895, l1: 0.00010760348444212771, l2: 0.0003722721031365273   Iteration 85 of 100, tot loss = 4.7873219293706555, l1: 0.00010769690874012133, l2: 0.0003710352848215467   Iteration 86 of 100, tot loss = 4.8020912796952, l1: 0.00010768556142487119, l2: 0.00037252356763252295   Iteration 87 of 100, tot loss = 4.798556270270512, l1: 0.00010789570806918655, l2: 0.00037195991999710556   Iteration 88 of 100, tot loss = 4.801328461278569, l1: 0.000107852868950431, l2: 0.0003722799782818501   Iteration 89 of 100, tot loss = 4.799642822715674, l1: 0.00010800624430262003, l2: 0.00037195803901688145   Iteration 90 of 100, tot loss = 4.769789436128405, l1: 0.00010725991350126504, l2: 0.00036971903124746557   Iteration 91 of 100, tot loss = 4.756526491144201, l1: 0.00010700102363473859, l2: 0.0003686516267787358   Iteration 92 of 100, tot loss = 4.7399812237076135, l1: 0.00010688243329913466, l2: 0.0003671156904918304   Iteration 93 of 100, tot loss = 4.762252984508391, l1: 0.00010708213253516293, l2: 0.00036914316750812515   Iteration 94 of 100, tot loss = 4.766679051074576, l1: 0.0001070954184932758, l2: 0.00036957248812541366   Iteration 95 of 100, tot loss = 4.790862321853638, l1: 0.0001074754728362773, l2: 0.00037161076110542605   Iteration 96 of 100, tot loss = 4.776997638245423, l1: 0.00010697023193036632, l2: 0.00037072953364258865   Iteration 97 of 100, tot loss = 4.788350107743568, l1: 0.00010730767083719143, l2: 0.0003715273417105188   Iteration 98 of 100, tot loss = 4.78325677161314, l1: 0.00010732318629862322, l2: 0.00037100249265881294   Iteration 99 of 100, tot loss = 4.771563339715052, l1: 0.0001073126622941345, l2: 0.0003698436735019163   Iteration 100 of 100, tot loss = 4.7856422352790835, l1: 0.00010778649564599618, l2: 0.0003707777295494452
   End of epoch 1314; saving model... 

Epoch 1315 of 2000
   Iteration 1 of 100, tot loss = 5.508841037750244, l1: 0.00012284702097531408, l2: 0.00042803710675798357   Iteration 2 of 100, tot loss = 4.494492888450623, l1: 9.502656757831573e-05, l2: 0.0003544227365637198   Iteration 3 of 100, tot loss = 4.278177499771118, l1: 9.833353396970779e-05, l2: 0.0003294842317700386   Iteration 4 of 100, tot loss = 4.83363550901413, l1: 0.000105595427157823, l2: 0.00037776812678202987   Iteration 5 of 100, tot loss = 4.459010362625122, l1: 0.0001041583702317439, l2: 0.0003417426662053913   Iteration 6 of 100, tot loss = 4.549840490023295, l1: 0.00010901694016259474, l2: 0.00034596710368835676   Iteration 7 of 100, tot loss = 4.707431350435529, l1: 0.00010976156460986073, l2: 0.0003609815695589142   Iteration 8 of 100, tot loss = 4.351835444569588, l1: 0.00010306472495358321, l2: 0.00033211881782335695   Iteration 9 of 100, tot loss = 4.342561893992954, l1: 0.0001034740437009734, l2: 0.0003307821445762076   Iteration 10 of 100, tot loss = 4.630958425998688, l1: 0.00010566449382167775, l2: 0.0003574313464923762   Iteration 11 of 100, tot loss = 4.841516635634682, l1: 0.00010914673045838505, l2: 0.00037500493371308863   Iteration 12 of 100, tot loss = 4.772856483856837, l1: 0.00010737107398502606, l2: 0.0003699145757612617   Iteration 13 of 100, tot loss = 4.849893395717327, l1: 0.00010968218003444445, l2: 0.0003753071572506227   Iteration 14 of 100, tot loss = 5.025362466062818, l1: 0.00011005854451338694, l2: 0.00039247769849940335   Iteration 15 of 100, tot loss = 5.150122936566671, l1: 0.00010908745922885524, l2: 0.0004059248303140824   Iteration 16 of 100, tot loss = 5.190037943422794, l1: 0.00011050035004700476, l2: 0.0004085034406671184   Iteration 17 of 100, tot loss = 5.397096332381754, l1: 0.00011380446748636827, l2: 0.00042590515958556137   Iteration 18 of 100, tot loss = 5.276016665829553, l1: 0.00011220511542988889, l2: 0.00041539654598131566   Iteration 19 of 100, tot loss = 5.193260751272502, l1: 0.00011141748128514002, l2: 0.0004079085889603256   Iteration 20 of 100, tot loss = 5.076312392950058, l1: 0.00010939149924524827, l2: 0.00039823973493184893   Iteration 21 of 100, tot loss = 5.139684058371044, l1: 0.00010986471707361662, l2: 0.00040410368403952034   Iteration 22 of 100, tot loss = 5.217952636155215, l1: 0.00011099643349800978, l2: 0.00041079882595857436   Iteration 23 of 100, tot loss = 5.179950460143711, l1: 0.00011023980424278821, l2: 0.00040775523698159856   Iteration 24 of 100, tot loss = 5.122279152274132, l1: 0.00010949812425072498, l2: 0.0004027297873108182   Iteration 25 of 100, tot loss = 5.028492302894592, l1: 0.00010841047958820127, l2: 0.0003944387473165989   Iteration 26 of 100, tot loss = 5.016401864015139, l1: 0.00010902284800697368, l2: 0.0003926173353997561   Iteration 27 of 100, tot loss = 4.927791538061919, l1: 0.00010729229729299227, l2: 0.0003854868533650275   Iteration 28 of 100, tot loss = 4.925002059766224, l1: 0.00010695673557035792, l2: 0.0003855434671485065   Iteration 29 of 100, tot loss = 4.939648689894841, l1: 0.00010676596076368226, l2: 0.00038719890440476994   Iteration 30 of 100, tot loss = 4.887748126188914, l1: 0.0001059288916925046, l2: 0.0003828459171927534   Iteration 31 of 100, tot loss = 4.844699309718225, l1: 0.00010543824171162992, l2: 0.00037903168601089067   Iteration 32 of 100, tot loss = 4.890906412154436, l1: 0.00010666879632026394, l2: 0.00038242184155024006   Iteration 33 of 100, tot loss = 4.789149363835652, l1: 0.00010454635500509264, l2: 0.0003743685777987015   Iteration 34 of 100, tot loss = 4.827198105699876, l1: 0.00010548160111391232, l2: 0.00037723820573506495   Iteration 35 of 100, tot loss = 4.873647928237915, l1: 0.00010575236613346663, l2: 0.0003816124232668829   Iteration 36 of 100, tot loss = 4.833418700430128, l1: 0.00010538208546980361, l2: 0.00037795978096255567   Iteration 37 of 100, tot loss = 4.7784384650153084, l1: 0.00010466416977185479, l2: 0.00037317967331005464   Iteration 38 of 100, tot loss = 4.777225921028538, l1: 0.00010462471283589318, l2: 0.0003730978762752401   Iteration 39 of 100, tot loss = 4.798921633989383, l1: 0.00010487472965719346, l2: 0.0003750174317899972   Iteration 40 of 100, tot loss = 4.77790721654892, l1: 0.00010491857401575543, l2: 0.00037287214563548333   Iteration 41 of 100, tot loss = 4.859797140447105, l1: 0.00010442154625041706, l2: 0.00038155816440760135   Iteration 42 of 100, tot loss = 4.846656708490281, l1: 0.00010429690545216934, l2: 0.00038036876243755354   Iteration 43 of 100, tot loss = 4.847268448319546, l1: 0.00010416892566650477, l2: 0.00038055791624491426   Iteration 44 of 100, tot loss = 4.806647642092272, l1: 0.0001035872237480362, l2: 0.00037707753736785065   Iteration 45 of 100, tot loss = 4.798048384984335, l1: 0.00010362581403266328, l2: 0.00037617902093592826   Iteration 46 of 100, tot loss = 4.789647221565247, l1: 0.00010342838465041258, l2: 0.0003755363338669944   Iteration 47 of 100, tot loss = 4.783531102728336, l1: 0.00010341076801736601, l2: 0.0003749423388186149   Iteration 48 of 100, tot loss = 4.7792349110047025, l1: 0.00010296495133843564, l2: 0.00037495853590977884   Iteration 49 of 100, tot loss = 4.790409715808168, l1: 0.00010311081228903685, l2: 0.0003759301552960022   Iteration 50 of 100, tot loss = 4.896320633888244, l1: 0.00010465235252922867, l2: 0.0003849797062866855   Iteration 51 of 100, tot loss = 4.891474382550109, l1: 0.00010507283071752218, l2: 0.0003840746034185548   Iteration 52 of 100, tot loss = 4.898628505376669, l1: 0.00010532145034421074, l2: 0.0003845413964602183   Iteration 53 of 100, tot loss = 4.92961856104293, l1: 0.00010574906524918745, l2: 0.00038721278776032497   Iteration 54 of 100, tot loss = 4.885532440962614, l1: 0.00010495209715844356, l2: 0.0003836011443733393   Iteration 55 of 100, tot loss = 4.919194958426735, l1: 0.00010562384737898934, l2: 0.0003862956463391046   Iteration 56 of 100, tot loss = 4.886589650596891, l1: 0.00010556136729584458, l2: 0.0003830975959187656   Iteration 57 of 100, tot loss = 4.88092553406431, l1: 0.00010590337153285203, l2: 0.00038218918001675455   Iteration 58 of 100, tot loss = 4.873882347139819, l1: 0.0001057483233157184, l2: 0.0003816399098311728   Iteration 59 of 100, tot loss = 4.855044599306786, l1: 0.0001050898468223503, l2: 0.000380414611708286   Iteration 60 of 100, tot loss = 4.843887376785278, l1: 0.00010508141889052544, l2: 0.0003793073174165329   Iteration 61 of 100, tot loss = 4.913387892676181, l1: 0.00010610131679730657, l2: 0.0003852374699266368   Iteration 62 of 100, tot loss = 4.9072194407063146, l1: 0.00010610254984414731, l2: 0.0003846193915617938   Iteration 63 of 100, tot loss = 4.929115870642284, l1: 0.00010656634924724507, l2: 0.0003863452350035206   Iteration 64 of 100, tot loss = 4.931360378861427, l1: 0.00010655780437218709, l2: 0.00038657823108678713   Iteration 65 of 100, tot loss = 4.932370706704947, l1: 0.00010637752874082742, l2: 0.000386859539811177   Iteration 66 of 100, tot loss = 4.915578477310412, l1: 0.00010563390848498481, l2: 0.0003859239369569078   Iteration 67 of 100, tot loss = 4.917952042907031, l1: 0.00010583584597369017, l2: 0.00038595935566535   Iteration 68 of 100, tot loss = 4.920374719535603, l1: 0.0001056688310842112, l2: 0.0003863686379190767   Iteration 69 of 100, tot loss = 4.92588015915691, l1: 0.00010572769911116178, l2: 0.00038686031410285017   Iteration 70 of 100, tot loss = 4.938691401481629, l1: 0.00010622699940410842, l2: 0.00038764213854197547   Iteration 71 of 100, tot loss = 4.970587109176206, l1: 0.00010657624130852154, l2: 0.00039048246724914735   Iteration 72 of 100, tot loss = 4.983519279294544, l1: 0.00010696280009546576, l2: 0.0003913891253735831   Iteration 73 of 100, tot loss = 4.967330939149203, l1: 0.00010671860686732992, l2: 0.00039001448490188064   Iteration 74 of 100, tot loss = 4.95253641863127, l1: 0.00010649830004990076, l2: 0.0003887553396674087   Iteration 75 of 100, tot loss = 4.916658143997193, l1: 0.0001059152605982187, l2: 0.00038575055164983494   Iteration 76 of 100, tot loss = 4.957413494586945, l1: 0.00010625495509118896, l2: 0.0003894863920753491   Iteration 77 of 100, tot loss = 4.960450113593758, l1: 0.00010632814141368349, l2: 0.0003897168681323347   Iteration 78 of 100, tot loss = 4.997054873368679, l1: 0.00010667307586304676, l2: 0.0003930324094723731   Iteration 79 of 100, tot loss = 5.059370864795733, l1: 0.00010733840509267713, l2: 0.0003985986803382741   Iteration 80 of 100, tot loss = 5.062549415230751, l1: 0.00010756372366813594, l2: 0.0003986912172877055   Iteration 81 of 100, tot loss = 5.0576197394618285, l1: 0.00010729165453848969, l2: 0.0003984703188855488   Iteration 82 of 100, tot loss = 5.070812455037745, l1: 0.00010740873607928956, l2: 0.0003996725087146354   Iteration 83 of 100, tot loss = 5.087628419140735, l1: 0.00010748203466534165, l2: 0.0004012808064314402   Iteration 84 of 100, tot loss = 5.079310016972678, l1: 0.00010736847633822999, l2: 0.0004005625247546483   Iteration 85 of 100, tot loss = 5.08593394616071, l1: 0.00010738088475430714, l2: 0.0004012125093140639   Iteration 86 of 100, tot loss = 5.073809565499771, l1: 0.00010717181354520171, l2: 0.0004002091422884151   Iteration 87 of 100, tot loss = 5.0932728586525755, l1: 0.00010774839556061437, l2: 0.0004015788898419257   Iteration 88 of 100, tot loss = 5.108532485636798, l1: 0.00010784345869813643, l2: 0.0004030097897239929   Iteration 89 of 100, tot loss = 5.085442612680157, l1: 0.00010754988818845366, l2: 0.000400994373030903   Iteration 90 of 100, tot loss = 5.064573091930813, l1: 0.00010703511959743789, l2: 0.00039942218936630527   Iteration 91 of 100, tot loss = 5.060852831536597, l1: 0.00010718034004989582, l2: 0.0003989049426828777   Iteration 92 of 100, tot loss = 5.064407353815825, l1: 0.00010722062040219063, l2: 0.0003992201145592844   Iteration 93 of 100, tot loss = 5.062632791457638, l1: 0.00010748484219807471, l2: 0.0003987784364907902   Iteration 94 of 100, tot loss = 5.072670373510807, l1: 0.00010754414649861073, l2: 0.000399722890545084   Iteration 95 of 100, tot loss = 5.0640607532701996, l1: 0.00010744479659479112, l2: 0.0003989612785517238   Iteration 96 of 100, tot loss = 5.042515893777211, l1: 0.00010686928374070703, l2: 0.0003973823053608309   Iteration 97 of 100, tot loss = 5.063702878263808, l1: 0.00010727419599395044, l2: 0.00039909609190497987   Iteration 98 of 100, tot loss = 5.049703503141598, l1: 0.00010726089211096465, l2: 0.0003977094582259436   Iteration 99 of 100, tot loss = 5.0569649103916055, l1: 0.00010742393524104946, l2: 0.0003982725559241101   Iteration 100 of 100, tot loss = 5.042638981342316, l1: 0.00010739055327576353, l2: 0.00039687334479822313
   End of epoch 1315; saving model... 

Epoch 1316 of 2000
   Iteration 1 of 100, tot loss = 4.808305263519287, l1: 9.862677688943222e-05, l2: 0.0003822037542704493   Iteration 2 of 100, tot loss = 5.056405305862427, l1: 0.00010889786062762141, l2: 0.00039674268919043243   Iteration 3 of 100, tot loss = 5.095759550730388, l1: 0.00010289827817662929, l2: 0.0004066776891704649   Iteration 4 of 100, tot loss = 5.389683723449707, l1: 0.0001153121320385253, l2: 0.0004236562454025261   Iteration 5 of 100, tot loss = 5.059134674072266, l1: 0.0001055572007317096, l2: 0.00040035627316683533   Iteration 6 of 100, tot loss = 4.741398612658183, l1: 0.00010517601428243022, l2: 0.000368963851845668   Iteration 7 of 100, tot loss = 4.617149557386126, l1: 0.00010437826339122174, l2: 0.00035733669522284927   Iteration 8 of 100, tot loss = 4.538758397102356, l1: 0.00010179679884458892, l2: 0.0003520790432958165   Iteration 9 of 100, tot loss = 4.380452632904053, l1: 9.709029796391001e-05, l2: 0.00034095496812369674   Iteration 10 of 100, tot loss = 4.711647462844849, l1: 0.00010357232567912434, l2: 0.00036759242502739655   Iteration 11 of 100, tot loss = 5.045701590451327, l1: 0.00010728942850784568, l2: 0.00039728073567279023   Iteration 12 of 100, tot loss = 5.065792202949524, l1: 0.00010998645696721117, l2: 0.0003965927668711326   Iteration 13 of 100, tot loss = 5.502492207747239, l1: 0.00011584312848684091, l2: 0.0004344060938679971   Iteration 14 of 100, tot loss = 5.524450029645648, l1: 0.00011545096094778273, l2: 0.0004369940444511095   Iteration 15 of 100, tot loss = 5.590748023986817, l1: 0.0001172723867057357, l2: 0.00044180241820868105   Iteration 16 of 100, tot loss = 5.603452205657959, l1: 0.00011647509859358252, l2: 0.00044387012621882604   Iteration 17 of 100, tot loss = 5.560577112085679, l1: 0.00011580063758575467, l2: 0.00044025707842699965   Iteration 18 of 100, tot loss = 5.368758943345812, l1: 0.00011207160059711896, l2: 0.00042480429854347475   Iteration 19 of 100, tot loss = 5.368576978382311, l1: 0.00011291402035184499, l2: 0.0004239436821080744   Iteration 20 of 100, tot loss = 5.517112755775452, l1: 0.00011596226722758729, l2: 0.0004357490106485784   Iteration 21 of 100, tot loss = 5.39947231610616, l1: 0.00011524411959163401, l2: 0.00042470311330232235   Iteration 22 of 100, tot loss = 5.350491426207802, l1: 0.0001148726693248715, l2: 0.0004201764743563465   Iteration 23 of 100, tot loss = 5.276361092277195, l1: 0.00011308199906995034, l2: 0.00041455411068771196   Iteration 24 of 100, tot loss = 5.311674972375234, l1: 0.0001135737114357956, l2: 0.0004175937865511514   Iteration 25 of 100, tot loss = 5.185164175033569, l1: 0.00011098647359176538, l2: 0.0004075299447868019   Iteration 26 of 100, tot loss = 5.311264982590308, l1: 0.00011360221259775035, l2: 0.0004175242871637098   Iteration 27 of 100, tot loss = 5.26580023765564, l1: 0.00011334923503993734, l2: 0.00041323079063591584   Iteration 28 of 100, tot loss = 5.184048329080854, l1: 0.00011189725423069572, l2: 0.00040650757988000156   Iteration 29 of 100, tot loss = 5.078577534905795, l1: 0.00010985525659634732, l2: 0.00039800249816898   Iteration 30 of 100, tot loss = 4.9816223303476965, l1: 0.00010832471392253258, l2: 0.0003898375206820977   Iteration 31 of 100, tot loss = 4.943636978826215, l1: 0.00010818471597421223, l2: 0.0003861789831012908   Iteration 32 of 100, tot loss = 4.903606377542019, l1: 0.00010745463771399955, l2: 0.00038290600105028716   Iteration 33 of 100, tot loss = 4.8824482036359385, l1: 0.00010610302166297157, l2: 0.0003821417997118481   Iteration 34 of 100, tot loss = 4.846299753469579, l1: 0.00010464839803770541, l2: 0.0003799815788036486   Iteration 35 of 100, tot loss = 4.85188581602914, l1: 0.00010515054964344017, l2: 0.0003800380338881431   Iteration 36 of 100, tot loss = 4.869130763742659, l1: 0.00010574542072087449, l2: 0.0003811676575019697   Iteration 37 of 100, tot loss = 4.866726611111615, l1: 0.0001058167803716355, l2: 0.00038085588233583177   Iteration 38 of 100, tot loss = 4.805435393985949, l1: 0.00010507448266588127, l2: 0.0003754690585896912   Iteration 39 of 100, tot loss = 4.8015666374793415, l1: 0.00010507192266045306, l2: 0.0003750847427932641   Iteration 40 of 100, tot loss = 4.7837025165557865, l1: 0.00010510848615012947, l2: 0.0003732617671630578   Iteration 41 of 100, tot loss = 4.8210863950775895, l1: 0.00010556843521842937, l2: 0.000376540205296606   Iteration 42 of 100, tot loss = 4.785771267754691, l1: 0.00010496290884648694, l2: 0.00037361421917531904   Iteration 43 of 100, tot loss = 4.7689995211224225, l1: 0.00010512986375752642, l2: 0.00037177008892095453   Iteration 44 of 100, tot loss = 4.72849364714189, l1: 0.00010431942800178976, l2: 0.00036852993699457414   Iteration 45 of 100, tot loss = 4.711243078443739, l1: 0.00010436860247864388, l2: 0.0003667557059088722   Iteration 46 of 100, tot loss = 4.717320960500966, l1: 0.00010468602601537461, l2: 0.00036704607033322606   Iteration 47 of 100, tot loss = 4.6966135704771, l1: 0.00010439838808180982, l2: 0.00036526296908243934   Iteration 48 of 100, tot loss = 4.647113914291064, l1: 0.00010363836865205182, l2: 0.0003610730227592285   Iteration 49 of 100, tot loss = 4.6461547491501785, l1: 0.00010366273881470765, l2: 0.0003609527362752896   Iteration 50 of 100, tot loss = 4.601426725387573, l1: 0.00010286192664352712, l2: 0.00035728074581129474   Iteration 51 of 100, tot loss = 4.641613576926437, l1: 0.00010350215705008904, l2: 0.0003606592012254302   Iteration 52 of 100, tot loss = 4.681063422789941, l1: 0.00010403684056934883, l2: 0.0003640695020014671   Iteration 53 of 100, tot loss = 4.648840719798826, l1: 0.00010354599232053963, l2: 0.0003613380795972914   Iteration 54 of 100, tot loss = 4.66592702600691, l1: 0.00010395020857416696, l2: 0.00036264249340294756   Iteration 55 of 100, tot loss = 4.646921318227594, l1: 0.00010380892801384273, l2: 0.0003608832032610239   Iteration 56 of 100, tot loss = 4.637305732284274, l1: 0.00010335555043639033, l2: 0.0003603750226050449   Iteration 57 of 100, tot loss = 4.600065557580245, l1: 0.00010244712859531047, l2: 0.0003575594271655734   Iteration 58 of 100, tot loss = 4.5789163975880065, l1: 0.00010214714874846635, l2: 0.0003557444910382338   Iteration 59 of 100, tot loss = 4.55603477106256, l1: 0.00010224484830652758, l2: 0.00035335862850257335   Iteration 60 of 100, tot loss = 4.562389775117238, l1: 0.00010198780449475938, l2: 0.00035425117312115617   Iteration 61 of 100, tot loss = 4.554689012590002, l1: 0.00010180806578423247, l2: 0.00035366083549709653   Iteration 62 of 100, tot loss = 4.593487851081356, l1: 0.0001025462384390149, l2: 0.0003568025471571262   Iteration 63 of 100, tot loss = 4.585822298413231, l1: 0.00010286370213309835, l2: 0.0003557185282550072   Iteration 64 of 100, tot loss = 4.583443310111761, l1: 0.00010279264336077176, l2: 0.00035555168847167806   Iteration 65 of 100, tot loss = 4.599440614993756, l1: 0.0001029038105536109, l2: 0.0003570402517932682   Iteration 66 of 100, tot loss = 4.594040794806047, l1: 0.0001027141299731959, l2: 0.00035668995031190923   Iteration 67 of 100, tot loss = 4.590481704740382, l1: 0.00010250402176179298, l2: 0.00035654414951655347   Iteration 68 of 100, tot loss = 4.606269117663889, l1: 0.00010277606184792924, l2: 0.00035785085085936934   Iteration 69 of 100, tot loss = 4.627036346905474, l1: 0.00010311693620388868, l2: 0.0003595866987977744   Iteration 70 of 100, tot loss = 4.643719479015895, l1: 0.00010314753297799534, l2: 0.00036122441545428176   Iteration 71 of 100, tot loss = 4.651987112743754, l1: 0.0001028877569028032, l2: 0.0003623109547385264   Iteration 72 of 100, tot loss = 4.648490134212706, l1: 0.0001031577323576332, l2: 0.00036169128139186394   Iteration 73 of 100, tot loss = 4.6244039764143015, l1: 0.00010252016911854049, l2: 0.00035992022906749014   Iteration 74 of 100, tot loss = 4.635404474026448, l1: 0.00010276010269425003, l2: 0.00036078034483236134   Iteration 75 of 100, tot loss = 4.641702165603638, l1: 0.00010282382922014222, l2: 0.0003613463874595861   Iteration 76 of 100, tot loss = 4.672718007313578, l1: 0.00010347677581242016, l2: 0.0003637950247043352   Iteration 77 of 100, tot loss = 4.634637781551906, l1: 0.00010258903591432488, l2: 0.0003608747420593637   Iteration 78 of 100, tot loss = 4.648215524661235, l1: 0.00010281495442307997, l2: 0.00036200659796565724   Iteration 79 of 100, tot loss = 4.657829916929897, l1: 0.0001032524861456514, l2: 0.0003625305055985463   Iteration 80 of 100, tot loss = 4.636513207852841, l1: 0.000102725547958471, l2: 0.00036092577283852734   Iteration 81 of 100, tot loss = 4.649154967731899, l1: 0.00010234360798890127, l2: 0.0003625718893966189   Iteration 82 of 100, tot loss = 4.64744882903448, l1: 0.00010217125791092346, l2: 0.00036257362559653544   Iteration 83 of 100, tot loss = 4.64982570366687, l1: 0.00010230255618106562, l2: 0.0003626800148540277   Iteration 84 of 100, tot loss = 4.655474031255359, l1: 0.00010222119077038119, l2: 0.00036332621294561597   Iteration 85 of 100, tot loss = 4.669797217144685, l1: 0.0001023615818720667, l2: 0.00036461814032757984   Iteration 86 of 100, tot loss = 4.67484095346096, l1: 0.00010247132804367941, l2: 0.0003650127676570095   Iteration 87 of 100, tot loss = 4.6885601117693145, l1: 0.0001026374419957758, l2: 0.000366218570063169   Iteration 88 of 100, tot loss = 4.693434356288477, l1: 0.00010270542716997707, l2: 0.0003666380090924742   Iteration 89 of 100, tot loss = 4.660300787915005, l1: 0.00010206512855843306, l2: 0.00036396495092089887   Iteration 90 of 100, tot loss = 4.6622585323121815, l1: 0.00010233771677626969, l2: 0.00036388813734649576   Iteration 91 of 100, tot loss = 4.651803297000927, l1: 0.00010242708530923055, l2: 0.00036275324524907333   Iteration 92 of 100, tot loss = 4.6796897779340325, l1: 0.00010297811403016449, l2: 0.0003649908645931409   Iteration 93 of 100, tot loss = 4.68296028208989, l1: 0.00010327421106374321, l2: 0.0003650218177328928   Iteration 94 of 100, tot loss = 4.672595128099969, l1: 0.00010318706316614592, l2: 0.00036407245042550555   Iteration 95 of 100, tot loss = 4.6592512306414156, l1: 0.00010323088580135894, l2: 0.00036269423818387285   Iteration 96 of 100, tot loss = 4.642281124989192, l1: 0.00010297469009401539, l2: 0.00036125342345864436   Iteration 97 of 100, tot loss = 4.647314356774399, l1: 0.00010310442119533167, l2: 0.0003616270157940609   Iteration 98 of 100, tot loss = 4.673997489773497, l1: 0.00010347119540692431, l2: 0.00036392855512605484   Iteration 99 of 100, tot loss = 4.684967132529827, l1: 0.00010377944606877252, l2: 0.0003647172686554529   Iteration 100 of 100, tot loss = 4.6803693771362305, l1: 0.00010369379288022174, l2: 0.00036434314650250597
   End of epoch 1316; saving model... 

Epoch 1317 of 2000
   Iteration 1 of 100, tot loss = 5.8440399169921875, l1: 0.0001065167598426342, l2: 0.00047788722440600395   Iteration 2 of 100, tot loss = 5.494466304779053, l1: 0.00011181976879015565, l2: 0.0004376268479973078   Iteration 3 of 100, tot loss = 6.4703874588012695, l1: 0.0001257443412517508, l2: 0.0005212944039764503   Iteration 4 of 100, tot loss = 6.301699757575989, l1: 0.00012553441774798557, l2: 0.0005046355508966371   Iteration 5 of 100, tot loss = 6.01747875213623, l1: 0.00012033496313961223, l2: 0.0004814129089936614   Iteration 6 of 100, tot loss = 5.55661141872406, l1: 0.00011789606893823172, l2: 0.00043776507178942364   Iteration 7 of 100, tot loss = 5.545502764838083, l1: 0.00011857000104750373, l2: 0.0004359802738430777   Iteration 8 of 100, tot loss = 5.375739961862564, l1: 0.00011252341664658161, l2: 0.00042505057717789896   Iteration 9 of 100, tot loss = 5.529623058107164, l1: 0.00011318926731797142, l2: 0.00043977303236412507   Iteration 10 of 100, tot loss = 5.400090146064758, l1: 0.00011327832253300585, l2: 0.00042673068819567563   Iteration 11 of 100, tot loss = 5.117087949406017, l1: 0.00010984211324036799, l2: 0.0004018666778310117   Iteration 12 of 100, tot loss = 4.885175963242848, l1: 0.00010412991074796689, l2: 0.00038438768266739015   Iteration 13 of 100, tot loss = 4.855568867463332, l1: 0.00010550946087907784, l2: 0.0003800474234874575   Iteration 14 of 100, tot loss = 4.904702952929905, l1: 0.00010481808593405211, l2: 0.00038565220997302925   Iteration 15 of 100, tot loss = 4.975213448206584, l1: 0.00010776396108364376, l2: 0.00038975738474012665   Iteration 16 of 100, tot loss = 4.890066206455231, l1: 0.00010711789013839734, l2: 0.0003818887316811015   Iteration 17 of 100, tot loss = 5.030524253845215, l1: 0.00010802080706686385, l2: 0.0003950316193582052   Iteration 18 of 100, tot loss = 5.148511966069539, l1: 0.00010967704757705279, l2: 0.0004051741505261614   Iteration 19 of 100, tot loss = 5.134988533823114, l1: 0.00011005228991148454, l2: 0.00040344656429768195   Iteration 20 of 100, tot loss = 5.063814353942871, l1: 0.00010904928403761005, l2: 0.00039733215162414127   Iteration 21 of 100, tot loss = 5.136528469267345, l1: 0.0001103303690061217, l2: 0.0004033224783294524   Iteration 22 of 100, tot loss = 5.0714251886714585, l1: 0.00011033120658380953, l2: 0.00039681131271539596   Iteration 23 of 100, tot loss = 5.134297920309979, l1: 0.00011138312671075413, l2: 0.0004020466655239706   Iteration 24 of 100, tot loss = 5.140806506077449, l1: 0.00011210108596060309, l2: 0.00040197956514020916   Iteration 25 of 100, tot loss = 5.169505071640015, l1: 0.00011194716833415441, l2: 0.00040500333823729307   Iteration 26 of 100, tot loss = 5.109811443548936, l1: 0.00011170347673984137, l2: 0.00039927766696424584   Iteration 27 of 100, tot loss = 5.087925707852399, l1: 0.00011191917413574022, l2: 0.00039687339503197345   Iteration 28 of 100, tot loss = 5.097764568669455, l1: 0.0001121252003680898, l2: 0.00039765125620760955   Iteration 29 of 100, tot loss = 5.108828618608672, l1: 0.00011254043169299558, l2: 0.0003983424309087146   Iteration 30 of 100, tot loss = 5.019783671696981, l1: 0.00011096539637946989, l2: 0.0003910129710372227   Iteration 31 of 100, tot loss = 5.0980362584514, l1: 0.00011122136340653074, l2: 0.0003985822620433605   Iteration 32 of 100, tot loss = 5.1002455949783325, l1: 0.0001118462108706808, l2: 0.0003981783479503065   Iteration 33 of 100, tot loss = 5.046369928302187, l1: 0.00011112894847456394, l2: 0.00039350804405385685   Iteration 34 of 100, tot loss = 5.028428189894733, l1: 0.00011128411610758015, l2: 0.0003915587033535463   Iteration 35 of 100, tot loss = 4.977901050022671, l1: 0.0001098713329286381, l2: 0.0003879187724253695   Iteration 36 of 100, tot loss = 4.9806923733817206, l1: 0.00011006344205573744, l2: 0.0003880057956848759   Iteration 37 of 100, tot loss = 4.9952411264986605, l1: 0.00011013151605446772, l2: 0.0003893925966474706   Iteration 38 of 100, tot loss = 4.978328428770366, l1: 0.00010972720673108645, l2: 0.00038810563694967543   Iteration 39 of 100, tot loss = 4.93074046037136, l1: 0.00010882695222300335, l2: 0.00038424709465545724   Iteration 40 of 100, tot loss = 4.843134450912475, l1: 0.00010692213872971479, l2: 0.00037739130730187755   Iteration 41 of 100, tot loss = 4.873721564688334, l1: 0.00010772498690795789, l2: 0.00037964717034256177   Iteration 42 of 100, tot loss = 4.952718893686931, l1: 0.00010810666710659418, l2: 0.00038716522370426293   Iteration 43 of 100, tot loss = 4.970259211784185, l1: 0.0001081762678696514, l2: 0.00038884965541128197   Iteration 44 of 100, tot loss = 4.997601541605863, l1: 0.00010848966650717722, l2: 0.0003912704892586176   Iteration 45 of 100, tot loss = 5.0227478451199, l1: 0.00010915137504020499, l2: 0.0003931234113729766   Iteration 46 of 100, tot loss = 5.025220933167831, l1: 0.00010933153569804627, l2: 0.00039319055924905746   Iteration 47 of 100, tot loss = 4.970483790052698, l1: 0.00010818256216341669, l2: 0.0003888658181320816   Iteration 48 of 100, tot loss = 4.97657310962677, l1: 0.00010809593019682022, l2: 0.00038956138223511516   Iteration 49 of 100, tot loss = 4.955276727676392, l1: 0.00010707334283268915, l2: 0.0003884543312240715   Iteration 50 of 100, tot loss = 4.9482475805282595, l1: 0.00010699647551518864, l2: 0.0003878282838559244   Iteration 51 of 100, tot loss = 4.951120895497939, l1: 0.00010735181878736316, l2: 0.0003877602719904983   Iteration 52 of 100, tot loss = 4.925235221019158, l1: 0.00010745190509344236, l2: 0.00038507161843881477   Iteration 53 of 100, tot loss = 4.926570321029088, l1: 0.00010761294504605621, l2: 0.00038504408837739765   Iteration 54 of 100, tot loss = 4.90693419067948, l1: 0.00010732426542542978, l2: 0.00038336915477648963   Iteration 55 of 100, tot loss = 4.892122780192982, l1: 0.00010682121421930126, l2: 0.00038239106453891674   Iteration 56 of 100, tot loss = 4.91403671673366, l1: 0.00010745929466793314, l2: 0.00038394437758272815   Iteration 57 of 100, tot loss = 4.922487309104518, l1: 0.00010787896437186486, l2: 0.00038436976715354573   Iteration 58 of 100, tot loss = 4.902047194283584, l1: 0.00010719582132878713, l2: 0.0003830088988189377   Iteration 59 of 100, tot loss = 4.8647983316647805, l1: 0.0001068973704882487, l2: 0.0003795824633956634   Iteration 60 of 100, tot loss = 4.880009353160858, l1: 0.00010696410972741433, l2: 0.00038103682636574375   Iteration 61 of 100, tot loss = 4.853270741759753, l1: 0.00010634806891235325, l2: 0.0003789790058422635   Iteration 62 of 100, tot loss = 4.8496427459101525, l1: 0.00010656773297195022, l2: 0.00037839654220489695   Iteration 63 of 100, tot loss = 4.874228636423747, l1: 0.00010672670182502193, l2: 0.00038069616234029776   Iteration 64 of 100, tot loss = 4.83636686578393, l1: 0.00010596553903496897, l2: 0.0003776711481577877   Iteration 65 of 100, tot loss = 4.819121382786677, l1: 0.00010532762193738912, l2: 0.0003765845168695355   Iteration 66 of 100, tot loss = 4.7975420662851045, l1: 0.00010474949086721719, l2: 0.0003750047162060527   Iteration 67 of 100, tot loss = 4.836001929952137, l1: 0.00010521329489559172, l2: 0.0003783868984345333   Iteration 68 of 100, tot loss = 4.8388387175167304, l1: 0.00010500353447937146, l2: 0.00037888033725988967   Iteration 69 of 100, tot loss = 4.857150126194608, l1: 0.00010534538576367608, l2: 0.0003803696265516708   Iteration 70 of 100, tot loss = 4.882595184871128, l1: 0.00010538754270653175, l2: 0.0003828719752034106   Iteration 71 of 100, tot loss = 4.869375067697445, l1: 0.00010523904650103183, l2: 0.0003816984596122778   Iteration 72 of 100, tot loss = 4.8456480569309655, l1: 0.00010497208803725598, l2: 0.0003795927169550042   Iteration 73 of 100, tot loss = 4.8529753815637875, l1: 0.00010552369154812704, l2: 0.00037977384546475026   Iteration 74 of 100, tot loss = 4.878336700233254, l1: 0.00010608460301276914, l2: 0.00038174906581549   Iteration 75 of 100, tot loss = 4.914807980855306, l1: 0.00010686443910041514, l2: 0.00038461635733256117   Iteration 76 of 100, tot loss = 4.903206091178091, l1: 0.00010683990709175735, l2: 0.0003834807004367528   Iteration 77 of 100, tot loss = 4.893319656322529, l1: 0.00010697990237295253, l2: 0.0003823520616263865   Iteration 78 of 100, tot loss = 4.897049922209519, l1: 0.00010682627324944732, l2: 0.00038287871715370926   Iteration 79 of 100, tot loss = 4.882060597214518, l1: 0.00010665184384413783, l2: 0.00038155421415340935   Iteration 80 of 100, tot loss = 4.8670581668615345, l1: 0.00010636936608534597, l2: 0.00038033644914321486   Iteration 81 of 100, tot loss = 4.835721545749241, l1: 0.00010599646514122098, l2: 0.00037757568785354474   Iteration 82 of 100, tot loss = 4.821160450214293, l1: 0.00010603655360549654, l2: 0.0003760794897971209   Iteration 83 of 100, tot loss = 4.8205712961863325, l1: 0.00010580906579768879, l2: 0.00037624806238279444   Iteration 84 of 100, tot loss = 4.809377159391131, l1: 0.00010576536394745788, l2: 0.0003751723505133885   Iteration 85 of 100, tot loss = 4.808360660777373, l1: 0.00010567929767763844, l2: 0.00037515676667680963   Iteration 86 of 100, tot loss = 4.799781034159106, l1: 0.00010563752995188034, l2: 0.0003743405716302859   Iteration 87 of 100, tot loss = 4.800730496987529, l1: 0.00010583479257317891, l2: 0.00037423825524038177   Iteration 88 of 100, tot loss = 4.798704672943462, l1: 0.00010604377021908559, l2: 0.00037382669515037293   Iteration 89 of 100, tot loss = 4.817154171761502, l1: 0.00010581793096943318, l2: 0.0003758974845378576   Iteration 90 of 100, tot loss = 4.806968381669786, l1: 0.00010574562834840941, l2: 0.00037495120802001717   Iteration 91 of 100, tot loss = 4.8016054551680005, l1: 0.0001052687108752722, l2: 0.0003748918328770682   Iteration 92 of 100, tot loss = 4.808897961740908, l1: 0.0001055294964237913, l2: 0.00037536029784583343   Iteration 93 of 100, tot loss = 4.792242119389195, l1: 0.00010527620574438153, l2: 0.0003739480043902883   Iteration 94 of 100, tot loss = 4.810555445387008, l1: 0.00010563763758097032, l2: 0.00037541790523856105   Iteration 95 of 100, tot loss = 4.819445652710764, l1: 0.00010591443075865851, l2: 0.0003760301328025219   Iteration 96 of 100, tot loss = 4.797214617331822, l1: 0.00010564246436691367, l2: 0.0003740789957949649   Iteration 97 of 100, tot loss = 4.797495404469598, l1: 0.00010572875874355848, l2: 0.0003740207805057037   Iteration 98 of 100, tot loss = 4.813913549695696, l1: 0.00010594576908298056, l2: 0.0003754455846115443   Iteration 99 of 100, tot loss = 4.802106561082782, l1: 0.00010576600055568945, l2: 0.0003744446541825921   Iteration 100 of 100, tot loss = 4.800488798618317, l1: 0.0001058477807600866, l2: 0.0003742010975111043
   End of epoch 1317; saving model... 

Epoch 1318 of 2000
   Iteration 1 of 100, tot loss = 5.174806594848633, l1: 0.00011800156062236056, l2: 0.0003994791186414659   Iteration 2 of 100, tot loss = 4.245369553565979, l1: 9.82368437689729e-05, l2: 0.0003263001126470044   Iteration 3 of 100, tot loss = 4.603979825973511, l1: 9.871875954559073e-05, l2: 0.0003616792382672429   Iteration 4 of 100, tot loss = 4.767317473888397, l1: 0.00010306243348168209, l2: 0.00037366933247540146   Iteration 5 of 100, tot loss = 4.832013559341431, l1: 0.0001048619335051626, l2: 0.0003783394407946616   Iteration 6 of 100, tot loss = 4.740052580833435, l1: 0.00010534734246903099, l2: 0.00036865792935714126   Iteration 7 of 100, tot loss = 4.864126580102103, l1: 0.00010873130668187514, l2: 0.00037768136410574826   Iteration 8 of 100, tot loss = 4.88546285033226, l1: 0.00010696668869059067, l2: 0.0003815796080743894   Iteration 9 of 100, tot loss = 4.775439951154921, l1: 0.00010542187293241214, l2: 0.0003721221324263348   Iteration 10 of 100, tot loss = 4.670383143424988, l1: 0.00010391971809440293, l2: 0.0003631186060374603   Iteration 11 of 100, tot loss = 4.756026072935625, l1: 0.0001064063868728805, l2: 0.00036919622851366347   Iteration 12 of 100, tot loss = 4.7693496743837995, l1: 0.00010741877182833075, l2: 0.0003695162061679487   Iteration 13 of 100, tot loss = 5.035568915880644, l1: 0.00010928034036903857, l2: 0.00039427655820663157   Iteration 14 of 100, tot loss = 5.046797156333923, l1: 0.00011045580790128693, l2: 0.00039422391273546964   Iteration 15 of 100, tot loss = 5.080293226242065, l1: 0.00011125258024549111, l2: 0.0003967767484330883   Iteration 16 of 100, tot loss = 5.077284052968025, l1: 0.00011219190355404862, l2: 0.00039553650640300475   Iteration 17 of 100, tot loss = 5.232131775687723, l1: 0.00011487130498554667, l2: 0.00040834187872379143   Iteration 18 of 100, tot loss = 5.1494848595725164, l1: 0.00011312779598584812, l2: 0.0004018206948078134   Iteration 19 of 100, tot loss = 5.1368920050169296, l1: 0.0001131128159805054, l2: 0.0004005763897255651   Iteration 20 of 100, tot loss = 5.030216574668884, l1: 0.00011124276170448866, l2: 0.0003917789006663952   Iteration 21 of 100, tot loss = 5.096146356491816, l1: 0.00011334290355166776, l2: 0.00039627173724251667   Iteration 22 of 100, tot loss = 5.019404747269371, l1: 0.00011151248029570773, l2: 0.000390427998685151   Iteration 23 of 100, tot loss = 5.016488956368488, l1: 0.00011255473572932912, l2: 0.00038909416395219284   Iteration 24 of 100, tot loss = 4.989447623491287, l1: 0.0001121607774621225, l2: 0.0003867839895974612   Iteration 25 of 100, tot loss = 5.000394620895386, l1: 0.00011177753360243514, l2: 0.0003882619325304404   Iteration 26 of 100, tot loss = 5.028235206237206, l1: 0.0001124248606524466, l2: 0.0003903986636522369   Iteration 27 of 100, tot loss = 4.981651058903447, l1: 0.00011200250689095508, l2: 0.00038616260226936666   Iteration 28 of 100, tot loss = 4.995952367782593, l1: 0.0001121453915402526, l2: 0.0003874498480789563   Iteration 29 of 100, tot loss = 5.034326290262157, l1: 0.0001131642194854192, l2: 0.0003902684121416753   Iteration 30 of 100, tot loss = 5.030593458811442, l1: 0.00011303508047906992, l2: 0.0003900242673504787   Iteration 31 of 100, tot loss = 5.0078791033837105, l1: 0.00011334794919915317, l2: 0.0003874399638246565   Iteration 32 of 100, tot loss = 4.917378760874271, l1: 0.00011172792687830224, l2: 0.00038000995164111373   Iteration 33 of 100, tot loss = 4.923970489790945, l1: 0.00011201748125269219, l2: 0.0003803795698331669   Iteration 34 of 100, tot loss = 4.9089579371845025, l1: 0.00011142780482152696, l2: 0.0003794679912115338   Iteration 35 of 100, tot loss = 4.816495408330645, l1: 0.00010928387491730973, l2: 0.00037236566796699274   Iteration 36 of 100, tot loss = 4.780763920810488, l1: 0.00010770741048165494, l2: 0.0003703689835674595   Iteration 37 of 100, tot loss = 4.80232923417478, l1: 0.00010793330383839438, l2: 0.0003722996220489763   Iteration 38 of 100, tot loss = 4.834069204957862, l1: 0.00010808453015143689, l2: 0.0003753223928996656   Iteration 39 of 100, tot loss = 4.802393194956657, l1: 0.00010715570626929641, l2: 0.00037308361564870353   Iteration 40 of 100, tot loss = 4.861749807000161, l1: 0.00010836890014616074, l2: 0.00037780608363391366   Iteration 41 of 100, tot loss = 4.87488755074943, l1: 0.0001089849731836627, l2: 0.0003785037846594095   Iteration 42 of 100, tot loss = 4.856048689002082, l1: 0.0001092769314363111, l2: 0.0003763279400161645   Iteration 43 of 100, tot loss = 4.835728420767674, l1: 0.00010837304039024424, l2: 0.0003751998041014642   Iteration 44 of 100, tot loss = 4.834714409979907, l1: 0.00010786865459522232, l2: 0.0003756027888977604   Iteration 45 of 100, tot loss = 4.837485088242425, l1: 0.00010785966571549781, l2: 0.0003758888455599339   Iteration 46 of 100, tot loss = 4.925483193086541, l1: 0.00010952251092100556, l2: 0.00038302581062611273   Iteration 47 of 100, tot loss = 4.921950246425385, l1: 0.00010922826155326626, l2: 0.00038296676536853563   Iteration 48 of 100, tot loss = 4.942124413947265, l1: 0.00010981361932257035, l2: 0.00038439882428065175   Iteration 49 of 100, tot loss = 4.983573745708076, l1: 0.0001106223083167736, l2: 0.0003877350682159886   Iteration 50 of 100, tot loss = 5.016795394420623, l1: 0.00011135849956190214, l2: 0.000390321041631978   Iteration 51 of 100, tot loss = 5.004820447342069, l1: 0.00011130207841394141, l2: 0.00038917996839611045   Iteration 52 of 100, tot loss = 4.96708041200271, l1: 0.00011055462290152299, l2: 0.0003861534202699729   Iteration 53 of 100, tot loss = 4.90993568807278, l1: 0.00010931241311756719, l2: 0.000381681157653835   Iteration 54 of 100, tot loss = 4.904074640185745, l1: 0.00010945927216973225, l2: 0.00038094819346615286   Iteration 55 of 100, tot loss = 4.915170207890597, l1: 0.0001097400074782358, l2: 0.0003817770143293522   Iteration 56 of 100, tot loss = 4.924313168440547, l1: 0.00011031477463672803, l2: 0.0003821165430833519   Iteration 57 of 100, tot loss = 4.916713126918726, l1: 0.00011036176555647041, l2: 0.00038130954827034945   Iteration 58 of 100, tot loss = 4.890831369778206, l1: 0.0001093519919683378, l2: 0.0003797311460246043   Iteration 59 of 100, tot loss = 4.896672998444509, l1: 0.00010951295929370937, l2: 0.00038015434194861326   Iteration 60 of 100, tot loss = 4.894735906521479, l1: 0.00010968448059429647, l2: 0.00037978911180592455   Iteration 61 of 100, tot loss = 4.861989128785055, l1: 0.00010896796346537113, l2: 0.00037723095094227826   Iteration 62 of 100, tot loss = 4.833834180908818, l1: 0.00010850287746448416, l2: 0.0003748805421280615   Iteration 63 of 100, tot loss = 4.824727779343014, l1: 0.00010857817335543963, l2: 0.0003738946058244134   Iteration 64 of 100, tot loss = 4.9011836145073175, l1: 0.00010967499434855199, l2: 0.00038044336929488054   Iteration 65 of 100, tot loss = 4.935743451118469, l1: 0.00010976718168911667, l2: 0.0003838071655571604   Iteration 66 of 100, tot loss = 4.901357952392463, l1: 0.0001089198355630012, l2: 0.0003812159618863723   Iteration 67 of 100, tot loss = 4.858275578982794, l1: 0.00010800740173140388, l2: 0.00037782015813229633   Iteration 68 of 100, tot loss = 4.856953370220521, l1: 0.00010788780101018561, l2: 0.0003778075378624421   Iteration 69 of 100, tot loss = 4.8351543692574985, l1: 0.00010747628819418436, l2: 0.0003760391506754483   Iteration 70 of 100, tot loss = 4.8259748612131395, l1: 0.00010715087498413466, l2: 0.0003754466129716353   Iteration 71 of 100, tot loss = 4.8282554737279115, l1: 0.00010686964414559606, l2: 0.0003759559052488045   Iteration 72 of 100, tot loss = 4.821763980719778, l1: 0.00010673526953218647, l2: 0.00037544113062419154   Iteration 73 of 100, tot loss = 4.81517043342329, l1: 0.00010665260391486872, l2: 0.00037486444140484636   Iteration 74 of 100, tot loss = 4.817133763351956, l1: 0.00010633377162259815, l2: 0.00037537960668339873   Iteration 75 of 100, tot loss = 4.834841748873393, l1: 0.00010632227325307516, l2: 0.0003771619031128163   Iteration 76 of 100, tot loss = 4.901439561655647, l1: 0.00010763581758510554, l2: 0.00038250814031925984   Iteration 77 of 100, tot loss = 4.9220226290938145, l1: 0.00010802325116608038, l2: 0.00038417901328988613   Iteration 78 of 100, tot loss = 4.900498886903127, l1: 0.00010776350679337226, l2: 0.00038228638340325025   Iteration 79 of 100, tot loss = 4.871029960958263, l1: 0.00010704748329553234, l2: 0.0003800555145223566   Iteration 80 of 100, tot loss = 4.863728956878186, l1: 0.00010682692409318406, l2: 0.0003795459733737516   Iteration 81 of 100, tot loss = 4.852946288791704, l1: 0.00010671561670469488, l2: 0.00037857901393349664   Iteration 82 of 100, tot loss = 4.849075202534839, l1: 0.00010662213718183566, l2: 0.0003782853847241779   Iteration 83 of 100, tot loss = 4.864649808550456, l1: 0.00010716884115750127, l2: 0.00037929614082242484   Iteration 84 of 100, tot loss = 4.909286648035049, l1: 0.00010782775072064916, l2: 0.0003831009157043549   Iteration 85 of 100, tot loss = 4.9113174508599675, l1: 0.0001080371908542207, l2: 0.00038309455558191983   Iteration 86 of 100, tot loss = 4.890080045822055, l1: 0.00010763790394480037, l2: 0.00038137010195113847   Iteration 87 of 100, tot loss = 4.929722428321838, l1: 0.00010809074559091087, l2: 0.0003848814988581078   Iteration 88 of 100, tot loss = 4.928018752824176, l1: 0.0001081527975243675, l2: 0.0003846490791064835   Iteration 89 of 100, tot loss = 4.949595471446433, l1: 0.00010858113026269972, l2: 0.00038637841833896643   Iteration 90 of 100, tot loss = 4.930698178874122, l1: 0.00010823327781205687, l2: 0.0003848365415857794   Iteration 91 of 100, tot loss = 4.937405535152981, l1: 0.00010841336044685015, l2: 0.000385327194785382   Iteration 92 of 100, tot loss = 4.935776198687761, l1: 0.0001085315102302601, l2: 0.0003850461113831008   Iteration 93 of 100, tot loss = 4.923593535218187, l1: 0.00010822486709452845, l2: 0.0003841344881439782   Iteration 94 of 100, tot loss = 4.91249639556763, l1: 0.00010803033922244101, l2: 0.00038321930193713807   Iteration 95 of 100, tot loss = 4.909432879247164, l1: 0.0001078492780307945, l2: 0.0003830940111871123   Iteration 96 of 100, tot loss = 4.907102268189192, l1: 0.00010744710615047855, l2: 0.00038326312202722573   Iteration 97 of 100, tot loss = 4.881851583412013, l1: 0.00010702274846256298, l2: 0.00038116241111564114   Iteration 98 of 100, tot loss = 4.889511150973184, l1: 0.00010692130278900494, l2: 0.0003820298137428353   Iteration 99 of 100, tot loss = 4.887016314448732, l1: 0.00010683937793458822, l2: 0.00038186225492149744   Iteration 100 of 100, tot loss = 4.880771306753158, l1: 0.0001068364609091077, l2: 0.000381240671267733
   End of epoch 1318; saving model... 

Epoch 1319 of 2000
   Iteration 1 of 100, tot loss = 5.118934154510498, l1: 0.00011591989459702745, l2: 0.0003959735040552914   Iteration 2 of 100, tot loss = 5.5104241371154785, l1: 0.00011873386029037647, l2: 0.0004323085304349661   Iteration 3 of 100, tot loss = 5.6310272216796875, l1: 0.00011989115106795604, l2: 0.00044321155291981995   Iteration 4 of 100, tot loss = 6.078137040138245, l1: 0.00012502864956331905, l2: 0.0004827850279980339   Iteration 5 of 100, tot loss = 5.437142181396484, l1: 0.00011102499265689403, l2: 0.00043268920562695713   Iteration 6 of 100, tot loss = 5.284295717875163, l1: 0.00010883047798415646, l2: 0.00041959907927472767   Iteration 7 of 100, tot loss = 5.10792487008231, l1: 0.00010850453898975892, l2: 0.0004022879355553804   Iteration 8 of 100, tot loss = 4.9719425439834595, l1: 0.00010876322357944446, l2: 0.00038843101901875343   Iteration 9 of 100, tot loss = 5.085273795657688, l1: 0.00011178677555613426, l2: 0.000396740595331519   Iteration 10 of 100, tot loss = 5.115564203262329, l1: 0.00011139281414216385, l2: 0.00040016359853325414   Iteration 11 of 100, tot loss = 5.138769149780273, l1: 0.00011123360507719389, l2: 0.00040264330428644   Iteration 12 of 100, tot loss = 5.421343247095744, l1: 0.00011695825317777538, l2: 0.0004251760656188708   Iteration 13 of 100, tot loss = 5.442490944495568, l1: 0.00011920373239823115, l2: 0.00042504535499924363   Iteration 14 of 100, tot loss = 5.1737392374447415, l1: 0.00011431422847506058, l2: 0.0004030596891035592   Iteration 15 of 100, tot loss = 5.125047755241394, l1: 0.00011397970617205525, l2: 0.0003985250611246253   Iteration 16 of 100, tot loss = 5.059105791151524, l1: 0.0001124578279814159, l2: 0.00039345274399238406   Iteration 17 of 100, tot loss = 5.009108438211329, l1: 0.00011089291750549284, l2: 0.00039001791960532395   Iteration 18 of 100, tot loss = 4.986131674713558, l1: 0.00011017723833598818, l2: 0.0003884359232162953   Iteration 19 of 100, tot loss = 4.841486573219299, l1: 0.00010725541276195527, l2: 0.00037689323883234084   Iteration 20 of 100, tot loss = 4.82554458975792, l1: 0.00010628284690028522, l2: 0.00037627160709234885   Iteration 21 of 100, tot loss = 4.909211380141122, l1: 0.00010719818480235214, l2: 0.0003837229499670987   Iteration 22 of 100, tot loss = 4.811386742375114, l1: 0.00010644872434733605, l2: 0.0003746899462633089   Iteration 23 of 100, tot loss = 4.7736722127251, l1: 0.00010669166184253181, l2: 0.00037067555683239806   Iteration 24 of 100, tot loss = 4.704586510856946, l1: 0.00010594183155869057, l2: 0.0003645168168683692   Iteration 25 of 100, tot loss = 4.710919709205627, l1: 0.0001068995866808109, l2: 0.0003641923808027059   Iteration 26 of 100, tot loss = 4.762144295068888, l1: 0.0001069955536737465, l2: 0.00036921887322723004   Iteration 27 of 100, tot loss = 4.7063876302153975, l1: 0.00010590829441315252, l2: 0.0003647304655276929   Iteration 28 of 100, tot loss = 4.777990958520344, l1: 0.00010717451342705837, l2: 0.0003706245801627769   Iteration 29 of 100, tot loss = 4.742839250071295, l1: 0.00010752406900604095, l2: 0.0003667598538068605   Iteration 30 of 100, tot loss = 4.738662779331207, l1: 0.00010792967853679632, l2: 0.00036593659703309335   Iteration 31 of 100, tot loss = 4.703990817070007, l1: 0.00010772201212896635, l2: 0.00036267706778861825   Iteration 32 of 100, tot loss = 4.696758855134249, l1: 0.00010749593548098346, l2: 0.00036217994784237817   Iteration 33 of 100, tot loss = 4.72571337223053, l1: 0.0001075677013652625, l2: 0.00036500363300244015   Iteration 34 of 100, tot loss = 4.67438935882905, l1: 0.00010667130289434949, l2: 0.00036076763054177934   Iteration 35 of 100, tot loss = 4.701229350907462, l1: 0.00010749678310405995, l2: 0.00036262614926922004   Iteration 36 of 100, tot loss = 4.64683343635665, l1: 0.00010675868199743693, l2: 0.00035792465900562494   Iteration 37 of 100, tot loss = 4.64065826583553, l1: 0.00010676913665815232, l2: 0.0003572966867264964   Iteration 38 of 100, tot loss = 4.685821184986516, l1: 0.00010756095262446539, l2: 0.00036102116313199267   Iteration 39 of 100, tot loss = 4.702953329453101, l1: 0.00010776862286431238, l2: 0.00036252670822880016   Iteration 40 of 100, tot loss = 4.710861578583717, l1: 0.00010798744388011983, l2: 0.00036309871211415157   Iteration 41 of 100, tot loss = 4.669351656262467, l1: 0.00010677725385459957, l2: 0.0003601579100643171   Iteration 42 of 100, tot loss = 4.689218631812504, l1: 0.00010676959833030455, l2: 0.00036215226351660456   Iteration 43 of 100, tot loss = 4.647490886754768, l1: 0.00010640907820806904, l2: 0.00035834000926453967   Iteration 44 of 100, tot loss = 4.679524939168584, l1: 0.00010664461495418271, l2: 0.0003613078770285938   Iteration 45 of 100, tot loss = 4.672911916838752, l1: 0.00010662585280240617, l2: 0.0003606653368074654   Iteration 46 of 100, tot loss = 4.67138154351193, l1: 0.00010685676834723183, l2: 0.0003602813835048279   Iteration 47 of 100, tot loss = 4.648336489149865, l1: 0.00010660658277784217, l2: 0.0003582270635772457   Iteration 48 of 100, tot loss = 4.647367961704731, l1: 0.00010627693594263594, l2: 0.0003584598571251263   Iteration 49 of 100, tot loss = 4.605163440412404, l1: 0.00010524592215340224, l2: 0.00035527041918427055   Iteration 50 of 100, tot loss = 4.568673098087311, l1: 0.00010473924114194233, l2: 0.0003521280660061166   Iteration 51 of 100, tot loss = 4.578062639516943, l1: 0.00010504542403984541, l2: 0.00035276083736771757   Iteration 52 of 100, tot loss = 4.573233845142218, l1: 0.00010467015013441247, l2: 0.0003526532320672861   Iteration 53 of 100, tot loss = 4.618685517670973, l1: 0.00010519662883213967, l2: 0.00035667192039325974   Iteration 54 of 100, tot loss = 4.598965011261128, l1: 0.0001045583048161697, l2: 0.00035533819377801763   Iteration 55 of 100, tot loss = 4.631835133379156, l1: 0.0001048243631719908, l2: 0.00035835914737121624   Iteration 56 of 100, tot loss = 4.6458277468170435, l1: 0.00010546036388145044, l2: 0.00035912240803424666   Iteration 57 of 100, tot loss = 4.628971720996656, l1: 0.00010479347551345679, l2: 0.0003581036938141966   Iteration 58 of 100, tot loss = 4.598225562736906, l1: 0.00010456981362949591, l2: 0.0003552527401740823   Iteration 59 of 100, tot loss = 4.600531539674533, l1: 0.0001046962352736924, l2: 0.00035535691599993   Iteration 60 of 100, tot loss = 4.6763053675492605, l1: 0.00010572827953486315, l2: 0.0003619022546142029   Iteration 61 of 100, tot loss = 4.6648324024481855, l1: 0.00010584339352284527, l2: 0.0003606398441050141   Iteration 62 of 100, tot loss = 4.6973151987598785, l1: 0.00010671847350397823, l2: 0.000363013043346244   Iteration 63 of 100, tot loss = 4.672706462088085, l1: 0.00010613564547153175, l2: 0.0003611349980235987   Iteration 64 of 100, tot loss = 4.658607533201575, l1: 0.00010612659974640337, l2: 0.00035973415106127504   Iteration 65 of 100, tot loss = 4.637091242350065, l1: 0.0001058741766498245, l2: 0.00035783494509255084   Iteration 66 of 100, tot loss = 4.6398722782279505, l1: 0.0001058763765203449, l2: 0.00035811084848264176   Iteration 67 of 100, tot loss = 4.627991073167146, l1: 0.00010566213916432905, l2: 0.0003571369656563194   Iteration 68 of 100, tot loss = 4.687328978496439, l1: 0.00010678663436599035, l2: 0.0003619462612037109   Iteration 69 of 100, tot loss = 4.750554466593092, l1: 0.00010766937428844108, l2: 0.0003673860704546551   Iteration 70 of 100, tot loss = 4.759776852812085, l1: 0.00010782954138578913, l2: 0.00036814814167363306   Iteration 71 of 100, tot loss = 4.759279353517882, l1: 0.00010761883330956088, l2: 0.00036830909986337515   Iteration 72 of 100, tot loss = 4.750776921709378, l1: 0.00010790157299804075, l2: 0.0003671761171264936   Iteration 73 of 100, tot loss = 4.753724426439364, l1: 0.00010820436114979564, l2: 0.0003671680791590567   Iteration 74 of 100, tot loss = 4.767800120083061, l1: 0.00010862547723927246, l2: 0.0003681545329871713   Iteration 75 of 100, tot loss = 4.783887343406677, l1: 0.00010886615350803671, l2: 0.00036952257893669107   Iteration 76 of 100, tot loss = 4.788744482554887, l1: 0.00010866872794155682, l2: 0.0003702057185460274   Iteration 77 of 100, tot loss = 4.792861574656003, l1: 0.00010901799939713417, l2: 0.0003702681562288773   Iteration 78 of 100, tot loss = 4.82964587975771, l1: 0.00010986605537674713, l2: 0.00037309853098868654   Iteration 79 of 100, tot loss = 4.82035145729403, l1: 0.0001099421247148067, l2: 0.0003720930194676724   Iteration 80 of 100, tot loss = 4.824609722197056, l1: 0.00011001219741046953, l2: 0.000372448772759526   Iteration 81 of 100, tot loss = 4.793202463491463, l1: 0.00010939961798556643, l2: 0.00036992062634880435   Iteration 82 of 100, tot loss = 4.79098967081163, l1: 0.00010918152044568404, l2: 0.0003699174448251497   Iteration 83 of 100, tot loss = 4.78174511064966, l1: 0.00010889750502615079, l2: 0.0003692770042984062   Iteration 84 of 100, tot loss = 4.736401242159662, l1: 0.00010792663689634148, l2: 0.0003657134855616494   Iteration 85 of 100, tot loss = 4.731802274900324, l1: 0.00010766820883231155, l2: 0.00036551201662993714   Iteration 86 of 100, tot loss = 4.738554385512374, l1: 0.00010783626230688749, l2: 0.0003660191744762245   Iteration 87 of 100, tot loss = 4.749889825267354, l1: 0.00010808716165908778, l2: 0.0003669018190301923   Iteration 88 of 100, tot loss = 4.753476638008248, l1: 0.00010817942209293754, l2: 0.0003671682399561343   Iteration 89 of 100, tot loss = 4.774350698744313, l1: 0.00010855740052402632, l2: 0.0003688776672829539   Iteration 90 of 100, tot loss = 4.772989306847254, l1: 0.0001086468751155836, l2: 0.000368652053374616   Iteration 91 of 100, tot loss = 4.8139761954873475, l1: 0.00010937707256644089, l2: 0.0003720205447509895   Iteration 92 of 100, tot loss = 4.824675800359768, l1: 0.00010932967291449566, l2: 0.0003731379048353436   Iteration 93 of 100, tot loss = 4.811686756149415, l1: 0.00010920482610120133, l2: 0.0003719638469935675   Iteration 94 of 100, tot loss = 4.8141685337462325, l1: 0.00010902646613914762, l2: 0.0003723903848483732   Iteration 95 of 100, tot loss = 4.831274212661542, l1: 0.00010918840387986213, l2: 0.00037393901492530285   Iteration 96 of 100, tot loss = 4.832703738783796, l1: 0.0001090492878764356, l2: 0.00037422108357532125   Iteration 97 of 100, tot loss = 4.8404900652846115, l1: 0.00010927952992920068, l2: 0.00037476947399386754   Iteration 98 of 100, tot loss = 4.839760648352759, l1: 0.00010945895161169073, l2: 0.0003745171105000903   Iteration 99 of 100, tot loss = 4.835302260789004, l1: 0.00010942901282778805, l2: 0.00037410121060957226   Iteration 100 of 100, tot loss = 4.838619846701622, l1: 0.00010952739148706314, l2: 0.0003743345908151241
   End of epoch 1319; saving model... 

Epoch 1320 of 2000
   Iteration 1 of 100, tot loss = 3.3172309398651123, l1: 6.928170478204265e-05, l2: 0.0002624414046294987   Iteration 2 of 100, tot loss = 3.974589467048645, l1: 8.793345114099793e-05, l2: 0.00030952549423091114   Iteration 3 of 100, tot loss = 4.785278876622518, l1: 0.00010979965494091933, l2: 0.0003687282248089711   Iteration 4 of 100, tot loss = 4.471484422683716, l1: 0.00010693128206185065, l2: 0.0003402171569177881   Iteration 5 of 100, tot loss = 4.50285005569458, l1: 0.00010486433457117528, l2: 0.0003454206686001271   Iteration 6 of 100, tot loss = 4.199201703071594, l1: 9.60083095075485e-05, l2: 0.00032391185717036325   Iteration 7 of 100, tot loss = 4.802519219262259, l1: 0.0001042721866854533, l2: 0.0003759797296619841   Iteration 8 of 100, tot loss = 4.746113508939743, l1: 0.00010278507988914498, l2: 0.00037182626692811027   Iteration 9 of 100, tot loss = 4.726584884855482, l1: 0.00010418394756723299, l2: 0.0003684745396539155   Iteration 10 of 100, tot loss = 4.618561935424805, l1: 0.00010373412333137821, l2: 0.00035812206915579735   Iteration 11 of 100, tot loss = 4.5526792786338115, l1: 0.00010418082324163565, l2: 0.0003510871028993279   Iteration 12 of 100, tot loss = 4.475605467955272, l1: 0.00010363711498939665, l2: 0.00034392343028836575   Iteration 13 of 100, tot loss = 4.383925731365498, l1: 0.00010013525220878924, l2: 0.00033825731952674687   Iteration 14 of 100, tot loss = 4.353196927479336, l1: 9.929230327543337e-05, l2: 0.000336027388194842   Iteration 15 of 100, tot loss = 4.37900889714559, l1: 0.00010046434011504365, l2: 0.00033743654882224897   Iteration 16 of 100, tot loss = 4.440077602863312, l1: 0.00010117440911017184, l2: 0.0003428333529882366   Iteration 17 of 100, tot loss = 4.419167518615723, l1: 0.00010151601578828002, l2: 0.00034040073788834405   Iteration 18 of 100, tot loss = 4.641393608517117, l1: 0.00010510489927481912, l2: 0.00035903446486271505   Iteration 19 of 100, tot loss = 4.668118527061061, l1: 0.00010654293510015123, l2: 0.0003602689202556289   Iteration 20 of 100, tot loss = 4.551904475688934, l1: 0.00010390104525868083, l2: 0.0003512894050800242   Iteration 21 of 100, tot loss = 4.450282596406483, l1: 0.00010254432241448999, l2: 0.00034248393993558627   Iteration 22 of 100, tot loss = 4.523662307045677, l1: 0.00010413494254118467, l2: 0.00034823129085866225   Iteration 23 of 100, tot loss = 4.488387916399085, l1: 0.00010325853778024042, l2: 0.00034558025705020714   Iteration 24 of 100, tot loss = 4.462797433137894, l1: 0.0001028978981594264, l2: 0.0003433818480213328   Iteration 25 of 100, tot loss = 4.52209174156189, l1: 0.0001033091485442128, l2: 0.00034890002803876995   Iteration 26 of 100, tot loss = 4.439265938905569, l1: 0.00010149898060682553, l2: 0.0003424276152285389   Iteration 27 of 100, tot loss = 4.580906929793181, l1: 0.0001038383773077469, l2: 0.0003542523169286411   Iteration 28 of 100, tot loss = 4.598659626075199, l1: 0.00010462347602567337, l2: 0.0003552424875254344   Iteration 29 of 100, tot loss = 4.654841973863799, l1: 0.0001053373912170706, l2: 0.00036014680718553477   Iteration 30 of 100, tot loss = 4.605133183797201, l1: 0.00010356767537208118, l2: 0.0003569456438223521   Iteration 31 of 100, tot loss = 4.61235107914094, l1: 0.00010302446929516361, l2: 0.00035821063905924316   Iteration 32 of 100, tot loss = 4.609587728977203, l1: 0.00010310318486972392, l2: 0.00035785558884526836   Iteration 33 of 100, tot loss = 4.586393970431703, l1: 0.00010278239256656502, l2: 0.000355857005519228   Iteration 34 of 100, tot loss = 4.626522211467519, l1: 0.0001030461583414923, l2: 0.00035960606456843805   Iteration 35 of 100, tot loss = 4.575850766045707, l1: 0.00010219780992234258, l2: 0.00035538726868773144   Iteration 36 of 100, tot loss = 4.619170831309424, l1: 0.00010289318854322321, l2: 0.0003590238969385003   Iteration 37 of 100, tot loss = 4.553911769712293, l1: 0.00010194186275790294, l2: 0.0003534493160849387   Iteration 38 of 100, tot loss = 4.525398241846185, l1: 0.00010163868893486276, l2: 0.0003509011370816121   Iteration 39 of 100, tot loss = 4.544998535743127, l1: 0.00010249913863775034, l2: 0.0003520007169124885   Iteration 40 of 100, tot loss = 4.543327736854553, l1: 0.00010269948352288338, l2: 0.00035163329230272213   Iteration 41 of 100, tot loss = 4.581260820714439, l1: 0.0001036190815949983, l2: 0.00035450700244571014   Iteration 42 of 100, tot loss = 4.5618386041550405, l1: 0.00010355045525925344, l2: 0.0003526334074281511   Iteration 43 of 100, tot loss = 4.564363335454186, l1: 0.00010363827451053725, l2: 0.0003527980609147181   Iteration 44 of 100, tot loss = 4.5386828780174255, l1: 0.00010320665379285029, l2: 0.00035066163598623297   Iteration 45 of 100, tot loss = 4.52238097190857, l1: 0.00010236166474189505, l2: 0.0003498764339989672   Iteration 46 of 100, tot loss = 4.505246867304263, l1: 0.00010221877109079682, l2: 0.00034830591681858766   Iteration 47 of 100, tot loss = 4.515622321595537, l1: 0.0001018132270280361, l2: 0.0003497490059941056   Iteration 48 of 100, tot loss = 4.482021525502205, l1: 0.00010115337598411618, l2: 0.0003470487775606064   Iteration 49 of 100, tot loss = 4.475562674658639, l1: 0.00010068857264574332, l2: 0.00034686769578577383   Iteration 50 of 100, tot loss = 4.489851622581482, l1: 0.00010111357383721042, l2: 0.0003478715891833417   Iteration 51 of 100, tot loss = 4.447345214731553, l1: 0.00010012914258279089, l2: 0.00034460537968769524   Iteration 52 of 100, tot loss = 4.443797702972706, l1: 0.00010006810939208102, l2: 0.0003443116617675584   Iteration 53 of 100, tot loss = 4.459326928516604, l1: 0.0001007444370770827, l2: 0.00034518825672215446   Iteration 54 of 100, tot loss = 4.4302249175530894, l1: 0.00010035028134114798, l2: 0.0003426722113444056   Iteration 55 of 100, tot loss = 4.413823522220959, l1: 0.00010054916836617684, l2: 0.0003408331847326322   Iteration 56 of 100, tot loss = 4.405631197350366, l1: 0.0001006785795263048, l2: 0.000339884541192857   Iteration 57 of 100, tot loss = 4.405051653845268, l1: 0.00010054235207733877, l2: 0.00033996281471397534   Iteration 58 of 100, tot loss = 4.373645803024029, l1: 0.0001000110016069937, l2: 0.0003373535803583419   Iteration 59 of 100, tot loss = 4.36291351560819, l1: 9.978018549144647e-05, l2: 0.0003365111676384142   Iteration 60 of 100, tot loss = 4.355781968434652, l1: 9.980235384621968e-05, l2: 0.0003357758444811528   Iteration 61 of 100, tot loss = 4.3588437096017305, l1: 0.0001001020265575193, l2: 0.0003357823455294014   Iteration 62 of 100, tot loss = 4.404473250912082, l1: 0.00010114474547839152, l2: 0.0003393025807811007   Iteration 63 of 100, tot loss = 4.3840091152796665, l1: 0.00010082640431983959, l2: 0.00033757450809288357   Iteration 64 of 100, tot loss = 4.3667298667132854, l1: 0.0001005204634338952, l2: 0.000336152524141653   Iteration 65 of 100, tot loss = 4.32999940101917, l1: 9.956532572127448e-05, l2: 0.0003334346153468897   Iteration 66 of 100, tot loss = 4.349283599492275, l1: 9.983498856661643e-05, l2: 0.00033509337178353843   Iteration 67 of 100, tot loss = 4.335237704106231, l1: 9.969664064719022e-05, l2: 0.0003338271303279603   Iteration 68 of 100, tot loss = 4.324089111650691, l1: 9.946205757253611e-05, l2: 0.0003329468542206622   Iteration 69 of 100, tot loss = 4.3196452648743335, l1: 9.979708409156191e-05, l2: 0.00033216744275810873   Iteration 70 of 100, tot loss = 4.31325181382043, l1: 9.953058553427191e-05, l2: 0.00033179459631875425   Iteration 71 of 100, tot loss = 4.318452302838715, l1: 9.971417521100334e-05, l2: 0.0003321310556510096   Iteration 72 of 100, tot loss = 4.32017330163055, l1: 9.969036788485634e-05, l2: 0.0003323269631639252   Iteration 73 of 100, tot loss = 4.309650775504439, l1: 9.943739720779688e-05, l2: 0.00033152768127652117   Iteration 74 of 100, tot loss = 4.295538581706382, l1: 9.926277670200688e-05, l2: 0.0003302910825712732   Iteration 75 of 100, tot loss = 4.307823367118836, l1: 9.972655747939522e-05, l2: 0.0003310557806010668   Iteration 76 of 100, tot loss = 4.314063707464619, l1: 9.997596642183213e-05, l2: 0.0003314304058700432   Iteration 77 of 100, tot loss = 4.317547118508971, l1: 0.0001001704975398705, l2: 0.00033158421582185556   Iteration 78 of 100, tot loss = 4.288479091265263, l1: 9.967996569078726e-05, l2: 0.000329167944959735   Iteration 79 of 100, tot loss = 4.280355273922788, l1: 9.950130822025003e-05, l2: 0.0003285342206680393   Iteration 80 of 100, tot loss = 4.280251391232014, l1: 9.951790989362052e-05, l2: 0.0003285072309154202   Iteration 81 of 100, tot loss = 4.264148996200091, l1: 9.885102010656861e-05, l2: 0.00032756388145010275   Iteration 82 of 100, tot loss = 4.276900973261856, l1: 9.898083199670243e-05, l2: 0.00032870926757246576   Iteration 83 of 100, tot loss = 4.292854109442378, l1: 9.931649647005105e-05, l2: 0.000329968916267684   Iteration 84 of 100, tot loss = 4.299395324218841, l1: 9.962895750331449e-05, l2: 0.0003303105770014337   Iteration 85 of 100, tot loss = 4.332488908487208, l1: 0.00010011208454242852, l2: 0.0003331368085448904   Iteration 86 of 100, tot loss = 4.35783339794292, l1: 0.00010052948405351598, l2: 0.0003352538578626483   Iteration 87 of 100, tot loss = 4.386638604361435, l1: 0.00010103353671528848, l2: 0.0003376303258386772   Iteration 88 of 100, tot loss = 4.374309803951871, l1: 0.0001007701629615357, l2: 0.00033666081948137037   Iteration 89 of 100, tot loss = 4.385130655899476, l1: 0.00010101330929940692, l2: 0.00033749975777477067   Iteration 90 of 100, tot loss = 4.353997815979851, l1: 0.00010048257172456942, l2: 0.00033491721131011015   Iteration 91 of 100, tot loss = 4.370264469922244, l1: 0.0001005352724050581, l2: 0.0003364911759630419   Iteration 92 of 100, tot loss = 4.365299660226573, l1: 0.00010069835962380951, l2: 0.0003358316076312052   Iteration 93 of 100, tot loss = 4.367379552574568, l1: 0.0001007758142139953, l2: 0.00033596214232465595   Iteration 94 of 100, tot loss = 4.353411017580235, l1: 0.00010055512881235893, l2: 0.00033478597423651236   Iteration 95 of 100, tot loss = 4.343296896783929, l1: 0.00010057063030360855, l2: 0.00033375906059518456   Iteration 96 of 100, tot loss = 4.351736250023047, l1: 0.00010083739554526498, l2: 0.00033433623078356806   Iteration 97 of 100, tot loss = 4.337398428277871, l1: 0.00010071834781838428, l2: 0.00033302149632177553   Iteration 98 of 100, tot loss = 4.324272206851414, l1: 0.00010060350623276389, l2: 0.0003318237157108509   Iteration 99 of 100, tot loss = 4.319629153819999, l1: 0.00010047279875384494, l2: 0.00033149011788838025   Iteration 100 of 100, tot loss = 4.343290195465088, l1: 0.00010077363760501611, l2: 0.00033355538282194176
   End of epoch 1320; saving model... 

Epoch 1321 of 2000
   Iteration 1 of 100, tot loss = 4.955430030822754, l1: 0.00011811841250164434, l2: 0.00037742461427114904   Iteration 2 of 100, tot loss = 4.163295388221741, l1: 0.00010329156430088915, l2: 0.0003130379773210734   Iteration 3 of 100, tot loss = 4.195939143498738, l1: 0.00010071427095681429, l2: 0.0003188796399626881   Iteration 4 of 100, tot loss = 4.468285381793976, l1: 0.00010409055357740726, l2: 0.0003427379851927981   Iteration 5 of 100, tot loss = 4.304796743392944, l1: 9.951146348612383e-05, l2: 0.000330968206981197   Iteration 6 of 100, tot loss = 4.566277066866557, l1: 0.00010300533540430479, l2: 0.0003536223666742444   Iteration 7 of 100, tot loss = 4.336717299052647, l1: 0.00010072835721075535, l2: 0.0003329433696178187   Iteration 8 of 100, tot loss = 4.2667830884456635, l1: 0.0001018208986351965, l2: 0.0003248574084864231   Iteration 9 of 100, tot loss = 4.288367509841919, l1: 0.00010286500926465831, l2: 0.00032597173736172006   Iteration 10 of 100, tot loss = 4.441704344749451, l1: 0.00010585894997348078, l2: 0.0003383114832104184   Iteration 11 of 100, tot loss = 4.300018592314287, l1: 0.00010299397135068747, l2: 0.00032700788737698036   Iteration 12 of 100, tot loss = 4.283600389957428, l1: 0.00010284505090870273, l2: 0.00032551498831404996   Iteration 13 of 100, tot loss = 4.226395295216487, l1: 0.0001021666231771143, l2: 0.00032047290677348006   Iteration 14 of 100, tot loss = 4.301327858652387, l1: 0.00010333370145027792, l2: 0.0003267990853471149   Iteration 15 of 100, tot loss = 4.405558061599732, l1: 0.00010426763716774682, l2: 0.0003362881698800872   Iteration 16 of 100, tot loss = 4.470225229859352, l1: 0.0001049266697918938, l2: 0.00034209585464850534   Iteration 17 of 100, tot loss = 4.648790457669427, l1: 0.00010890047868837, l2: 0.0003559785699882709   Iteration 18 of 100, tot loss = 4.560160252783033, l1: 0.00010730383574910875, l2: 0.0003487121915289511   Iteration 19 of 100, tot loss = 4.6773599197990015, l1: 0.00010893745715484808, l2: 0.0003587985350015132   Iteration 20 of 100, tot loss = 4.745248639583588, l1: 0.00011033620685338975, l2: 0.00036418865784071386   Iteration 21 of 100, tot loss = 4.622021413984752, l1: 0.0001071868867993111, l2: 0.00035501525521145334   Iteration 22 of 100, tot loss = 4.652389558878812, l1: 0.00010672502966454803, l2: 0.0003585139268331907   Iteration 23 of 100, tot loss = 4.65698274322178, l1: 0.0001057846654831614, l2: 0.0003599136101041475   Iteration 24 of 100, tot loss = 4.706401973962784, l1: 0.0001072306069242283, l2: 0.00036340959316779237   Iteration 25 of 100, tot loss = 4.74821623802185, l1: 0.00010865004893275909, l2: 0.0003661715763155371   Iteration 26 of 100, tot loss = 4.87845214513632, l1: 0.00011073429763760381, l2: 0.00037711091863457114   Iteration 27 of 100, tot loss = 4.848563379711575, l1: 0.00010965056154182767, l2: 0.0003752057781201546   Iteration 28 of 100, tot loss = 4.826686765466418, l1: 0.00010830391940024648, l2: 0.00037436475810994   Iteration 29 of 100, tot loss = 4.837930736870601, l1: 0.00010833377282010745, l2: 0.00037545930285103106   Iteration 30 of 100, tot loss = 4.782736142476399, l1: 0.00010746393102939086, l2: 0.00037080968516723566   Iteration 31 of 100, tot loss = 4.7905778884887695, l1: 0.00010763532531376357, l2: 0.00037142246559773   Iteration 32 of 100, tot loss = 4.835732892155647, l1: 0.00010818692464908963, l2: 0.00037538636661338387   Iteration 33 of 100, tot loss = 4.824162598812219, l1: 0.00010794550976380847, l2: 0.0003744707518666418   Iteration 34 of 100, tot loss = 4.851222220589133, l1: 0.00010859813622065464, l2: 0.00037652408782913187   Iteration 35 of 100, tot loss = 4.895922715323312, l1: 0.00010998416120336124, l2: 0.00037960811335194326   Iteration 36 of 100, tot loss = 4.897383530934651, l1: 0.0001094900450577067, l2: 0.00038024831155780703   Iteration 37 of 100, tot loss = 4.932804558728193, l1: 0.0001095896957456323, l2: 0.0003836907641458753   Iteration 38 of 100, tot loss = 4.923962141338148, l1: 0.00010883473451509704, l2: 0.00038356148395809884   Iteration 39 of 100, tot loss = 4.900513037657126, l1: 0.00010860312473736345, l2: 0.0003814481837579455   Iteration 40 of 100, tot loss = 4.901712906360626, l1: 0.00010885175406656344, l2: 0.0003813195420661941   Iteration 41 of 100, tot loss = 4.871223717201047, l1: 0.00010871480205933926, l2: 0.0003784075748493395   Iteration 42 of 100, tot loss = 4.945446025757563, l1: 0.00011020342303492639, l2: 0.0003843411853137825   Iteration 43 of 100, tot loss = 5.013550159543059, l1: 0.00011123093865408433, l2: 0.00039012408355077686   Iteration 44 of 100, tot loss = 4.952872297980568, l1: 0.00011002387335875855, l2: 0.00038526336257663473   Iteration 45 of 100, tot loss = 4.968791304694282, l1: 0.00011023381982037488, l2: 0.0003866453170000265   Iteration 46 of 100, tot loss = 4.987892990526945, l1: 0.00010978913103273321, l2: 0.0003890001737644248   Iteration 47 of 100, tot loss = 4.973529805528357, l1: 0.0001094690846815517, l2: 0.00038788390179917695   Iteration 48 of 100, tot loss = 4.964540968338649, l1: 0.00010931718111351074, l2: 0.0003871369214418034   Iteration 49 of 100, tot loss = 4.962696815023617, l1: 0.00010934614165678944, l2: 0.000386923545619891   Iteration 50 of 100, tot loss = 4.992580976486206, l1: 0.00010945147390884812, l2: 0.0003898066293913871   Iteration 51 of 100, tot loss = 5.0594875204796885, l1: 0.00011062619980258009, l2: 0.00039532255751135596   Iteration 52 of 100, tot loss = 5.047622845723079, l1: 0.00011052629319097972, l2: 0.00039423599642199965   Iteration 53 of 100, tot loss = 5.017445879162483, l1: 0.000109712588109344, l2: 0.0003920320046951397   Iteration 54 of 100, tot loss = 5.020714124043782, l1: 0.00010984081146253393, l2: 0.000392230606478363   Iteration 55 of 100, tot loss = 5.045203859155828, l1: 0.00010993471256700683, l2: 0.0003945856794333932   Iteration 56 of 100, tot loss = 5.04265694958823, l1: 0.00011021779255315778, l2: 0.00039404790836020505   Iteration 57 of 100, tot loss = 5.024332975086413, l1: 0.00010978703185652481, l2: 0.0003926462714413279   Iteration 58 of 100, tot loss = 5.05815548732363, l1: 0.00011028485503880677, l2: 0.00039553069931069583   Iteration 59 of 100, tot loss = 5.055561615248858, l1: 0.0001102941204084407, l2: 0.00039526204661450393   Iteration 60 of 100, tot loss = 5.0472438414891565, l1: 0.00011014768321426042, l2: 0.00039457670597281926   Iteration 61 of 100, tot loss = 5.0370006404939245, l1: 0.00010978866464670243, l2: 0.0003939114041534848   Iteration 62 of 100, tot loss = 5.069135289038381, l1: 0.00011029111045636162, l2: 0.0003966224227746528   Iteration 63 of 100, tot loss = 5.043657454233321, l1: 0.00011012085757563851, l2: 0.00039424489174659055   Iteration 64 of 100, tot loss = 5.098297223448753, l1: 0.00011075896208012637, l2: 0.0003990707637058222   Iteration 65 of 100, tot loss = 5.085444046900823, l1: 0.0001109765848713533, l2: 0.0003975678230814922   Iteration 66 of 100, tot loss = 5.07633451981978, l1: 0.00011083290282289398, l2: 0.0003968005524560188   Iteration 67 of 100, tot loss = 5.0714605103677775, l1: 0.00011112232932103311, l2: 0.00039602372510386493   Iteration 68 of 100, tot loss = 5.047069759929881, l1: 0.00011045316082077882, l2: 0.00039425381863325397   Iteration 69 of 100, tot loss = 5.0427695633708565, l1: 0.00011025518403545468, l2: 0.00039402177545538956   Iteration 70 of 100, tot loss = 5.024708894320897, l1: 0.00010992462552426982, l2: 0.00039254626691607493   Iteration 71 of 100, tot loss = 5.043672578435548, l1: 0.00011035676744912462, l2: 0.00039401049332194767   Iteration 72 of 100, tot loss = 5.03760411673122, l1: 0.00011020216702996145, l2: 0.0003935582479041639   Iteration 73 of 100, tot loss = 5.0100551957953465, l1: 0.00011005455424509981, l2: 0.0003909509685225402   Iteration 74 of 100, tot loss = 5.003440508971343, l1: 0.00011004162911991148, l2: 0.00039030242506005625   Iteration 75 of 100, tot loss = 4.986065012613932, l1: 0.00010963695070434673, l2: 0.00038896955375093966   Iteration 76 of 100, tot loss = 4.971536426167739, l1: 0.00010946509423535502, l2: 0.00038768855167865314   Iteration 77 of 100, tot loss = 4.993309853912948, l1: 0.00010981597058292079, l2: 0.0003895150176657654   Iteration 78 of 100, tot loss = 4.9797672766905565, l1: 0.00010967786506509397, l2: 0.00038829886556740326   Iteration 79 of 100, tot loss = 4.998893692523619, l1: 0.00010997534646097088, l2: 0.0003899140259519196   Iteration 80 of 100, tot loss = 5.0074003547430035, l1: 0.0001105156537505536, l2: 0.0003902243848642684   Iteration 81 of 100, tot loss = 4.980051570468479, l1: 0.000110047646120182, l2: 0.00038795751400211434   Iteration 82 of 100, tot loss = 5.058795835913681, l1: 0.00011116546024490295, l2: 0.0003947141262980905   Iteration 83 of 100, tot loss = 5.04248545543257, l1: 0.00011070199370915799, l2: 0.00039354655491917516   Iteration 84 of 100, tot loss = 5.021231764838809, l1: 0.00011004665824051092, l2: 0.0003920765215991109   Iteration 85 of 100, tot loss = 5.027403607087977, l1: 0.00011025028254473855, l2: 0.0003924900813110392   Iteration 86 of 100, tot loss = 5.026922270309093, l1: 0.00011008034515577682, l2: 0.00039261188540813446   Iteration 87 of 100, tot loss = 5.029013513148516, l1: 0.00011043556782399604, l2: 0.0003924657866337347   Iteration 88 of 100, tot loss = 4.9985585754567925, l1: 0.00010977135384977869, l2: 0.00039008450667924165   Iteration 89 of 100, tot loss = 4.98668695567699, l1: 0.00010949787104160923, l2: 0.0003891708273728368   Iteration 90 of 100, tot loss = 4.982522373729282, l1: 0.00010944629009625512, l2: 0.0003888059498017861   Iteration 91 of 100, tot loss = 4.970183071199354, l1: 0.0001092469553571261, l2: 0.0003877713541362107   Iteration 92 of 100, tot loss = 4.9672666606695755, l1: 0.00010938743945070715, l2: 0.0003873392289160224   Iteration 93 of 100, tot loss = 4.960333631884668, l1: 0.00010946511083015901, l2: 0.0003865682548137322   Iteration 94 of 100, tot loss = 4.95020333503155, l1: 0.00010944511601298879, l2: 0.0003855752200135922   Iteration 95 of 100, tot loss = 4.939464385885941, l1: 0.00010942585056335176, l2: 0.00038452059042787083   Iteration 96 of 100, tot loss = 4.946206845343113, l1: 0.00010931512296489625, l2: 0.00038530556381980813   Iteration 97 of 100, tot loss = 4.929529691479869, l1: 0.0001090792953693812, l2: 0.0003838736759546722   Iteration 98 of 100, tot loss = 4.916997520291075, l1: 0.00010898334143608476, l2: 0.00038271641260136525   Iteration 99 of 100, tot loss = 4.921373073500816, l1: 0.00010916035840049094, l2: 0.00038297695103054396   Iteration 100 of 100, tot loss = 4.936496224403381, l1: 0.00010948130020551616, l2: 0.00038416832452639935
   End of epoch 1321; saving model... 

Epoch 1322 of 2000
   Iteration 1 of 100, tot loss = 3.2100791931152344, l1: 5.237850564299151e-05, l2: 0.0002686294028535485   Iteration 2 of 100, tot loss = 4.329031705856323, l1: 8.613393220002763e-05, l2: 0.0003467692295089364   Iteration 3 of 100, tot loss = 4.247766971588135, l1: 8.10610120727991e-05, l2: 0.0003437156750199695   Iteration 4 of 100, tot loss = 3.8899084329605103, l1: 7.741663648630492e-05, l2: 0.0003115742038062308   Iteration 5 of 100, tot loss = 3.8961586952209473, l1: 7.996581261977553e-05, l2: 0.0003096500498941168   Iteration 6 of 100, tot loss = 3.691965182622274, l1: 8.001647559770693e-05, l2: 0.0002891800346939514   Iteration 7 of 100, tot loss = 3.6271358217511858, l1: 7.867218976441239e-05, l2: 0.00028404138616419265   Iteration 8 of 100, tot loss = 3.674407124519348, l1: 8.154381430358626e-05, l2: 0.0002858968946384266   Iteration 9 of 100, tot loss = 3.5977364116244845, l1: 8.107194541177402e-05, l2: 0.00027870169368624274   Iteration 10 of 100, tot loss = 3.5252933740615844, l1: 8.012550097191706e-05, l2: 0.00027240383496973664   Iteration 11 of 100, tot loss = 3.4508937922391025, l1: 7.971462103093721e-05, l2: 0.00026537475721190936   Iteration 12 of 100, tot loss = 3.5725831588109336, l1: 8.162152213723554e-05, l2: 0.0002756367915329368   Iteration 13 of 100, tot loss = 3.7485967049231896, l1: 8.58513997697558e-05, l2: 0.00028900827097599034   Iteration 14 of 100, tot loss = 3.861696856362479, l1: 8.970499922205428e-05, l2: 0.00029646468361274207   Iteration 15 of 100, tot loss = 3.8804566065470376, l1: 8.860684368604174e-05, l2: 0.00029943881381768735   Iteration 16 of 100, tot loss = 3.986477553844452, l1: 9.042937972481013e-05, l2: 0.0003082183729929966   Iteration 17 of 100, tot loss = 4.053744372199564, l1: 9.232224008027355e-05, l2: 0.00031305219454225153   Iteration 18 of 100, tot loss = 4.09405239423116, l1: 9.315991741863804e-05, l2: 0.00031624531887549284   Iteration 19 of 100, tot loss = 4.23224411512676, l1: 9.54231019070568e-05, l2: 0.00032780130660332933   Iteration 20 of 100, tot loss = 4.342142558097839, l1: 9.747198710101657e-05, l2: 0.00033674226651783103   Iteration 21 of 100, tot loss = 4.316834960665021, l1: 9.681858535345999e-05, l2: 0.00033486490844682394   Iteration 22 of 100, tot loss = 4.350960872390053, l1: 9.845597775314342e-05, l2: 0.00033664010764120826   Iteration 23 of 100, tot loss = 4.410108701042507, l1: 9.913957309554857e-05, l2: 0.00034187129336793947   Iteration 24 of 100, tot loss = 4.413306464751561, l1: 9.794519246497657e-05, l2: 0.0003433854502266816   Iteration 25 of 100, tot loss = 4.394443101882935, l1: 9.731054626172409e-05, l2: 0.00034213376056868583   Iteration 26 of 100, tot loss = 4.32239031791687, l1: 9.659582652742616e-05, l2: 0.00033564320172165305   Iteration 27 of 100, tot loss = 4.405538294050428, l1: 9.76321386621782e-05, l2: 0.0003429216874927213   Iteration 28 of 100, tot loss = 4.45576412337167, l1: 9.868476419277223e-05, l2: 0.00034689164593666125   Iteration 29 of 100, tot loss = 4.4786864149159396, l1: 9.882818417752097e-05, l2: 0.0003490404558070967   Iteration 30 of 100, tot loss = 4.576117372512817, l1: 0.00010004801733884961, l2: 0.0003575637175041872   Iteration 31 of 100, tot loss = 4.680190686256655, l1: 0.00010145780669840713, l2: 0.00036656125889895783   Iteration 32 of 100, tot loss = 4.628214038908482, l1: 0.00010083562847285066, l2: 0.00036198577254253905   Iteration 33 of 100, tot loss = 4.677206523490675, l1: 0.00010239973815976444, l2: 0.0003653209114969341   Iteration 34 of 100, tot loss = 4.711357530425577, l1: 0.0001021857030590421, l2: 0.00036895004707142054   Iteration 35 of 100, tot loss = 4.80907107080732, l1: 0.00010373969001063545, l2: 0.00037716741457448474   Iteration 36 of 100, tot loss = 4.836692737208472, l1: 0.00010376684157462377, l2: 0.0003799024295100632   Iteration 37 of 100, tot loss = 4.903299802058452, l1: 0.00010519498696462629, l2: 0.0003851349898173499   Iteration 38 of 100, tot loss = 4.920707872039394, l1: 0.00010549344293203351, l2: 0.00038657734158301825   Iteration 39 of 100, tot loss = 5.024170160293579, l1: 0.00010678033066095594, l2: 0.00039563668318665947   Iteration 40 of 100, tot loss = 5.069445496797561, l1: 0.00010801263324538013, l2: 0.00039893191424198446   Iteration 41 of 100, tot loss = 5.035968169933412, l1: 0.00010780981357107166, l2: 0.00039578700103092847   Iteration 42 of 100, tot loss = 5.048704141662235, l1: 0.00010806308429372231, l2: 0.0003968073274119801   Iteration 43 of 100, tot loss = 5.1156261078147, l1: 0.00010918254769802483, l2: 0.0004023800598630725   Iteration 44 of 100, tot loss = 5.18933247978037, l1: 0.0001107017238113754, l2: 0.00040823152018922633   Iteration 45 of 100, tot loss = 5.162753433651394, l1: 0.00011047588470521279, l2: 0.00040579945521636144   Iteration 46 of 100, tot loss = 5.184437109076458, l1: 0.00011110004387497294, l2: 0.0004073436632651188   Iteration 47 of 100, tot loss = 5.162748519410479, l1: 0.0001107720425042839, l2: 0.00040550280654644397   Iteration 48 of 100, tot loss = 5.096477128565311, l1: 0.00010968034735014953, l2: 0.00039996736268221866   Iteration 49 of 100, tot loss = 5.124944733113659, l1: 0.00011036336908713268, l2: 0.00040213110184353986   Iteration 50 of 100, tot loss = 5.134661242961884, l1: 0.0001108623077016091, l2: 0.00040260381472762675   Iteration 51 of 100, tot loss = 5.149089923091963, l1: 0.00011094793237633912, l2: 0.0004039610577427654   Iteration 52 of 100, tot loss = 5.151012950218641, l1: 0.00011120013613645615, l2: 0.00040390115632012917   Iteration 53 of 100, tot loss = 5.115154925382362, l1: 0.00011047108609828975, l2: 0.0004010444039145028   Iteration 54 of 100, tot loss = 5.0669484822838395, l1: 0.00010936205479892023, l2: 0.0003973327906418557   Iteration 55 of 100, tot loss = 5.055493361299688, l1: 0.00010968228261018257, l2: 0.00039586705097462984   Iteration 56 of 100, tot loss = 5.060621455311775, l1: 0.00011005790572328676, l2: 0.00039600423692068683   Iteration 57 of 100, tot loss = 5.084738285917985, l1: 0.00011059711834062462, l2: 0.0003978767069240444   Iteration 58 of 100, tot loss = 5.102474342132437, l1: 0.0001103008970199755, l2: 0.000399946534438139   Iteration 59 of 100, tot loss = 5.106937640804356, l1: 0.00010977092023171342, l2: 0.000400922841035319   Iteration 60 of 100, tot loss = 5.1053841809431715, l1: 0.00010990830238976438, l2: 0.00040063011329039   Iteration 61 of 100, tot loss = 5.120538779946624, l1: 0.00011017886972098733, l2: 0.00040187500584221706   Iteration 62 of 100, tot loss = 5.142824209505512, l1: 0.00011040818296145693, l2: 0.0004038742352408477   Iteration 63 of 100, tot loss = 5.135565521225097, l1: 0.00011013805251134309, l2: 0.0004034184964857848   Iteration 64 of 100, tot loss = 5.146427055820823, l1: 0.00011056092381522831, l2: 0.0004040817786972184   Iteration 65 of 100, tot loss = 5.166396373968858, l1: 0.00011070441872177001, l2: 0.0004059352153070414   Iteration 66 of 100, tot loss = 5.161174519495531, l1: 0.00011057018728575768, l2: 0.00040554726133863863   Iteration 67 of 100, tot loss = 5.1766722077754, l1: 0.00011065957700501808, l2: 0.0004070076403823167   Iteration 68 of 100, tot loss = 5.153430635438246, l1: 0.0001104608621009153, l2: 0.00040488219832802904   Iteration 69 of 100, tot loss = 5.15616902579432, l1: 0.00011060446827879944, l2: 0.00040501243147182453   Iteration 70 of 100, tot loss = 5.1443797639438085, l1: 0.00011076601632729373, l2: 0.00040367195761064066   Iteration 71 of 100, tot loss = 5.131261454501622, l1: 0.00011085414443071604, l2: 0.00040227199866088814   Iteration 72 of 100, tot loss = 5.130209397938517, l1: 0.0001110079145594985, l2: 0.00040201302282285824   Iteration 73 of 100, tot loss = 5.11288934047908, l1: 0.0001109985387800357, l2: 0.00040029039289898   Iteration 74 of 100, tot loss = 5.092421604169382, l1: 0.00011056200855819043, l2: 0.000398680149472155   Iteration 75 of 100, tot loss = 5.10292141755422, l1: 0.00011052259501108589, l2: 0.0003997695440193638   Iteration 76 of 100, tot loss = 5.091170061575739, l1: 0.00011038459060249519, l2: 0.0003987324125773739   Iteration 77 of 100, tot loss = 5.090088635296016, l1: 0.00011040243771276437, l2: 0.0003986064227095309   Iteration 78 of 100, tot loss = 5.070090952592018, l1: 0.00010972356694065429, l2: 0.00039728552506764967   Iteration 79 of 100, tot loss = 5.04500467716893, l1: 0.00010941500046630861, l2: 0.0003950854642184074   Iteration 80 of 100, tot loss = 5.0516378119587895, l1: 0.00010944219616249029, l2: 0.00039572158220835263   Iteration 81 of 100, tot loss = 5.029405351038332, l1: 0.00010930390357159159, l2: 0.0003936366286965225   Iteration 82 of 100, tot loss = 5.038794411391747, l1: 0.00010969317545681318, l2: 0.0003941862628491391   Iteration 83 of 100, tot loss = 5.003742862896747, l1: 0.00010888252119955898, l2: 0.0003914917624241347   Iteration 84 of 100, tot loss = 5.010560213100343, l1: 0.00010930570425400566, l2: 0.0003917503144337042   Iteration 85 of 100, tot loss = 5.03100166461047, l1: 0.00010984450726776713, l2: 0.00039325565620161156   Iteration 86 of 100, tot loss = 5.02682948805565, l1: 0.00010952653894153542, l2: 0.0003931564069082311   Iteration 87 of 100, tot loss = 5.034986415128598, l1: 0.00010964015336344608, l2: 0.0003938584848729915   Iteration 88 of 100, tot loss = 5.02579181980003, l1: 0.00010943934818318336, l2: 0.00039313983043856837   Iteration 89 of 100, tot loss = 5.028633445836185, l1: 0.00010893883698147701, l2: 0.0003939245044635713   Iteration 90 of 100, tot loss = 5.048076609770457, l1: 0.00010909991764896808, l2: 0.0003957077404872204   Iteration 91 of 100, tot loss = 5.0469986750529365, l1: 0.0001090661127898704, l2: 0.00039563375201516346   Iteration 92 of 100, tot loss = 5.035701172507328, l1: 0.00010884189732282918, l2: 0.00039472821695001227   Iteration 93 of 100, tot loss = 5.012830012588091, l1: 0.00010842723819340819, l2: 0.000392855760129419   Iteration 94 of 100, tot loss = 5.015680720197394, l1: 0.00010860884964479719, l2: 0.00039295921925123426   Iteration 95 of 100, tot loss = 5.051703032694365, l1: 0.00010901569740350456, l2: 0.00039615460294666456   Iteration 96 of 100, tot loss = 5.044040298710267, l1: 0.0001091386273325649, l2: 0.00039526539982640924   Iteration 97 of 100, tot loss = 5.03228510900871, l1: 0.0001091319646013627, l2: 0.0003940965438720089   Iteration 98 of 100, tot loss = 5.019885300373544, l1: 0.00010920310980546507, l2: 0.0003927854177534903   Iteration 99 of 100, tot loss = 5.040932206192402, l1: 0.0001093512546871389, l2: 0.0003947419635634023   Iteration 100 of 100, tot loss = 5.011752566099167, l1: 0.00010876956916035852, l2: 0.000392405685124686
   End of epoch 1322; saving model... 

Epoch 1323 of 2000
   Iteration 1 of 100, tot loss = 2.301055908203125, l1: 6.103743362473324e-05, l2: 0.0001690681674517691   Iteration 2 of 100, tot loss = 4.885494709014893, l1: 0.00010853592175408266, l2: 0.0003800135455094278   Iteration 3 of 100, tot loss = 6.7213560740153, l1: 0.00012644935971669233, l2: 0.0005456862466720244   Iteration 4 of 100, tot loss = 6.59502899646759, l1: 0.0001241468999069184, l2: 0.0005353559972718358   Iteration 5 of 100, tot loss = 6.095483779907227, l1: 0.00011659120063995942, l2: 0.0004929571703542023   Iteration 6 of 100, tot loss = 5.768472512563069, l1: 0.00011454265040811151, l2: 0.0004623045970220119   Iteration 7 of 100, tot loss = 5.505526031766619, l1: 0.00010921249382330902, l2: 0.00044134010594072085   Iteration 8 of 100, tot loss = 5.360549300909042, l1: 0.00011084968900831882, l2: 0.00042520523857092485   Iteration 9 of 100, tot loss = 5.436422480477227, l1: 0.0001129737857910287, l2: 0.0004306684616797914   Iteration 10 of 100, tot loss = 5.4444430589675905, l1: 0.0001148977389675565, l2: 0.00042954656528308987   Iteration 11 of 100, tot loss = 5.329224304719404, l1: 0.0001152688352952034, l2: 0.00041765359293838793   Iteration 12 of 100, tot loss = 5.4040005803108215, l1: 0.00011494275107300685, l2: 0.0004254573044211914   Iteration 13 of 100, tot loss = 5.396834355134231, l1: 0.0001141668645476994, l2: 0.00042551656952127814   Iteration 14 of 100, tot loss = 5.253562143870762, l1: 0.00011214058914837162, l2: 0.00041321562561539134   Iteration 15 of 100, tot loss = 5.218205229441325, l1: 0.0001110519515350461, l2: 0.00041076857208584744   Iteration 16 of 100, tot loss = 5.171077132225037, l1: 0.00011110519244539319, l2: 0.00040600252214062493   Iteration 17 of 100, tot loss = 5.008249941994162, l1: 0.00010811779943383847, l2: 0.0003927071957150474   Iteration 18 of 100, tot loss = 4.917729046609667, l1: 0.0001070638417230738, l2: 0.0003847090645447477   Iteration 19 of 100, tot loss = 5.00383764819095, l1: 0.00010908428080974293, l2: 0.00039129948479393005   Iteration 20 of 100, tot loss = 4.8727907538414, l1: 0.00010751957943284651, l2: 0.0003797594967181794   Iteration 21 of 100, tot loss = 4.9289579050881525, l1: 0.00010754346831596368, l2: 0.00038535232306458056   Iteration 22 of 100, tot loss = 4.919131094759161, l1: 0.00010829848394099496, l2: 0.0003836146259511059   Iteration 23 of 100, tot loss = 4.886411843092545, l1: 0.00010782889955368339, l2: 0.0003808122862170896   Iteration 24 of 100, tot loss = 4.905436168114345, l1: 0.00010885005076488596, l2: 0.00038169356897318113   Iteration 25 of 100, tot loss = 4.90431565284729, l1: 0.00010812499487656169, l2: 0.00038230657344684004   Iteration 26 of 100, tot loss = 5.01881206035614, l1: 0.00010957801420702778, l2: 0.00039230319537007465   Iteration 27 of 100, tot loss = 5.0208590560489235, l1: 0.00010925938072937747, l2: 0.00039282652842639773   Iteration 28 of 100, tot loss = 4.9571270772389004, l1: 0.00010817472879612719, l2: 0.00038753798227324817   Iteration 29 of 100, tot loss = 4.927401559106235, l1: 0.00010790195502240436, l2: 0.00038483820389956236   Iteration 30 of 100, tot loss = 4.9821838219960535, l1: 0.00010849622473566949, l2: 0.00038972216037412485   Iteration 31 of 100, tot loss = 4.967673901588686, l1: 0.00010781536184367724, l2: 0.00038895203127345493   Iteration 32 of 100, tot loss = 5.027691885828972, l1: 0.00010890957071296725, l2: 0.0003938596209991374   Iteration 33 of 100, tot loss = 4.937642465938222, l1: 0.00010760517398011871, l2: 0.0003861590759532357   Iteration 34 of 100, tot loss = 4.8715746262494255, l1: 0.0001064490988937905, l2: 0.0003807083669337718   Iteration 35 of 100, tot loss = 4.838939537320818, l1: 0.00010622631723111096, l2: 0.0003776676395708429   Iteration 36 of 100, tot loss = 4.809639599588182, l1: 0.00010626797049755826, l2: 0.0003746959927310753   Iteration 37 of 100, tot loss = 4.813288688659668, l1: 0.00010678470809236043, l2: 0.00037454416412180543   Iteration 38 of 100, tot loss = 4.752897613926938, l1: 0.00010575630844150989, l2: 0.0003695334556763747   Iteration 39 of 100, tot loss = 4.798083293132293, l1: 0.00010690384330630947, l2: 0.00037290448903476295   Iteration 40 of 100, tot loss = 4.769861960411072, l1: 0.00010641576363923377, l2: 0.0003705704348249128   Iteration 41 of 100, tot loss = 4.743602851542031, l1: 0.00010576641757435892, l2: 0.0003685938700516822   Iteration 42 of 100, tot loss = 4.7082991770335605, l1: 0.00010458823799992734, l2: 0.00036624168188566166   Iteration 43 of 100, tot loss = 4.69368639103202, l1: 0.00010371562327003799, l2: 0.0003656530180853943   Iteration 44 of 100, tot loss = 4.792431479150599, l1: 0.00010512478101273618, l2: 0.00037411836853028615   Iteration 45 of 100, tot loss = 4.822719367345174, l1: 0.00010539807893413429, l2: 0.000376873858880976   Iteration 46 of 100, tot loss = 4.790014826733133, l1: 0.00010502988500419356, l2: 0.00037397159884795144   Iteration 47 of 100, tot loss = 4.772407582465639, l1: 0.0001050836092079098, l2: 0.0003721571500794566   Iteration 48 of 100, tot loss = 4.783914764722188, l1: 0.00010493519660788782, l2: 0.00037345628121935687   Iteration 49 of 100, tot loss = 4.808962812229079, l1: 0.00010541349079825782, l2: 0.00037548279274511625   Iteration 50 of 100, tot loss = 4.800567789077759, l1: 0.00010476816780283116, l2: 0.00037528861343162135   Iteration 51 of 100, tot loss = 4.793249036751542, l1: 0.00010427199580875572, l2: 0.00037505291031408763   Iteration 52 of 100, tot loss = 4.76831639271516, l1: 0.00010385234558130972, l2: 0.0003729792960467211   Iteration 53 of 100, tot loss = 4.765436392910075, l1: 0.00010356779102091063, l2: 0.00037297585024718055   Iteration 54 of 100, tot loss = 4.791919977576645, l1: 0.0001040113004112047, l2: 0.0003751807000500978   Iteration 55 of 100, tot loss = 4.776167613809759, l1: 0.00010394233977422118, l2: 0.00037367442458741026   Iteration 56 of 100, tot loss = 4.7845122175557275, l1: 0.00010438612584299076, l2: 0.00037406509857516667   Iteration 57 of 100, tot loss = 4.761763673079641, l1: 0.00010412245830051241, l2: 0.00037205391160106255   Iteration 58 of 100, tot loss = 4.760075626702144, l1: 0.00010461586812041411, l2: 0.00037139169656037323   Iteration 59 of 100, tot loss = 4.736962314379418, l1: 0.00010417472922353673, l2: 0.0003695215043805192   Iteration 60 of 100, tot loss = 4.734043498833974, l1: 0.00010433649804326705, l2: 0.0003690678541412732   Iteration 61 of 100, tot loss = 4.773131710584046, l1: 0.00010494977001054976, l2: 0.00037236340352540196   Iteration 62 of 100, tot loss = 4.764765274140142, l1: 0.00010462770050282651, l2: 0.00037184882896854695   Iteration 63 of 100, tot loss = 4.829437660792517, l1: 0.00010582898863099722, l2: 0.0003771147788188139   Iteration 64 of 100, tot loss = 4.801562689244747, l1: 0.0001055214079315192, l2: 0.0003746348620552453   Iteration 65 of 100, tot loss = 4.766546014639047, l1: 0.0001047412928668424, l2: 0.00037191330943292435   Iteration 66 of 100, tot loss = 4.798139788887718, l1: 0.00010496459391574855, l2: 0.00037484938586589345   Iteration 67 of 100, tot loss = 4.790187607950239, l1: 0.00010493295317178647, l2: 0.00037408580872362287   Iteration 68 of 100, tot loss = 4.772235375993392, l1: 0.00010488317821000237, l2: 0.0003723403606272768   Iteration 69 of 100, tot loss = 4.7914375118587325, l1: 0.00010510440631111598, l2: 0.0003740393455001944   Iteration 70 of 100, tot loss = 4.803438442093985, l1: 0.00010520980285946279, l2: 0.00037513404199022   Iteration 71 of 100, tot loss = 4.79463280086786, l1: 0.00010503433420549108, l2: 0.00037442894654945324   Iteration 72 of 100, tot loss = 4.7830760147836475, l1: 0.00010434327764920049, l2: 0.00037396432425238244   Iteration 73 of 100, tot loss = 4.751330434459529, l1: 0.00010353220319327554, l2: 0.00037160084051820005   Iteration 74 of 100, tot loss = 4.779132933230014, l1: 0.00010425613328226137, l2: 0.00037365716078155046   Iteration 75 of 100, tot loss = 4.782530981699626, l1: 0.00010419107226577277, l2: 0.00037406202677326897   Iteration 76 of 100, tot loss = 4.748816725454833, l1: 0.00010350443297188337, l2: 0.00037137724039463426   Iteration 77 of 100, tot loss = 4.770270747023743, l1: 0.00010404920140462747, l2: 0.00037297787365515704   Iteration 78 of 100, tot loss = 4.7662653892468185, l1: 0.00010386854512781168, l2: 0.0003727579941579069   Iteration 79 of 100, tot loss = 4.741884451878222, l1: 0.00010323162192273944, l2: 0.00037095682360998157   Iteration 80 of 100, tot loss = 4.736214724183083, l1: 0.00010324082163606362, l2: 0.0003703806509292917   Iteration 81 of 100, tot loss = 4.725547007572504, l1: 0.00010298085063918678, l2: 0.00036957385025654035   Iteration 82 of 100, tot loss = 4.747303642877719, l1: 0.00010340529128947442, l2: 0.00037132507288067533   Iteration 83 of 100, tot loss = 4.7504768256681515, l1: 0.00010364814899007633, l2: 0.00037139953367961906   Iteration 84 of 100, tot loss = 4.8252454825810025, l1: 0.00010491740868885591, l2: 0.00037760713998328095   Iteration 85 of 100, tot loss = 4.804829434787526, l1: 0.00010472247441237628, l2: 0.0003757604697431602   Iteration 86 of 100, tot loss = 4.824666067611339, l1: 0.00010522982496653175, l2: 0.00037723678271800584   Iteration 87 of 100, tot loss = 4.8261153286901015, l1: 0.00010514279706878641, l2: 0.0003774687368489652   Iteration 88 of 100, tot loss = 4.829552341591228, l1: 0.00010522064716993059, l2: 0.0003777345880900446   Iteration 89 of 100, tot loss = 4.822907528180755, l1: 0.00010534575900809807, l2: 0.0003769449950810626   Iteration 90 of 100, tot loss = 4.815671576393975, l1: 0.00010503600929546842, l2: 0.00037653114947412784   Iteration 91 of 100, tot loss = 4.829384337414752, l1: 0.00010556491678881275, l2: 0.00037737351769944757   Iteration 92 of 100, tot loss = 4.822987774144048, l1: 0.000105398199158789, l2: 0.00037690057908311365   Iteration 93 of 100, tot loss = 4.814835020290908, l1: 0.00010531512006925749, l2: 0.00037616838294867507   Iteration 94 of 100, tot loss = 4.812702138373193, l1: 0.00010533438046664703, l2: 0.00037593583460585116   Iteration 95 of 100, tot loss = 4.81580874292474, l1: 0.00010550471377332302, l2: 0.00037607616165922467   Iteration 96 of 100, tot loss = 4.810512766242027, l1: 0.00010555310348081548, l2: 0.00037549817428346916   Iteration 97 of 100, tot loss = 4.796898871352992, l1: 0.0001055109581280591, l2: 0.00037417893004531677   Iteration 98 of 100, tot loss = 4.8027668047924434, l1: 0.00010576979232665712, l2: 0.0003745068894872651   Iteration 99 of 100, tot loss = 4.826930681864421, l1: 0.00010604507284976232, l2: 0.00037664799692904146   Iteration 100 of 100, tot loss = 4.830476307868958, l1: 0.00010610566820105304, l2: 0.00037694196405936966
   End of epoch 1323; saving model... 

Epoch 1324 of 2000
   Iteration 1 of 100, tot loss = 4.821098804473877, l1: 0.00010888950782828033, l2: 0.0003732203913386911   Iteration 2 of 100, tot loss = 3.3463445901870728, l1: 8.149393215717282e-05, l2: 0.0002531405334593728   Iteration 3 of 100, tot loss = 4.075146754582723, l1: 9.155429749322745e-05, l2: 0.00031596038995000225   Iteration 4 of 100, tot loss = 4.003990054130554, l1: 8.28211295811343e-05, l2: 0.0003175778838340193   Iteration 5 of 100, tot loss = 4.071867179870606, l1: 8.687169683980755e-05, l2: 0.0003203150350600481   Iteration 6 of 100, tot loss = 4.362784385681152, l1: 9.404789489053655e-05, l2: 0.00034223055505814653   Iteration 7 of 100, tot loss = 4.3185939107622415, l1: 9.465787349784347e-05, l2: 0.00033720152818464806   Iteration 8 of 100, tot loss = 4.441931307315826, l1: 9.86595709946414e-05, l2: 0.00034553357181721367   Iteration 9 of 100, tot loss = 4.356544176737468, l1: 9.795298481347143e-05, l2: 0.00033770144283254113   Iteration 10 of 100, tot loss = 4.3778329372406, l1: 9.95622332993662e-05, l2: 0.00033822106779552997   Iteration 11 of 100, tot loss = 4.506707234816118, l1: 9.971059989766218e-05, l2: 0.0003509601334702562   Iteration 12 of 100, tot loss = 4.6654229164123535, l1: 0.00010338096368892972, l2: 0.00036316133628133684   Iteration 13 of 100, tot loss = 4.583452756588276, l1: 0.0001023684932218972, l2: 0.0003559767904404837   Iteration 14 of 100, tot loss = 4.657733389309475, l1: 0.00010092736374645028, l2: 0.0003648459803246494   Iteration 15 of 100, tot loss = 4.77542290687561, l1: 0.0001033953951264266, l2: 0.00037414690013974904   Iteration 16 of 100, tot loss = 4.7602840811014175, l1: 0.00010273471957589209, l2: 0.0003732936911546858   Iteration 17 of 100, tot loss = 4.808884774937349, l1: 0.00010286957202718921, l2: 0.0003780189076411154   Iteration 18 of 100, tot loss = 4.891634451018439, l1: 0.00010421719647032053, l2: 0.000384946251870133   Iteration 19 of 100, tot loss = 5.004025747901515, l1: 0.00010609904079740915, l2: 0.0003943035369861479   Iteration 20 of 100, tot loss = 4.992895352840423, l1: 0.00010564342428551755, l2: 0.00039364611293422056   Iteration 21 of 100, tot loss = 4.860778831300282, l1: 0.00010377489230623247, l2: 0.00038230299278872   Iteration 22 of 100, tot loss = 4.850045399232344, l1: 0.00010399773517829917, l2: 0.0003810068063268607   Iteration 23 of 100, tot loss = 4.787637140439904, l1: 0.00010317333979638416, l2: 0.0003755903757258278   Iteration 24 of 100, tot loss = 4.816259394089381, l1: 0.00010317589885744383, l2: 0.0003784500428688868   Iteration 25 of 100, tot loss = 4.826861085891724, l1: 0.00010386294437921607, l2: 0.0003788231662474573   Iteration 26 of 100, tot loss = 4.931755900382996, l1: 0.00010627058887061699, l2: 0.00038690500346442254   Iteration 27 of 100, tot loss = 4.9340991355754715, l1: 0.00010531140301113569, l2: 0.00038809851415593313   Iteration 28 of 100, tot loss = 4.970151841640472, l1: 0.00010509851394350076, l2: 0.00039191667331449153   Iteration 29 of 100, tot loss = 4.9476016883192395, l1: 0.0001052583557511439, l2: 0.0003895018157420744   Iteration 30 of 100, tot loss = 4.90673177242279, l1: 0.00010360628281584165, l2: 0.0003870668966555968   Iteration 31 of 100, tot loss = 4.910409181348739, l1: 0.00010329515017445139, l2: 0.00038774577044551413   Iteration 32 of 100, tot loss = 5.029905758798122, l1: 0.00010502387192445894, l2: 0.0003979667071689619   Iteration 33 of 100, tot loss = 5.010106007258098, l1: 0.00010562957752633586, l2: 0.0003953810267954726   Iteration 34 of 100, tot loss = 5.03704297542572, l1: 0.00010650758774906589, l2: 0.0003971967140815276   Iteration 35 of 100, tot loss = 4.968172570637294, l1: 0.00010470984852872789, l2: 0.0003921074127512319   Iteration 36 of 100, tot loss = 4.909672922558254, l1: 0.00010399528178418727, l2: 0.000386972014692017   Iteration 37 of 100, tot loss = 4.884893675108214, l1: 0.00010371379765864411, l2: 0.0003847755736363045   Iteration 38 of 100, tot loss = 4.913318383066278, l1: 0.00010386294525233097, l2: 0.0003874688972198208   Iteration 39 of 100, tot loss = 4.891577378297463, l1: 0.0001042008495856769, l2: 0.0003849568926782992   Iteration 40 of 100, tot loss = 4.912210309505463, l1: 0.00010402421448816313, l2: 0.0003871968208841281   Iteration 41 of 100, tot loss = 4.895674845067466, l1: 0.0001045387235069761, l2: 0.00038502876548857497   Iteration 42 of 100, tot loss = 4.898632208506267, l1: 0.00010464854079015953, l2: 0.00038521468377439305   Iteration 43 of 100, tot loss = 4.889029713564141, l1: 0.00010424825653222628, l2: 0.0003846547181236146   Iteration 44 of 100, tot loss = 4.863329854878512, l1: 0.00010380795984846455, l2: 0.00038252502847833307   Iteration 45 of 100, tot loss = 4.842976956897312, l1: 0.00010331194304550688, l2: 0.00038098575540870014   Iteration 46 of 100, tot loss = 4.792113008706466, l1: 0.00010240753877239868, l2: 0.00037680376471658036   Iteration 47 of 100, tot loss = 4.790659919698188, l1: 0.00010280763904410513, l2: 0.00037625835597901507   Iteration 48 of 100, tot loss = 4.812216058373451, l1: 0.00010329580421360636, l2: 0.00037792580496898154   Iteration 49 of 100, tot loss = 4.846331182791262, l1: 0.00010355802508883597, l2: 0.00038107509614320053   Iteration 50 of 100, tot loss = 4.859978728294372, l1: 0.00010359839201555587, l2: 0.00038239948422415183   Iteration 51 of 100, tot loss = 4.831718192381017, l1: 0.00010355799710012826, l2: 0.0003796138255126482   Iteration 52 of 100, tot loss = 4.810561514817751, l1: 0.00010320502014320272, l2: 0.00037785113426462677   Iteration 53 of 100, tot loss = 4.81136343164264, l1: 0.00010330407668282892, l2: 0.000377832269427841   Iteration 54 of 100, tot loss = 4.824142690058108, l1: 0.00010393233457795793, l2: 0.0003784819375680484   Iteration 55 of 100, tot loss = 4.833030999790538, l1: 0.0001039695568827235, l2: 0.0003793335456231778   Iteration 56 of 100, tot loss = 4.833246609994343, l1: 0.00010363758350909588, l2: 0.0003796870800475257   Iteration 57 of 100, tot loss = 4.8216289344586825, l1: 0.00010366977132211455, l2: 0.0003784931246436348   Iteration 58 of 100, tot loss = 4.814738680576456, l1: 0.00010349418893195528, l2: 0.0003779796815220395   Iteration 59 of 100, tot loss = 4.811930668556084, l1: 0.00010359553068726308, l2: 0.00037759753837944717   Iteration 60 of 100, tot loss = 4.848857430617015, l1: 0.00010449117277554857, l2: 0.0003803945723727035   Iteration 61 of 100, tot loss = 4.83766333392409, l1: 0.00010462493476358654, l2: 0.00037914140056437035   Iteration 62 of 100, tot loss = 4.869251162775101, l1: 0.0001050598518979255, l2: 0.00038186526686448846   Iteration 63 of 100, tot loss = 4.832004603885469, l1: 0.00010412121374097224, l2: 0.0003790792492624845   Iteration 64 of 100, tot loss = 4.792096361517906, l1: 0.0001032729449548242, l2: 0.00037593669367197435   Iteration 65 of 100, tot loss = 4.795170952723576, l1: 0.00010332458248460451, l2: 0.0003761925152502954   Iteration 66 of 100, tot loss = 4.770739259141864, l1: 0.00010305733253930653, l2: 0.00037401659541170704   Iteration 67 of 100, tot loss = 4.784171794777486, l1: 0.00010365579656712752, l2: 0.00037476138442879846   Iteration 68 of 100, tot loss = 4.780282539479873, l1: 0.00010360909163864562, l2: 0.00037441916399128623   Iteration 69 of 100, tot loss = 4.780896421791851, l1: 0.00010323574178324054, l2: 0.0003748539020307362   Iteration 70 of 100, tot loss = 4.784195375442505, l1: 0.00010331410194339697, l2: 0.0003751054377062246   Iteration 71 of 100, tot loss = 4.762103725487078, l1: 0.00010262951933250854, l2: 0.00037358085539611715   Iteration 72 of 100, tot loss = 4.75319963031345, l1: 0.00010225109619770794, l2: 0.000373068868712936   Iteration 73 of 100, tot loss = 4.720846375373945, l1: 0.00010165823221865288, l2: 0.0003704264070695164   Iteration 74 of 100, tot loss = 4.71613676805754, l1: 0.00010177061252564985, l2: 0.0003698430658472274   Iteration 75 of 100, tot loss = 4.714760001500448, l1: 0.00010167965175545153, l2: 0.0003697963495505974   Iteration 76 of 100, tot loss = 4.725588042485087, l1: 0.00010167790103303942, l2: 0.00037088090462738795   Iteration 77 of 100, tot loss = 4.717646193194699, l1: 0.00010143849516231507, l2: 0.00037032612557855567   Iteration 78 of 100, tot loss = 4.738340759888674, l1: 0.00010203132147487336, l2: 0.0003718027555106136   Iteration 79 of 100, tot loss = 4.731333445899094, l1: 0.00010216525002254491, l2: 0.00037096809544983684   Iteration 80 of 100, tot loss = 4.722518840432167, l1: 0.00010207607951997488, l2: 0.00037017580525571246   Iteration 81 of 100, tot loss = 4.705968918623747, l1: 0.0001015475100746384, l2: 0.0003690493825239555   Iteration 82 of 100, tot loss = 4.67738491151391, l1: 0.0001008465649026287, l2: 0.0003668919268487271   Iteration 83 of 100, tot loss = 4.677135714565415, l1: 0.00010109994080442239, l2: 0.0003666136308968718   Iteration 84 of 100, tot loss = 4.662670081570035, l1: 0.00010099919849640823, l2: 0.00036526781003062966   Iteration 85 of 100, tot loss = 4.680708893607645, l1: 0.00010094782277626698, l2: 0.0003671230667832253   Iteration 86 of 100, tot loss = 4.678728045419205, l1: 0.00010081523603645998, l2: 0.000367057568774028   Iteration 87 of 100, tot loss = 4.699580842051013, l1: 0.00010099912294114395, l2: 0.0003689589614822026   Iteration 88 of 100, tot loss = 4.682394100861116, l1: 0.00010041958293269008, l2: 0.0003678198275090174   Iteration 89 of 100, tot loss = 4.6729332677434, l1: 0.00010012994231616953, l2: 0.00036716338471389176   Iteration 90 of 100, tot loss = 4.657021681467692, l1: 9.973508519275734e-05, l2: 0.00036596708313380884   Iteration 91 of 100, tot loss = 4.665516743293176, l1: 9.982881018224141e-05, l2: 0.0003667228641231534   Iteration 92 of 100, tot loss = 4.6663757977278335, l1: 9.985511892211989e-05, l2: 0.0003667824609653073   Iteration 93 of 100, tot loss = 4.649490441045454, l1: 9.936723975159268e-05, l2: 0.0003655818045347108   Iteration 94 of 100, tot loss = 4.669510575051003, l1: 9.928256057467846e-05, l2: 0.0003676684974244439   Iteration 95 of 100, tot loss = 4.659826313821893, l1: 9.899708836057567e-05, l2: 0.0003669855434533307   Iteration 96 of 100, tot loss = 4.657867183287938, l1: 9.892621847029659e-05, l2: 0.0003668605005865781   Iteration 97 of 100, tot loss = 4.672753166906612, l1: 9.933899540199716e-05, l2: 0.00036793632223390847   Iteration 98 of 100, tot loss = 4.658884605582879, l1: 9.923786452344182e-05, l2: 0.00036665059703910646   Iteration 99 of 100, tot loss = 4.64578470557627, l1: 9.880699236428064e-05, l2: 0.0003657714792143911   Iteration 100 of 100, tot loss = 4.660668969154358, l1: 9.897887844999787e-05, l2: 0.0003670880194113124
   End of epoch 1324; saving model... 

Epoch 1325 of 2000
   Iteration 1 of 100, tot loss = 4.093911170959473, l1: 0.00012019301357213408, l2: 0.0002891980984713882   Iteration 2 of 100, tot loss = 4.73022723197937, l1: 0.00011954488218179904, l2: 0.00035347782250028104   Iteration 3 of 100, tot loss = 4.487003485361735, l1: 0.00011186753302657355, l2: 0.0003368328034412116   Iteration 4 of 100, tot loss = 4.597868204116821, l1: 0.00010995187039952725, l2: 0.000349834946973715   Iteration 5 of 100, tot loss = 4.615486145019531, l1: 0.00011460031964816153, l2: 0.0003469482937362045   Iteration 6 of 100, tot loss = 4.918701887130737, l1: 0.00011886101128766313, l2: 0.0003730091799904282   Iteration 7 of 100, tot loss = 4.679928370884487, l1: 0.00011617886775638908, l2: 0.0003518139731438298   Iteration 8 of 100, tot loss = 4.7190011739730835, l1: 0.000117421188406297, l2: 0.00035447893060336355   Iteration 9 of 100, tot loss = 4.746842119428846, l1: 0.00011599088692390878, l2: 0.00035869332916465483   Iteration 10 of 100, tot loss = 4.793930578231811, l1: 0.00011625579645624384, l2: 0.0003631372659583576   Iteration 11 of 100, tot loss = 4.695114439184016, l1: 0.0001150586196507158, l2: 0.00035445282992441207   Iteration 12 of 100, tot loss = 4.574610908826192, l1: 0.00011272069847715709, l2: 0.0003447403990624783   Iteration 13 of 100, tot loss = 4.429031757208017, l1: 0.00010783232965667803, l2: 0.00033507085194847046   Iteration 14 of 100, tot loss = 4.80000068460192, l1: 0.00011381347316533461, l2: 0.00036618659942178056   Iteration 15 of 100, tot loss = 4.7379552841186525, l1: 0.00011244536435697227, l2: 0.00036135016804716237   Iteration 16 of 100, tot loss = 4.6904619336128235, l1: 0.00011236364889555261, l2: 0.0003566825480447733   Iteration 17 of 100, tot loss = 4.650788896224078, l1: 0.00011111155703581651, l2: 0.0003539673352326431   Iteration 18 of 100, tot loss = 4.584527360068427, l1: 0.00010977544540461774, l2: 0.0003486772936109143   Iteration 19 of 100, tot loss = 4.623843569504587, l1: 0.00010994561996899153, l2: 0.00035243873916394815   Iteration 20 of 100, tot loss = 4.747143840789795, l1: 0.00011034529125026893, l2: 0.00036436909649637526   Iteration 21 of 100, tot loss = 4.674597717466808, l1: 0.00010920538036208156, l2: 0.000358254395125966   Iteration 22 of 100, tot loss = 4.638749285177751, l1: 0.00010757974946913733, l2: 0.00035629518176640636   Iteration 23 of 100, tot loss = 4.6410061276477315, l1: 0.00010855340983455434, l2: 0.00035554720544377744   Iteration 24 of 100, tot loss = 4.592321773370107, l1: 0.0001075813142961124, l2: 0.00035165086592314765   Iteration 25 of 100, tot loss = 4.667489986419678, l1: 0.00010753589594969526, l2: 0.0003592131054028869   Iteration 26 of 100, tot loss = 4.570406647828909, l1: 0.00010562019759583144, l2: 0.0003514204699268493   Iteration 27 of 100, tot loss = 4.536920344388044, l1: 0.00010461560978674916, l2: 0.0003490764275839966   Iteration 28 of 100, tot loss = 4.543303055422647, l1: 0.00010440715777804144, l2: 0.0003499231497698929   Iteration 29 of 100, tot loss = 4.598948355378775, l1: 0.00010573756549876698, l2: 0.0003541572714939007   Iteration 30 of 100, tot loss = 4.531779591242472, l1: 0.00010468844096370352, l2: 0.000348489520062382   Iteration 31 of 100, tot loss = 4.531770813849665, l1: 0.00010519016852354511, l2: 0.00034798691593741456   Iteration 32 of 100, tot loss = 4.484605528414249, l1: 0.00010419587420074095, l2: 0.00034426468209858285   Iteration 33 of 100, tot loss = 4.511559204621748, l1: 0.00010513769181367631, l2: 0.00034601823162202806   Iteration 34 of 100, tot loss = 4.482397927957423, l1: 0.00010429177350220819, l2: 0.0003439480214557775   Iteration 35 of 100, tot loss = 4.534692239761353, l1: 0.00010407215408382139, l2: 0.0003493970717369978   Iteration 36 of 100, tot loss = 4.538877268632253, l1: 0.00010468418663044253, l2: 0.0003492035426057151   Iteration 37 of 100, tot loss = 4.557061897741781, l1: 0.00010567262959100205, l2: 0.0003500335623283644   Iteration 38 of 100, tot loss = 4.635961024384749, l1: 0.00010645394879822178, l2: 0.0003571421548258513   Iteration 39 of 100, tot loss = 4.574813916133, l1: 0.00010507263020069028, l2: 0.00035240876231676876   Iteration 40 of 100, tot loss = 4.56355117559433, l1: 0.0001052666462783236, l2: 0.0003510884722345509   Iteration 41 of 100, tot loss = 4.576870360025546, l1: 0.00010557814835565073, l2: 0.00035210888869719717   Iteration 42 of 100, tot loss = 4.581043970017206, l1: 0.00010529961928980247, l2: 0.0003528047784043121   Iteration 43 of 100, tot loss = 4.562832022822181, l1: 0.00010489732542123948, l2: 0.0003513858774081306   Iteration 44 of 100, tot loss = 4.551559849218889, l1: 0.00010494328109251165, l2: 0.0003502127041098323   Iteration 45 of 100, tot loss = 4.522479142083062, l1: 0.00010463454333754877, l2: 0.00034761337155941874   Iteration 46 of 100, tot loss = 4.495340958885524, l1: 0.00010390371133563231, l2: 0.000345630385551055   Iteration 47 of 100, tot loss = 4.517111504331548, l1: 0.00010402515893455278, l2: 0.0003476859930471735   Iteration 48 of 100, tot loss = 4.534087965885798, l1: 0.00010446406152671746, l2: 0.00034894473689443356   Iteration 49 of 100, tot loss = 4.527452809470041, l1: 0.00010390782536349583, l2: 0.0003488374566920672   Iteration 50 of 100, tot loss = 4.516993894577026, l1: 0.00010415136624942533, l2: 0.0003475480244378559   Iteration 51 of 100, tot loss = 4.532990493026435, l1: 0.000103976662977206, l2: 0.0003493223884302721   Iteration 52 of 100, tot loss = 4.531909841757554, l1: 0.00010415284710828788, l2: 0.0003490381392251808   Iteration 53 of 100, tot loss = 4.625395981770642, l1: 0.00010571790143039148, l2: 0.0003568216988636743   Iteration 54 of 100, tot loss = 4.60742442254667, l1: 0.000105002619723867, l2: 0.00035573982475196116   Iteration 55 of 100, tot loss = 4.610522699356079, l1: 0.0001047314501854337, l2: 0.00035632082224103874   Iteration 56 of 100, tot loss = 4.614911092179162, l1: 0.00010497025622108984, l2: 0.000356520855348208   Iteration 57 of 100, tot loss = 4.621358290053251, l1: 0.00010493565366088755, l2: 0.00035720017688602146   Iteration 58 of 100, tot loss = 4.614742118736793, l1: 0.00010470661223296011, l2: 0.00035676760082173256   Iteration 59 of 100, tot loss = 4.6054213774406305, l1: 0.0001041704963427037, l2: 0.0003563716429999982   Iteration 60 of 100, tot loss = 4.6451360503832495, l1: 0.00010438516862147178, l2: 0.0003601284379935047   Iteration 61 of 100, tot loss = 4.6228691124525225, l1: 0.00010372691555902912, l2: 0.0003585599973076405   Iteration 62 of 100, tot loss = 4.577007234096527, l1: 0.00010261379429721273, l2: 0.00035508693080402973   Iteration 63 of 100, tot loss = 4.579402562171694, l1: 0.0001026475379365452, l2: 0.0003552927198128716   Iteration 64 of 100, tot loss = 4.5866880882531404, l1: 0.00010283540223099408, l2: 0.0003558334085482784   Iteration 65 of 100, tot loss = 4.5533068638581495, l1: 0.00010224507445505319, l2: 0.00035308561368415563   Iteration 66 of 100, tot loss = 4.557190849925533, l1: 0.0001022392432126005, l2: 0.00035347984345262927   Iteration 67 of 100, tot loss = 4.5567374140469, l1: 0.00010169286629682017, l2: 0.00035398087677990654   Iteration 68 of 100, tot loss = 4.538608216187534, l1: 0.00010147911930021913, l2: 0.00035238170419367687   Iteration 69 of 100, tot loss = 4.5091206630071, l1: 0.00010094274013839743, l2: 0.0003499693279002753   Iteration 70 of 100, tot loss = 4.50644097839083, l1: 0.00010039234517275222, l2: 0.00035025175457121805   Iteration 71 of 100, tot loss = 4.554232018094667, l1: 0.00010113472559220406, l2: 0.00035428847854425736   Iteration 72 of 100, tot loss = 4.515186258488232, l1: 0.0001005344699175718, l2: 0.0003509841581035289   Iteration 73 of 100, tot loss = 4.54128214594436, l1: 0.00010092611434990668, l2: 0.0003532021028772698   Iteration 74 of 100, tot loss = 4.574321316706167, l1: 0.00010157652640545338, l2: 0.00035585560771004247   Iteration 75 of 100, tot loss = 4.581831800142924, l1: 0.0001019409498743092, l2: 0.00035624223285897944   Iteration 76 of 100, tot loss = 4.562052094622662, l1: 0.0001012820449958733, l2: 0.0003549231668761135   Iteration 77 of 100, tot loss = 4.560098005579664, l1: 0.000101117560042404, l2: 0.0003548922430528872   Iteration 78 of 100, tot loss = 4.538450695001162, l1: 0.00010055306978383436, l2: 0.00035329200228517997   Iteration 79 of 100, tot loss = 4.532808733891837, l1: 0.00010033121481360985, l2: 0.00035294966101336966   Iteration 80 of 100, tot loss = 4.500248555839062, l1: 9.965837475647276e-05, l2: 0.00035036648323512055   Iteration 81 of 100, tot loss = 4.492282347914613, l1: 9.948556602646505e-05, l2: 0.0003497426711234126   Iteration 82 of 100, tot loss = 4.473390979010884, l1: 9.922554556574252e-05, l2: 0.0003481135545149389   Iteration 83 of 100, tot loss = 4.471074941646622, l1: 9.909690039881775e-05, l2: 0.0003480105958030424   Iteration 84 of 100, tot loss = 4.487921963135402, l1: 9.966937167995465e-05, l2: 0.0003491228267410493   Iteration 85 of 100, tot loss = 4.4655103529200835, l1: 9.913782645633225e-05, l2: 0.00034741321090704706   Iteration 86 of 100, tot loss = 4.459064767804256, l1: 9.87304944807642e-05, l2: 0.00034717598455879557   Iteration 87 of 100, tot loss = 4.462293028831482, l1: 9.900673567558285e-05, l2: 0.00034722256942199887   Iteration 88 of 100, tot loss = 4.4455375684933225, l1: 9.856072847469477e-05, l2: 0.000345993030764062   Iteration 89 of 100, tot loss = 4.472272809971584, l1: 9.88467915468912e-05, l2: 0.00034838049187589166   Iteration 90 of 100, tot loss = 4.48969966702991, l1: 9.855695971054957e-05, l2: 0.0003504130094370339   Iteration 91 of 100, tot loss = 4.492895316291642, l1: 9.868854033481842e-05, l2: 0.0003506009938524125   Iteration 92 of 100, tot loss = 4.495268102573312, l1: 9.860974893419315e-05, l2: 0.0003509170641653169   Iteration 93 of 100, tot loss = 4.511998911057749, l1: 9.884261825029308e-05, l2: 0.0003523572761301322   Iteration 94 of 100, tot loss = 4.512531433967834, l1: 9.871193663244373e-05, l2: 0.000352541209917461   Iteration 95 of 100, tot loss = 4.546728045062015, l1: 9.930624502502676e-05, l2: 0.0003553665626800227   Iteration 96 of 100, tot loss = 4.541908702502648, l1: 9.92037616166878e-05, l2: 0.0003549871117532651   Iteration 97 of 100, tot loss = 4.552989415286743, l1: 9.929173430679942e-05, l2: 0.0003560072105268698   Iteration 98 of 100, tot loss = 4.529995924356032, l1: 9.903065928279384e-05, l2: 0.0003539689363494791   Iteration 99 of 100, tot loss = 4.539069210640108, l1: 9.904755507254823e-05, l2: 0.0003548593692054662   Iteration 100 of 100, tot loss = 4.521876350641251, l1: 9.86552438189392e-05, l2: 0.0003535323942924151
   End of epoch 1325; saving model... 

Epoch 1326 of 2000
   Iteration 1 of 100, tot loss = 4.358141899108887, l1: 8.10663404990919e-05, l2: 0.00035474784090183675   Iteration 2 of 100, tot loss = 3.6966632604599, l1: 6.7908302298747e-05, l2: 0.00030175801657605916   Iteration 3 of 100, tot loss = 5.311850468317668, l1: 0.00010084748403945316, l2: 0.000430337570530052   Iteration 4 of 100, tot loss = 5.642437040805817, l1: 0.00011307404929539189, l2: 0.0004511696533882059   Iteration 5 of 100, tot loss = 5.756805181503296, l1: 0.00011374430323485285, l2: 0.00046193621237762273   Iteration 6 of 100, tot loss = 5.4929810762405396, l1: 0.00010984526306856424, l2: 0.00043945284172271687   Iteration 7 of 100, tot loss = 5.3047104222433905, l1: 0.00010608699501192729, l2: 0.00042438404385133514   Iteration 8 of 100, tot loss = 5.245053917169571, l1: 0.00010794096851896029, l2: 0.0004165644204476848   Iteration 9 of 100, tot loss = 5.530404700173272, l1: 0.00011521034078517307, l2: 0.0004378301285517712   Iteration 10 of 100, tot loss = 5.315251255035401, l1: 0.00011119688278995454, l2: 0.0004203282413072884   Iteration 11 of 100, tot loss = 5.137373317371715, l1: 0.00010789500679079951, l2: 0.00040584232457066804   Iteration 12 of 100, tot loss = 4.957395851612091, l1: 0.00010570168706180993, l2: 0.0003900378966742816   Iteration 13 of 100, tot loss = 4.8250229542072, l1: 0.00010422561452007637, l2: 0.0003782766803991623   Iteration 14 of 100, tot loss = 4.965382235390799, l1: 0.00010683231188782624, l2: 0.00038970591309147755   Iteration 15 of 100, tot loss = 4.883488925298055, l1: 0.00010708606035526221, l2: 0.00038126283325254915   Iteration 16 of 100, tot loss = 4.702654160559177, l1: 0.00010431950795464218, l2: 0.00036594590983440867   Iteration 17 of 100, tot loss = 4.729264350498424, l1: 0.00010548803787541521, l2: 0.00036743839925346784   Iteration 18 of 100, tot loss = 4.833961519930098, l1: 0.00010843299322813336, l2: 0.00037496316023559   Iteration 19 of 100, tot loss = 4.995966967783477, l1: 0.00010861058437608575, l2: 0.000390986111597158   Iteration 20 of 100, tot loss = 4.897540813684463, l1: 0.00010709500566008501, l2: 0.0003826590756943915   Iteration 21 of 100, tot loss = 4.877625085058666, l1: 0.00010686233526073574, l2: 0.0003809001736087902   Iteration 22 of 100, tot loss = 4.802276638421145, l1: 0.00010610373746286231, l2: 0.00037412392703117803   Iteration 23 of 100, tot loss = 4.700075019960818, l1: 0.0001038399410469498, l2: 0.0003661675614041641   Iteration 24 of 100, tot loss = 4.734024837613106, l1: 0.00010410118405464648, l2: 0.00036930130105853703   Iteration 25 of 100, tot loss = 4.748872389793396, l1: 0.00010515059431781993, l2: 0.000369736646534875   Iteration 26 of 100, tot loss = 4.795398524174323, l1: 0.00010630700321948658, l2: 0.0003732328511362609   Iteration 27 of 100, tot loss = 4.752553652834009, l1: 0.00010558472045145377, l2: 0.00036967064672218704   Iteration 28 of 100, tot loss = 4.7916140513760705, l1: 0.00010603926836795705, l2: 0.0003731221388859142   Iteration 29 of 100, tot loss = 4.785986353611124, l1: 0.00010662457448077099, l2: 0.000371974062953337   Iteration 30 of 100, tot loss = 4.766410617033641, l1: 0.00010576379233195136, l2: 0.00037087727105244994   Iteration 31 of 100, tot loss = 4.68882772614879, l1: 0.0001043182557858076, l2: 0.00036456451878973074   Iteration 32 of 100, tot loss = 4.720640312880278, l1: 0.00010492380511095689, l2: 0.0003671402282634517   Iteration 33 of 100, tot loss = 4.750042868383003, l1: 0.00010491718752415512, l2: 0.00037008710117361534   Iteration 34 of 100, tot loss = 4.666059704387889, l1: 0.00010299715647895081, l2: 0.00036360881580656176   Iteration 35 of 100, tot loss = 4.661907250540597, l1: 0.00010306587989491942, l2: 0.0003631248467302482   Iteration 36 of 100, tot loss = 4.68407146135966, l1: 0.00010316989604082967, l2: 0.0003652372514706157   Iteration 37 of 100, tot loss = 4.706134280642948, l1: 0.00010330942284388191, l2: 0.0003673040061855588   Iteration 38 of 100, tot loss = 4.710590851934333, l1: 0.00010329122453281583, l2: 0.00036776786107925306   Iteration 39 of 100, tot loss = 4.680609171207134, l1: 0.00010280125915857915, l2: 0.0003652596582795899   Iteration 40 of 100, tot loss = 4.734874564409256, l1: 0.00010340883773096721, l2: 0.0003700786193803651   Iteration 41 of 100, tot loss = 4.776927151331088, l1: 0.00010419091223611315, l2: 0.00037350180301894777   Iteration 42 of 100, tot loss = 4.7468895968936735, l1: 0.00010395553103340458, l2: 0.00037073342848868506   Iteration 43 of 100, tot loss = 4.764863984529362, l1: 0.00010378665074524686, l2: 0.00037269974716256786   Iteration 44 of 100, tot loss = 4.74204400994561, l1: 0.00010318331219009865, l2: 0.00037102108796137725   Iteration 45 of 100, tot loss = 4.728424967659844, l1: 0.00010315893555849067, l2: 0.0003696835603073446   Iteration 46 of 100, tot loss = 4.69693850952646, l1: 0.00010281738990591333, l2: 0.00036687646016893586   Iteration 47 of 100, tot loss = 4.807157633152414, l1: 0.00010426709342666188, l2: 0.00037644866716483253   Iteration 48 of 100, tot loss = 4.808532769481341, l1: 0.00010465690828217096, l2: 0.00037619636562643183   Iteration 49 of 100, tot loss = 4.772777372477006, l1: 0.00010399988263534211, l2: 0.0003732778518033043   Iteration 50 of 100, tot loss = 4.737995328903199, l1: 0.00010309800672985148, l2: 0.0003707015234977007   Iteration 51 of 100, tot loss = 4.737512008816588, l1: 0.00010287373801225377, l2: 0.00037087746051267956   Iteration 52 of 100, tot loss = 4.721435863238114, l1: 0.0001027724291326684, l2: 0.0003693711551022716   Iteration 53 of 100, tot loss = 4.745307935858673, l1: 0.00010356988546708687, l2: 0.0003709609061259917   Iteration 54 of 100, tot loss = 4.68141613624714, l1: 0.00010223926080841381, l2: 0.0003659023506635869   Iteration 55 of 100, tot loss = 4.687611272118309, l1: 0.00010242348871543072, l2: 0.00036633763705718923   Iteration 56 of 100, tot loss = 4.6377745015280585, l1: 0.00010165102042135134, l2: 0.0003621264282368689   Iteration 57 of 100, tot loss = 4.639249199315121, l1: 0.00010156333659675478, l2: 0.0003623615819123459   Iteration 58 of 100, tot loss = 4.654570768619406, l1: 0.00010171675170693498, l2: 0.0003637403231174364   Iteration 59 of 100, tot loss = 4.633514456829782, l1: 0.00010143566963714705, l2: 0.0003619157742231421   Iteration 60 of 100, tot loss = 4.664141492048899, l1: 0.00010218098377663409, l2: 0.00036423316317571637   Iteration 61 of 100, tot loss = 4.690005157814651, l1: 0.000102554554404542, l2: 0.0003664459588032857   Iteration 62 of 100, tot loss = 4.712010602797231, l1: 0.00010286045518671838, l2: 0.0003683406030177139   Iteration 63 of 100, tot loss = 4.6723476061745295, l1: 0.00010222426990078511, l2: 0.00036501048857139214   Iteration 64 of 100, tot loss = 4.68367875367403, l1: 0.00010273370031654849, l2: 0.0003656341732494184   Iteration 65 of 100, tot loss = 4.685505815652701, l1: 0.00010269739572182656, l2: 0.00036585318381324985   Iteration 66 of 100, tot loss = 4.659727172418074, l1: 0.0001023290120484131, l2: 0.0003636437029468432   Iteration 67 of 100, tot loss = 4.6582115123521035, l1: 0.00010235314593005772, l2: 0.0003634680029084739   Iteration 68 of 100, tot loss = 4.648322221110849, l1: 0.00010233488645847982, l2: 0.00036249733303146274   Iteration 69 of 100, tot loss = 4.640307008356288, l1: 0.00010173671556834091, l2: 0.0003622939826606813   Iteration 70 of 100, tot loss = 4.638386845588684, l1: 0.00010151377287651745, l2: 0.0003623249090326551   Iteration 71 of 100, tot loss = 4.639935866208144, l1: 0.00010144253697739848, l2: 0.00036255104699283575   Iteration 72 of 100, tot loss = 4.678288542562061, l1: 0.0001020140139189607, l2: 0.0003658148382075726   Iteration 73 of 100, tot loss = 4.655039640322124, l1: 0.00010171396854574384, l2: 0.0003637899933717762   Iteration 74 of 100, tot loss = 4.635562191138396, l1: 0.00010159390212966419, l2: 0.0003619623151010003   Iteration 75 of 100, tot loss = 4.630969222386678, l1: 0.00010128981089413477, l2: 0.0003618071092447887   Iteration 76 of 100, tot loss = 4.617008080607967, l1: 0.00010116155504204614, l2: 0.0003605392508018811   Iteration 77 of 100, tot loss = 4.622832158943275, l1: 0.00010115317887870002, l2: 0.00036113003500156684   Iteration 78 of 100, tot loss = 4.677168781940754, l1: 0.00010191640304140329, l2: 0.0003658004731196576   Iteration 79 of 100, tot loss = 4.685738783848437, l1: 0.00010197967073704777, l2: 0.0003665942062116876   Iteration 80 of 100, tot loss = 4.724335399270058, l1: 0.00010256536174892971, l2: 0.00036986817649449223   Iteration 81 of 100, tot loss = 4.751800769641076, l1: 0.00010308600430567979, l2: 0.00037209407058486966   Iteration 82 of 100, tot loss = 4.741714105373475, l1: 0.0001027795223774042, l2: 0.0003713918860473602   Iteration 83 of 100, tot loss = 4.743890865739569, l1: 0.0001026968997591091, l2: 0.000371692184266264   Iteration 84 of 100, tot loss = 4.74780071349371, l1: 0.00010282444630623407, l2: 0.0003719556227137911   Iteration 85 of 100, tot loss = 4.742658099006204, l1: 0.00010270774480030763, l2: 0.0003715580630609218   Iteration 86 of 100, tot loss = 4.745819596357124, l1: 0.00010291919115420463, l2: 0.00037166276685406216   Iteration 87 of 100, tot loss = 4.749690592974082, l1: 0.00010262230674946285, l2: 0.00037234675121793374   Iteration 88 of 100, tot loss = 4.741748278791254, l1: 0.00010271147985721737, l2: 0.000371463347123195   Iteration 89 of 100, tot loss = 4.739316404535529, l1: 0.0001027650761714315, l2: 0.0003711665631271899   Iteration 90 of 100, tot loss = 4.762933662202623, l1: 0.00010340169655036233, l2: 0.00037289166874769663   Iteration 91 of 100, tot loss = 4.766215591640263, l1: 0.00010372541326252938, l2: 0.000372896144923928   Iteration 92 of 100, tot loss = 4.770328355872112, l1: 0.00010385000697841979, l2: 0.000373182827783951   Iteration 93 of 100, tot loss = 4.771912626040879, l1: 0.00010377188147920665, l2: 0.0003734193801734677   Iteration 94 of 100, tot loss = 4.758277596311366, l1: 0.00010348910891093027, l2: 0.0003723386496161804   Iteration 95 of 100, tot loss = 4.741188779630159, l1: 0.00010320288064050195, l2: 0.0003709159961497215   Iteration 96 of 100, tot loss = 4.745075263082981, l1: 0.00010343014078747122, l2: 0.0003710773839884496   Iteration 97 of 100, tot loss = 4.733191082158039, l1: 0.00010336814660515357, l2: 0.0003699509601874431   Iteration 98 of 100, tot loss = 4.709970999737175, l1: 0.00010306824559829322, l2: 0.00036792885286410394   Iteration 99 of 100, tot loss = 4.732816387908628, l1: 0.0001034170528153555, l2: 0.00036986458416313233   Iteration 100 of 100, tot loss = 4.7172750782966615, l1: 0.0001032848585236934, l2: 0.000368442647595657
   End of epoch 1326; saving model... 

Epoch 1327 of 2000
   Iteration 1 of 100, tot loss = 6.47007942199707, l1: 0.00012651590805035084, l2: 0.0005204920307733119   Iteration 2 of 100, tot loss = 6.971995115280151, l1: 0.00013805137132294476, l2: 0.000559148145839572   Iteration 3 of 100, tot loss = 5.912195920944214, l1: 0.0001244985469384119, l2: 0.0004667210450861603   Iteration 4 of 100, tot loss = 5.237379789352417, l1: 0.0001061248885889654, l2: 0.00041761308966670185   Iteration 5 of 100, tot loss = 4.985844373703003, l1: 0.00010716126780607738, l2: 0.0003914231667295098   Iteration 6 of 100, tot loss = 4.58657431602478, l1: 0.00010245030049797303, l2: 0.0003562071287888102   Iteration 7 of 100, tot loss = 4.90370191846575, l1: 0.00010718664738565817, l2: 0.00038318353680162024   Iteration 8 of 100, tot loss = 4.639455825090408, l1: 0.00010264054571962333, l2: 0.00036130503031017724   Iteration 9 of 100, tot loss = 4.5928739706675215, l1: 0.00010021217369487406, l2: 0.0003590752199266313   Iteration 10 of 100, tot loss = 4.378274989128113, l1: 9.752549340191763e-05, l2: 0.0003403020018595271   Iteration 11 of 100, tot loss = 4.325484362515536, l1: 9.936604295613837e-05, l2: 0.00033318239068400794   Iteration 12 of 100, tot loss = 4.131314506133397, l1: 9.646105687958577e-05, l2: 0.00031667039123324986   Iteration 13 of 100, tot loss = 4.336674754436199, l1: 0.00010092001181104794, l2: 0.00033274745958176657   Iteration 14 of 100, tot loss = 4.317960185664041, l1: 0.00010163446833238205, l2: 0.0003301615475460754   Iteration 15 of 100, tot loss = 4.270556871096293, l1: 0.00010014829434415637, l2: 0.0003269073892928039   Iteration 16 of 100, tot loss = 4.375091411173344, l1: 0.0001025788276365347, l2: 0.00033493031060061185   Iteration 17 of 100, tot loss = 4.410293698310852, l1: 0.00010237125538611402, l2: 0.00033865811265594163   Iteration 18 of 100, tot loss = 4.464466472466786, l1: 0.0001021846302238474, l2: 0.00034426201455062255   Iteration 19 of 100, tot loss = 4.545469967942489, l1: 0.00010394054602090221, l2: 0.00035060644930988357   Iteration 20 of 100, tot loss = 4.595871287584305, l1: 0.00010399020629847655, l2: 0.0003555969196895603   Iteration 21 of 100, tot loss = 4.711081283433097, l1: 0.000106259836944186, l2: 0.00036484828853558396   Iteration 22 of 100, tot loss = 4.685750728303736, l1: 0.00010647864126605617, l2: 0.0003620964304321784   Iteration 23 of 100, tot loss = 4.633198349372201, l1: 0.00010451980035767242, l2: 0.00035880003364898425   Iteration 24 of 100, tot loss = 4.6226876229047775, l1: 0.00010436112233946915, l2: 0.0003579076395302157   Iteration 25 of 100, tot loss = 4.696911053657532, l1: 0.00010636758888722397, l2: 0.0003633235167944804   Iteration 26 of 100, tot loss = 4.775416782269111, l1: 0.00010782717953798755, l2: 0.0003697144997064382   Iteration 27 of 100, tot loss = 4.754678306756197, l1: 0.00010730482595304407, l2: 0.0003681630063241486   Iteration 28 of 100, tot loss = 4.7484016461031775, l1: 0.00010782792755240475, l2: 0.00036701223819233874   Iteration 29 of 100, tot loss = 4.762897462680422, l1: 0.00010789696271190066, l2: 0.00036839278392766697   Iteration 30 of 100, tot loss = 4.754702937602997, l1: 0.00010731656766438391, l2: 0.0003681537263522235   Iteration 31 of 100, tot loss = 4.771911009665458, l1: 0.00010764649015336838, l2: 0.00036954461088058567   Iteration 32 of 100, tot loss = 4.7532926090061665, l1: 0.00010667535923403193, l2: 0.00036865390075035975   Iteration 33 of 100, tot loss = 4.751454718185194, l1: 0.0001064310154381956, l2: 0.00036871445561039514   Iteration 34 of 100, tot loss = 4.746221868430867, l1: 0.00010655360882297186, l2: 0.0003680685780743849   Iteration 35 of 100, tot loss = 4.833650360788618, l1: 0.00010761753042710812, l2: 0.0003757475056253108   Iteration 36 of 100, tot loss = 4.843392640352249, l1: 0.0001065433523561094, l2: 0.00037779591083461937   Iteration 37 of 100, tot loss = 4.781373832676862, l1: 0.00010543292680538159, l2: 0.0003727044551507444   Iteration 38 of 100, tot loss = 4.82772917182822, l1: 0.00010599335377934806, l2: 0.0003767795631000282   Iteration 39 of 100, tot loss = 4.842732169689277, l1: 0.00010681106439919975, l2: 0.0003774621522424217   Iteration 40 of 100, tot loss = 4.794142642617226, l1: 0.00010561737044554321, l2: 0.0003737968934728997   Iteration 41 of 100, tot loss = 4.832083635213898, l1: 0.00010649002664339183, l2: 0.00037671833777538975   Iteration 42 of 100, tot loss = 4.817961468583062, l1: 0.00010638482516307184, l2: 0.00037541132249836143   Iteration 43 of 100, tot loss = 4.800521221271781, l1: 0.00010656492050593654, l2: 0.000373487202734903   Iteration 44 of 100, tot loss = 4.791744917631149, l1: 0.00010681847090216417, l2: 0.000372356021795316   Iteration 45 of 100, tot loss = 4.80361365477244, l1: 0.00010709973503253423, l2: 0.00037326163049632063   Iteration 46 of 100, tot loss = 4.8437129855155945, l1: 0.00010753285641840193, l2: 0.00037683844193056956   Iteration 47 of 100, tot loss = 4.9069393061577005, l1: 0.0001086888253841143, l2: 0.00038200510483720596   Iteration 48 of 100, tot loss = 4.917115978896618, l1: 0.00010860094236401589, l2: 0.00038311065418383805   Iteration 49 of 100, tot loss = 4.869326228998145, l1: 0.00010772868603342023, l2: 0.00037920393604471596   Iteration 50 of 100, tot loss = 4.809665954113006, l1: 0.00010674453835235909, l2: 0.000374222056416329   Iteration 51 of 100, tot loss = 4.803039810236762, l1: 0.00010657706297934055, l2: 0.00037372691754260847   Iteration 52 of 100, tot loss = 4.84829898751699, l1: 0.00010751505564817657, l2: 0.00037731484273815743   Iteration 53 of 100, tot loss = 4.8828255073079525, l1: 0.00010809478847494454, l2: 0.000380187762411364   Iteration 54 of 100, tot loss = 4.843779159916772, l1: 0.00010735672320286674, l2: 0.0003770211928726726   Iteration 55 of 100, tot loss = 4.816077971458435, l1: 0.00010668208825253797, l2: 0.00037492570938246154   Iteration 56 of 100, tot loss = 4.8052108734846115, l1: 0.00010656828479633467, l2: 0.00037395280300448315   Iteration 57 of 100, tot loss = 4.801919897397359, l1: 0.00010620611934236397, l2: 0.00037398587084138405   Iteration 58 of 100, tot loss = 4.8060928069312, l1: 0.00010628038342411886, l2: 0.00037432889778223207   Iteration 59 of 100, tot loss = 4.776591993994632, l1: 0.00010594373612794868, l2: 0.0003717154638360302   Iteration 60 of 100, tot loss = 4.756511829296747, l1: 0.00010610347041316951, l2: 0.0003695477130046735   Iteration 61 of 100, tot loss = 4.768982267770611, l1: 0.00010650094477345282, l2: 0.00037039728256965394   Iteration 62 of 100, tot loss = 4.729302696643337, l1: 0.00010590831762754298, l2: 0.00036702195235017326   Iteration 63 of 100, tot loss = 4.722577501857091, l1: 0.00010595092721309306, l2: 0.00036630682307221585   Iteration 64 of 100, tot loss = 4.75979251973331, l1: 0.00010640201173828245, l2: 0.0003695772402352304   Iteration 65 of 100, tot loss = 4.746558473660396, l1: 0.00010624208874875107, l2: 0.0003684137585178877   Iteration 66 of 100, tot loss = 4.7482502153425505, l1: 0.00010593838978269504, l2: 0.00036888663210750866   Iteration 67 of 100, tot loss = 4.746824725350337, l1: 0.00010581408195552041, l2: 0.0003688683908835832   Iteration 68 of 100, tot loss = 4.71403778125258, l1: 0.00010535351569106912, l2: 0.00036605026295600347   Iteration 69 of 100, tot loss = 4.719379119251085, l1: 0.0001054710209364598, l2: 0.0003664668909260544   Iteration 70 of 100, tot loss = 4.726593901429857, l1: 0.00010548792769051423, l2: 0.0003671714628580958   Iteration 71 of 100, tot loss = 4.734719576969953, l1: 0.0001058426324292873, l2: 0.0003676293257945283   Iteration 72 of 100, tot loss = 4.706982862618235, l1: 0.00010510714628253481, l2: 0.000365591140507604   Iteration 73 of 100, tot loss = 4.667777579124659, l1: 0.00010441953448095512, l2: 0.00036235822405353545   Iteration 74 of 100, tot loss = 4.666987085664594, l1: 0.00010440703039962132, l2: 0.0003622916786273866   Iteration 75 of 100, tot loss = 4.652850060462952, l1: 0.00010428810487307298, l2: 0.0003609969018725678   Iteration 76 of 100, tot loss = 4.622153597442727, l1: 0.00010383438056075982, l2: 0.00035838097985680707   Iteration 77 of 100, tot loss = 4.63605762611736, l1: 0.00010432417094578979, l2: 0.0003592815927105847   Iteration 78 of 100, tot loss = 4.628881755547646, l1: 0.00010435598414915148, l2: 0.00035853219238700916   Iteration 79 of 100, tot loss = 4.616474358341362, l1: 0.00010421759163617463, l2: 0.00035742984526890883   Iteration 80 of 100, tot loss = 4.6324828043580055, l1: 0.00010426039193589531, l2: 0.0003589878897400922   Iteration 81 of 100, tot loss = 4.642441545003726, l1: 0.00010447795374389382, l2: 0.000359766202350722   Iteration 82 of 100, tot loss = 4.632381242949788, l1: 0.00010457851416637224, l2: 0.0003586596116869392   Iteration 83 of 100, tot loss = 4.625764038189348, l1: 0.00010410226622550097, l2: 0.0003584741391395836   Iteration 84 of 100, tot loss = 4.670334664129076, l1: 0.00010496238588703342, l2: 0.00036207108236599846   Iteration 85 of 100, tot loss = 4.665336683217217, l1: 0.00010453572283629054, l2: 0.0003619979469530175   Iteration 86 of 100, tot loss = 4.657821262991706, l1: 0.0001043857332555236, l2: 0.00036139639455224064   Iteration 87 of 100, tot loss = 4.633991886829508, l1: 0.00010372748465147458, l2: 0.00035967170555230185   Iteration 88 of 100, tot loss = 4.628382703120058, l1: 0.00010363912104532409, l2: 0.0003591991507247175   Iteration 89 of 100, tot loss = 4.632095389151841, l1: 0.00010375686367061983, l2: 0.00035945267685992495   Iteration 90 of 100, tot loss = 4.617624459001753, l1: 0.00010337799156129929, l2: 0.0003583844557094077   Iteration 91 of 100, tot loss = 4.603123543026683, l1: 0.00010307246155027495, l2: 0.00035723989412372745   Iteration 92 of 100, tot loss = 4.5826739189417465, l1: 0.00010277174382993152, l2: 0.00035549564958985326   Iteration 93 of 100, tot loss = 4.557419908943997, l1: 0.00010220122297347012, l2: 0.00035354076954786495   Iteration 94 of 100, tot loss = 4.539464852911361, l1: 0.00010211552673470268, l2: 0.0003518309602712063   Iteration 95 of 100, tot loss = 4.534373538117659, l1: 0.00010223820618672395, l2: 0.0003511991492766691   Iteration 96 of 100, tot loss = 4.53217473005255, l1: 0.00010247850074544355, l2: 0.00035073897409650573   Iteration 97 of 100, tot loss = 4.519742428641958, l1: 0.00010229882825593243, l2: 0.0003496754165023527   Iteration 98 of 100, tot loss = 4.5095570221239205, l1: 0.00010230351784100224, l2: 0.00034865218616204755   Iteration 99 of 100, tot loss = 4.500572993297769, l1: 0.00010225470554121215, l2: 0.0003478025954047387   Iteration 100 of 100, tot loss = 4.5258330428600315, l1: 0.00010262451178277843, l2: 0.00034995879395864903
   End of epoch 1327; saving model... 

Epoch 1328 of 2000
   Iteration 1 of 100, tot loss = 5.049463748931885, l1: 0.00010291457147104666, l2: 0.000402031815610826   Iteration 2 of 100, tot loss = 4.374050140380859, l1: 9.767663141246885e-05, l2: 0.0003397283871890977   Iteration 3 of 100, tot loss = 4.4560214678446455, l1: 9.275479533243924e-05, l2: 0.0003528473510717352   Iteration 4 of 100, tot loss = 5.228839993476868, l1: 0.00011118869588244706, l2: 0.0004116953059565276   Iteration 5 of 100, tot loss = 5.069879055023193, l1: 0.00011541493877302856, l2: 0.0003915729757864028   Iteration 6 of 100, tot loss = 4.676679929097493, l1: 0.00010643826681189239, l2: 0.00036122972960583866   Iteration 7 of 100, tot loss = 4.4650707585471014, l1: 0.00010220891272183508, l2: 0.0003442981667051624   Iteration 8 of 100, tot loss = 4.50747874379158, l1: 0.00010185243536398048, l2: 0.00034889544076577295   Iteration 9 of 100, tot loss = 4.372012615203857, l1: 9.914201978568194e-05, l2: 0.00033805924370729673   Iteration 10 of 100, tot loss = 4.217468929290772, l1: 9.831402348936535e-05, l2: 0.0003234328702092171   Iteration 11 of 100, tot loss = 4.172555511648005, l1: 9.648842453977771e-05, l2: 0.00032076712655411524   Iteration 12 of 100, tot loss = 4.19227260351181, l1: 9.739143024489749e-05, l2: 0.00032183583001218113   Iteration 13 of 100, tot loss = 4.084984669318566, l1: 9.55819229532678e-05, l2: 0.0003129165427078708   Iteration 14 of 100, tot loss = 4.137846401759556, l1: 9.823881503377509e-05, l2: 0.0003155458252876997   Iteration 15 of 100, tot loss = 4.070185852050781, l1: 9.575543784497616e-05, l2: 0.00031126314812960726   Iteration 16 of 100, tot loss = 4.13552188873291, l1: 9.495722474639479e-05, l2: 0.00031859496266406495   Iteration 17 of 100, tot loss = 4.207868211409625, l1: 9.526210552019834e-05, l2: 0.0003255247138440609   Iteration 18 of 100, tot loss = 4.270512739817302, l1: 9.564292546807944e-05, l2: 0.00033140834582607367   Iteration 19 of 100, tot loss = 4.303188625134919, l1: 9.621680338137881e-05, l2: 0.0003341020568960199   Iteration 20 of 100, tot loss = 4.228770864009857, l1: 9.399126938660629e-05, l2: 0.00032888581554288974   Iteration 21 of 100, tot loss = 4.233467113404047, l1: 9.43301600359735e-05, l2: 0.00032901654915241084   Iteration 22 of 100, tot loss = 4.448955069888722, l1: 9.815004556333984e-05, l2: 0.00034674545895541087   Iteration 23 of 100, tot loss = 4.404621497444484, l1: 9.797788357979658e-05, l2: 0.000342484263060412   Iteration 24 of 100, tot loss = 4.491427322228749, l1: 9.851205989737839e-05, l2: 0.0003506306705579239   Iteration 25 of 100, tot loss = 4.454432239532471, l1: 9.850188711425289e-05, l2: 0.0003469413350103423   Iteration 26 of 100, tot loss = 4.417458616770231, l1: 9.832386590110567e-05, l2: 0.00034342199405028415   Iteration 27 of 100, tot loss = 4.35788596117938, l1: 9.704699591060894e-05, l2: 0.00033874159918114956   Iteration 28 of 100, tot loss = 4.272641782249723, l1: 9.500694484034154e-05, l2: 0.000332257232262886   Iteration 29 of 100, tot loss = 4.248674988746643, l1: 9.463928290642798e-05, l2: 0.0003302282149951648   Iteration 30 of 100, tot loss = 4.3160565257072445, l1: 9.651872290608784e-05, l2: 0.0003350869286805391   Iteration 31 of 100, tot loss = 4.3297754756865965, l1: 9.758016723583663e-05, l2: 0.00033539737905225446   Iteration 32 of 100, tot loss = 4.335051316767931, l1: 9.770013093657326e-05, l2: 0.0003358049998496426   Iteration 33 of 100, tot loss = 4.324301000797387, l1: 9.781729750102386e-05, l2: 0.00033461280117714495   Iteration 34 of 100, tot loss = 4.360812429119559, l1: 9.853042908670271e-05, l2: 0.0003375508129144745   Iteration 35 of 100, tot loss = 4.3765492473329815, l1: 9.917407026348104e-05, l2: 0.0003384808527438768   Iteration 36 of 100, tot loss = 4.339645177125931, l1: 9.788628590791227e-05, l2: 0.00033607823085427907   Iteration 37 of 100, tot loss = 4.338527695552723, l1: 9.858811009090041e-05, l2: 0.0003352646583791923   Iteration 38 of 100, tot loss = 4.387883164380726, l1: 9.939859304914104e-05, l2: 0.0003393897219977685   Iteration 39 of 100, tot loss = 4.399901784383333, l1: 9.953528168784358e-05, l2: 0.0003404548952881342   Iteration 40 of 100, tot loss = 4.381246665120125, l1: 9.899626202241052e-05, l2: 0.000339128402993083   Iteration 41 of 100, tot loss = 4.453017702916774, l1: 9.986832872794078e-05, l2: 0.0003454334389350218   Iteration 42 of 100, tot loss = 4.460909255913326, l1: 9.999150123413918e-05, l2: 0.0003460994214817349   Iteration 43 of 100, tot loss = 4.491439234378726, l1: 0.00010053956133941578, l2: 0.0003486043586785537   Iteration 44 of 100, tot loss = 4.481867110187357, l1: 0.00010013614949209362, l2: 0.00034805055739442736   Iteration 45 of 100, tot loss = 4.487042164802551, l1: 9.999719833204936e-05, l2: 0.0003487070142808888   Iteration 46 of 100, tot loss = 4.454654602900796, l1: 9.899132680363002e-05, l2: 0.0003464741299546364   Iteration 47 of 100, tot loss = 4.434718520083326, l1: 9.868784005409403e-05, l2: 0.0003447840083142782   Iteration 48 of 100, tot loss = 4.418547573188941, l1: 9.803625433354075e-05, l2: 0.000343818499459303   Iteration 49 of 100, tot loss = 4.386074457849775, l1: 9.760651130131826e-05, l2: 0.0003410009310903902   Iteration 50 of 100, tot loss = 4.353205096721649, l1: 9.711118131235708e-05, l2: 0.0003382093249820173   Iteration 51 of 100, tot loss = 4.322230692003288, l1: 9.62105977854591e-05, l2: 0.0003360124680387112   Iteration 52 of 100, tot loss = 4.313128033509622, l1: 9.623495728961783e-05, l2: 0.0003350778430243596   Iteration 53 of 100, tot loss = 4.3574416614928335, l1: 9.741258011518028e-05, l2: 0.0003383315837160104   Iteration 54 of 100, tot loss = 4.38728869623608, l1: 9.776132874658624e-05, l2: 0.00034096753882261474   Iteration 55 of 100, tot loss = 4.3581876342946835, l1: 9.739425870727493e-05, l2: 0.0003384245027766817   Iteration 56 of 100, tot loss = 4.358451104589871, l1: 9.716669598804271e-05, l2: 0.0003386784127152558   Iteration 57 of 100, tot loss = 4.375506051799707, l1: 9.770565897301902e-05, l2: 0.0003398449449665158   Iteration 58 of 100, tot loss = 4.365086035481815, l1: 9.765664793215772e-05, l2: 0.00033885195468961874   Iteration 59 of 100, tot loss = 4.334955956976293, l1: 9.738509704790994e-05, l2: 0.0003361104976709427   Iteration 60 of 100, tot loss = 4.343909245729447, l1: 9.762175765596719e-05, l2: 0.0003367691659756626   Iteration 61 of 100, tot loss = 4.399106797624807, l1: 9.795735473506016e-05, l2: 0.0003419533242150897   Iteration 62 of 100, tot loss = 4.388483703136444, l1: 9.789749586367003e-05, l2: 0.00034095087387193474   Iteration 63 of 100, tot loss = 4.4166770832879205, l1: 9.794145149672182e-05, l2: 0.0003437262563411856   Iteration 64 of 100, tot loss = 4.401161780580878, l1: 9.782282171499901e-05, l2: 0.00034229335597046884   Iteration 65 of 100, tot loss = 4.373699461496793, l1: 9.709070951570398e-05, l2: 0.0003402792362836548   Iteration 66 of 100, tot loss = 4.387780863227266, l1: 9.748893948695199e-05, l2: 0.0003412891460219257   Iteration 67 of 100, tot loss = 4.426876539614663, l1: 9.784241779898501e-05, l2: 0.00034484523543983753   Iteration 68 of 100, tot loss = 4.431691767538295, l1: 9.817782622977735e-05, l2: 0.0003449913494799094   Iteration 69 of 100, tot loss = 4.412528907043346, l1: 9.782529725824766e-05, l2: 0.00034342759223047483   Iteration 70 of 100, tot loss = 4.3875421336718965, l1: 9.736141510074958e-05, l2: 0.0003413927972100542   Iteration 71 of 100, tot loss = 4.36833149446568, l1: 9.693953745201571e-05, l2: 0.00033989361086433987   Iteration 72 of 100, tot loss = 4.370345772968398, l1: 9.678448188626337e-05, l2: 0.0003402500945715777   Iteration 73 of 100, tot loss = 4.348918416728712, l1: 9.644192293420281e-05, l2: 0.0003384499179122791   Iteration 74 of 100, tot loss = 4.3575841913352145, l1: 9.650712269610046e-05, l2: 0.0003392512955690256   Iteration 75 of 100, tot loss = 4.35359651406606, l1: 9.6432214777451e-05, l2: 0.0003389274359991153   Iteration 76 of 100, tot loss = 4.374540032524812, l1: 9.700349035387932e-05, l2: 0.0003404505118133353   Iteration 77 of 100, tot loss = 4.347059576542346, l1: 9.663855107410436e-05, l2: 0.00033806740550487604   Iteration 78 of 100, tot loss = 4.361467891778702, l1: 9.668369621533949e-05, l2: 0.0003394630921553844   Iteration 79 of 100, tot loss = 4.397769280626804, l1: 9.746612635399864e-05, l2: 0.0003423108006429658   Iteration 80 of 100, tot loss = 4.40895874351263, l1: 9.758763007994276e-05, l2: 0.0003433082429182832   Iteration 81 of 100, tot loss = 4.436844624119041, l1: 9.801346836697862e-05, l2: 0.0003456709926939328   Iteration 82 of 100, tot loss = 4.441786607591117, l1: 9.825606400959148e-05, l2: 0.00034592259535424   Iteration 83 of 100, tot loss = 4.413401826318488, l1: 9.787034905019093e-05, l2: 0.00034346983221735444   Iteration 84 of 100, tot loss = 4.424674753631864, l1: 9.826829504856418e-05, l2: 0.0003441991792074294   Iteration 85 of 100, tot loss = 4.42311013025396, l1: 9.808138651458328e-05, l2: 0.0003442296253257048   Iteration 86 of 100, tot loss = 4.428574649400489, l1: 9.800815855986763e-05, l2: 0.00034484930548324183   Iteration 87 of 100, tot loss = 4.431014081527447, l1: 9.820316520107418e-05, l2: 0.00034489824209795817   Iteration 88 of 100, tot loss = 4.476064160466194, l1: 9.881109128235204e-05, l2: 0.00034879532391972714   Iteration 89 of 100, tot loss = 4.479480083069105, l1: 9.876186068345049e-05, l2: 0.00034918614680842195   Iteration 90 of 100, tot loss = 4.4727102796236675, l1: 9.887725294296008e-05, l2: 0.00034839377411925753   Iteration 91 of 100, tot loss = 4.502886849445301, l1: 9.894247557315463e-05, l2: 0.00035134620838517136   Iteration 92 of 100, tot loss = 4.522108848976052, l1: 9.903839410348472e-05, l2: 0.0003531724896279427   Iteration 93 of 100, tot loss = 4.524122841896549, l1: 9.899120801316214e-05, l2: 0.00035342107535094546   Iteration 94 of 100, tot loss = 4.527153353741828, l1: 9.921791051772046e-05, l2: 0.00035349742408490124   Iteration 95 of 100, tot loss = 4.5423318699786535, l1: 9.947992021855163e-05, l2: 0.00035475326600901193   Iteration 96 of 100, tot loss = 4.545012557258208, l1: 9.96255545639239e-05, l2: 0.00035487570054707856   Iteration 97 of 100, tot loss = 4.539159065669345, l1: 9.96379876784145e-05, l2: 0.000354277918303683   Iteration 98 of 100, tot loss = 4.521395959416214, l1: 9.931569455110715e-05, l2: 0.00035282390085832993   Iteration 99 of 100, tot loss = 4.5033342802163325, l1: 9.906895667716691e-05, l2: 0.0003512644709024409   Iteration 100 of 100, tot loss = 4.497062257528305, l1: 9.89285258401651e-05, l2: 0.00035077769935014657
   End of epoch 1328; saving model... 

Epoch 1329 of 2000
   Iteration 1 of 100, tot loss = 5.9491682052612305, l1: 0.0001295088732149452, l2: 0.0004654079384636134   Iteration 2 of 100, tot loss = 4.497723579406738, l1: 0.00010397874939371832, l2: 0.0003457936109043658   Iteration 3 of 100, tot loss = 4.522609869639079, l1: 0.00010839336270388837, l2: 0.00034386761641750735   Iteration 4 of 100, tot loss = 4.496120810508728, l1: 0.00010252198262605816, l2: 0.0003470900992397219   Iteration 5 of 100, tot loss = 4.4630941390991214, l1: 0.00010401365288998931, l2: 0.00034229576122015717   Iteration 6 of 100, tot loss = 4.835145711898804, l1: 0.00011292909039184451, l2: 0.00037058548574956757   Iteration 7 of 100, tot loss = 4.809577669416155, l1: 0.00011518454370421491, l2: 0.00036577322836300094   Iteration 8 of 100, tot loss = 4.952495098114014, l1: 0.00011683111188176554, l2: 0.0003784184082178399   Iteration 9 of 100, tot loss = 5.042852454715305, l1: 0.00011978485013565255, l2: 0.0003845004086744868   Iteration 10 of 100, tot loss = 4.999320793151855, l1: 0.00012072996178176254, l2: 0.00037920213071629405   Iteration 11 of 100, tot loss = 5.33378115567294, l1: 0.00012427822697315025, l2: 0.00040909989779307085   Iteration 12 of 100, tot loss = 5.255973656972249, l1: 0.00012282718247054922, l2: 0.00040277019191610936   Iteration 13 of 100, tot loss = 5.321137685042161, l1: 0.00012392303156397806, l2: 0.00040819074358576193   Iteration 14 of 100, tot loss = 5.192605035645621, l1: 0.0001207762979902327, l2: 0.00039848421043383757   Iteration 15 of 100, tot loss = 5.1798820972442625, l1: 0.00011971979304992904, l2: 0.00039826842063727476   Iteration 16 of 100, tot loss = 5.186718061566353, l1: 0.00011824404191429494, l2: 0.00040042776709015016   Iteration 17 of 100, tot loss = 5.068350763881908, l1: 0.00011747799298438408, l2: 0.00038935708717497834   Iteration 18 of 100, tot loss = 5.110648446612888, l1: 0.00011848603632339898, l2: 0.0003925788138682644   Iteration 19 of 100, tot loss = 5.053051145453202, l1: 0.00011782638658156717, l2: 0.00038747873316567977   Iteration 20 of 100, tot loss = 5.120920443534851, l1: 0.00011956983144045807, l2: 0.0003925222175894305   Iteration 21 of 100, tot loss = 5.14363754363287, l1: 0.00011766182606585236, l2: 0.00039670193412651617   Iteration 22 of 100, tot loss = 5.0949485301971436, l1: 0.00011637221541604958, l2: 0.00039312264231160623   Iteration 23 of 100, tot loss = 5.0849310626154365, l1: 0.00011565134469571564, l2: 0.00039284176613285166   Iteration 24 of 100, tot loss = 5.128605961799622, l1: 0.00011613210184198881, l2: 0.0003967284998604252   Iteration 25 of 100, tot loss = 5.2064055061340335, l1: 0.00011775826947996393, l2: 0.0004028822865802795   Iteration 26 of 100, tot loss = 5.088671262447651, l1: 0.00011495778772559088, l2: 0.00039390934342428896   Iteration 27 of 100, tot loss = 5.080451982992667, l1: 0.00011383684829558695, l2: 0.00039420835512121103   Iteration 28 of 100, tot loss = 5.088419454438346, l1: 0.00011367329482579538, l2: 0.00039516865373505946   Iteration 29 of 100, tot loss = 5.058124706662935, l1: 0.00011190096772444467, l2: 0.0003939115072014453   Iteration 30 of 100, tot loss = 5.0287049293518065, l1: 0.00011137279749770338, l2: 0.000391497700184118   Iteration 31 of 100, tot loss = 5.071690036404517, l1: 0.00011202618723825341, l2: 0.00039514282111814545   Iteration 32 of 100, tot loss = 5.093249350786209, l1: 0.00011172027848260768, l2: 0.00039760466052030097   Iteration 33 of 100, tot loss = 5.148535771803423, l1: 0.00011306274069339094, l2: 0.00040179083898141414   Iteration 34 of 100, tot loss = 5.140875521828146, l1: 0.00011383399631834917, l2: 0.00040025355830199686   Iteration 35 of 100, tot loss = 5.159091881343296, l1: 0.00011374368269961061, l2: 0.0004021655082137191   Iteration 36 of 100, tot loss = 5.19509674443139, l1: 0.00011357709809090011, l2: 0.0004059325789664096   Iteration 37 of 100, tot loss = 5.190524513657029, l1: 0.00011349084603795279, l2: 0.0004055616072209149   Iteration 38 of 100, tot loss = 5.114219972961827, l1: 0.00011175400763213954, l2: 0.00039966799139572114   Iteration 39 of 100, tot loss = 5.089176453076876, l1: 0.00011112658467027359, l2: 0.0003977910625205065   Iteration 40 of 100, tot loss = 5.027961242198944, l1: 0.00011018200539183453, l2: 0.000392614120937651   Iteration 41 of 100, tot loss = 5.02093191844661, l1: 0.00011080725868473897, l2: 0.0003912859353543509   Iteration 42 of 100, tot loss = 5.023610239937192, l1: 0.00011080539003333321, l2: 0.00039155563683293407   Iteration 43 of 100, tot loss = 4.993041853572047, l1: 0.00011041921285606466, l2: 0.00038888497510924935   Iteration 44 of 100, tot loss = 4.95884400064295, l1: 0.00011009933826823148, l2: 0.00038578506395093757   Iteration 45 of 100, tot loss = 5.0567753156026205, l1: 0.00011124835913910324, l2: 0.00039442917461403543   Iteration 46 of 100, tot loss = 5.034580448399419, l1: 0.00011121114434702221, l2: 0.000392246902819075   Iteration 47 of 100, tot loss = 5.060649668916743, l1: 0.00011107949854278481, l2: 0.0003949854711189549   Iteration 48 of 100, tot loss = 5.076070964336395, l1: 0.00011060451659735311, l2: 0.00039700258275843225   Iteration 49 of 100, tot loss = 5.1011245688613585, l1: 0.00011093477335133666, l2: 0.0003991776861117354   Iteration 50 of 100, tot loss = 5.076766853332519, l1: 0.00011046241204894614, l2: 0.00039721427601762115   Iteration 51 of 100, tot loss = 5.067308566149543, l1: 0.00011047481979510528, l2: 0.00039625603947605865   Iteration 52 of 100, tot loss = 5.075630123798664, l1: 0.00011022421809824524, l2: 0.0003973387966442925   Iteration 53 of 100, tot loss = 5.041807606535138, l1: 0.00010972764947888758, l2: 0.00039445311336268514   Iteration 54 of 100, tot loss = 5.081544355109886, l1: 0.00011029798548002468, l2: 0.0003978564518210651   Iteration 55 of 100, tot loss = 5.0828606952320445, l1: 0.00011016440105529248, l2: 0.0003981216704811562   Iteration 56 of 100, tot loss = 5.0510494112968445, l1: 0.00010922037167152407, l2: 0.00039588457158450704   Iteration 57 of 100, tot loss = 5.026289458860431, l1: 0.00010850569397964711, l2: 0.0003941232535636804   Iteration 58 of 100, tot loss = 5.044889848807762, l1: 0.00010861869173070239, l2: 0.0003958702953492195   Iteration 59 of 100, tot loss = 5.036026198985213, l1: 0.00010849176914681743, l2: 0.00039511085320185173   Iteration 60 of 100, tot loss = 5.105513306458791, l1: 0.0001097723906544464, l2: 0.00040077894179072854   Iteration 61 of 100, tot loss = 5.078552093662199, l1: 0.00010903779882222002, l2: 0.0003988174125208657   Iteration 62 of 100, tot loss = 5.081863261038257, l1: 0.00010948091391671164, l2: 0.0003987054139631049   Iteration 63 of 100, tot loss = 5.093256098883493, l1: 0.0001096110670279438, l2: 0.00039971454506961717   Iteration 64 of 100, tot loss = 5.083286490291357, l1: 0.00010977530331501839, l2: 0.0003985533480772574   Iteration 65 of 100, tot loss = 5.080360328234159, l1: 0.00011009861230447244, l2: 0.0003979374229227408   Iteration 66 of 100, tot loss = 5.050358457998796, l1: 0.00010960015209672169, l2: 0.00039543569613614994   Iteration 67 of 100, tot loss = 5.029928154020167, l1: 0.00010915372027440428, l2: 0.00039383909732812266   Iteration 68 of 100, tot loss = 4.98235952503541, l1: 0.00010824044249388594, l2: 0.0003899955120098585   Iteration 69 of 100, tot loss = 4.994768920152084, l1: 0.00010846842765138295, l2: 0.0003910084662915113   Iteration 70 of 100, tot loss = 4.977537080219814, l1: 0.00010811555803229567, l2: 0.00038963815209821666   Iteration 71 of 100, tot loss = 4.9594688079726525, l1: 0.00010811928976335379, l2: 0.00038782759298222846   Iteration 72 of 100, tot loss = 4.959412846300337, l1: 0.00010846841324665648, l2: 0.0003874728732221734   Iteration 73 of 100, tot loss = 4.946259123005279, l1: 0.0001080790823162055, l2: 0.0003865468317773576   Iteration 74 of 100, tot loss = 4.9175481892920825, l1: 0.00010772870799071923, l2: 0.000384026112639018   Iteration 75 of 100, tot loss = 4.907253586451213, l1: 0.0001075209148984868, l2: 0.00038320444563093287   Iteration 76 of 100, tot loss = 4.881024128512332, l1: 0.00010701242617391185, l2: 0.0003810899883989661   Iteration 77 of 100, tot loss = 4.951163805924453, l1: 0.00010791744910408864, l2: 0.00038719893294815943   Iteration 78 of 100, tot loss = 4.973900697170159, l1: 0.0001084078447979081, l2: 0.0003889822268646616   Iteration 79 of 100, tot loss = 4.969475082204312, l1: 0.0001081744240890876, l2: 0.0003887730865382672   Iteration 80 of 100, tot loss = 4.978301244974136, l1: 0.0001083760747405904, l2: 0.00038945405194681373   Iteration 81 of 100, tot loss = 4.98601159343013, l1: 0.00010840751879803985, l2: 0.00039019364303834506   Iteration 82 of 100, tot loss = 5.0225410810331015, l1: 0.0001091371095975685, l2: 0.0003931170008954501   Iteration 83 of 100, tot loss = 5.032084700572922, l1: 0.00010935531906167842, l2: 0.00039385315272760725   Iteration 84 of 100, tot loss = 5.018229975586846, l1: 0.00010893418181824797, l2: 0.0003928888176493014   Iteration 85 of 100, tot loss = 5.011890251496259, l1: 0.00010902017978985574, l2: 0.00039216884709757696   Iteration 86 of 100, tot loss = 5.00942965163741, l1: 0.00010897640480157692, l2: 0.00039196656228362607   Iteration 87 of 100, tot loss = 5.030781277294817, l1: 0.00010937301080905425, l2: 0.0003937051185586051   Iteration 88 of 100, tot loss = 5.0050796514207665, l1: 0.00010872010608926013, l2: 0.00039178786077668934   Iteration 89 of 100, tot loss = 4.999750552552469, l1: 0.00010866426878799737, l2: 0.00039131078837057473   Iteration 90 of 100, tot loss = 5.010761936505635, l1: 0.00010905053487577889, l2: 0.00039202566048414963   Iteration 91 of 100, tot loss = 5.014503287745046, l1: 0.00010937752989235918, l2: 0.00039207280001342094   Iteration 92 of 100, tot loss = 5.000449600427047, l1: 0.00010903679978345399, l2: 0.0003910081613867078   Iteration 93 of 100, tot loss = 4.99157690745528, l1: 0.0001090714899336128, l2: 0.0003900862020698266   Iteration 94 of 100, tot loss = 5.009646359910357, l1: 0.00010918475960894052, l2: 0.0003917798778584643   Iteration 95 of 100, tot loss = 4.988559916144923, l1: 0.0001088562427627805, l2: 0.00038999975030923166   Iteration 96 of 100, tot loss = 4.980681983133157, l1: 0.0001088742583306157, l2: 0.00038919394122179557   Iteration 97 of 100, tot loss = 4.99055194608944, l1: 0.00010904044089310792, l2: 0.00039001475535357157   Iteration 98 of 100, tot loss = 4.998412601801814, l1: 0.00010914479727424415, l2: 0.0003906964644853843   Iteration 99 of 100, tot loss = 5.020783527932986, l1: 0.00010971451866689768, l2: 0.0003923638357787929   Iteration 100 of 100, tot loss = 5.0195685505867, l1: 0.00010960193430946674, l2: 0.00039235492222360334
   End of epoch 1329; saving model... 

Epoch 1330 of 2000
   Iteration 1 of 100, tot loss = 3.274625062942505, l1: 7.80909467721358e-05, l2: 0.0002493715437594801   Iteration 2 of 100, tot loss = 3.523097276687622, l1: 6.977427983656526e-05, l2: 0.0002825354313245043   Iteration 3 of 100, tot loss = 4.401905218760173, l1: 8.488926444745933e-05, l2: 0.0003553012599392484   Iteration 4 of 100, tot loss = 4.061140954494476, l1: 7.908571569714695e-05, l2: 0.0003270283850724809   Iteration 5 of 100, tot loss = 4.109038209915161, l1: 7.507427508244291e-05, l2: 0.0003358295536600053   Iteration 6 of 100, tot loss = 4.406054854393005, l1: 8.056545266299509e-05, l2: 0.00036004003292570513   Iteration 7 of 100, tot loss = 4.232453380312238, l1: 8.099766273517162e-05, l2: 0.0003422476750399385   Iteration 8 of 100, tot loss = 4.297761410474777, l1: 8.356774742424022e-05, l2: 0.00034620839505805634   Iteration 9 of 100, tot loss = 4.332830243640476, l1: 8.68943752720952e-05, l2: 0.00034638865165308945   Iteration 10 of 100, tot loss = 4.422324156761169, l1: 8.975835517048836e-05, l2: 0.00035247406049165875   Iteration 11 of 100, tot loss = 4.308721802451394, l1: 9.074226082620126e-05, l2: 0.0003401299195088954   Iteration 12 of 100, tot loss = 4.38526435693105, l1: 9.333008468577948e-05, l2: 0.0003451963530096691   Iteration 13 of 100, tot loss = 4.447022144611065, l1: 9.228781821618143e-05, l2: 0.00035241439763922244   Iteration 14 of 100, tot loss = 4.4595542294638495, l1: 9.41926461694363e-05, l2: 0.0003517627784666339   Iteration 15 of 100, tot loss = 4.492053826649983, l1: 9.583405529459318e-05, l2: 0.0003533713296443845   Iteration 16 of 100, tot loss = 4.48418065905571, l1: 9.515473220744752e-05, l2: 0.0003532633354552672   Iteration 17 of 100, tot loss = 4.506392058204202, l1: 9.558918784879257e-05, l2: 0.0003550500199815039   Iteration 18 of 100, tot loss = 4.507635884814793, l1: 9.551281901723187e-05, l2: 0.00035525077206936356   Iteration 19 of 100, tot loss = 4.484246504934211, l1: 9.406785791629533e-05, l2: 0.0003543567949428076   Iteration 20 of 100, tot loss = 4.504045486450195, l1: 9.575808035151567e-05, l2: 0.0003546464700775687   Iteration 21 of 100, tot loss = 4.596012342543829, l1: 9.702611194890258e-05, l2: 0.00036257512415648394   Iteration 22 of 100, tot loss = 4.603938687931407, l1: 9.792591332701373e-05, l2: 0.0003624679567027752   Iteration 23 of 100, tot loss = 4.551469326019287, l1: 9.724938022175237e-05, l2: 0.00035789755423573536   Iteration 24 of 100, tot loss = 4.617254773775737, l1: 9.941433260488945e-05, l2: 0.00036231114851640694   Iteration 25 of 100, tot loss = 4.571028871536255, l1: 9.957323200069368e-05, l2: 0.0003575296589406207   Iteration 26 of 100, tot loss = 4.51633464373075, l1: 9.807560784863129e-05, l2: 0.0003535578597131042   Iteration 27 of 100, tot loss = 4.47862387586523, l1: 9.838369913739842e-05, l2: 0.0003494786915662526   Iteration 28 of 100, tot loss = 4.396326116153172, l1: 9.719128099407368e-05, l2: 0.0003424413340066427   Iteration 29 of 100, tot loss = 4.388632149531923, l1: 9.715760285777426e-05, l2: 0.00034170561562420736   Iteration 30 of 100, tot loss = 4.418850723902384, l1: 9.854726243550734e-05, l2: 0.0003433378141683837   Iteration 31 of 100, tot loss = 4.386137823904714, l1: 9.694810620614237e-05, l2: 0.00034166568037753383   Iteration 32 of 100, tot loss = 4.402446269989014, l1: 9.73984772372205e-05, l2: 0.00034284615412616404   Iteration 33 of 100, tot loss = 4.425766843737978, l1: 9.791094150083761e-05, l2: 0.00034466574839880747   Iteration 34 of 100, tot loss = 4.446422731175142, l1: 9.854753678904243e-05, l2: 0.0003460947422461365   Iteration 35 of 100, tot loss = 4.525637040819441, l1: 9.853200379958643e-05, l2: 0.0003540317053973143   Iteration 36 of 100, tot loss = 4.496849258740743, l1: 9.826023657903231e-05, l2: 0.00035142469439759024   Iteration 37 of 100, tot loss = 4.500557345312995, l1: 9.817949683418044e-05, l2: 0.00035187624303649207   Iteration 38 of 100, tot loss = 4.617722574033235, l1: 0.0001004958130100644, l2: 0.0003612764512102953   Iteration 39 of 100, tot loss = 4.576356080862192, l1: 9.962297861840433e-05, l2: 0.000358012636216023   Iteration 40 of 100, tot loss = 4.542021954059601, l1: 9.973991382139502e-05, l2: 0.00035446228794171474   Iteration 41 of 100, tot loss = 4.556609770146812, l1: 0.00010004510326458641, l2: 0.0003556158796815974   Iteration 42 of 100, tot loss = 4.516250337873187, l1: 9.93512374060672e-05, l2: 0.00035227380223139856   Iteration 43 of 100, tot loss = 4.514174051063005, l1: 9.933520760928649e-05, l2: 0.00035208220313422297   Iteration 44 of 100, tot loss = 4.656908067789945, l1: 0.00010117555310981433, l2: 0.0003645152600339233   Iteration 45 of 100, tot loss = 4.641239494747586, l1: 0.00010077648443661423, l2: 0.00036334747144590237   Iteration 46 of 100, tot loss = 4.659557549849801, l1: 0.00010165419893303846, l2: 0.000364301562038473   Iteration 47 of 100, tot loss = 4.651844907314219, l1: 0.00010152808341019331, l2: 0.00036365641287201025   Iteration 48 of 100, tot loss = 4.668972382942836, l1: 0.00010195205959462328, l2: 0.0003649451843254307   Iteration 49 of 100, tot loss = 4.661615634451107, l1: 0.00010197417325297446, l2: 0.0003641873953878233   Iteration 50 of 100, tot loss = 4.698693895339966, l1: 0.00010219075513305142, l2: 0.0003676786398864351   Iteration 51 of 100, tot loss = 4.708846933701459, l1: 0.00010208344097276601, l2: 0.0003688012580181856   Iteration 52 of 100, tot loss = 4.702638708628141, l1: 0.0001017755424142636, l2: 0.00036848833378126775   Iteration 53 of 100, tot loss = 4.680316731614886, l1: 0.00010126029903987282, l2: 0.00036677137925509223   Iteration 54 of 100, tot loss = 4.658639479566504, l1: 0.00010076372335013864, l2: 0.0003651002294241658   Iteration 55 of 100, tot loss = 4.695393601330844, l1: 0.00010168574160409413, l2: 0.00036785362304230646   Iteration 56 of 100, tot loss = 4.685629031487873, l1: 0.00010133603349718865, l2: 0.0003672268740047002   Iteration 57 of 100, tot loss = 4.667602104053163, l1: 0.000101002495508159, l2: 0.00036575771912139956   Iteration 58 of 100, tot loss = 4.62747427923926, l1: 0.00010031484509517182, l2: 0.0003624325871551891   Iteration 59 of 100, tot loss = 4.701652094469232, l1: 0.00010132113058747471, l2: 0.00036884408263623776   Iteration 60 of 100, tot loss = 4.723079860210419, l1: 0.00010186846860354611, l2: 0.0003704395205810821   Iteration 61 of 100, tot loss = 4.720037292261592, l1: 0.00010221409356149035, l2: 0.000369789639199687   Iteration 62 of 100, tot loss = 4.7146089192359675, l1: 0.00010218395841694738, l2: 0.00036927693661475074   Iteration 63 of 100, tot loss = 4.69182018249754, l1: 0.00010164778983120673, l2: 0.0003675342311561551   Iteration 64 of 100, tot loss = 4.689917661249638, l1: 0.00010185492180880829, l2: 0.00036713684653477685   Iteration 65 of 100, tot loss = 4.656293513224675, l1: 0.00010105721773400616, l2: 0.00036457213573157785   Iteration 66 of 100, tot loss = 4.630295724579782, l1: 0.0001004347240902434, l2: 0.00036259485028550085   Iteration 67 of 100, tot loss = 4.603850510582995, l1: 9.999272026731494e-05, l2: 0.00036039233291104656   Iteration 68 of 100, tot loss = 4.6635596506735855, l1: 0.00010124075341831057, l2: 0.00036511521423509457   Iteration 69 of 100, tot loss = 4.654783763747284, l1: 0.00010123592680126119, l2: 0.000364242452264264   Iteration 70 of 100, tot loss = 4.670404614721026, l1: 0.00010171764988626819, l2: 0.00036532281415670045   Iteration 71 of 100, tot loss = 4.7232966658095235, l1: 0.00010266551272623883, l2: 0.00036966415617326406   Iteration 72 of 100, tot loss = 4.698194702466329, l1: 0.00010193348988549487, l2: 0.00036788598279397574   Iteration 73 of 100, tot loss = 4.671222918654141, l1: 0.00010135740720290589, l2: 0.00036576488721088747   Iteration 74 of 100, tot loss = 4.664864749521823, l1: 0.00010132656933746075, l2: 0.00036515990792100345   Iteration 75 of 100, tot loss = 4.641888742446899, l1: 0.00010095786177165186, l2: 0.000363231014731961   Iteration 76 of 100, tot loss = 4.646916718859422, l1: 0.00010123829490207994, l2: 0.0003634533792424726   Iteration 77 of 100, tot loss = 4.637292462509948, l1: 0.00010116330365719903, l2: 0.00036256594485875414   Iteration 78 of 100, tot loss = 4.635539369705396, l1: 0.0001014647207809433, l2: 0.00036208921851655946   Iteration 79 of 100, tot loss = 4.633618671682816, l1: 0.00010148165552269762, l2: 0.00036188021403165603   Iteration 80 of 100, tot loss = 4.635115215182305, l1: 0.00010143312010768568, l2: 0.0003620784040322178   Iteration 81 of 100, tot loss = 4.634857422039833, l1: 0.00010160716219279156, l2: 0.000361878582798117   Iteration 82 of 100, tot loss = 4.650226735487217, l1: 0.00010161527390620194, l2: 0.00036340740224681566   Iteration 83 of 100, tot loss = 4.650426292993936, l1: 0.00010176990528349732, l2: 0.000363272726809303   Iteration 84 of 100, tot loss = 4.646998306115468, l1: 0.00010172218156055481, l2: 0.00036297765178633633   Iteration 85 of 100, tot loss = 4.637205656837015, l1: 0.0001012744739829727, l2: 0.00036244609423995237   Iteration 86 of 100, tot loss = 4.611891208693039, l1: 0.00010097603825310808, l2: 0.0003602130850912397   Iteration 87 of 100, tot loss = 4.63724527687862, l1: 0.00010155628140261076, l2: 0.00036216824852069307   Iteration 88 of 100, tot loss = 4.609345306049693, l1: 0.00010108523586247678, l2: 0.000359849296893861   Iteration 89 of 100, tot loss = 4.603674679659726, l1: 0.00010111675727059489, l2: 0.00035925071295403113   Iteration 90 of 100, tot loss = 4.606362162695991, l1: 0.00010099578167783976, l2: 0.00035964043701015825   Iteration 91 of 100, tot loss = 4.619218145098005, l1: 0.00010137658862551869, l2: 0.0003605452279821243   Iteration 92 of 100, tot loss = 4.6126063388326894, l1: 0.00010106275678244064, l2: 0.0003601978792373658   Iteration 93 of 100, tot loss = 4.591212754608483, l1: 0.0001007955000996815, l2: 0.0003583257777243352   Iteration 94 of 100, tot loss = 4.614781633336493, l1: 0.00010109386621866662, l2: 0.0003603842997108586   Iteration 95 of 100, tot loss = 4.606732157657021, l1: 0.00010108087647638871, l2: 0.00035959234182404254   Iteration 96 of 100, tot loss = 4.593170990546544, l1: 0.00010084403542502211, l2: 0.0003584730661714275   Iteration 97 of 100, tot loss = 4.560713673375316, l1: 0.00010015211516952852, l2: 0.0003559192546256866   Iteration 98 of 100, tot loss = 4.5988103358113035, l1: 0.00010059741105912824, l2: 0.0003592836246196162   Iteration 99 of 100, tot loss = 4.617313012932286, l1: 0.00010078811766391602, l2: 0.00036094318578523026   Iteration 100 of 100, tot loss = 4.604833289384842, l1: 0.00010068677322124131, l2: 0.0003597965578956064
   End of epoch 1330; saving model... 

Epoch 1331 of 2000
   Iteration 1 of 100, tot loss = 5.6047539710998535, l1: 0.0001375665597151965, l2: 0.00042290883720852435   Iteration 2 of 100, tot loss = 6.255197525024414, l1: 0.00013954668975202367, l2: 0.00048597306886222214   Iteration 3 of 100, tot loss = 6.281832695007324, l1: 0.0001362546366484215, l2: 0.0004919286293443292   Iteration 4 of 100, tot loss = 5.406910002231598, l1: 0.0001210292011819547, l2: 0.0004196617956040427   Iteration 5 of 100, tot loss = 5.401077318191528, l1: 0.00011913710623048245, l2: 0.00042097062105312945   Iteration 6 of 100, tot loss = 5.213292161623637, l1: 0.00011871285702606353, l2: 0.0004026163563442727   Iteration 7 of 100, tot loss = 5.070252452577863, l1: 0.00011441668487220471, l2: 0.00039260856075478453   Iteration 8 of 100, tot loss = 4.958250910043716, l1: 0.0001127084979088977, l2: 0.00038311659591272473   Iteration 9 of 100, tot loss = 4.838762945599026, l1: 0.00011111777388982268, l2: 0.0003727585232506196   Iteration 10 of 100, tot loss = 4.814461636543274, l1: 0.00010932293516816572, l2: 0.0003721232293173671   Iteration 11 of 100, tot loss = 4.861109972000122, l1: 0.0001097677126050588, l2: 0.00037634328790855676   Iteration 12 of 100, tot loss = 4.658205091953278, l1: 0.0001046039721283402, l2: 0.0003612165395073437   Iteration 13 of 100, tot loss = 4.509885109387911, l1: 0.00010236953032570175, l2: 0.0003486189827806531   Iteration 14 of 100, tot loss = 4.490165489060538, l1: 0.00010173260177128083, l2: 0.00034728395049959157   Iteration 15 of 100, tot loss = 4.463891394933065, l1: 0.00010093132514157332, l2: 0.0003454578157591944   Iteration 16 of 100, tot loss = 4.480032458901405, l1: 0.00010024906646322052, l2: 0.0003477541804386419   Iteration 17 of 100, tot loss = 4.501179035972147, l1: 0.00010184259757379015, l2: 0.00034827530736733666   Iteration 18 of 100, tot loss = 4.459571811887953, l1: 0.00010085325973502929, l2: 0.0003451039220736776   Iteration 19 of 100, tot loss = 4.537560889595433, l1: 0.0001034435709267487, l2: 0.00035031251900363714   Iteration 20 of 100, tot loss = 4.6158743619918825, l1: 0.00010534666380408453, l2: 0.000356240773544414   Iteration 21 of 100, tot loss = 4.6788271722339445, l1: 0.00010628475259485033, l2: 0.00036159796443479577   Iteration 22 of 100, tot loss = 4.62917947769165, l1: 0.00010558292127494828, l2: 0.0003573350265717388   Iteration 23 of 100, tot loss = 4.491013345511063, l1: 0.00010235594179349668, l2: 0.0003467453925227544   Iteration 24 of 100, tot loss = 4.42109311123689, l1: 0.00010081565596919972, l2: 0.0003412936545525251   Iteration 25 of 100, tot loss = 4.475931477546692, l1: 0.00010109431663295254, l2: 0.0003464988296036609   Iteration 26 of 100, tot loss = 4.469582397204179, l1: 0.00010092877010510374, l2: 0.00034602946862623165   Iteration 27 of 100, tot loss = 4.502983132998149, l1: 0.00010134822244893897, l2: 0.000348950090820263   Iteration 28 of 100, tot loss = 4.510647454432079, l1: 0.00010165994353883434, l2: 0.00034940480197422275   Iteration 29 of 100, tot loss = 4.485950802934581, l1: 0.00010091961880318885, l2: 0.00034767546182185217   Iteration 30 of 100, tot loss = 4.422546970844269, l1: 9.957481815945358e-05, l2: 0.00034267987866769544   Iteration 31 of 100, tot loss = 4.394666198761232, l1: 9.989508379799043e-05, l2: 0.0003395715360848924   Iteration 32 of 100, tot loss = 4.379347268491983, l1: 9.964536911866162e-05, l2: 0.0003382893576144852   Iteration 33 of 100, tot loss = 4.395230773723487, l1: 0.00010022756804105346, l2: 0.00033929550982341453   Iteration 34 of 100, tot loss = 4.404980648966396, l1: 9.989458703785203e-05, l2: 0.00034060347880928446   Iteration 35 of 100, tot loss = 4.4908940758023945, l1: 0.00010063176762612005, l2: 0.00034845764104310156   Iteration 36 of 100, tot loss = 4.501075059175491, l1: 0.00010119110124327967, l2: 0.00034891640603341837   Iteration 37 of 100, tot loss = 4.481106013865085, l1: 0.00010152889326737086, l2: 0.0003465817097930011   Iteration 38 of 100, tot loss = 4.547180982012498, l1: 0.00010199192643324893, l2: 0.0003527261741462433   Iteration 39 of 100, tot loss = 4.490569123855004, l1: 0.00010127180165652998, l2: 0.0003477851130763809   Iteration 40 of 100, tot loss = 4.459055861830711, l1: 0.00010074091496790061, l2: 0.0003451646729445201   Iteration 41 of 100, tot loss = 4.466472692605926, l1: 0.00010055406679251663, l2: 0.0003460932044928507   Iteration 42 of 100, tot loss = 4.537762048698607, l1: 0.00010158565280670744, l2: 0.0003521905540158817   Iteration 43 of 100, tot loss = 4.550670110902121, l1: 0.00010160506103285264, l2: 0.0003534619526214746   Iteration 44 of 100, tot loss = 4.546834997155449, l1: 0.00010126743638300633, l2: 0.0003534160656603159   Iteration 45 of 100, tot loss = 4.533112925953335, l1: 0.00010123982922070557, l2: 0.0003520714657497592   Iteration 46 of 100, tot loss = 4.534685567669246, l1: 0.00010104995406278329, l2: 0.00035241860519897233   Iteration 47 of 100, tot loss = 4.530746883534371, l1: 0.00010014823156451073, l2: 0.00035292645956592713   Iteration 48 of 100, tot loss = 4.538108733793099, l1: 0.00010005724599674674, l2: 0.00035375362995182513   Iteration 49 of 100, tot loss = 4.5197100809642246, l1: 0.00010019406367138465, l2: 0.0003517769469834427   Iteration 50 of 100, tot loss = 4.5138390517234805, l1: 9.999422276450787e-05, l2: 0.0003513896850927267   Iteration 51 of 100, tot loss = 4.508982347507103, l1: 9.926841741224623e-05, l2: 0.00035162981985908406   Iteration 52 of 100, tot loss = 4.492764296439978, l1: 9.876191220078348e-05, l2: 0.0003505145198757348   Iteration 53 of 100, tot loss = 4.499763513511082, l1: 9.89734070108685e-05, l2: 0.00035100294629063564   Iteration 54 of 100, tot loss = 4.467025149751593, l1: 9.84058697213186e-05, l2: 0.00034829664717242985   Iteration 55 of 100, tot loss = 4.501093528487465, l1: 9.82522728339642e-05, l2: 0.00035185708190758967   Iteration 56 of 100, tot loss = 4.528954107846532, l1: 9.85760449501478e-05, l2: 0.0003543193671638229   Iteration 57 of 100, tot loss = 4.5413618443305035, l1: 9.87695704296991e-05, l2: 0.0003553666160370452   Iteration 58 of 100, tot loss = 4.5541949457135695, l1: 9.905782312200935e-05, l2: 0.000356361673013829   Iteration 59 of 100, tot loss = 4.553190621279054, l1: 9.91894420452408e-05, l2: 0.0003561296218353071   Iteration 60 of 100, tot loss = 4.527340831359227, l1: 9.899170903130047e-05, l2: 0.0003537423761978668   Iteration 61 of 100, tot loss = 4.515757355533663, l1: 9.857867249205601e-05, l2: 0.00035299706503225976   Iteration 62 of 100, tot loss = 4.510403861922603, l1: 9.843479165283497e-05, l2: 0.0003526055970137203   Iteration 63 of 100, tot loss = 4.496139055206662, l1: 9.829375359538336e-05, l2: 0.0003513201542470294   Iteration 64 of 100, tot loss = 4.490150740370154, l1: 9.802925882240743e-05, l2: 0.0003509858178176728   Iteration 65 of 100, tot loss = 4.5132583306385925, l1: 9.802305234748368e-05, l2: 0.0003533027832208273   Iteration 66 of 100, tot loss = 4.499815343004284, l1: 9.762758346562006e-05, l2: 0.0003523539532827933   Iteration 67 of 100, tot loss = 4.5078890163507035, l1: 9.747248275662467e-05, l2: 0.00035331642101848944   Iteration 68 of 100, tot loss = 4.513307187487097, l1: 9.776390946653384e-05, l2: 0.0003535668112068807   Iteration 69 of 100, tot loss = 4.4841112002082495, l1: 9.701247367100196e-05, l2: 0.0003513986483322578   Iteration 70 of 100, tot loss = 4.487126420225416, l1: 9.727658938832715e-05, l2: 0.0003514360543964098   Iteration 71 of 100, tot loss = 4.491379182103654, l1: 9.756784877513574e-05, l2: 0.00035157007102536457   Iteration 72 of 100, tot loss = 4.491576878560914, l1: 9.803924260470214e-05, l2: 0.0003511184468152351   Iteration 73 of 100, tot loss = 4.483032260855583, l1: 9.807992368946107e-05, l2: 0.0003502233039024796   Iteration 74 of 100, tot loss = 4.469289678174096, l1: 9.789385542201155e-05, l2: 0.00034903511403024676   Iteration 75 of 100, tot loss = 4.480158394177755, l1: 9.819002589210868e-05, l2: 0.00034982581574392196   Iteration 76 of 100, tot loss = 4.492524926599703, l1: 9.861012911546583e-05, l2: 0.0003506423660215468   Iteration 77 of 100, tot loss = 4.497234457499021, l1: 9.871819032002362e-05, l2: 0.00035100525754198507   Iteration 78 of 100, tot loss = 4.497325611420167, l1: 9.892300117942982e-05, l2: 0.0003508095623356684   Iteration 79 of 100, tot loss = 4.507477435884597, l1: 9.938905575928644e-05, l2: 0.00035135869054297644   Iteration 80 of 100, tot loss = 4.520426608622074, l1: 9.932806342476397e-05, l2: 0.00035271460028525327   Iteration 81 of 100, tot loss = 4.507646055869114, l1: 9.94643476722693e-05, l2: 0.0003513002605213393   Iteration 82 of 100, tot loss = 4.5465075693479395, l1: 0.00010011445716017766, l2: 0.0003545363022833665   Iteration 83 of 100, tot loss = 4.551692039133554, l1: 0.00010034088765941448, l2: 0.0003548283186910887   Iteration 84 of 100, tot loss = 4.531756342876525, l1: 0.000100132313881269, l2: 0.0003530433228401567   Iteration 85 of 100, tot loss = 4.51711268565234, l1: 9.96357062831521e-05, l2: 0.00035207556459628154   Iteration 86 of 100, tot loss = 4.5013965271240055, l1: 9.964817463485308e-05, l2: 0.00035049148040301124   Iteration 87 of 100, tot loss = 4.538140185948076, l1: 9.98819399599639e-05, l2: 0.00035393208052211003   Iteration 88 of 100, tot loss = 4.527002455158667, l1: 9.95675305106984e-05, l2: 0.0003531327168027267   Iteration 89 of 100, tot loss = 4.538346468732598, l1: 9.97279920933109e-05, l2: 0.00035410665662035745   Iteration 90 of 100, tot loss = 4.539125512705909, l1: 9.985956825807484e-05, l2: 0.0003540529849285829   Iteration 91 of 100, tot loss = 4.509834552859212, l1: 9.914321196777234e-05, l2: 0.00035184024518757426   Iteration 92 of 100, tot loss = 4.526139271000157, l1: 9.919224185464175e-05, l2: 0.00035342168698464417   Iteration 93 of 100, tot loss = 4.536497122497969, l1: 9.940812679981532e-05, l2: 0.0003542415870469995   Iteration 94 of 100, tot loss = 4.537514449434077, l1: 9.95560774444652e-05, l2: 0.0003541953694675999   Iteration 95 of 100, tot loss = 4.552165248519496, l1: 9.974058584224335e-05, l2: 0.0003554759410020013   Iteration 96 of 100, tot loss = 4.547301606585582, l1: 9.970898994045758e-05, l2: 0.0003550211726330114   Iteration 97 of 100, tot loss = 4.556867399166539, l1: 9.95969534527568e-05, l2: 0.0003560897888952972   Iteration 98 of 100, tot loss = 4.555728227508311, l1: 9.942658363463718e-05, l2: 0.00035614624142240524   Iteration 99 of 100, tot loss = 4.58654766973823, l1: 0.00010009475807586422, l2: 0.0003585600109113559   Iteration 100 of 100, tot loss = 4.577997111082077, l1: 0.00010003170486015733, l2: 0.00035776800832536536
   End of epoch 1331; saving model... 

Epoch 1332 of 2000
   Iteration 1 of 100, tot loss = 10.497392654418945, l1: 0.0001889954146463424, l2: 0.00086074392311275   Iteration 2 of 100, tot loss = 7.697822093963623, l1: 0.00015950739907566458, l2: 0.000610274844802916   Iteration 3 of 100, tot loss = 6.4654364585876465, l1: 0.00013809410544733206, l2: 0.0005084495642222464   Iteration 4 of 100, tot loss = 6.327454924583435, l1: 0.0001400893488607835, l2: 0.0004926561523461714   Iteration 5 of 100, tot loss = 5.674791145324707, l1: 0.0001306399251916446, l2: 0.00043683919357135893   Iteration 6 of 100, tot loss = 5.436633904774983, l1: 0.00012505474417897253, l2: 0.0004186086540964122   Iteration 7 of 100, tot loss = 5.372177873338972, l1: 0.00012408572261587584, l2: 0.0004131320705969951   Iteration 8 of 100, tot loss = 5.5501590967178345, l1: 0.00012485417482821504, l2: 0.00043016174095100723   Iteration 9 of 100, tot loss = 5.525950484805637, l1: 0.00012379677274212654, l2: 0.0004287982818722311   Iteration 10 of 100, tot loss = 5.53107180595398, l1: 0.00012209752603666856, l2: 0.0004310096555855125   Iteration 11 of 100, tot loss = 5.525499257174405, l1: 0.00011942707615989175, l2: 0.0004331228529653427   Iteration 12 of 100, tot loss = 5.474248011906941, l1: 0.00011987977632088587, l2: 0.0004275450264685787   Iteration 13 of 100, tot loss = 5.489597357236422, l1: 0.00011951110186950806, l2: 0.0004294486335801104   Iteration 14 of 100, tot loss = 5.508536849703107, l1: 0.00012174392863276548, l2: 0.00042910975961214196   Iteration 15 of 100, tot loss = 5.296781587600708, l1: 0.00011687464405743716, l2: 0.0004128035167620207   Iteration 16 of 100, tot loss = 5.18770158290863, l1: 0.00011648218401205668, l2: 0.00040228797570307506   Iteration 17 of 100, tot loss = 5.111643959494198, l1: 0.00011481331282137784, l2: 0.0003963510843117119   Iteration 18 of 100, tot loss = 5.178161064783732, l1: 0.0001165306516163077, l2: 0.0004012854567716002   Iteration 19 of 100, tot loss = 5.279477696669729, l1: 0.00011743158795166222, l2: 0.0004105161818810494   Iteration 20 of 100, tot loss = 5.218938779830933, l1: 0.00011459471679700072, l2: 0.00040729915999691   Iteration 21 of 100, tot loss = 5.301762217567081, l1: 0.00011543411424749397, l2: 0.00041474210627798344   Iteration 22 of 100, tot loss = 5.335936849767512, l1: 0.00011597461806641977, l2: 0.0004176190658737059   Iteration 23 of 100, tot loss = 5.282537045686142, l1: 0.00011509013711474836, l2: 0.00041316356699731523   Iteration 24 of 100, tot loss = 5.235341966152191, l1: 0.00011407163553182424, l2: 0.00040946256134096376   Iteration 25 of 100, tot loss = 5.211322154998779, l1: 0.00011387711274437606, l2: 0.00040725510276388375   Iteration 26 of 100, tot loss = 5.237900055371798, l1: 0.00011379000940136253, l2: 0.00040999999719833094   Iteration 27 of 100, tot loss = 5.262607804051152, l1: 0.00011420286615180817, l2: 0.00041205791543082643   Iteration 28 of 100, tot loss = 5.292186941419329, l1: 0.00011456402431317006, l2: 0.00041465467022914836   Iteration 29 of 100, tot loss = 5.280870273195464, l1: 0.0001147498556308384, l2: 0.00041333717195686466   Iteration 30 of 100, tot loss = 5.259141985575358, l1: 0.00011438423947159511, l2: 0.0004115299588496176   Iteration 31 of 100, tot loss = 5.199742255672332, l1: 0.00011370302977851562, l2: 0.00040627119550105907   Iteration 32 of 100, tot loss = 5.167163819074631, l1: 0.00011306545661682321, l2: 0.0004036509258185106   Iteration 33 of 100, tot loss = 5.143326788237601, l1: 0.00011244905357615966, l2: 0.00040188362552064723   Iteration 34 of 100, tot loss = 5.150929100373212, l1: 0.00011204064353745814, l2: 0.0004030522665262277   Iteration 35 of 100, tot loss = 5.11637452670506, l1: 0.00011153030084512596, l2: 0.00040010715144619876   Iteration 36 of 100, tot loss = 5.1124363674057856, l1: 0.00011134522173961159, l2: 0.000399898414697317   Iteration 37 of 100, tot loss = 5.102358489423184, l1: 0.00011122313004012244, l2: 0.0003990127183528416   Iteration 38 of 100, tot loss = 5.062592198974208, l1: 0.000111054212634611, l2: 0.0003952050070725626   Iteration 39 of 100, tot loss = 5.0946727226942015, l1: 0.00011184234869851467, l2: 0.00039762492293121817   Iteration 40 of 100, tot loss = 5.073483425378799, l1: 0.00011171678634127601, l2: 0.0003956315555114998   Iteration 41 of 100, tot loss = 5.088815601860604, l1: 0.00011184748663941807, l2: 0.00039703407283679285   Iteration 42 of 100, tot loss = 5.004728127093542, l1: 0.00011026307211501436, l2: 0.0003902097400679763   Iteration 43 of 100, tot loss = 4.972545959228693, l1: 0.00011017585855497177, l2: 0.0003870787367368182   Iteration 44 of 100, tot loss = 4.9617200358347455, l1: 0.00011005999013749798, l2: 0.00038611201324039774   Iteration 45 of 100, tot loss = 4.969146540429857, l1: 0.00011026976539546417, l2: 0.00038664488901203083   Iteration 46 of 100, tot loss = 4.9548878177352575, l1: 0.00011062461714533603, l2: 0.0003848641653071947   Iteration 47 of 100, tot loss = 4.959298917587767, l1: 0.00011077292645573021, l2: 0.00038515696592475386   Iteration 48 of 100, tot loss = 5.069605263570945, l1: 0.00011239022463390332, l2: 0.0003945703036454991   Iteration 49 of 100, tot loss = 5.045180483740204, l1: 0.00011250187051292433, l2: 0.0003920161797562899   Iteration 50 of 100, tot loss = 5.012801172733307, l1: 0.00011195440732990392, l2: 0.0003893257120216731   Iteration 51 of 100, tot loss = 5.053133116048925, l1: 0.00011239673353973594, l2: 0.00039291657937129084   Iteration 52 of 100, tot loss = 5.031407358554693, l1: 0.0001125802156401243, l2: 0.00039056052138711006   Iteration 53 of 100, tot loss = 4.987409306022356, l1: 0.00011193233828789573, l2: 0.0003868085934883774   Iteration 54 of 100, tot loss = 4.970380167166392, l1: 0.00011172969634235078, l2: 0.00038530832117194896   Iteration 55 of 100, tot loss = 5.0039681889794085, l1: 0.00011247563035629521, l2: 0.00038792118946069177   Iteration 56 of 100, tot loss = 4.971062470759664, l1: 0.00011180466429193205, l2: 0.00038530158352451896   Iteration 57 of 100, tot loss = 4.986296626559475, l1: 0.00011235681653999021, l2: 0.00038627284696926117   Iteration 58 of 100, tot loss = 5.001048036690416, l1: 0.00011277949883590517, l2: 0.0003873253057275273   Iteration 59 of 100, tot loss = 4.977858472678621, l1: 0.00011216264064680248, l2: 0.0003856232075831632   Iteration 60 of 100, tot loss = 4.992229694128037, l1: 0.00011250443628038434, l2: 0.0003867185338701044   Iteration 61 of 100, tot loss = 4.969884022337491, l1: 0.00011205531467805753, l2: 0.000384933088470029   Iteration 62 of 100, tot loss = 4.963350251797707, l1: 0.00011188359595478452, l2: 0.0003844514303050199   Iteration 63 of 100, tot loss = 4.9899749547716175, l1: 0.00011225549828922672, l2: 0.0003867419985158088   Iteration 64 of 100, tot loss = 4.9701715391129255, l1: 0.000111471848185829, l2: 0.0003855453068126735   Iteration 65 of 100, tot loss = 4.972153801184434, l1: 0.00011111770906539348, l2: 0.00038609767160288844   Iteration 66 of 100, tot loss = 4.994820101694628, l1: 0.00011151737569228068, l2: 0.00038796463515168534   Iteration 67 of 100, tot loss = 5.018493593628727, l1: 0.00011181851135264498, l2: 0.0003900308492758534   Iteration 68 of 100, tot loss = 5.009654436041327, l1: 0.0001117939303666764, l2: 0.00038917151462502424   Iteration 69 of 100, tot loss = 4.9730043290317925, l1: 0.00011119050021448672, l2: 0.0003861099342601306   Iteration 70 of 100, tot loss = 4.991156818185534, l1: 0.00011106835060802821, l2: 0.0003880473328795883   Iteration 71 of 100, tot loss = 4.979689196801521, l1: 0.00011063684390867653, l2: 0.00038733207762100353   Iteration 72 of 100, tot loss = 5.0076844443877535, l1: 0.00011109652960941376, l2: 0.0003896719169157829   Iteration 73 of 100, tot loss = 5.049143283334497, l1: 0.00011141072212772648, l2: 0.0003935036087792992   Iteration 74 of 100, tot loss = 5.023935719116314, l1: 0.00011093809823521312, l2: 0.0003914554762485676   Iteration 75 of 100, tot loss = 5.006470510164896, l1: 0.00011034341451401512, l2: 0.0003903036390935692   Iteration 76 of 100, tot loss = 5.010599388888008, l1: 0.00011024566064358958, l2: 0.00039081428076315206   Iteration 77 of 100, tot loss = 5.022145006563757, l1: 0.00011028488275864125, l2: 0.0003919296206448336   Iteration 78 of 100, tot loss = 5.028391637863257, l1: 0.00011055936817846929, l2: 0.00039227979859555117   Iteration 79 of 100, tot loss = 5.0052658591089365, l1: 0.00011024322451749534, l2: 0.0003902833645204023   Iteration 80 of 100, tot loss = 4.980748696625232, l1: 0.00010989721395162632, l2: 0.00038817765862404486   Iteration 81 of 100, tot loss = 5.015952353124265, l1: 0.000110443412178242, l2: 0.00039115182548419305   Iteration 82 of 100, tot loss = 4.997041776412871, l1: 0.00011008465786822276, l2: 0.0003896195221608561   Iteration 83 of 100, tot loss = 5.017904986818153, l1: 0.00011040856647978436, l2: 0.00039138193412316424   Iteration 84 of 100, tot loss = 5.0050174068836935, l1: 0.00010989320279415031, l2: 0.0003906085397223554   Iteration 85 of 100, tot loss = 5.002918962871327, l1: 0.00010996642248595462, l2: 0.000390325475788867   Iteration 86 of 100, tot loss = 5.006197038084962, l1: 0.00011008527432094588, l2: 0.00039053443140082256   Iteration 87 of 100, tot loss = 4.99344734898929, l1: 0.0001101073484142557, l2: 0.0003892373884579798   Iteration 88 of 100, tot loss = 5.026942406188358, l1: 0.00011083974442788696, l2: 0.00039185449788254283   Iteration 89 of 100, tot loss = 5.045982694357969, l1: 0.00011070595408025919, l2: 0.0003938923166898665   Iteration 90 of 100, tot loss = 5.032412911785974, l1: 0.00011048359228880145, l2: 0.00039275770031963473   Iteration 91 of 100, tot loss = 5.02091739334903, l1: 0.00011014338615695833, l2: 0.0003919483548328332   Iteration 92 of 100, tot loss = 5.013397583495015, l1: 0.00011021617209063034, l2: 0.00039112358773190465   Iteration 93 of 100, tot loss = 5.011783957481384, l1: 0.00011007625101727524, l2: 0.000391102146300342   Iteration 94 of 100, tot loss = 4.996111939562128, l1: 0.00010977229380551627, l2: 0.00038983890183946177   Iteration 95 of 100, tot loss = 5.008061573379918, l1: 0.00011013704038065809, l2: 0.00039066911872635645   Iteration 96 of 100, tot loss = 5.006797416756551, l1: 0.00011000110422780078, l2: 0.00039067863940545067   Iteration 97 of 100, tot loss = 5.0215487910300185, l1: 0.00011029975913637846, l2: 0.0003918551218956373   Iteration 98 of 100, tot loss = 5.017748865546013, l1: 0.00011031214790466261, l2: 0.0003914627403511466   Iteration 99 of 100, tot loss = 5.018458601200219, l1: 0.00011013006955218228, l2: 0.00039171579217766364   Iteration 100 of 100, tot loss = 5.028240903615951, l1: 0.00011022319486073684, l2: 0.00039260089724848515
   End of epoch 1332; saving model... 

Epoch 1333 of 2000
   Iteration 1 of 100, tot loss = 5.571542263031006, l1: 0.00011866876593558118, l2: 0.00043848546920344234   Iteration 2 of 100, tot loss = 6.403928995132446, l1: 0.0001243962578882929, l2: 0.0005159966531209648   Iteration 3 of 100, tot loss = 5.8458835283915205, l1: 0.00011999672521293785, l2: 0.00046459164392823976   Iteration 4 of 100, tot loss = 5.437551259994507, l1: 0.00011710940452758223, l2: 0.0004266457326593809   Iteration 5 of 100, tot loss = 5.269849872589111, l1: 0.0001156576196081005, l2: 0.00041132737533189356   Iteration 6 of 100, tot loss = 5.061228275299072, l1: 0.0001127939637323531, l2: 0.00039332887293615687   Iteration 7 of 100, tot loss = 4.985142980303083, l1: 0.0001049857097054233, l2: 0.00039352859520087283   Iteration 8 of 100, tot loss = 4.676935791969299, l1: 9.735646381159313e-05, l2: 0.0003703371203300776   Iteration 9 of 100, tot loss = 4.680138481987847, l1: 9.717608433372031e-05, l2: 0.0003708377658363639   Iteration 10 of 100, tot loss = 4.538323450088501, l1: 9.664763711043633e-05, l2: 0.00035718470899155365   Iteration 11 of 100, tot loss = 4.450059587305242, l1: 9.827118761181323e-05, l2: 0.000346734771896577   Iteration 12 of 100, tot loss = 4.421584685643514, l1: 0.00010042737449111883, l2: 0.0003417310957350613   Iteration 13 of 100, tot loss = 4.378044146757859, l1: 0.00010008941563473154, l2: 0.0003377149998693942   Iteration 14 of 100, tot loss = 4.7260867697852, l1: 0.00010591916309619722, l2: 0.00036668951569091793   Iteration 15 of 100, tot loss = 4.834638388951619, l1: 0.0001078112802739876, l2: 0.00037565256061498074   Iteration 16 of 100, tot loss = 4.750421896576881, l1: 0.00010807286707859021, l2: 0.00036696932420454687   Iteration 17 of 100, tot loss = 4.690151144476498, l1: 0.00010757952558753245, l2: 0.00036143559041460426   Iteration 18 of 100, tot loss = 4.779495120048523, l1: 0.00010898292303964909, l2: 0.0003689665876057309   Iteration 19 of 100, tot loss = 4.816785021832115, l1: 0.00011016868091629524, l2: 0.00037150982212838963   Iteration 20 of 100, tot loss = 4.868066847324371, l1: 0.00011142863368149847, l2: 0.00037537805110332556   Iteration 21 of 100, tot loss = 4.88857779048738, l1: 0.00011144306786480316, l2: 0.000377414711346362   Iteration 22 of 100, tot loss = 4.850029739466581, l1: 0.0001108366609101226, l2: 0.00037416631411443547   Iteration 23 of 100, tot loss = 4.904130386269611, l1: 0.00011190368177662806, l2: 0.0003785093576880172   Iteration 24 of 100, tot loss = 4.939300745725632, l1: 0.0001135208182555895, l2: 0.00038040925755922217   Iteration 25 of 100, tot loss = 4.946282434463501, l1: 0.00011374639492714778, l2: 0.0003808818483958021   Iteration 26 of 100, tot loss = 5.069811133237986, l1: 0.00011553074923890214, l2: 0.00039145036414713383   Iteration 27 of 100, tot loss = 4.990868577250728, l1: 0.00011382268443143133, l2: 0.00038526417295172533   Iteration 28 of 100, tot loss = 5.094322536672864, l1: 0.00011581255629217984, l2: 0.00039361969746616   Iteration 29 of 100, tot loss = 5.189664947575536, l1: 0.00011774381709983572, l2: 0.0004012226779415304   Iteration 30 of 100, tot loss = 5.141071200370789, l1: 0.0001171011814828186, l2: 0.00039700593915767966   Iteration 31 of 100, tot loss = 5.135157562071277, l1: 0.00011615946348155699, l2: 0.00039735629275861766   Iteration 32 of 100, tot loss = 5.1542064025998116, l1: 0.00011674425286400947, l2: 0.0003986763886132394   Iteration 33 of 100, tot loss = 5.079913905172637, l1: 0.00011604029978942972, l2: 0.0003919510918697624   Iteration 34 of 100, tot loss = 5.109617107054767, l1: 0.00011654802395188359, l2: 0.00039441368768594284   Iteration 35 of 100, tot loss = 5.110537379128592, l1: 0.00011573243308313458, l2: 0.0003953213055085923   Iteration 36 of 100, tot loss = 5.0973984532886085, l1: 0.00011538457511051092, l2: 0.0003943552704489169   Iteration 37 of 100, tot loss = 5.089068541655669, l1: 0.00011555092744340466, l2: 0.00039335592678194314   Iteration 38 of 100, tot loss = 5.068710414986861, l1: 0.00011524296630676345, l2: 0.00039162807469868933   Iteration 39 of 100, tot loss = 5.075838639185979, l1: 0.00011473017571589503, l2: 0.0003928536876068952   Iteration 40 of 100, tot loss = 5.039727216958999, l1: 0.00011400565654184902, l2: 0.0003899670649843756   Iteration 41 of 100, tot loss = 5.0317365309087245, l1: 0.00011386729353788967, l2: 0.00038930635953253906   Iteration 42 of 100, tot loss = 5.022986917268662, l1: 0.00011382115692304935, l2: 0.0003884775346488736   Iteration 43 of 100, tot loss = 5.025194960971211, l1: 0.00011393909500289145, l2: 0.0003885804021698531   Iteration 44 of 100, tot loss = 4.997945037755099, l1: 0.00011357712058022365, l2: 0.0003862173841546544   Iteration 45 of 100, tot loss = 4.9872220993042, l1: 0.00011323739696914951, l2: 0.0003854848140488482   Iteration 46 of 100, tot loss = 4.957904349202695, l1: 0.00011267333130767246, l2: 0.0003831171052297577   Iteration 47 of 100, tot loss = 4.934990086454026, l1: 0.00011219298764270671, l2: 0.00038130602247002436   Iteration 48 of 100, tot loss = 4.915370419621468, l1: 0.00011165274114925221, l2: 0.0003798843020679972   Iteration 49 of 100, tot loss = 4.9918996217299485, l1: 0.00011296259617665783, l2: 0.0003862273683343843   Iteration 50 of 100, tot loss = 4.967449140548706, l1: 0.00011293490664684214, l2: 0.00038381000980734824   Iteration 51 of 100, tot loss = 4.992377187691483, l1: 0.00011358004050242587, l2: 0.0003856576815950593   Iteration 52 of 100, tot loss = 4.953958731431228, l1: 0.00011270043978583999, l2: 0.000382695436341097   Iteration 53 of 100, tot loss = 4.998175881943613, l1: 0.00011307184797500804, l2: 0.0003867457433546595   Iteration 54 of 100, tot loss = 5.011868406225134, l1: 0.00011320471350228656, l2: 0.0003879821303632559   Iteration 55 of 100, tot loss = 5.003629979220304, l1: 0.00011247112111463635, l2: 0.00038789187965448946   Iteration 56 of 100, tot loss = 4.962099879980087, l1: 0.00011153221255101795, l2: 0.00038467777812911663   Iteration 57 of 100, tot loss = 4.9652733259033734, l1: 0.00011175997497048229, l2: 0.0003847673603255923   Iteration 58 of 100, tot loss = 4.94428341553129, l1: 0.00011138070790209519, l2: 0.00038304763614272315   Iteration 59 of 100, tot loss = 4.987586251759933, l1: 0.00011183273602647991, l2: 0.0003869258919513737   Iteration 60 of 100, tot loss = 4.964040994644165, l1: 0.00011176164819820163, l2: 0.0003846424542037615   Iteration 61 of 100, tot loss = 4.935766403792335, l1: 0.00011111399302923228, l2: 0.000382462650256949   Iteration 62 of 100, tot loss = 4.95095048796746, l1: 0.00011158283567994894, l2: 0.0003835122157808303   Iteration 63 of 100, tot loss = 4.959403215892731, l1: 0.00011172690770647947, l2: 0.0003842134164953013   Iteration 64 of 100, tot loss = 4.938377220183611, l1: 0.00011159763175783155, l2: 0.0003822400924491376   Iteration 65 of 100, tot loss = 4.959251796282254, l1: 0.0001118526993265662, l2: 0.0003840724823441213   Iteration 66 of 100, tot loss = 4.960176457058299, l1: 0.0001120566345035155, l2: 0.0003839610124652443   Iteration 67 of 100, tot loss = 4.973619827583654, l1: 0.00011225296301704678, l2: 0.0003851090209061669   Iteration 68 of 100, tot loss = 4.982853395097396, l1: 0.0001122591372276314, l2: 0.0003860262032129172   Iteration 69 of 100, tot loss = 4.99153400849605, l1: 0.00011218649761639941, l2: 0.0003869669042456814   Iteration 70 of 100, tot loss = 4.9894586256572175, l1: 0.00011199608343304135, l2: 0.0003869497801393404   Iteration 71 of 100, tot loss = 4.999082642541805, l1: 0.00011153123136544206, l2: 0.00038837703394959354   Iteration 72 of 100, tot loss = 5.010812630256017, l1: 0.00011153049227788061, l2: 0.00038955077151412197   Iteration 73 of 100, tot loss = 4.992568806426166, l1: 0.00011116715994494494, l2: 0.0003880897215136952   Iteration 74 of 100, tot loss = 4.98509582957706, l1: 0.00011115322342595539, l2: 0.0003873563607182429   Iteration 75 of 100, tot loss = 4.9487118085225426, l1: 0.00011053524280820663, l2: 0.0003843359393067658   Iteration 76 of 100, tot loss = 4.937300864018892, l1: 0.00011003191033715236, l2: 0.00038369817699085136   Iteration 77 of 100, tot loss = 4.947137913146577, l1: 0.00011049682507291436, l2: 0.0003842169667636468   Iteration 78 of 100, tot loss = 4.92180763452481, l1: 0.00010999697154217686, l2: 0.00038218379240089026   Iteration 79 of 100, tot loss = 4.901040544992761, l1: 0.00010987058984811264, l2: 0.00038023346492024494   Iteration 80 of 100, tot loss = 4.892967417836189, l1: 0.00010947117107207305, l2: 0.00037982557096256643   Iteration 81 of 100, tot loss = 4.907904798601881, l1: 0.000109527682160447, l2: 0.00038126279742968624   Iteration 82 of 100, tot loss = 4.881019763830231, l1: 0.0001091821522561254, l2: 0.00037891982409325086   Iteration 83 of 100, tot loss = 4.878730386136526, l1: 0.00010918125935201813, l2: 0.00037869177909472576   Iteration 84 of 100, tot loss = 4.859882456915719, l1: 0.00010878106037298927, l2: 0.0003772071852706306   Iteration 85 of 100, tot loss = 4.875255747402416, l1: 0.0001091341205584981, l2: 0.0003783914543044589   Iteration 86 of 100, tot loss = 4.889529344647429, l1: 0.00010923846334768567, l2: 0.00037971447133269636   Iteration 87 of 100, tot loss = 4.865693892555675, l1: 0.00010889315718099817, l2: 0.0003776762324409431   Iteration 88 of 100, tot loss = 4.88409598307176, l1: 0.00010945371220507447, l2: 0.00037895588667941576   Iteration 89 of 100, tot loss = 4.876050295454733, l1: 0.00010913410721074664, l2: 0.0003784709229733104   Iteration 90 of 100, tot loss = 4.875037203894721, l1: 0.00010909770450477178, l2: 0.00037840601693864707   Iteration 91 of 100, tot loss = 4.876645994710398, l1: 0.00010923274257348256, l2: 0.00037843185770555635   Iteration 92 of 100, tot loss = 4.8605106861694996, l1: 0.00010878954059950522, l2: 0.00037726152890093584   Iteration 93 of 100, tot loss = 4.839368130571099, l1: 0.00010871745873510497, l2: 0.00037521935538739284   Iteration 94 of 100, tot loss = 4.84840445569221, l1: 0.00010881648569843396, l2: 0.00037602396084966656   Iteration 95 of 100, tot loss = 4.830849115472091, l1: 0.00010848194835585003, l2: 0.0003746029642091966   Iteration 96 of 100, tot loss = 4.823756302396457, l1: 0.00010829982572128453, l2: 0.0003740758056665072   Iteration 97 of 100, tot loss = 4.802936337657811, l1: 0.00010798291870892125, l2: 0.0003723107164632049   Iteration 98 of 100, tot loss = 4.781301702771868, l1: 0.00010760223523568723, l2: 0.00037052793658756634   Iteration 99 of 100, tot loss = 4.773446497291025, l1: 0.00010773864286043916, l2: 0.00036960600838864766   Iteration 100 of 100, tot loss = 4.782029671669006, l1: 0.00010800578049384057, l2: 0.0003701971884584054
   End of epoch 1333; saving model... 

Epoch 1334 of 2000
   Iteration 1 of 100, tot loss = 5.471174240112305, l1: 0.00012336327927187085, l2: 0.00042375418706797063   Iteration 2 of 100, tot loss = 6.223925590515137, l1: 0.00012965551286470145, l2: 0.0004927370609948412   Iteration 3 of 100, tot loss = 6.008315086364746, l1: 0.00012139310274505988, l2: 0.0004794384294655174   Iteration 4 of 100, tot loss = 5.9049729108810425, l1: 0.00012383408284222241, l2: 0.000466663230326958   Iteration 5 of 100, tot loss = 5.281289768218994, l1: 0.00011396016489015893, l2: 0.0004141688346862793   Iteration 6 of 100, tot loss = 5.651212374369304, l1: 0.00011831736264866777, l2: 0.0004468038969207555   Iteration 7 of 100, tot loss = 5.436137608119419, l1: 0.00011503094719955698, l2: 0.0004285828326828778   Iteration 8 of 100, tot loss = 5.409152507781982, l1: 0.0001121682707889704, l2: 0.0004287469964765478   Iteration 9 of 100, tot loss = 5.470163769192165, l1: 0.00011153507188686894, l2: 0.00043548131684979633   Iteration 10 of 100, tot loss = 5.424709987640381, l1: 0.00011379840143490582, l2: 0.00042867260926868764   Iteration 11 of 100, tot loss = 5.65489690954035, l1: 0.00011497126251924783, l2: 0.00045051843996837056   Iteration 12 of 100, tot loss = 5.478538393974304, l1: 0.00010937059929953345, l2: 0.00043848325003637   Iteration 13 of 100, tot loss = 5.320766082176795, l1: 0.00010736920292122074, l2: 0.00042470741587189527   Iteration 14 of 100, tot loss = 5.136100190026419, l1: 0.00010333540558349341, l2: 0.00041027462326123247   Iteration 15 of 100, tot loss = 5.286046028137207, l1: 0.00010684080965196093, l2: 0.00042176380520686507   Iteration 16 of 100, tot loss = 5.348658382892609, l1: 0.00010670061465134495, l2: 0.00042816523273359053   Iteration 17 of 100, tot loss = 5.268421818228329, l1: 0.00010585484971456667, l2: 0.0004209873406901298   Iteration 18 of 100, tot loss = 5.168685171339247, l1: 0.00010413688788604405, l2: 0.00041273163676831045   Iteration 19 of 100, tot loss = 5.205768233851383, l1: 0.00010470770365301225, l2: 0.0004158691291721832   Iteration 20 of 100, tot loss = 5.218372964859009, l1: 0.00010606155374262016, l2: 0.00041577575175324457   Iteration 21 of 100, tot loss = 5.167054630461193, l1: 0.00010580473012634597, l2: 0.0004109007422812283   Iteration 22 of 100, tot loss = 5.067085547880693, l1: 0.00010454746502959593, l2: 0.000402161098794419   Iteration 23 of 100, tot loss = 5.170390004697054, l1: 0.00010690553260602705, l2: 0.0004101334754681296   Iteration 24 of 100, tot loss = 5.105965693791707, l1: 0.00010690803416461374, l2: 0.0004036885426709584   Iteration 25 of 100, tot loss = 5.082150154113769, l1: 0.00010661292384611442, l2: 0.00040160209871828553   Iteration 26 of 100, tot loss = 5.052297518803523, l1: 0.00010538407262244548, l2: 0.00039984568586358085   Iteration 27 of 100, tot loss = 5.032911088731554, l1: 0.0001050231763252264, l2: 0.0003982679390227767   Iteration 28 of 100, tot loss = 5.116456849234445, l1: 0.00010591866547266753, l2: 0.0004057270260610884   Iteration 29 of 100, tot loss = 5.0958739313586, l1: 0.00010521721535418504, l2: 0.00040437018394405983   Iteration 30 of 100, tot loss = 5.083758354187012, l1: 0.00010471764398971573, l2: 0.00040365819780466456   Iteration 31 of 100, tot loss = 5.056038841124503, l1: 0.0001038746721009844, l2: 0.00040172921839140113   Iteration 32 of 100, tot loss = 4.974065348505974, l1: 0.00010267587867929251, l2: 0.0003947306622649194   Iteration 33 of 100, tot loss = 4.998927477634314, l1: 0.00010365011853123592, l2: 0.0003962426359948674   Iteration 34 of 100, tot loss = 4.953458568629096, l1: 0.00010312205267837271, l2: 0.00039222381105098655   Iteration 35 of 100, tot loss = 4.9098158427647185, l1: 0.00010266893036064825, l2: 0.0003883126607563879   Iteration 36 of 100, tot loss = 4.8722455104192095, l1: 0.00010248095006520291, l2: 0.0003847436075577409   Iteration 37 of 100, tot loss = 4.919659408363136, l1: 0.00010329486139124965, l2: 0.00038867108633968274   Iteration 38 of 100, tot loss = 4.899718560670552, l1: 0.00010325785842724144, l2: 0.00038671400474605004   Iteration 39 of 100, tot loss = 4.817824320915418, l1: 0.00010149360064249963, l2: 0.000380288838268592   Iteration 40 of 100, tot loss = 4.751391035318375, l1: 0.00010061508419312304, l2: 0.000374524025755818   Iteration 41 of 100, tot loss = 4.751723644210071, l1: 0.00010101679338443252, l2: 0.00037415557726933735   Iteration 42 of 100, tot loss = 4.733550622349694, l1: 0.00010023532238500636, l2: 0.0003731197457888075   Iteration 43 of 100, tot loss = 4.6977898575538815, l1: 9.95581911749553e-05, l2: 0.00037022080065325164   Iteration 44 of 100, tot loss = 4.704582496122881, l1: 0.00010016222668954552, l2: 0.0003702960295248142   Iteration 45 of 100, tot loss = 4.661829365624322, l1: 9.916558135753601e-05, l2: 0.0003670173619563381   Iteration 46 of 100, tot loss = 4.650477585585221, l1: 9.920992883765038e-05, l2: 0.0003658378358839242   Iteration 47 of 100, tot loss = 4.64756559818349, l1: 9.920357401727976e-05, l2: 0.0003655529915394777   Iteration 48 of 100, tot loss = 4.676159153381984, l1: 9.956907759563667e-05, l2: 0.000368046842898669   Iteration 49 of 100, tot loss = 4.700848511287144, l1: 0.00010034692886805789, l2: 0.00036973792736475565   Iteration 50 of 100, tot loss = 4.758127298355102, l1: 0.00010108013309945819, l2: 0.0003747326019220054   Iteration 51 of 100, tot loss = 4.807921091715495, l1: 0.00010225480396983068, l2: 0.00037853731020518085   Iteration 52 of 100, tot loss = 4.801260471343994, l1: 0.0001026657898994297, l2: 0.0003774602625456352   Iteration 53 of 100, tot loss = 4.801692494806254, l1: 0.00010230360992795366, l2: 0.0003778656443826995   Iteration 54 of 100, tot loss = 4.808234020515725, l1: 0.00010256357691778922, l2: 0.00037825983060799816   Iteration 55 of 100, tot loss = 4.821072925220836, l1: 0.00010268225801568902, l2: 0.0003794250401287255   Iteration 56 of 100, tot loss = 4.852849338735853, l1: 0.000103185814558466, l2: 0.00038209912444082353   Iteration 57 of 100, tot loss = 4.858267851043165, l1: 0.00010347637030543155, l2: 0.0003823504197051781   Iteration 58 of 100, tot loss = 4.878318400218569, l1: 0.00010364850678687899, l2: 0.00038418333760687504   Iteration 59 of 100, tot loss = 4.862685975381884, l1: 0.00010326280589343712, l2: 0.00038300579615032016   Iteration 60 of 100, tot loss = 4.849223450819651, l1: 0.0001035392234067937, l2: 0.00038138312617472063   Iteration 61 of 100, tot loss = 4.826110996183802, l1: 0.00010324971454330628, l2: 0.00037936138973922515   Iteration 62 of 100, tot loss = 4.825850640573809, l1: 0.00010344216013524396, l2: 0.00037914290864606416   Iteration 63 of 100, tot loss = 4.823048319135394, l1: 0.00010349081189598932, l2: 0.00037881402466963563   Iteration 64 of 100, tot loss = 4.844165802001953, l1: 0.00010391601512083071, l2: 0.0003805005703725328   Iteration 65 of 100, tot loss = 4.8218508390279915, l1: 0.00010314167951806807, l2: 0.00037904340961876395   Iteration 66 of 100, tot loss = 4.781435233173949, l1: 0.00010238743925583549, l2: 0.00037575608946622884   Iteration 67 of 100, tot loss = 4.782534538809933, l1: 0.00010190084509276993, l2: 0.0003763526142053584   Iteration 68 of 100, tot loss = 4.7839420437812805, l1: 0.00010183172419013263, l2: 0.00037656248519744943   Iteration 69 of 100, tot loss = 4.807232777277629, l1: 0.00010237420044005916, l2: 0.000378349082261   Iteration 70 of 100, tot loss = 4.7740852151598245, l1: 0.00010171938946379149, l2: 0.0003756891370617918   Iteration 71 of 100, tot loss = 4.738027377867363, l1: 0.00010115420942819468, l2: 0.00037264853324839504   Iteration 72 of 100, tot loss = 4.73881956603792, l1: 0.00010147168081352397, l2: 0.0003724102803567399   Iteration 73 of 100, tot loss = 4.736411003217305, l1: 0.00010123993432562326, l2: 0.00037240117053941453   Iteration 74 of 100, tot loss = 4.730201991828713, l1: 0.00010130639059974094, l2: 0.0003717138132668805   Iteration 75 of 100, tot loss = 4.731168562571208, l1: 0.00010125175957606795, l2: 0.0003718651010422036   Iteration 76 of 100, tot loss = 4.776510872338948, l1: 0.00010224711805724502, l2: 0.00037540397380852433   Iteration 77 of 100, tot loss = 4.799764373085716, l1: 0.00010295327044698386, l2: 0.00037702317096371593   Iteration 78 of 100, tot loss = 4.770042511133047, l1: 0.00010262584444904067, l2: 0.0003743784109246917   Iteration 79 of 100, tot loss = 4.750372225725198, l1: 0.00010254026742989515, l2: 0.00037249695945124414   Iteration 80 of 100, tot loss = 4.779784360527993, l1: 0.00010329532929063135, l2: 0.0003746831103853765   Iteration 81 of 100, tot loss = 4.811043394936456, l1: 0.00010413903909926533, l2: 0.000376965304266052   Iteration 82 of 100, tot loss = 4.781760451270313, l1: 0.00010373690042242965, l2: 0.00037443914866676874   Iteration 83 of 100, tot loss = 4.768441694328584, l1: 0.00010360727660499614, l2: 0.00037323689671704566   Iteration 84 of 100, tot loss = 4.77196821144649, l1: 0.00010364957229501757, l2: 0.0003735472526473348   Iteration 85 of 100, tot loss = 4.777018120709587, l1: 0.00010358137784779126, l2: 0.0003741204379099038   Iteration 86 of 100, tot loss = 4.795678720917812, l1: 0.00010404772576801534, l2: 0.00037552014996330144   Iteration 87 of 100, tot loss = 4.766903364795378, l1: 0.00010347401679944429, l2: 0.0003732163231584361   Iteration 88 of 100, tot loss = 4.749959818341515, l1: 0.0001032285627347433, l2: 0.00037176742262090556   Iteration 89 of 100, tot loss = 4.757752453343252, l1: 0.00010337247147801688, l2: 0.0003724027771026608   Iteration 90 of 100, tot loss = 4.750323377715217, l1: 0.00010350925751683665, l2: 0.0003715230834334054   Iteration 91 of 100, tot loss = 4.742606710601639, l1: 0.00010359922145233632, l2: 0.0003706614527170761   Iteration 92 of 100, tot loss = 4.740481285945229, l1: 0.0001033078843924475, l2: 0.00037074024737154576   Iteration 93 of 100, tot loss = 4.739047647804342, l1: 0.00010316366470144731, l2: 0.0003707411033051309   Iteration 94 of 100, tot loss = 4.734372349495583, l1: 0.00010308967068069367, l2: 0.0003703475674054526   Iteration 95 of 100, tot loss = 4.730112520017122, l1: 0.00010302909117614802, l2: 0.0003699821640590304   Iteration 96 of 100, tot loss = 4.722179529567559, l1: 0.000102951882013258, l2: 0.00036926607390341815   Iteration 97 of 100, tot loss = 4.740849625204027, l1: 0.0001035335889045922, l2: 0.0003705513765603381   Iteration 98 of 100, tot loss = 4.742961399409236, l1: 0.0001034506899022559, l2: 0.0003708454531413142   Iteration 99 of 100, tot loss = 4.7341135824569545, l1: 0.00010302545247167922, l2: 0.0003703859087310242   Iteration 100 of 100, tot loss = 4.743046727180481, l1: 0.00010323572732886532, l2: 0.0003710689482977614
   End of epoch 1334; saving model... 

Epoch 1335 of 2000
   Iteration 1 of 100, tot loss = 6.855167388916016, l1: 0.00016185487038455904, l2: 0.0005236618453636765   Iteration 2 of 100, tot loss = 8.12506103515625, l1: 0.00016519037308171391, l2: 0.0006473157554864883   Iteration 3 of 100, tot loss = 7.431621233622233, l1: 0.00015283656830433756, l2: 0.0005903255757099638   Iteration 4 of 100, tot loss = 6.2440502643585205, l1: 0.00013287258298078086, l2: 0.0004915324570902158   Iteration 5 of 100, tot loss = 6.320272159576416, l1: 0.00013571945164585485, l2: 0.0004963077750289813   Iteration 6 of 100, tot loss = 5.798702716827393, l1: 0.00012867996823236658, l2: 0.0004511903098318726   Iteration 7 of 100, tot loss = 5.52771384375436, l1: 0.00011959644949196704, l2: 0.00043317493899459284   Iteration 8 of 100, tot loss = 5.253559172153473, l1: 0.00011416679535614094, l2: 0.00041118912486126646   Iteration 9 of 100, tot loss = 5.083264138963488, l1: 0.0001108862003699566, l2: 0.0003974402142274711   Iteration 10 of 100, tot loss = 5.237289810180664, l1: 0.00011170860088896006, l2: 0.00041202037828043104   Iteration 11 of 100, tot loss = 5.463270274075595, l1: 0.00011703754055567764, l2: 0.00042928948956118387   Iteration 12 of 100, tot loss = 5.320966402689616, l1: 0.0001161418355574521, l2: 0.0004159548067642997   Iteration 13 of 100, tot loss = 5.301891180185171, l1: 0.00011598116263765126, l2: 0.0004142079582939354   Iteration 14 of 100, tot loss = 5.233831882476807, l1: 0.00011404383102282216, l2: 0.0004093393591964351   Iteration 15 of 100, tot loss = 5.11839272181193, l1: 0.00011104984684304023, l2: 0.0004007894254755229   Iteration 16 of 100, tot loss = 5.094326615333557, l1: 0.0001092667721422913, l2: 0.00040016588900471106   Iteration 17 of 100, tot loss = 4.971442783580107, l1: 0.00010738717024009126, l2: 0.00038975710734067597   Iteration 18 of 100, tot loss = 4.811154948340522, l1: 0.00010412430634055959, l2: 0.00037699118790139136   Iteration 19 of 100, tot loss = 4.768480526773553, l1: 0.00010418019385389543, l2: 0.00037266785741514087   Iteration 20 of 100, tot loss = 4.652802562713623, l1: 0.00010219117339147488, l2: 0.0003630890816566534   Iteration 21 of 100, tot loss = 4.6827717281523205, l1: 0.00010260285091048682, l2: 0.00036567431968814203   Iteration 22 of 100, tot loss = 4.6176606525074355, l1: 0.00010212352687879253, l2: 0.0003596425356756134   Iteration 23 of 100, tot loss = 4.561062574386597, l1: 0.0001013404703964783, l2: 0.00035476578341331333   Iteration 24 of 100, tot loss = 4.67555500070254, l1: 0.00010293917163532267, l2: 0.00036461632468369015   Iteration 25 of 100, tot loss = 4.589646377563477, l1: 0.00010203224388533272, l2: 0.00035693239071406426   Iteration 26 of 100, tot loss = 4.7382456706120415, l1: 0.00010403886489467158, l2: 0.0003697856994292054   Iteration 27 of 100, tot loss = 4.710612155773021, l1: 0.00010321382311910081, l2: 0.00036784738983476053   Iteration 28 of 100, tot loss = 4.767468299184527, l1: 0.00010405198801371236, l2: 0.0003726948385259935   Iteration 29 of 100, tot loss = 4.739324668358112, l1: 0.00010409105053785707, l2: 0.000369841413540316   Iteration 30 of 100, tot loss = 4.745887200037639, l1: 0.00010432559368685664, l2: 0.0003702631229922796   Iteration 31 of 100, tot loss = 4.69002535266261, l1: 0.00010273528330857985, l2: 0.00036626724871776755   Iteration 32 of 100, tot loss = 4.7004623636603355, l1: 0.0001027846504939589, l2: 0.0003672615821415093   Iteration 33 of 100, tot loss = 4.645387468915997, l1: 0.00010222775932616611, l2: 0.0003623109836498219   Iteration 34 of 100, tot loss = 4.621214340714848, l1: 0.00010122718776983675, l2: 0.00036089424290420375   Iteration 35 of 100, tot loss = 4.572616713387625, l1: 0.00010023875542434066, l2: 0.00035702291227478   Iteration 36 of 100, tot loss = 4.5814544094933405, l1: 0.0001007613182082423, l2: 0.0003573841192216302   Iteration 37 of 100, tot loss = 4.642167168694574, l1: 0.00010179915723809343, l2: 0.0003624175568237095   Iteration 38 of 100, tot loss = 4.686053589770668, l1: 0.000102392904936048, l2: 0.00036621245089918375   Iteration 39 of 100, tot loss = 4.709308587587797, l1: 0.00010333585156238255, l2: 0.0003675950037387128   Iteration 40 of 100, tot loss = 4.653215593099594, l1: 0.0001024502188556653, l2: 0.0003628713373473147   Iteration 41 of 100, tot loss = 4.656736240154359, l1: 0.00010305090827609004, l2: 0.00036262271240092357   Iteration 42 of 100, tot loss = 4.633885474432082, l1: 0.00010270428962636895, l2: 0.00036068425418177085   Iteration 43 of 100, tot loss = 4.597957666530165, l1: 0.00010239031884296755, l2: 0.0003574054439817447   Iteration 44 of 100, tot loss = 4.564977049827576, l1: 0.00010135048341908259, l2: 0.0003551472178845539   Iteration 45 of 100, tot loss = 4.527803749508328, l1: 0.00010076310047427088, l2: 0.0003520172710220019   Iteration 46 of 100, tot loss = 4.601417738458385, l1: 0.00010175438372876353, l2: 0.0003583873875196213   Iteration 47 of 100, tot loss = 4.594052253885472, l1: 0.00010136906604067065, l2: 0.0003580361576205952   Iteration 48 of 100, tot loss = 4.6150027712186175, l1: 0.00010169054371544917, l2: 0.00035980973128365196   Iteration 49 of 100, tot loss = 4.601806149190786, l1: 0.00010181931481513727, l2: 0.00035836129824687936   Iteration 50 of 100, tot loss = 4.598889670372009, l1: 0.00010138603429368231, l2: 0.00035850293061230333   Iteration 51 of 100, tot loss = 4.648027097477632, l1: 0.00010229419472036134, l2: 0.00036250851311100024   Iteration 52 of 100, tot loss = 4.614513461406414, l1: 0.00010147422346143195, l2: 0.00035997712117163104   Iteration 53 of 100, tot loss = 4.636473241842018, l1: 0.0001017023668677177, l2: 0.00036194495541561956   Iteration 54 of 100, tot loss = 4.655056008586177, l1: 0.00010222138964174385, l2: 0.0003632842091502863   Iteration 55 of 100, tot loss = 4.6616772998463025, l1: 0.00010183785026046363, l2: 0.0003643298767697574   Iteration 56 of 100, tot loss = 4.669122406414577, l1: 0.00010166258042383041, l2: 0.000365249656949475   Iteration 57 of 100, tot loss = 4.681613587496574, l1: 0.00010200757242273539, l2: 0.00036615378242762137   Iteration 58 of 100, tot loss = 4.67695193455137, l1: 0.0001018839633114363, l2: 0.0003658112262451925   Iteration 59 of 100, tot loss = 4.670228230751167, l1: 0.0001021771526735639, l2: 0.00036484566670092676   Iteration 60 of 100, tot loss = 4.684676154454549, l1: 0.00010257146871784547, l2: 0.0003658961439214181   Iteration 61 of 100, tot loss = 4.695047355089032, l1: 0.00010298895871469018, l2: 0.00036651577412143165   Iteration 62 of 100, tot loss = 4.688021106104697, l1: 0.00010293706920140633, l2: 0.00036586503870973   Iteration 63 of 100, tot loss = 4.689645063309443, l1: 0.00010327830802582736, l2: 0.00036568619566784786   Iteration 64 of 100, tot loss = 4.716722168028355, l1: 0.00010384393522144819, l2: 0.00036782827942261065   Iteration 65 of 100, tot loss = 4.701559851719783, l1: 0.00010314488603258863, l2: 0.00036701109694639366   Iteration 66 of 100, tot loss = 4.691805731166493, l1: 0.00010304456158006691, l2: 0.00036613600946800557   Iteration 67 of 100, tot loss = 4.680446315167555, l1: 0.00010263888603432535, l2: 0.0003654057437592228   Iteration 68 of 100, tot loss = 4.714247714070713, l1: 0.0001030788301196641, l2: 0.0003683459395046264   Iteration 69 of 100, tot loss = 4.707692674968554, l1: 0.00010289773685592766, l2: 0.0003678715284626958   Iteration 70 of 100, tot loss = 4.723641310419355, l1: 0.00010331784157772615, l2: 0.00036904628733671935   Iteration 71 of 100, tot loss = 4.714540565517587, l1: 0.00010302547341037187, l2: 0.00036842858113027115   Iteration 72 of 100, tot loss = 4.696099821064207, l1: 0.00010262138013988281, l2: 0.0003669886001363112   Iteration 73 of 100, tot loss = 4.705745014425826, l1: 0.00010273709237593073, l2: 0.0003678374072578289   Iteration 74 of 100, tot loss = 4.706147686855213, l1: 0.00010272279497577394, l2: 0.0003678919717974684   Iteration 75 of 100, tot loss = 4.678372538884481, l1: 0.00010225340001246271, l2: 0.0003655838520110895   Iteration 76 of 100, tot loss = 4.701671892090848, l1: 0.00010302119828434355, l2: 0.0003671459891943653   Iteration 77 of 100, tot loss = 4.696199686496289, l1: 0.00010299408923065011, l2: 0.00036662587799228633   Iteration 78 of 100, tot loss = 4.707390708801074, l1: 0.00010297009397711522, l2: 0.00036776897561652825   Iteration 79 of 100, tot loss = 4.686585142642637, l1: 0.0001027027229852886, l2: 0.0003659557901128325   Iteration 80 of 100, tot loss = 4.643661724030972, l1: 0.00010180903464060976, l2: 0.0003625571366683289   Iteration 81 of 100, tot loss = 4.64397715786357, l1: 0.0001018364720813999, l2: 0.00036256124269444333   Iteration 82 of 100, tot loss = 4.631616838094665, l1: 0.00010170657421950614, l2: 0.00036145510851539014   Iteration 83 of 100, tot loss = 4.608510142349335, l1: 0.00010129207359831668, l2: 0.00035955893944918056   Iteration 84 of 100, tot loss = 4.605527274665379, l1: 0.0001012485093572953, l2: 0.00035930421715117217   Iteration 85 of 100, tot loss = 4.657537010136773, l1: 0.00010177711425486075, l2: 0.00036397658550413325   Iteration 86 of 100, tot loss = 4.660104472969854, l1: 0.00010185218351855224, l2: 0.00036415826269698895   Iteration 87 of 100, tot loss = 4.655962668616196, l1: 0.00010211522046653233, l2: 0.00036348104545506704   Iteration 88 of 100, tot loss = 4.6555562981150365, l1: 0.00010203550713909367, l2: 0.00036352012177138187   Iteration 89 of 100, tot loss = 4.638948495468397, l1: 0.00010185688006458804, l2: 0.00036203796872960695   Iteration 90 of 100, tot loss = 4.62873471710417, l1: 0.0001019206323285794, l2: 0.00036095283859241236   Iteration 91 of 100, tot loss = 4.614397435397892, l1: 0.00010193960858217267, l2: 0.00035950013445651895   Iteration 92 of 100, tot loss = 4.629447645467261, l1: 0.00010219078752055319, l2: 0.0003607539760117146   Iteration 93 of 100, tot loss = 4.620736095213121, l1: 0.00010235145436449637, l2: 0.0003597221540829157   Iteration 94 of 100, tot loss = 4.6368718236050706, l1: 0.0001026673697274515, l2: 0.0003610198111849233   Iteration 95 of 100, tot loss = 4.643120829682601, l1: 0.00010265303319197541, l2: 0.000361659048863466   Iteration 96 of 100, tot loss = 4.636062399794658, l1: 0.00010253797063342063, l2: 0.00036106826829988375   Iteration 97 of 100, tot loss = 4.630015089339817, l1: 0.00010247881984525389, l2: 0.0003605226878246014   Iteration 98 of 100, tot loss = 4.631408060083584, l1: 0.00010237060877796243, l2: 0.00036077019593699797   Iteration 99 of 100, tot loss = 4.61682502549104, l1: 0.00010198512175731887, l2: 0.0003596973794922844   Iteration 100 of 100, tot loss = 4.623470443487167, l1: 0.00010223963283351623, l2: 0.0003601074103062274
   End of epoch 1335; saving model... 

Epoch 1336 of 2000
   Iteration 1 of 100, tot loss = 4.448333740234375, l1: 0.0001139426021836698, l2: 0.00033089076168835163   Iteration 2 of 100, tot loss = 4.712846279144287, l1: 0.00011605152030824684, l2: 0.0003552330890670419   Iteration 3 of 100, tot loss = 3.959512710571289, l1: 0.00010308973772528891, l2: 0.0002928615188769375   Iteration 4 of 100, tot loss = 5.023392915725708, l1: 0.0001133172645495506, l2: 0.0003890220068569761   Iteration 5 of 100, tot loss = 5.449442768096924, l1: 0.00012121367763029411, l2: 0.0004237305925926194   Iteration 6 of 100, tot loss = 5.237712542215983, l1: 0.00012009678175672889, l2: 0.0004036744624803153   Iteration 7 of 100, tot loss = 5.78821781703404, l1: 0.00012893138461679752, l2: 0.00044989038516567755   Iteration 8 of 100, tot loss = 5.909642934799194, l1: 0.00013013008901907597, l2: 0.00046083419692877214   Iteration 9 of 100, tot loss = 5.690327220492893, l1: 0.0001281596026577366, l2: 0.0004408731134996439   Iteration 10 of 100, tot loss = 5.469154644012451, l1: 0.0001244082515768241, l2: 0.000422507208713796   Iteration 11 of 100, tot loss = 5.469416531649503, l1: 0.00012530533885265785, l2: 0.0004216363105859438   Iteration 12 of 100, tot loss = 5.739458163579305, l1: 0.00012818606834722837, l2: 0.0004457597448587573   Iteration 13 of 100, tot loss = 5.5775768390068645, l1: 0.00012428650818317413, l2: 0.00043347117137342977   Iteration 14 of 100, tot loss = 5.588829466274807, l1: 0.00012560712795155787, l2: 0.00043327581598922346   Iteration 15 of 100, tot loss = 5.714209508895874, l1: 0.00012802412335683282, l2: 0.0004433968249941245   Iteration 16 of 100, tot loss = 5.570066750049591, l1: 0.0001251746493835526, l2: 0.0004318320216043503   Iteration 17 of 100, tot loss = 5.491253179662368, l1: 0.00012384016650578226, l2: 0.0004252851487014114   Iteration 18 of 100, tot loss = 5.543849574195014, l1: 0.00012456382319214754, l2: 0.0004298211319514343   Iteration 19 of 100, tot loss = 5.667808884068539, l1: 0.0001263185454023953, l2: 0.00044046234152598406   Iteration 20 of 100, tot loss = 5.546222901344299, l1: 0.00012413718068273737, l2: 0.00043048510851804165   Iteration 21 of 100, tot loss = 5.494699160257976, l1: 0.00012305137733208192, l2: 0.0004264185374181363   Iteration 22 of 100, tot loss = 5.422401243990118, l1: 0.00012193776935667053, l2: 0.0004203023537146774   Iteration 23 of 100, tot loss = 5.339545592017796, l1: 0.00012055442335701588, l2: 0.00041340013495241493   Iteration 24 of 100, tot loss = 5.342825164397557, l1: 0.00012093115492461948, l2: 0.00041335136120324023   Iteration 25 of 100, tot loss = 5.312950963973999, l1: 0.00012105229805456475, l2: 0.00041024279897101224   Iteration 26 of 100, tot loss = 5.254465488287119, l1: 0.0001190068396681454, l2: 0.00040643971050695446   Iteration 27 of 100, tot loss = 5.147256250734682, l1: 0.0001167616843573611, l2: 0.00039796394142925875   Iteration 28 of 100, tot loss = 5.141608119010925, l1: 0.00011591223327351534, l2: 0.00039824857906621346   Iteration 29 of 100, tot loss = 5.03138253606599, l1: 0.00011405821388852301, l2: 0.00038908004002035436   Iteration 30 of 100, tot loss = 5.06726807753245, l1: 0.00011373142115189694, l2: 0.0003929953857247407   Iteration 31 of 100, tot loss = 5.0362354017073105, l1: 0.0001130300975825277, l2: 0.00039059344224721916   Iteration 32 of 100, tot loss = 5.066960997879505, l1: 0.00011354835442034528, l2: 0.0003931477440346498   Iteration 33 of 100, tot loss = 5.014565670129024, l1: 0.00011241791093418601, l2: 0.00038903865479920626   Iteration 34 of 100, tot loss = 4.936536305090961, l1: 0.00011100175063748953, l2: 0.0003826518783599193   Iteration 35 of 100, tot loss = 4.884184203829084, l1: 0.0001099411302545507, l2: 0.0003784772890087749   Iteration 36 of 100, tot loss = 4.852826409869724, l1: 0.00010955772348905966, l2: 0.00037572491661800694   Iteration 37 of 100, tot loss = 4.865104597968024, l1: 0.00010913245264296042, l2: 0.0003773780072397376   Iteration 38 of 100, tot loss = 4.825889587402344, l1: 0.00010907736213994212, l2: 0.00037351159660733844   Iteration 39 of 100, tot loss = 4.759236023976253, l1: 0.00010727367901917882, l2: 0.0003686499235650095   Iteration 40 of 100, tot loss = 4.778256994485855, l1: 0.00010723357445385773, l2: 0.00037059212481835855   Iteration 41 of 100, tot loss = 4.834815182336947, l1: 0.00010849747248845765, l2: 0.00037498404584234443   Iteration 42 of 100, tot loss = 4.889278281302679, l1: 0.00010980363835447601, l2: 0.0003791241906583309   Iteration 43 of 100, tot loss = 4.910237351129221, l1: 0.00011065216439906074, l2: 0.00038037157203287404   Iteration 44 of 100, tot loss = 4.900703988292, l1: 0.00011028984772565309, l2: 0.0003797805521489036   Iteration 45 of 100, tot loss = 4.9168142053816055, l1: 0.00010976914377857207, l2: 0.00038191227811492154   Iteration 46 of 100, tot loss = 4.936949527781943, l1: 0.00010992511575434195, l2: 0.0003837698383469378   Iteration 47 of 100, tot loss = 4.876537678089548, l1: 0.00010854784170774981, l2: 0.0003791059273854889   Iteration 48 of 100, tot loss = 4.874441891908646, l1: 0.00010901903033300187, l2: 0.00037842515939701116   Iteration 49 of 100, tot loss = 4.865162265544035, l1: 0.00010824367690270728, l2: 0.00037827255051079377   Iteration 50 of 100, tot loss = 4.84497284412384, l1: 0.00010797126640682108, l2: 0.00037652601866284386   Iteration 51 of 100, tot loss = 4.8318802936404355, l1: 0.00010770661288452353, l2: 0.00037548141742316894   Iteration 52 of 100, tot loss = 4.80292351429279, l1: 0.00010685180663690866, l2: 0.00037344054586495843   Iteration 53 of 100, tot loss = 4.792531526313637, l1: 0.00010659241703457533, l2: 0.00037266073703440787   Iteration 54 of 100, tot loss = 4.789302296108669, l1: 0.00010666323591808409, l2: 0.00037226699485607377   Iteration 55 of 100, tot loss = 4.743624752218073, l1: 0.0001058413046518002, l2: 0.00036852117178072643   Iteration 56 of 100, tot loss = 4.734137454203197, l1: 0.00010577974938184655, l2: 0.00036763399761444556   Iteration 57 of 100, tot loss = 4.7778785479696175, l1: 0.00010624365371002473, l2: 0.0003715442024322488   Iteration 58 of 100, tot loss = 4.76978706080338, l1: 0.0001059107852252682, l2: 0.0003710679224680242   Iteration 59 of 100, tot loss = 4.78780821218329, l1: 0.00010598510771622819, l2: 0.0003727957149040175   Iteration 60 of 100, tot loss = 4.774820856253306, l1: 0.00010567494318820536, l2: 0.00037180714401377675   Iteration 61 of 100, tot loss = 4.791598081588745, l1: 0.00010601226259480979, l2: 0.00037314754755228574   Iteration 62 of 100, tot loss = 4.7961363753964825, l1: 0.00010606806139431654, l2: 0.0003735455780594249   Iteration 63 of 100, tot loss = 4.810014713378179, l1: 0.0001063001751419275, l2: 0.00037470129840556416   Iteration 64 of 100, tot loss = 4.816400114446878, l1: 0.00010668111644918099, l2: 0.000374958897509714   Iteration 65 of 100, tot loss = 4.805992298859817, l1: 0.00010674220259086444, l2: 0.0003738570297942855   Iteration 66 of 100, tot loss = 4.840779033574191, l1: 0.00010695099737142411, l2: 0.0003771269087640879   Iteration 67 of 100, tot loss = 4.832379351800947, l1: 0.00010690992131634657, l2: 0.0003763280169075744   Iteration 68 of 100, tot loss = 4.851272165775299, l1: 0.00010731617486967211, l2: 0.00037781104487401214   Iteration 69 of 100, tot loss = 4.836346774861433, l1: 0.0001065038574080867, l2: 0.0003771308231238357   Iteration 70 of 100, tot loss = 4.857432321139744, l1: 0.00010666010758931017, l2: 0.000379083127856055   Iteration 71 of 100, tot loss = 4.876706207302255, l1: 0.00010684370054022252, l2: 0.0003808269231714649   Iteration 72 of 100, tot loss = 4.830703692303763, l1: 0.00010598098944885553, l2: 0.00037708938270952785   Iteration 73 of 100, tot loss = 4.825800108583006, l1: 0.00010582699666871396, l2: 0.00037675301716997835   Iteration 74 of 100, tot loss = 4.827045450339446, l1: 0.00010586355317689201, l2: 0.0003768409944707339   Iteration 75 of 100, tot loss = 4.844671309789022, l1: 0.0001060757763722601, l2: 0.00037839135708054526   Iteration 76 of 100, tot loss = 4.852872776357751, l1: 0.0001063922671547564, l2: 0.0003788950130240808   Iteration 77 of 100, tot loss = 4.861897223955625, l1: 0.00010639328240075257, l2: 0.00037979644274584005   Iteration 78 of 100, tot loss = 4.871958448336675, l1: 0.00010672425644140906, l2: 0.00038047159101458814   Iteration 79 of 100, tot loss = 4.87250563766383, l1: 0.00010700514965581625, l2: 0.0003802454169239861   Iteration 80 of 100, tot loss = 4.897351410984993, l1: 0.00010749157600002946, l2: 0.0003822435676738678   Iteration 81 of 100, tot loss = 4.943261408511503, l1: 0.00010820072085702976, l2: 0.00038612542257509137   Iteration 82 of 100, tot loss = 4.945750722071019, l1: 0.00010823289276009834, l2: 0.00038634218204917   Iteration 83 of 100, tot loss = 4.9050260825329515, l1: 0.00010746295806876081, l2: 0.0003830396527486169   Iteration 84 of 100, tot loss = 4.906343167736416, l1: 0.00010766601631725539, l2: 0.0003829683028882192   Iteration 85 of 100, tot loss = 4.893624328164494, l1: 0.00010721355947145425, l2: 0.00038214887574534204   Iteration 86 of 100, tot loss = 4.888411893401035, l1: 0.00010723267329266555, l2: 0.00038160851833053195   Iteration 87 of 100, tot loss = 4.896584313491295, l1: 0.00010736782065975405, l2: 0.0003822906128125501   Iteration 88 of 100, tot loss = 4.9162162433971055, l1: 0.00010749560764948414, l2: 0.0003841260189470962   Iteration 89 of 100, tot loss = 4.8987394825796065, l1: 0.00010719488154484608, l2: 0.0003826790692002167   Iteration 90 of 100, tot loss = 4.888434947861565, l1: 0.00010698897426158914, l2: 0.0003818545231802596   Iteration 91 of 100, tot loss = 4.88015486905863, l1: 0.00010667905943202121, l2: 0.0003813364303793635   Iteration 92 of 100, tot loss = 4.855309727399246, l1: 0.00010624100729688982, l2: 0.0003792899684492073   Iteration 93 of 100, tot loss = 4.838566139180173, l1: 0.00010602355827339574, l2: 0.00037783305852308427   Iteration 94 of 100, tot loss = 4.809629513862285, l1: 0.0001054350896664162, l2: 0.0003755278646960815   Iteration 95 of 100, tot loss = 4.793528747558594, l1: 0.00010496046578689282, l2: 0.0003743924117818671   Iteration 96 of 100, tot loss = 4.786633720000585, l1: 0.00010463286863190054, l2: 0.0003740305058575662   Iteration 97 of 100, tot loss = 4.782022471280442, l1: 0.00010474504874905178, l2: 0.00037345720080601185   Iteration 98 of 100, tot loss = 4.765896400626825, l1: 0.0001044370969701012, l2: 0.0003721525453682039   Iteration 99 of 100, tot loss = 4.785167566453568, l1: 0.000104662437984694, l2: 0.0003738543209784448   Iteration 100 of 100, tot loss = 4.770818274021149, l1: 0.0001041826102300547, l2: 0.00037289921951014546
   End of epoch 1336; saving model... 

Epoch 1337 of 2000
   Iteration 1 of 100, tot loss = 9.94606876373291, l1: 0.00019762800366152078, l2: 0.0007969788275659084   Iteration 2 of 100, tot loss = 7.2887139320373535, l1: 0.00014689204908790998, l2: 0.0005819793150294572   Iteration 3 of 100, tot loss = 7.183770497639974, l1: 0.00014952532122454917, l2: 0.0005688517121598125   Iteration 4 of 100, tot loss = 6.522432327270508, l1: 0.00012987332411285024, l2: 0.0005223698972258717   Iteration 5 of 100, tot loss = 6.351250076293946, l1: 0.0001273924979614094, l2: 0.0005077325040474534   Iteration 6 of 100, tot loss = 5.939102570215861, l1: 0.00012003284427919425, l2: 0.0004738774102103586   Iteration 7 of 100, tot loss = 5.863434178488595, l1: 0.00011843140652802373, l2: 0.0004679120029322803   Iteration 8 of 100, tot loss = 6.322423994541168, l1: 0.00012630218589038122, l2: 0.000505940210132394   Iteration 9 of 100, tot loss = 6.244557539621989, l1: 0.0001245987667870294, l2: 0.0004998569881233076   Iteration 10 of 100, tot loss = 6.1572648048400875, l1: 0.00012127811642130837, l2: 0.000494448360404931   Iteration 11 of 100, tot loss = 5.880930337038907, l1: 0.00011664857910628515, l2: 0.00047144445257303727   Iteration 12 of 100, tot loss = 5.609683354695638, l1: 0.00011293758325336967, l2: 0.0004480307507037651   Iteration 13 of 100, tot loss = 5.562784378345196, l1: 0.0001140080270348475, l2: 0.0004422704100411815   Iteration 14 of 100, tot loss = 5.344922116824558, l1: 0.00010922431361645327, l2: 0.0004252678970390532   Iteration 15 of 100, tot loss = 5.119506422678629, l1: 0.00010478471716245016, l2: 0.00040716592338867484   Iteration 16 of 100, tot loss = 5.152101159095764, l1: 0.00010589796784188366, l2: 0.00040931214425654616   Iteration 17 of 100, tot loss = 5.043607473373413, l1: 0.00010477598569944829, l2: 0.0003995847586980637   Iteration 18 of 100, tot loss = 4.973458117908901, l1: 0.00010465861366052802, l2: 0.00039268719491600577   Iteration 19 of 100, tot loss = 4.862174159602115, l1: 0.00010348163126955594, l2: 0.0003827357814818817   Iteration 20 of 100, tot loss = 4.875928616523742, l1: 0.00010301581824023742, l2: 0.00038457704067695886   Iteration 21 of 100, tot loss = 5.050027688344319, l1: 0.00010599319198046855, l2: 0.0003990095740716372   Iteration 22 of 100, tot loss = 5.022059418938377, l1: 0.00010547469851603223, l2: 0.0003967312394789505   Iteration 23 of 100, tot loss = 5.106069689211638, l1: 0.00010781466730607107, l2: 0.0004027922974352765   Iteration 24 of 100, tot loss = 5.092793842156728, l1: 0.000107665446800335, l2: 0.0004016139331118514   Iteration 25 of 100, tot loss = 5.106334686279297, l1: 0.00010830612271092832, l2: 0.00040232734056189657   Iteration 26 of 100, tot loss = 5.084089260834914, l1: 0.00010765045842093129, l2: 0.0004007584637013049   Iteration 27 of 100, tot loss = 5.045567954028094, l1: 0.0001069364001590724, l2: 0.0003976203917301501   Iteration 28 of 100, tot loss = 4.985149562358856, l1: 0.00010694113630701654, l2: 0.00039157381615950726   Iteration 29 of 100, tot loss = 4.977163635451218, l1: 0.00010767814167373781, l2: 0.0003900382179036287   Iteration 30 of 100, tot loss = 5.011643036206563, l1: 0.00010867499804589897, l2: 0.0003924893016422478   Iteration 31 of 100, tot loss = 5.062268126395441, l1: 0.00010992902248436886, l2: 0.00039629778785315613   Iteration 32 of 100, tot loss = 5.089128993451595, l1: 0.00011068184721807484, l2: 0.0003982310504397901   Iteration 33 of 100, tot loss = 5.06202837192651, l1: 0.00011052936249391429, l2: 0.0003956734731080801   Iteration 34 of 100, tot loss = 5.171267320128048, l1: 0.0001122853892037015, l2: 0.0004048413414720391   Iteration 35 of 100, tot loss = 5.115214756556919, l1: 0.00011161525222373062, l2: 0.00039990622192687754   Iteration 36 of 100, tot loss = 5.131910589006212, l1: 0.00011193091975453879, l2: 0.00040126013684332266   Iteration 37 of 100, tot loss = 5.072515088158685, l1: 0.00011045231312400083, l2: 0.00039679919309737916   Iteration 38 of 100, tot loss = 5.0811844624971085, l1: 0.00011055292289759229, l2: 0.0003975655218757885   Iteration 39 of 100, tot loss = 5.0452130146515675, l1: 0.0001096164866258056, l2: 0.0003949048136396763   Iteration 40 of 100, tot loss = 5.017153650522232, l1: 0.0001090607040168834, l2: 0.0003926546596630942   Iteration 41 of 100, tot loss = 4.973540102563253, l1: 0.00010834227169852522, l2: 0.0003890117371977284   Iteration 42 of 100, tot loss = 4.96821297350384, l1: 0.00010760330821670192, l2: 0.00038921798806681876   Iteration 43 of 100, tot loss = 4.961294490237568, l1: 0.00010714125646911778, l2: 0.0003889881921823808   Iteration 44 of 100, tot loss = 4.937543039972132, l1: 0.00010680801543375392, l2: 0.00038694628826554185   Iteration 45 of 100, tot loss = 4.912334145439996, l1: 0.00010648554743966087, l2: 0.0003847478669033282   Iteration 46 of 100, tot loss = 4.896098852157593, l1: 0.00010651247114013961, l2: 0.0003830974139576859   Iteration 47 of 100, tot loss = 4.8323409151523675, l1: 0.000105109218488809, l2: 0.0003781248728665424   Iteration 48 of 100, tot loss = 4.853800947467486, l1: 0.00010573221220511186, l2: 0.00037964788255825016   Iteration 49 of 100, tot loss = 4.840617758887155, l1: 0.00010593006040185824, l2: 0.00037813171559032433   Iteration 50 of 100, tot loss = 4.803485140800476, l1: 0.00010516613176150713, l2: 0.00037518238270422443   Iteration 51 of 100, tot loss = 4.809128130183501, l1: 0.0001055942412480648, l2: 0.00037531857236115004   Iteration 52 of 100, tot loss = 4.801813772091498, l1: 0.00010510648231642196, l2: 0.00037507489609057444   Iteration 53 of 100, tot loss = 4.797573948806187, l1: 0.00010513720732184461, l2: 0.0003746201886624133   Iteration 54 of 100, tot loss = 4.775787711143494, l1: 0.00010490472959645558, l2: 0.00037267404264572113   Iteration 55 of 100, tot loss = 4.749291250922463, l1: 0.00010483979719786228, l2: 0.0003700893290277401   Iteration 56 of 100, tot loss = 4.723430833646229, l1: 0.00010425284322731645, l2: 0.00036809024121404424   Iteration 57 of 100, tot loss = 4.729766247565286, l1: 0.00010437116910460438, l2: 0.00036860545642731157   Iteration 58 of 100, tot loss = 4.772714413445572, l1: 0.0001048332568612311, l2: 0.0003724381855010569   Iteration 59 of 100, tot loss = 4.827610011828148, l1: 0.00010568008545275099, l2: 0.0003770809160383671   Iteration 60 of 100, tot loss = 4.816570444901784, l1: 0.00010590679952050171, l2: 0.0003757502454391215   Iteration 61 of 100, tot loss = 4.776654434985802, l1: 0.00010529634981467694, l2: 0.0003723690940399242   Iteration 62 of 100, tot loss = 4.7717055543776485, l1: 0.00010544359089004913, l2: 0.0003717269651280085   Iteration 63 of 100, tot loss = 4.794428117691525, l1: 0.00010609925944692574, l2: 0.0003733435529746145   Iteration 64 of 100, tot loss = 4.8015045039355755, l1: 0.00010591077494837009, l2: 0.0003742396763755096   Iteration 65 of 100, tot loss = 4.780708881524893, l1: 0.00010559466200692651, l2: 0.000372476227214345   Iteration 66 of 100, tot loss = 4.782329259496747, l1: 0.00010558246553542135, l2: 0.0003726504609486173   Iteration 67 of 100, tot loss = 4.770654966582113, l1: 0.00010506208865936332, l2: 0.0003720034083695526   Iteration 68 of 100, tot loss = 4.792134092134588, l1: 0.00010585521795292152, l2: 0.0003733581917699399   Iteration 69 of 100, tot loss = 4.797611951828003, l1: 0.00010624664081252126, l2: 0.00037351455450888074   Iteration 70 of 100, tot loss = 4.841695816176278, l1: 0.00010657196331261989, l2: 0.00037759761887303157   Iteration 71 of 100, tot loss = 4.855620508462611, l1: 0.00010648309588260149, l2: 0.0003790789553124956   Iteration 72 of 100, tot loss = 4.892482966184616, l1: 0.00010720179544579069, l2: 0.0003820465016259631   Iteration 73 of 100, tot loss = 4.875301547246437, l1: 0.000107075565995741, l2: 0.00038045458914233655   Iteration 74 of 100, tot loss = 4.9154652808163615, l1: 0.00010786742776061244, l2: 0.00038367910078581974   Iteration 75 of 100, tot loss = 4.881890977223714, l1: 0.00010742586722093014, l2: 0.0003807632310781628   Iteration 76 of 100, tot loss = 4.869344174861908, l1: 0.0001073621792544911, l2: 0.0003795722387669804   Iteration 77 of 100, tot loss = 4.843241183788745, l1: 0.00010665037226825408, l2: 0.0003776737464616051   Iteration 78 of 100, tot loss = 4.842971758964734, l1: 0.00010697879000732568, l2: 0.00037731838603623403   Iteration 79 of 100, tot loss = 4.845224084733408, l1: 0.00010704691314596766, l2: 0.0003774754958716516   Iteration 80 of 100, tot loss = 4.832140082120896, l1: 0.00010675572934815136, l2: 0.00037645827942469625   Iteration 81 of 100, tot loss = 4.84569251684495, l1: 0.00010694114493678991, l2: 0.00037762810717578287   Iteration 82 of 100, tot loss = 4.849696554788729, l1: 0.0001069241132471489, l2: 0.00037804554271931966   Iteration 83 of 100, tot loss = 4.824019366000072, l1: 0.00010626108558492241, l2: 0.0003761408513226737   Iteration 84 of 100, tot loss = 4.815530859288716, l1: 0.00010617803880969794, l2: 0.0003753750476919647   Iteration 85 of 100, tot loss = 4.863158150280223, l1: 0.00010695961579639355, l2: 0.00037935619954677187   Iteration 86 of 100, tot loss = 4.850800810858261, l1: 0.00010669650399968093, l2: 0.0003783835773626992   Iteration 87 of 100, tot loss = 4.860598681987017, l1: 0.00010665175051698675, l2: 0.00037940811826151676   Iteration 88 of 100, tot loss = 4.855444889176976, l1: 0.00010613122553464068, l2: 0.0003794132637928917   Iteration 89 of 100, tot loss = 4.8293767618329335, l1: 0.00010541592650390178, l2: 0.0003775217502673544   Iteration 90 of 100, tot loss = 4.82029693391588, l1: 0.00010541423156003778, l2: 0.0003766154626242092   Iteration 91 of 100, tot loss = 4.794932931334108, l1: 0.0001049584897483218, l2: 0.00037453480411556313   Iteration 92 of 100, tot loss = 4.802644309790238, l1: 0.00010493777905218809, l2: 0.00037532665231518206   Iteration 93 of 100, tot loss = 4.7870349422577885, l1: 0.00010472150946346422, l2: 0.00037398198506824915   Iteration 94 of 100, tot loss = 4.784297704696655, l1: 0.00010481587548596547, l2: 0.0003736138955610705   Iteration 95 of 100, tot loss = 4.788083854474519, l1: 0.00010502371189527606, l2: 0.0003737846744785968   Iteration 96 of 100, tot loss = 4.788929884632428, l1: 0.00010489811734260002, l2: 0.00037399487215831567   Iteration 97 of 100, tot loss = 4.786229802161148, l1: 0.0001049124617453607, l2: 0.0003737105196461888   Iteration 98 of 100, tot loss = 4.8212111336844305, l1: 0.00010529235308296142, l2: 0.0003768287611559832   Iteration 99 of 100, tot loss = 4.820594893561469, l1: 0.00010508571082909563, l2: 0.00037697377918062335   Iteration 100 of 100, tot loss = 4.833792839050293, l1: 0.00010529279301408678, l2: 0.00037808649183716623
   End of epoch 1337; saving model... 

Epoch 1338 of 2000
   Iteration 1 of 100, tot loss = 3.7035648822784424, l1: 0.00010278114496031776, l2: 0.000267575349425897   Iteration 2 of 100, tot loss = 3.970685601234436, l1: 9.423111987416632e-05, l2: 0.00030283743399195373   Iteration 3 of 100, tot loss = 4.633877674738566, l1: 0.00011031910010691111, l2: 0.0003530686565985282   Iteration 4 of 100, tot loss = 3.963424563407898, l1: 9.666397636465263e-05, l2: 0.0002996784714923706   Iteration 5 of 100, tot loss = 4.026850795745849, l1: 0.00010047836549347267, l2: 0.0003022067103302106   Iteration 6 of 100, tot loss = 4.063624620437622, l1: 9.784519109719743e-05, l2: 0.00030851727206027135   Iteration 7 of 100, tot loss = 4.7109567778451105, l1: 0.00010907068540940859, l2: 0.0003620249938519139   Iteration 8 of 100, tot loss = 4.664301633834839, l1: 0.00010926247432507807, l2: 0.0003571676879801089   Iteration 9 of 100, tot loss = 4.344760404692756, l1: 0.0001031958373560984, l2: 0.00033128020166057267   Iteration 10 of 100, tot loss = 4.312806832790375, l1: 0.00010349999502068385, l2: 0.0003277806841651909   Iteration 11 of 100, tot loss = 4.240023515441201, l1: 0.00010127241098829968, l2: 0.00032272993518166584   Iteration 12 of 100, tot loss = 4.266505469878514, l1: 0.0001016480697823378, l2: 0.0003250024734976857   Iteration 13 of 100, tot loss = 4.471686830887427, l1: 0.00010425631906120823, l2: 0.0003429123574348453   Iteration 14 of 100, tot loss = 4.390906972544534, l1: 0.00010339945947634988, l2: 0.00033569123063768657   Iteration 15 of 100, tot loss = 4.351751112937928, l1: 0.00010334084266408658, l2: 0.0003318342632458856   Iteration 16 of 100, tot loss = 4.239329285919666, l1: 0.00010144955058422056, l2: 0.00032248337265627924   Iteration 17 of 100, tot loss = 4.17895026066724, l1: 9.953101920580273e-05, l2: 0.00031836400226251604   Iteration 18 of 100, tot loss = 4.272616631454891, l1: 0.00010132437980953707, l2: 0.0003259372800433387   Iteration 19 of 100, tot loss = 4.28171065606569, l1: 0.00010072272600250711, l2: 0.00032744833458165984   Iteration 20 of 100, tot loss = 4.331164962053299, l1: 0.0001015146652207477, l2: 0.0003316018279292621   Iteration 21 of 100, tot loss = 4.214735207103548, l1: 9.940532029196176e-05, l2: 0.0003220681974198669   Iteration 22 of 100, tot loss = 4.20863242040981, l1: 9.88408778952858e-05, l2: 0.00032202236153270036   Iteration 23 of 100, tot loss = 4.164318058801734, l1: 9.69103117392439e-05, l2: 0.0003195214920434291   Iteration 24 of 100, tot loss = 4.222963685790698, l1: 9.792996115720598e-05, l2: 0.00032436640564507496   Iteration 25 of 100, tot loss = 4.171111226081848, l1: 9.719734865939245e-05, l2: 0.0003199137724004686   Iteration 26 of 100, tot loss = 4.1646589911901035, l1: 9.716972440051344e-05, l2: 0.00031929617398418486   Iteration 27 of 100, tot loss = 4.160094565815395, l1: 9.765561898583892e-05, l2: 0.000318353837226621   Iteration 28 of 100, tot loss = 4.150651944535119, l1: 9.724021505722444e-05, l2: 0.0003178249784728645   Iteration 29 of 100, tot loss = 4.106652140617371, l1: 9.566158355326103e-05, l2: 0.00031500362945672384   Iteration 30 of 100, tot loss = 4.10283651749293, l1: 9.600825020849394e-05, l2: 0.00031427540088770913   Iteration 31 of 100, tot loss = 4.134341551411536, l1: 9.682176718225463e-05, l2: 0.0003166123881988648   Iteration 32 of 100, tot loss = 4.110744547098875, l1: 9.589778164809104e-05, l2: 0.00031517667275693384   Iteration 33 of 100, tot loss = 4.0998747023669155, l1: 9.550263877661729e-05, l2: 0.00031448483059648424   Iteration 34 of 100, tot loss = 4.130558199742261, l1: 9.635657753439292e-05, l2: 0.00031669924218851304   Iteration 35 of 100, tot loss = 4.148269425119673, l1: 9.736715208938612e-05, l2: 0.00031745979005271305   Iteration 36 of 100, tot loss = 4.1778585149182215, l1: 9.817184319318685e-05, l2: 0.00031961400842798565   Iteration 37 of 100, tot loss = 4.166571774998227, l1: 9.772495969947478e-05, l2: 0.0003189322183921782   Iteration 38 of 100, tot loss = 4.167019445645182, l1: 9.763901700927435e-05, l2: 0.000319062928610947   Iteration 39 of 100, tot loss = 4.237136587118491, l1: 9.826277360284272e-05, l2: 0.00032545088600809087   Iteration 40 of 100, tot loss = 4.232681539654732, l1: 9.849325051618507e-05, l2: 0.0003247749042202486   Iteration 41 of 100, tot loss = 4.236230039015049, l1: 9.777050794715562e-05, l2: 0.0003258524971542789   Iteration 42 of 100, tot loss = 4.244292477766673, l1: 9.80754569657923e-05, l2: 0.00032635379154401435   Iteration 43 of 100, tot loss = 4.280332401741383, l1: 9.830922973315166e-05, l2: 0.00032972401112242236   Iteration 44 of 100, tot loss = 4.318088946017352, l1: 9.910097965489099e-05, l2: 0.000332707915896952   Iteration 45 of 100, tot loss = 4.321578854984708, l1: 9.98358703024375e-05, l2: 0.0003323220166041412   Iteration 46 of 100, tot loss = 4.3419487294943435, l1: 9.985864459810293e-05, l2: 0.0003343362290963895   Iteration 47 of 100, tot loss = 4.406799318942618, l1: 0.00010051924433123242, l2: 0.00034016068867516724   Iteration 48 of 100, tot loss = 4.388988407949607, l1: 0.00010021719875415631, l2: 0.0003386816433703643   Iteration 49 of 100, tot loss = 4.4127470936094015, l1: 0.00010091451362573675, l2: 0.0003403601968988814   Iteration 50 of 100, tot loss = 4.417714760303498, l1: 0.00010125756001798436, l2: 0.0003405139167443849   Iteration 51 of 100, tot loss = 4.432362932784884, l1: 0.00010193517864179597, l2: 0.00034130111481656556   Iteration 52 of 100, tot loss = 4.394342750310898, l1: 0.0001010890997489556, l2: 0.00033834517559556005   Iteration 53 of 100, tot loss = 4.411171856916176, l1: 0.0001012160476266389, l2: 0.00033990113791693355   Iteration 54 of 100, tot loss = 4.403162777423859, l1: 0.00010090075428942564, l2: 0.0003394155231675271   Iteration 55 of 100, tot loss = 4.384312627532266, l1: 0.00010012414131779223, l2: 0.0003383071214722639   Iteration 56 of 100, tot loss = 4.372256187455995, l1: 9.948219732385561e-05, l2: 0.0003377434212390134   Iteration 57 of 100, tot loss = 4.414010367895427, l1: 0.00010011087668540872, l2: 0.00034129016025792483   Iteration 58 of 100, tot loss = 4.401907326846287, l1: 0.0001001850072623648, l2: 0.0003400057256181628   Iteration 59 of 100, tot loss = 4.393715120978275, l1: 0.00010002554286768565, l2: 0.00033934596951624724   Iteration 60 of 100, tot loss = 4.383843539158503, l1: 9.967786366663253e-05, l2: 0.00033870649009865397   Iteration 61 of 100, tot loss = 4.401179765091568, l1: 0.00010020183504284283, l2: 0.00033991614196327377   Iteration 62 of 100, tot loss = 4.417237041458007, l1: 0.00010076650388298496, l2: 0.00034095720066914273   Iteration 63 of 100, tot loss = 4.43521303411514, l1: 0.00010101489585524957, l2: 0.00034250640809269887   Iteration 64 of 100, tot loss = 4.422054136171937, l1: 0.00010101382883931365, l2: 0.00034119158544854145   Iteration 65 of 100, tot loss = 4.404993506578299, l1: 0.00010075496583550166, l2: 0.00033974438556469975   Iteration 66 of 100, tot loss = 4.410134093327955, l1: 0.00010099173871849692, l2: 0.00034002167178021574   Iteration 67 of 100, tot loss = 4.427314311710756, l1: 0.00010117971548574296, l2: 0.0003415517167556586   Iteration 68 of 100, tot loss = 4.454715737525155, l1: 0.00010160499967990771, l2: 0.0003438665748142418   Iteration 69 of 100, tot loss = 4.459868937298872, l1: 0.00010155607267971273, l2: 0.000344430821934256   Iteration 70 of 100, tot loss = 4.450648745468684, l1: 0.00010140869178160626, l2: 0.0003436561836029536   Iteration 71 of 100, tot loss = 4.451932478958453, l1: 0.00010125804538789376, l2: 0.00034393520302690146   Iteration 72 of 100, tot loss = 4.466635540127754, l1: 0.00010144284887145559, l2: 0.0003452207060440236   Iteration 73 of 100, tot loss = 4.483216352658729, l1: 0.00010144458204581186, l2: 0.0003468770548152699   Iteration 74 of 100, tot loss = 4.533540559781565, l1: 0.0001021991298035278, l2: 0.00035115492740973224   Iteration 75 of 100, tot loss = 4.534821111361186, l1: 0.0001024166742960612, l2: 0.00035106543839598695   Iteration 76 of 100, tot loss = 4.5718195767779095, l1: 0.00010281472407912493, l2: 0.00035436723479314854   Iteration 77 of 100, tot loss = 4.592113576926194, l1: 0.00010300384397473331, l2: 0.0003562075147925356   Iteration 78 of 100, tot loss = 4.591414422560961, l1: 0.00010261423094795109, l2: 0.0003565272125296104   Iteration 79 of 100, tot loss = 4.598248000386395, l1: 0.00010237852026726108, l2: 0.00035744628101245417   Iteration 80 of 100, tot loss = 4.60457183867693, l1: 0.00010244461582260555, l2: 0.0003580125696316827   Iteration 81 of 100, tot loss = 4.602582067619135, l1: 0.00010226140893567469, l2: 0.00035799679922710325   Iteration 82 of 100, tot loss = 4.602659837501805, l1: 0.00010187641512347022, l2: 0.00035838956984396024   Iteration 83 of 100, tot loss = 4.586172254688768, l1: 0.00010156317008098206, l2: 0.0003570540568297617   Iteration 84 of 100, tot loss = 4.583085538375945, l1: 0.00010184653815702491, l2: 0.00035646201701768277   Iteration 85 of 100, tot loss = 4.585183902347789, l1: 0.00010193376527989612, l2: 0.0003565846264417119   Iteration 86 of 100, tot loss = 4.597841691139132, l1: 0.00010221893594567772, l2: 0.00035756523484028444   Iteration 87 of 100, tot loss = 4.582735142488589, l1: 0.00010198142129296288, l2: 0.00035629209449471926   Iteration 88 of 100, tot loss = 4.599069312214851, l1: 0.00010238642888990316, l2: 0.00035752050362961285   Iteration 89 of 100, tot loss = 4.602794983413782, l1: 0.00010246825106886707, l2: 0.00035781124861795834   Iteration 90 of 100, tot loss = 4.587373859352535, l1: 0.00010199754623398702, l2: 0.0003567398409359157   Iteration 91 of 100, tot loss = 4.586681574255556, l1: 0.0001019930432948468, l2: 0.0003566751154241981   Iteration 92 of 100, tot loss = 4.572920212279195, l1: 0.00010146191886198484, l2: 0.00035583010353350443   Iteration 93 of 100, tot loss = 4.562571929347131, l1: 0.00010124347318196669, l2: 0.00035501372097911294   Iteration 94 of 100, tot loss = 4.553275535715387, l1: 0.000101115028921916, l2: 0.0003542125257886352   Iteration 95 of 100, tot loss = 4.557118637938249, l1: 0.0001012753460121243, l2: 0.0003544365193719338   Iteration 96 of 100, tot loss = 4.559383413443963, l1: 0.00010138171842299926, l2: 0.0003545566244914274   Iteration 97 of 100, tot loss = 4.5539418852206355, l1: 0.00010134039291291085, l2: 0.0003540537970563032   Iteration 98 of 100, tot loss = 4.539056611304381, l1: 0.00010129664461805524, l2: 0.00035260901792326523   Iteration 99 of 100, tot loss = 4.539637901566246, l1: 0.00010124914798793863, l2: 0.0003527146434844142   Iteration 100 of 100, tot loss = 4.55655482172966, l1: 0.00010151469650736545, l2: 0.0003541407868033275
   End of epoch 1338; saving model... 

Epoch 1339 of 2000
   Iteration 1 of 100, tot loss = 4.54898738861084, l1: 0.00011481752153486013, l2: 0.0003400812274776399   Iteration 2 of 100, tot loss = 6.283268928527832, l1: 0.00013391455286182463, l2: 0.0004944123502355069   Iteration 3 of 100, tot loss = 5.771660327911377, l1: 0.00011919173751569663, l2: 0.0004579742962960154   Iteration 4 of 100, tot loss = 5.183067679405212, l1: 0.00011083133176725823, l2: 0.00040747543971519917   Iteration 5 of 100, tot loss = 5.248736572265625, l1: 0.00011478764208732173, l2: 0.00041008601547218857   Iteration 6 of 100, tot loss = 5.135085344314575, l1: 0.0001135152779170312, l2: 0.00039999325235839933   Iteration 7 of 100, tot loss = 5.3217348371233255, l1: 0.00011483433107579393, l2: 0.00041733914570483776   Iteration 8 of 100, tot loss = 5.093030393123627, l1: 0.00011053833532059798, l2: 0.00039876469963928685   Iteration 9 of 100, tot loss = 5.040121449364556, l1: 0.00011097945955245652, l2: 0.00039303268163671926   Iteration 10 of 100, tot loss = 5.345242071151733, l1: 0.00011728412719094194, l2: 0.0004172400775132701   Iteration 11 of 100, tot loss = 5.394648942080411, l1: 0.00011801433852683245, l2: 0.0004214505527422509   Iteration 12 of 100, tot loss = 5.2282816370328264, l1: 0.0001143720219261013, l2: 0.0004084561393635037   Iteration 13 of 100, tot loss = 5.281247047277597, l1: 0.00011364402934514846, l2: 0.000414480672379096   Iteration 14 of 100, tot loss = 5.3848879507609775, l1: 0.00011633645980119971, l2: 0.00042215233094923733   Iteration 15 of 100, tot loss = 5.209882895151774, l1: 0.00011156911374807047, l2: 0.0004094191710464656   Iteration 16 of 100, tot loss = 5.166577249765396, l1: 0.0001100882861919672, l2: 0.00040656943565409165   Iteration 17 of 100, tot loss = 5.214649901670568, l1: 0.00011044407887418536, l2: 0.0004110209085732041   Iteration 18 of 100, tot loss = 5.275994221369426, l1: 0.00011062868098734826, l2: 0.0004169707390246913   Iteration 19 of 100, tot loss = 5.35808831767032, l1: 0.00011213222004650896, l2: 0.00042367660851021737   Iteration 20 of 100, tot loss = 5.313358116149902, l1: 0.00011157290919072693, l2: 0.00041976290085585786   Iteration 21 of 100, tot loss = 5.315527938661122, l1: 0.00011245026014096635, l2: 0.0004191025329332444   Iteration 22 of 100, tot loss = 5.232744617895647, l1: 0.00011054766963008495, l2: 0.0004127267920094627   Iteration 23 of 100, tot loss = 5.245333827060202, l1: 0.00011185199195017998, l2: 0.00041268139055160725   Iteration 24 of 100, tot loss = 5.183209329843521, l1: 0.00011108984214539912, l2: 0.00040723109001798247   Iteration 25 of 100, tot loss = 5.224753694534302, l1: 0.0001110125168634113, l2: 0.00041146285249851647   Iteration 26 of 100, tot loss = 5.121775040259728, l1: 0.00010948171690800406, l2: 0.0004026957864810426   Iteration 27 of 100, tot loss = 5.093077871534559, l1: 0.00010893440331629891, l2: 0.0004003733839555126   Iteration 28 of 100, tot loss = 5.040328051362719, l1: 0.0001081242126019788, l2: 0.000395908592638859   Iteration 29 of 100, tot loss = 4.989681564528366, l1: 0.0001065785092640877, l2: 0.00039238964683167507   Iteration 30 of 100, tot loss = 5.083992584546407, l1: 0.00010888419116478568, l2: 0.0003995150681779099   Iteration 31 of 100, tot loss = 5.093292197873516, l1: 0.00010978018749043375, l2: 0.0003995490340223055   Iteration 32 of 100, tot loss = 5.06720881909132, l1: 0.00010882427625347191, l2: 0.0003978966074100754   Iteration 33 of 100, tot loss = 5.024863821087462, l1: 0.00010753919140728557, l2: 0.00039494719231063783   Iteration 34 of 100, tot loss = 4.943623290342443, l1: 0.0001060793054100085, l2: 0.0003882830248638878   Iteration 35 of 100, tot loss = 4.88613714490618, l1: 0.00010454895112031539, l2: 0.00038406476420017757   Iteration 36 of 100, tot loss = 4.917079144053989, l1: 0.00010556403281548733, l2: 0.00038614388217360503   Iteration 37 of 100, tot loss = 4.912249642449456, l1: 0.00010590483371571109, l2: 0.0003853201314750304   Iteration 38 of 100, tot loss = 4.846996759113512, l1: 0.00010455756627810555, l2: 0.0003801421109145801   Iteration 39 of 100, tot loss = 4.798038953389877, l1: 0.00010362696710403841, l2: 0.0003761769289550825   Iteration 40 of 100, tot loss = 4.822215121984482, l1: 0.0001042669709022448, l2: 0.00037795454300066923   Iteration 41 of 100, tot loss = 4.8461862482675695, l1: 0.00010534621274351461, l2: 0.0003792724139823737   Iteration 42 of 100, tot loss = 4.963018377621968, l1: 0.0001074043127681805, l2: 0.0003888975268290822   Iteration 43 of 100, tot loss = 5.030106028845144, l1: 0.00010858541924769037, l2: 0.00039442518535124266   Iteration 44 of 100, tot loss = 5.006007194519043, l1: 0.00010826618836290436, l2: 0.0003923345327414361   Iteration 45 of 100, tot loss = 4.972783199946085, l1: 0.00010740743899886082, l2: 0.00038987088256463824   Iteration 46 of 100, tot loss = 4.982520056807476, l1: 0.00010750674522094651, l2: 0.0003907452618991754   Iteration 47 of 100, tot loss = 4.933065947065962, l1: 0.0001062346515375774, l2: 0.0003870719442505667   Iteration 48 of 100, tot loss = 4.9626259952783585, l1: 0.00010701236298397514, l2: 0.0003892502381859231   Iteration 49 of 100, tot loss = 4.922016640098727, l1: 0.00010630116334401205, l2: 0.000385900502620569   Iteration 50 of 100, tot loss = 4.967110033035278, l1: 0.00010676354075258132, l2: 0.00038994746457319705   Iteration 51 of 100, tot loss = 4.962096307791915, l1: 0.00010698870220735175, l2: 0.00038922093032548827   Iteration 52 of 100, tot loss = 4.967876324286828, l1: 0.0001066969504021332, l2: 0.00039009068300052045   Iteration 53 of 100, tot loss = 5.007833660773511, l1: 0.00010704446342644141, l2: 0.00039373890316676135   Iteration 54 of 100, tot loss = 4.980563658255118, l1: 0.00010667756975669397, l2: 0.000391378796646475   Iteration 55 of 100, tot loss = 4.958172178268432, l1: 0.00010593847658707422, l2: 0.0003898787418041717   Iteration 56 of 100, tot loss = 5.010053383452552, l1: 0.00010670368367625218, l2: 0.0003943016553031547   Iteration 57 of 100, tot loss = 4.977451918417947, l1: 0.00010619625767535661, l2: 0.00039154893474596176   Iteration 58 of 100, tot loss = 4.96814121049026, l1: 0.00010622345263104292, l2: 0.0003905906690714142   Iteration 59 of 100, tot loss = 4.977397377208128, l1: 0.00010665815334196866, l2: 0.00039108158498647294   Iteration 60 of 100, tot loss = 4.9682001829147335, l1: 0.00010664274917265478, l2: 0.00039017726982516857   Iteration 61 of 100, tot loss = 4.975280198894564, l1: 0.00010672202742931464, l2: 0.000390805993238693   Iteration 62 of 100, tot loss = 4.946336938488868, l1: 0.00010628912206166499, l2: 0.00038834457260353733   Iteration 63 of 100, tot loss = 4.962461411006867, l1: 0.00010702546286564838, l2: 0.0003892206792472049   Iteration 64 of 100, tot loss = 4.939910300076008, l1: 0.00010667319321555624, l2: 0.0003873178377489239   Iteration 65 of 100, tot loss = 4.923645925521851, l1: 0.00010637442716800321, l2: 0.00038599016641875585   Iteration 66 of 100, tot loss = 4.9480487902959185, l1: 0.00010672674116821409, l2: 0.0003880781382163563   Iteration 67 of 100, tot loss = 4.913517129955007, l1: 0.0001062420646963231, l2: 0.0003851096485932566   Iteration 68 of 100, tot loss = 4.905144098926993, l1: 0.00010625081818836727, l2: 0.00038426359178094356   Iteration 69 of 100, tot loss = 4.918879609177078, l1: 0.00010644838181653303, l2: 0.00038543957908995503   Iteration 70 of 100, tot loss = 4.911234586579459, l1: 0.00010654323689647883, l2: 0.0003845802219335123   Iteration 71 of 100, tot loss = 4.94498180671477, l1: 0.00010707156375439143, l2: 0.0003874266168027198   Iteration 72 of 100, tot loss = 4.972349560923046, l1: 0.00010771599454528769, l2: 0.0003895189618358725   Iteration 73 of 100, tot loss = 4.943681863889302, l1: 0.00010693334678924732, l2: 0.00038743484007794257   Iteration 74 of 100, tot loss = 4.924563198476224, l1: 0.00010659044946314188, l2: 0.0003858658707719548   Iteration 75 of 100, tot loss = 4.919953832626343, l1: 0.000106881925118311, l2: 0.0003851134582267453   Iteration 76 of 100, tot loss = 4.938964451614179, l1: 0.0001074304645475135, l2: 0.00038646598075406233   Iteration 77 of 100, tot loss = 4.95103517445651, l1: 0.00010768702590472199, l2: 0.0003874164922072177   Iteration 78 of 100, tot loss = 4.931788890789717, l1: 0.00010741793868454316, l2: 0.00038576095129172196   Iteration 79 of 100, tot loss = 4.939109379732156, l1: 0.00010772734122274184, l2: 0.0003861835977221733   Iteration 80 of 100, tot loss = 4.95395594239235, l1: 0.00010790286187329911, l2: 0.0003874927333527012   Iteration 81 of 100, tot loss = 4.940751193482199, l1: 0.00010767938268448536, l2: 0.0003863957377846272   Iteration 82 of 100, tot loss = 4.941783329335655, l1: 0.00010780041933374865, l2: 0.0003863779147206692   Iteration 83 of 100, tot loss = 4.933738007602922, l1: 0.00010759582196387563, l2: 0.00038577797990965555   Iteration 84 of 100, tot loss = 4.953332957767305, l1: 0.00010769470775682879, l2: 0.0003876385890180245   Iteration 85 of 100, tot loss = 4.968567635031308, l1: 0.0001082071741727893, l2: 0.0003886495901764754   Iteration 86 of 100, tot loss = 4.952502319979113, l1: 0.00010753944509413614, l2: 0.00038771078783780507   Iteration 87 of 100, tot loss = 4.94414592885423, l1: 0.00010747532159539646, l2: 0.0003869392719767431   Iteration 88 of 100, tot loss = 4.925697708671743, l1: 0.00010737112849958727, l2: 0.00038519864293365657   Iteration 89 of 100, tot loss = 4.927997131026193, l1: 0.00010769842024120827, l2: 0.0003851012936976393   Iteration 90 of 100, tot loss = 4.925533456272549, l1: 0.0001078034911188297, l2: 0.0003847498556650761   Iteration 91 of 100, tot loss = 4.892676679642646, l1: 0.00010705630184320099, l2: 0.0003822113673207811   Iteration 92 of 100, tot loss = 4.893226801053338, l1: 0.00010713825126320286, l2: 0.000382184429959718   Iteration 93 of 100, tot loss = 4.91132136442328, l1: 0.0001075234469426896, l2: 0.0003836086903628643   Iteration 94 of 100, tot loss = 4.934968219158497, l1: 0.00010788728166594506, l2: 0.0003856095414796963   Iteration 95 of 100, tot loss = 4.931692174861306, l1: 0.00010801403213439412, l2: 0.0003851551865840233   Iteration 96 of 100, tot loss = 4.93137639388442, l1: 0.00010798484072438441, l2: 0.00038515279963273014   Iteration 97 of 100, tot loss = 4.9100971897852785, l1: 0.00010758162894377427, l2: 0.0003834280909096688   Iteration 98 of 100, tot loss = 4.9322168425637845, l1: 0.00010802344138715036, l2: 0.0003851982438224083   Iteration 99 of 100, tot loss = 4.920342203342553, l1: 0.00010790034447740199, l2: 0.0003841338767975832   Iteration 100 of 100, tot loss = 4.918324955701828, l1: 0.00010802131102536805, l2: 0.0003838111856020987
   End of epoch 1339; saving model... 

Epoch 1340 of 2000
   Iteration 1 of 100, tot loss = 4.938529014587402, l1: 9.195134043693542e-05, l2: 0.00040190157596953213   Iteration 2 of 100, tot loss = 4.175231337547302, l1: 8.386945773963816e-05, l2: 0.00033365369017701596   Iteration 3 of 100, tot loss = 4.530566771825154, l1: 8.867669384926558e-05, l2: 0.00036438000582469005   Iteration 4 of 100, tot loss = 4.775681674480438, l1: 9.119650167122018e-05, l2: 0.00038637168472632766   Iteration 5 of 100, tot loss = 4.803150224685669, l1: 9.98201299807988e-05, l2: 0.00038049491122364997   Iteration 6 of 100, tot loss = 4.654975692431132, l1: 0.0001004410575357421, l2: 0.00036505652921429527   Iteration 7 of 100, tot loss = 4.685494593211582, l1: 9.935612927490314e-05, l2: 0.00036919334421067366   Iteration 8 of 100, tot loss = 4.8075200617313385, l1: 0.00010159944940824062, l2: 0.00037915257053100504   Iteration 9 of 100, tot loss = 4.8511190679338245, l1: 0.00010320111000004949, l2: 0.00038191080481434864   Iteration 10 of 100, tot loss = 4.699092173576355, l1: 0.00010281981521984563, l2: 0.0003670894089736976   Iteration 11 of 100, tot loss = 4.915494333614003, l1: 0.00010613056656438857, l2: 0.0003854188742645254   Iteration 12 of 100, tot loss = 5.083145320415497, l1: 0.00010915768749934311, l2: 0.0003991568482888397   Iteration 13 of 100, tot loss = 5.2593834400177, l1: 0.00011258862818627116, l2: 0.0004133497187957311   Iteration 14 of 100, tot loss = 5.3462057283946445, l1: 0.00011378107176694487, l2: 0.0004208395060101923   Iteration 15 of 100, tot loss = 5.3361328601837155, l1: 0.00011441251166009655, l2: 0.0004192007812283312   Iteration 16 of 100, tot loss = 5.363659247756004, l1: 0.0001147877537732711, l2: 0.0004215781764287385   Iteration 17 of 100, tot loss = 5.403173600926118, l1: 0.0001154149863663513, l2: 0.0004249023785472245   Iteration 18 of 100, tot loss = 5.2807121011945934, l1: 0.00011247752142177585, l2: 0.00041559369371195015   Iteration 19 of 100, tot loss = 5.356219919104325, l1: 0.0001117998816832704, l2: 0.00042382211464236637   Iteration 20 of 100, tot loss = 5.336246967315674, l1: 0.00011062460143875796, l2: 0.0004230000973620918   Iteration 21 of 100, tot loss = 5.302902675810314, l1: 0.00011096076438358675, l2: 0.0004193295060845447   Iteration 22 of 100, tot loss = 5.2729387283325195, l1: 0.00011103235473538834, l2: 0.0004162615203478543   Iteration 23 of 100, tot loss = 5.4391569469286045, l1: 0.0001130955402597623, l2: 0.00043082015864733285   Iteration 24 of 100, tot loss = 5.4855965574582415, l1: 0.00011444247017304103, l2: 0.0004341171885850296   Iteration 25 of 100, tot loss = 5.4634108734130855, l1: 0.00011433540581492707, l2: 0.00043200568354222923   Iteration 26 of 100, tot loss = 5.453292626600999, l1: 0.00011398796414141543, l2: 0.00043134129989014653   Iteration 27 of 100, tot loss = 5.411385041696054, l1: 0.00011375137142345516, l2: 0.00042738713360719247   Iteration 28 of 100, tot loss = 5.386853013719831, l1: 0.00011290295437252748, l2: 0.0004257823472601428   Iteration 29 of 100, tot loss = 5.309863666008258, l1: 0.00011070430025875825, l2: 0.00042028206647454024   Iteration 30 of 100, tot loss = 5.293227990468343, l1: 0.00011065625391590098, l2: 0.00041866654501063747   Iteration 31 of 100, tot loss = 5.25709484469506, l1: 0.00011030735796309947, l2: 0.00041540212611923175   Iteration 32 of 100, tot loss = 5.2192928194999695, l1: 0.00011013580319740868, l2: 0.0004117934790883737   Iteration 33 of 100, tot loss = 5.269989967346191, l1: 0.0001107643128665531, l2: 0.0004162346846318211   Iteration 34 of 100, tot loss = 5.3588398484622735, l1: 0.00011252567898790243, l2: 0.00042335830521016544   Iteration 35 of 100, tot loss = 5.3182936259678435, l1: 0.00011229460235751633, l2: 0.0004195347598787131   Iteration 36 of 100, tot loss = 5.399377531475491, l1: 0.00011387427649525408, l2: 0.0004260634761724052   Iteration 37 of 100, tot loss = 5.333815104252583, l1: 0.00011309663229464629, l2: 0.00042028487754381587   Iteration 38 of 100, tot loss = 5.359642850725274, l1: 0.00011308039910019081, l2: 0.00042288388450335905   Iteration 39 of 100, tot loss = 5.311938230807964, l1: 0.00011237865337767662, l2: 0.00041881516778793855   Iteration 40 of 100, tot loss = 5.303681296110153, l1: 0.00011245052355661756, l2: 0.0004179176044999622   Iteration 41 of 100, tot loss = 5.3503774840657305, l1: 0.00011315592759851048, l2: 0.0004218818191098186   Iteration 42 of 100, tot loss = 5.398014823595683, l1: 0.00011376266203504721, l2: 0.0004260388183562706   Iteration 43 of 100, tot loss = 5.396551337353019, l1: 0.00011399152589187589, l2: 0.00042566360613374515   Iteration 44 of 100, tot loss = 5.3436925411224365, l1: 0.00011316365354022392, l2: 0.0004212055986995851   Iteration 45 of 100, tot loss = 5.325401401519775, l1: 0.00011233916140756466, l2: 0.00042020097704759487   Iteration 46 of 100, tot loss = 5.3166767825251044, l1: 0.0001127228510006756, l2: 0.00041894482495516297   Iteration 47 of 100, tot loss = 5.285113862220277, l1: 0.00011231933015725634, l2: 0.0004161920536562127   Iteration 48 of 100, tot loss = 5.275307854016622, l1: 0.00011204867632841342, l2: 0.00041548210689749493   Iteration 49 of 100, tot loss = 5.231286608442968, l1: 0.00011113895342699537, l2: 0.00041198970515955697   Iteration 50 of 100, tot loss = 5.190058798789978, l1: 0.00011043013146263547, l2: 0.00040857574582332746   Iteration 51 of 100, tot loss = 5.174226466347189, l1: 0.0001103536128271919, l2: 0.00040706903097338464   Iteration 52 of 100, tot loss = 5.157288143267999, l1: 0.00011057660314970865, l2: 0.00040515220881878544   Iteration 53 of 100, tot loss = 5.09407310215932, l1: 0.00010934391938685879, l2: 0.0004000633884713335   Iteration 54 of 100, tot loss = 5.07061023623855, l1: 0.00010876755914068781, l2: 0.0003982934620041676   Iteration 55 of 100, tot loss = 5.0391219832680445, l1: 0.00010857329894365235, l2: 0.0003953388968842443   Iteration 56 of 100, tot loss = 5.007320693561009, l1: 0.00010823493851473489, l2: 0.0003924971284245008   Iteration 57 of 100, tot loss = 5.03977612445229, l1: 0.00010901610387423844, l2: 0.0003949615066838369   Iteration 58 of 100, tot loss = 5.0088085306101835, l1: 0.00010852823997366017, l2: 0.00039235261095294346   Iteration 59 of 100, tot loss = 4.9723286305443715, l1: 0.00010749832006818535, l2: 0.00038973454107782976   Iteration 60 of 100, tot loss = 4.974283083279928, l1: 0.00010713481775989445, l2: 0.000390293488453608   Iteration 61 of 100, tot loss = 4.963738269493228, l1: 0.00010697386150255982, l2: 0.0003893999630951735   Iteration 62 of 100, tot loss = 4.951637483412219, l1: 0.00010678793701264376, l2: 0.00038837580875899163   Iteration 63 of 100, tot loss = 4.961540994190035, l1: 0.0001070132237232645, l2: 0.0003891408730793508   Iteration 64 of 100, tot loss = 4.98133534938097, l1: 0.0001074813570767219, l2: 0.0003906521756107395   Iteration 65 of 100, tot loss = 4.952830589734591, l1: 0.00010678052047016816, l2: 0.00038850253600125704   Iteration 66 of 100, tot loss = 4.948975422165611, l1: 0.00010669003556055638, l2: 0.0003882075045222529   Iteration 67 of 100, tot loss = 4.940336230975478, l1: 0.00010625329861198483, l2: 0.0003877803227176139   Iteration 68 of 100, tot loss = 4.932350239332984, l1: 0.00010614300502792877, l2: 0.000387092017370057   Iteration 69 of 100, tot loss = 4.945733308792114, l1: 0.00010652902246743062, l2: 0.00038804430674856013   Iteration 70 of 100, tot loss = 4.939633904184614, l1: 0.00010674819537338667, l2: 0.00038721519356061307   Iteration 71 of 100, tot loss = 4.925524382524087, l1: 0.00010645828881255076, l2: 0.0003860941481396136   Iteration 72 of 100, tot loss = 4.9272035294108925, l1: 0.00010647486043227318, l2: 0.000386245491325907   Iteration 73 of 100, tot loss = 4.937867465084547, l1: 0.00010664182906884267, l2: 0.00038714491648995   Iteration 74 of 100, tot loss = 4.946248466904099, l1: 0.0001068183799459901, l2: 0.00038780646541822906   Iteration 75 of 100, tot loss = 4.924914051691691, l1: 0.00010639679404751709, l2: 0.0003860946096635113   Iteration 76 of 100, tot loss = 4.909939894550725, l1: 0.00010618911045113284, l2: 0.0003848048773330734   Iteration 77 of 100, tot loss = 4.917670191108407, l1: 0.00010648836414238995, l2: 0.0003852786537102581   Iteration 78 of 100, tot loss = 4.914953638345767, l1: 0.00010667267251380671, l2: 0.0003848226900654248   Iteration 79 of 100, tot loss = 4.907944181297399, l1: 0.0001062807953795654, l2: 0.0003845136214041776   Iteration 80 of 100, tot loss = 4.885508558154106, l1: 0.0001058150736753305, l2: 0.00038273578102234753   Iteration 81 of 100, tot loss = 4.859907936166834, l1: 0.00010540647494008788, l2: 0.0003805843176159226   Iteration 82 of 100, tot loss = 4.859493118960683, l1: 0.0001054479324055847, l2: 0.0003805013785916721   Iteration 83 of 100, tot loss = 4.850289439580527, l1: 0.00010558302603319781, l2: 0.00037944591737419637   Iteration 84 of 100, tot loss = 4.810679272526786, l1: 0.00010480956249618127, l2: 0.0003762583641911901   Iteration 85 of 100, tot loss = 4.791934713195352, l1: 0.00010415268980603501, l2: 0.00037504078115007896   Iteration 86 of 100, tot loss = 4.7707181683806485, l1: 0.00010361340566813069, l2: 0.0003734584107553738   Iteration 87 of 100, tot loss = 4.742870332180769, l1: 0.00010321139395283803, l2: 0.000371075638913785   Iteration 88 of 100, tot loss = 4.745986538854512, l1: 0.00010315444759808915, l2: 0.0003714442055669761   Iteration 89 of 100, tot loss = 4.7330646260400835, l1: 0.00010295051163594824, l2: 0.00037035595032444034   Iteration 90 of 100, tot loss = 4.7186363365915085, l1: 0.00010269172307744156, l2: 0.0003691719099555889   Iteration 91 of 100, tot loss = 4.743807833273332, l1: 0.00010286280075388776, l2: 0.0003715179818055521   Iteration 92 of 100, tot loss = 4.749898344278336, l1: 0.00010304849871792375, l2: 0.00037194133509056763   Iteration 93 of 100, tot loss = 4.724727706242633, l1: 0.00010269905349911912, l2: 0.00036977371670335774   Iteration 94 of 100, tot loss = 4.75854658953687, l1: 0.00010315833451192915, l2: 0.00037269632392711543   Iteration 95 of 100, tot loss = 4.766197399089211, l1: 0.00010335353524025873, l2: 0.0003732662041377472   Iteration 96 of 100, tot loss = 4.771275618424018, l1: 0.0001035151324610221, l2: 0.00037361242872672545   Iteration 97 of 100, tot loss = 4.761336131194203, l1: 0.00010348004730779572, l2: 0.00037265356513671577   Iteration 98 of 100, tot loss = 4.731534599041452, l1: 0.00010283436521214233, l2: 0.00037031909403372176   Iteration 99 of 100, tot loss = 4.728640976578299, l1: 0.00010295298981340367, l2: 0.0003699111073710626   Iteration 100 of 100, tot loss = 4.7034981572628025, l1: 0.00010241529827908379, l2: 0.00036793451698031277
   End of epoch 1340; saving model... 

Epoch 1341 of 2000
   Iteration 1 of 100, tot loss = 5.325624465942383, l1: 0.00011245880887145177, l2: 0.00042010360630229115   Iteration 2 of 100, tot loss = 4.452889323234558, l1: 9.050206426763907e-05, l2: 0.000354786854586564   Iteration 3 of 100, tot loss = 4.3610743681589765, l1: 9.395296607787411e-05, l2: 0.0003421544679440558   Iteration 4 of 100, tot loss = 5.606626451015472, l1: 0.00011730207552318461, l2: 0.00044336057908367366   Iteration 5 of 100, tot loss = 5.030779218673706, l1: 0.0001100827124901116, l2: 0.0003929952217731625   Iteration 6 of 100, tot loss = 4.6905479828516645, l1: 0.00010374750127084553, l2: 0.0003653073047947449   Iteration 7 of 100, tot loss = 4.6728648117610385, l1: 0.00010173499945917033, l2: 0.0003655514883575961   Iteration 8 of 100, tot loss = 4.608114808797836, l1: 9.876335298031336e-05, l2: 0.0003620481347752502   Iteration 9 of 100, tot loss = 4.737861500846015, l1: 0.00010161171748121787, l2: 0.00037217444009406283   Iteration 10 of 100, tot loss = 4.711497044563293, l1: 0.00010144231710000895, l2: 0.00036970739456592127   Iteration 11 of 100, tot loss = 4.789018089121038, l1: 0.00010372380215399475, l2: 0.0003751780115470121   Iteration 12 of 100, tot loss = 4.6041140755017596, l1: 0.00010130309298498712, l2: 0.00035910831987469766   Iteration 13 of 100, tot loss = 4.485280220325176, l1: 9.81109064573852e-05, l2: 0.0003504171192896767   Iteration 14 of 100, tot loss = 4.431773168700082, l1: 9.803300937554533e-05, l2: 0.00034514431068341115   Iteration 15 of 100, tot loss = 4.268097615242004, l1: 9.579489706084132e-05, l2: 0.0003310148681824406   Iteration 16 of 100, tot loss = 4.400803051888943, l1: 9.758608302945504e-05, l2: 0.0003424942260608077   Iteration 17 of 100, tot loss = 4.517462583149181, l1: 0.00010070239744998296, l2: 0.0003510438648792093   Iteration 18 of 100, tot loss = 4.578829043441349, l1: 0.00010198693942382104, l2: 0.0003558959692276807   Iteration 19 of 100, tot loss = 4.599676627861826, l1: 0.00010187660901513147, l2: 0.00035809105764584323   Iteration 20 of 100, tot loss = 4.602330559492112, l1: 0.00010181315847148653, l2: 0.0003584199002943933   Iteration 21 of 100, tot loss = 4.782141702515738, l1: 0.00010465698938960919, l2: 0.0003735571850224265   Iteration 22 of 100, tot loss = 4.786708544601094, l1: 0.00010403084822676398, l2: 0.00037464001004330135   Iteration 23 of 100, tot loss = 5.033352245455203, l1: 0.00010748386367872034, l2: 0.0003958513666673199   Iteration 24 of 100, tot loss = 5.036913966139157, l1: 0.00010773262753597616, l2: 0.0003959587750917611   Iteration 25 of 100, tot loss = 4.970171961784363, l1: 0.00010703545151045546, l2: 0.0003899817506317049   Iteration 26 of 100, tot loss = 5.032720726269942, l1: 0.0001080640798206262, l2: 0.000395207998315947   Iteration 27 of 100, tot loss = 5.033950059502213, l1: 0.00010761158450299667, l2: 0.0003957834269385785   Iteration 28 of 100, tot loss = 4.94508541056088, l1: 0.00010568274709450114, l2: 0.0003888257997459732   Iteration 29 of 100, tot loss = 4.92366638265807, l1: 0.00010555327941707721, l2: 0.0003868133634537587   Iteration 30 of 100, tot loss = 4.928360180060069, l1: 0.00010654599209374283, l2: 0.0003862900300494706   Iteration 31 of 100, tot loss = 4.90022982705024, l1: 0.00010623949621672622, l2: 0.0003837834895333095   Iteration 32 of 100, tot loss = 4.853610496968031, l1: 0.00010564184287886746, l2: 0.0003797192102865665   Iteration 33 of 100, tot loss = 4.876106677633343, l1: 0.00010517547023923588, l2: 0.0003824352002773208   Iteration 34 of 100, tot loss = 4.846793206299052, l1: 0.00010529678965052452, l2: 0.00037938253337736516   Iteration 35 of 100, tot loss = 4.827001697676522, l1: 0.00010460734535757053, l2: 0.00037809282657690345   Iteration 36 of 100, tot loss = 4.820431977510452, l1: 0.00010427493311403345, l2: 0.000377768266642104   Iteration 37 of 100, tot loss = 4.7889277258434815, l1: 0.0001040015607007238, l2: 0.00037489121392797177   Iteration 38 of 100, tot loss = 4.740462187089418, l1: 0.00010344907892239892, l2: 0.0003705971417060171   Iteration 39 of 100, tot loss = 4.716207103851514, l1: 0.00010323962888856514, l2: 0.00036838108364743396   Iteration 40 of 100, tot loss = 4.7504982382059096, l1: 0.0001034029134643788, l2: 0.0003716469120263355   Iteration 41 of 100, tot loss = 4.713051194097938, l1: 0.0001027642717484169, l2: 0.0003685408492612357   Iteration 42 of 100, tot loss = 4.809105785120101, l1: 0.00010369314822171527, l2: 0.0003772174306851368   Iteration 43 of 100, tot loss = 4.758519363957782, l1: 0.0001033270581142143, l2: 0.00037252487882776837   Iteration 44 of 100, tot loss = 4.734132276339964, l1: 0.00010348221398585751, l2: 0.000369931014244106   Iteration 45 of 100, tot loss = 4.751745157771641, l1: 0.00010373098056233074, l2: 0.00037144353652062513   Iteration 46 of 100, tot loss = 4.822410021139228, l1: 0.00010532112980360145, l2: 0.00037691987349914956   Iteration 47 of 100, tot loss = 4.843844999658301, l1: 0.00010564241397623071, l2: 0.0003787420870874949   Iteration 48 of 100, tot loss = 4.832656778395176, l1: 0.00010551730648937034, l2: 0.00037774837195077754   Iteration 49 of 100, tot loss = 4.852642689432416, l1: 0.00010565180714273046, l2: 0.00037961246146421345   Iteration 50 of 100, tot loss = 4.842982013225555, l1: 0.00010599343520880212, l2: 0.00037830476590897886   Iteration 51 of 100, tot loss = 4.899424307486591, l1: 0.0001067284740166346, l2: 0.0003832139562148893   Iteration 52 of 100, tot loss = 4.8671077742026405, l1: 0.00010659469563296835, l2: 0.0003801160813824166   Iteration 53 of 100, tot loss = 4.915922999382019, l1: 0.0001074617781719184, l2: 0.0003841305214731585   Iteration 54 of 100, tot loss = 4.935182842943403, l1: 0.0001078643134759558, l2: 0.0003856539705237891   Iteration 55 of 100, tot loss = 4.9577368887988005, l1: 0.00010810604706586508, l2: 0.0003876676415199075   Iteration 56 of 100, tot loss = 4.949204246912684, l1: 0.00010822611219347371, l2: 0.00038669431238044387   Iteration 57 of 100, tot loss = 4.93426573694798, l1: 0.00010804047065392839, l2: 0.00038538610265516723   Iteration 58 of 100, tot loss = 4.940764112719174, l1: 0.00010801994903886225, l2: 0.0003860564613895459   Iteration 59 of 100, tot loss = 4.896394473011211, l1: 0.00010739203725726406, l2: 0.0003822474092762065   Iteration 60 of 100, tot loss = 4.8685319523016615, l1: 0.00010714839621262702, l2: 0.0003797047984941552   Iteration 61 of 100, tot loss = 4.83793559426167, l1: 0.00010708376013084132, l2: 0.00037670979866964103   Iteration 62 of 100, tot loss = 4.861512628293807, l1: 0.00010744715878137222, l2: 0.0003787041029183104   Iteration 63 of 100, tot loss = 4.858157844770522, l1: 0.00010720138743636198, l2: 0.0003786143961839289   Iteration 64 of 100, tot loss = 4.872113054618239, l1: 0.0001072126559051867, l2: 0.00037999864866833377   Iteration 65 of 100, tot loss = 4.881635341277489, l1: 0.0001074805813774359, l2: 0.000380682952191609   Iteration 66 of 100, tot loss = 4.854197110190536, l1: 0.00010704985090176871, l2: 0.00037836985928041747   Iteration 67 of 100, tot loss = 4.86723368737235, l1: 0.0001074890892256795, l2: 0.00037923427893773223   Iteration 68 of 100, tot loss = 4.871531758238287, l1: 0.00010730746185692945, l2: 0.0003798457137771252   Iteration 69 of 100, tot loss = 4.871540365011795, l1: 0.00010716573387321989, l2: 0.00037998830257624763   Iteration 70 of 100, tot loss = 4.83639212506158, l1: 0.00010643027749860527, l2: 0.0003772089349305523   Iteration 71 of 100, tot loss = 4.878864431045424, l1: 0.00010717739025212851, l2: 0.0003807090524531586   Iteration 72 of 100, tot loss = 4.8625219580199985, l1: 0.00010712561490638311, l2: 0.00037912658040618733   Iteration 73 of 100, tot loss = 4.902068227937777, l1: 0.00010800634521579574, l2: 0.00038220047670072075   Iteration 74 of 100, tot loss = 4.970975732481158, l1: 0.00010859791645261057, l2: 0.0003884996569123925   Iteration 75 of 100, tot loss = 4.944284580548604, l1: 0.00010806544528653225, l2: 0.00038636301275497923   Iteration 76 of 100, tot loss = 4.945548203430678, l1: 0.00010791304511689017, l2: 0.0003866417757403963   Iteration 77 of 100, tot loss = 4.945465628202859, l1: 0.00010791422892588663, l2: 0.000386632334663194   Iteration 78 of 100, tot loss = 4.909323861965766, l1: 0.0001073995651192187, l2: 0.0003835328219186825   Iteration 79 of 100, tot loss = 4.8899894711337515, l1: 0.00010721126823285241, l2: 0.0003817876798315917   Iteration 80 of 100, tot loss = 4.8906458362936975, l1: 0.0001074705096471007, l2: 0.00038159407504281264   Iteration 81 of 100, tot loss = 4.892191691163146, l1: 0.00010780629146424478, l2: 0.0003814128783379141   Iteration 82 of 100, tot loss = 4.912041319579613, l1: 0.00010797226032747591, l2: 0.00038323187245845383   Iteration 83 of 100, tot loss = 4.9346551650978, l1: 0.00010830062302671014, l2: 0.00038516489457176347   Iteration 84 of 100, tot loss = 4.911346774725687, l1: 0.00010783922455933255, l2: 0.00038329545402250787   Iteration 85 of 100, tot loss = 4.894192542749293, l1: 0.00010756895028273848, l2: 0.00038185030519825354   Iteration 86 of 100, tot loss = 4.886199883250303, l1: 0.00010740285675680308, l2: 0.00038121713289509114   Iteration 87 of 100, tot loss = 4.891375216944464, l1: 0.00010735562398036856, l2: 0.00038178189876409053   Iteration 88 of 100, tot loss = 4.859973979267207, l1: 0.0001068578440026613, l2: 0.00037913955500037196   Iteration 89 of 100, tot loss = 4.857899243912001, l1: 0.00010695218331287772, l2: 0.0003788377420932285   Iteration 90 of 100, tot loss = 4.879288287957509, l1: 0.00010746217554292848, l2: 0.0003804666544763475   Iteration 91 of 100, tot loss = 4.864655223521558, l1: 0.00010723677037826197, l2: 0.00037922875320533763   Iteration 92 of 100, tot loss = 4.856978656157203, l1: 0.00010699987844576674, l2: 0.0003786979886290678   Iteration 93 of 100, tot loss = 4.868140960252413, l1: 0.0001073702269325143, l2: 0.00037944387041798403   Iteration 94 of 100, tot loss = 4.898542197460824, l1: 0.00010771342211692753, l2: 0.00038214079912459635   Iteration 95 of 100, tot loss = 4.903484767361691, l1: 0.00010790993240478106, l2: 0.00038243854574638566   Iteration 96 of 100, tot loss = 4.879856361697118, l1: 0.00010739976517015748, l2: 0.00038058587233535945   Iteration 97 of 100, tot loss = 4.9074184120315865, l1: 0.00010775113876605377, l2: 0.0003829907035435905   Iteration 98 of 100, tot loss = 4.908018183951476, l1: 0.00010778341114631503, l2: 0.00038301840849809955   Iteration 99 of 100, tot loss = 4.904666157683941, l1: 0.00010764869420190643, l2: 0.0003828179227033009   Iteration 100 of 100, tot loss = 4.925879043340683, l1: 0.00010787726259877672, l2: 0.0003847106426837854
   End of epoch 1341; saving model... 

Epoch 1342 of 2000
   Iteration 1 of 100, tot loss = 4.277132034301758, l1: 7.648906466783956e-05, l2: 0.00035122415283694863   Iteration 2 of 100, tot loss = 5.187500238418579, l1: 0.00010808228762471117, l2: 0.0004106677515665069   Iteration 3 of 100, tot loss = 4.850715319315593, l1: 0.00010513423815912877, l2: 0.000379937303174908   Iteration 4 of 100, tot loss = 5.141239762306213, l1: 0.00010539635331952013, l2: 0.0004087276247446425   Iteration 5 of 100, tot loss = 5.171969604492188, l1: 0.00010447440145071596, l2: 0.00041272256057709453   Iteration 6 of 100, tot loss = 4.77846638361613, l1: 9.810457413550466e-05, l2: 0.0003797420674042466   Iteration 7 of 100, tot loss = 4.625398465565273, l1: 9.379419706030083e-05, l2: 0.00036874565038098287   Iteration 8 of 100, tot loss = 4.55266073346138, l1: 9.676945228420664e-05, l2: 0.00035849661981046665   Iteration 9 of 100, tot loss = 4.344139602449205, l1: 9.448621939453814e-05, l2: 0.000339927740343329   Iteration 10 of 100, tot loss = 4.442046713829041, l1: 9.589864275767468e-05, l2: 0.00034830602817237377   Iteration 11 of 100, tot loss = 4.616767731579867, l1: 9.983952292135324e-05, l2: 0.00036183725090020084   Iteration 12 of 100, tot loss = 4.429709454377492, l1: 9.64939063123893e-05, l2: 0.00034647704039040644   Iteration 13 of 100, tot loss = 4.311158345295833, l1: 9.445012591850873e-05, l2: 0.0003366657093955347   Iteration 14 of 100, tot loss = 4.549691591944013, l1: 9.710558307623225e-05, l2: 0.00035786358057521284   Iteration 15 of 100, tot loss = 4.544995387395223, l1: 9.87482097116299e-05, l2: 0.0003557513312747081   Iteration 16 of 100, tot loss = 4.434344008564949, l1: 9.809266612137435e-05, l2: 0.0003453417357377475   Iteration 17 of 100, tot loss = 4.46608567237854, l1: 9.91094806094599e-05, l2: 0.00034749908852117024   Iteration 18 of 100, tot loss = 4.428909526930915, l1: 9.829983981843624e-05, l2: 0.00034459111500634917   Iteration 19 of 100, tot loss = 4.381036131005538, l1: 9.743810943818014e-05, l2: 0.0003406655052554254   Iteration 20 of 100, tot loss = 4.30823245048523, l1: 9.638017836550716e-05, l2: 0.0003344430682773236   Iteration 21 of 100, tot loss = 4.469300292787098, l1: 9.709494805151952e-05, l2: 0.00034983508036627126   Iteration 22 of 100, tot loss = 4.442109552296725, l1: 9.677274019023488e-05, l2: 0.0003474382147859697   Iteration 23 of 100, tot loss = 4.439384201298589, l1: 9.737248784285444e-05, l2: 0.00034656593102820057   Iteration 24 of 100, tot loss = 4.58151317636172, l1: 9.892092596904452e-05, l2: 0.00035923038922192063   Iteration 25 of 100, tot loss = 4.670165128707886, l1: 0.00010031260637333617, l2: 0.00036670390341896563   Iteration 26 of 100, tot loss = 4.796267757048974, l1: 0.00010134051570695682, l2: 0.00037828625611907157   Iteration 27 of 100, tot loss = 4.684202039683306, l1: 9.901984486970361e-05, l2: 0.0003694003556760166   Iteration 28 of 100, tot loss = 4.7361346525805335, l1: 9.948657212329895e-05, l2: 0.0003741268888656383   Iteration 29 of 100, tot loss = 4.755574723769879, l1: 0.00010021496965582, l2: 0.0003753424986418144   Iteration 30 of 100, tot loss = 4.695053525765737, l1: 9.968283969404486e-05, l2: 0.0003698225094315906   Iteration 31 of 100, tot loss = 4.725078748118493, l1: 0.0001007812358194872, l2: 0.000371726636301666   Iteration 32 of 100, tot loss = 4.696232732385397, l1: 0.0001000418349121901, l2: 0.0003695814357342897   Iteration 33 of 100, tot loss = 4.7085634831226235, l1: 0.00010042217785887647, l2: 0.00037043416787954896   Iteration 34 of 100, tot loss = 4.690289732287912, l1: 0.00010051626047831687, l2: 0.0003685127106591073   Iteration 35 of 100, tot loss = 4.722997198786055, l1: 0.00010114301819287772, l2: 0.0003711566985917411   Iteration 36 of 100, tot loss = 4.676187505324681, l1: 0.00010032314599407578, l2: 0.00036729560128555424   Iteration 37 of 100, tot loss = 4.644351363182068, l1: 0.00010006489291963344, l2: 0.0003643702401785885   Iteration 38 of 100, tot loss = 4.605379390089135, l1: 9.946566474115428e-05, l2: 0.0003610722713730004   Iteration 39 of 100, tot loss = 4.642742997560745, l1: 9.975414156006314e-05, l2: 0.00036452015532025445   Iteration 40 of 100, tot loss = 4.62688395678997, l1: 9.923667876137188e-05, l2: 0.0003634517135651549   Iteration 41 of 100, tot loss = 4.743766525896584, l1: 0.00010097353054045858, l2: 0.0003734031197382137   Iteration 42 of 100, tot loss = 4.737078959033603, l1: 0.00010152642772438758, l2: 0.0003721814660128162   Iteration 43 of 100, tot loss = 4.712220876715904, l1: 0.0001012805885185899, l2: 0.0003699414966070245   Iteration 44 of 100, tot loss = 4.746749875220385, l1: 0.0001025284741031514, l2: 0.0003721465112572663   Iteration 45 of 100, tot loss = 4.743688527743021, l1: 0.00010291324191105863, l2: 0.00037145560846612273   Iteration 46 of 100, tot loss = 4.843359333017598, l1: 0.00010428115416763593, l2: 0.0003800547770958434   Iteration 47 of 100, tot loss = 4.77195185296079, l1: 0.00010288556633044054, l2: 0.00037430961705506484   Iteration 48 of 100, tot loss = 4.7862470000982285, l1: 0.0001033045031514727, l2: 0.0003753201951136968   Iteration 49 of 100, tot loss = 4.7617224771149305, l1: 0.00010302743337851264, l2: 0.00037314481284868507   Iteration 50 of 100, tot loss = 4.808727960586548, l1: 0.00010416507058835122, l2: 0.0003767077234806493   Iteration 51 of 100, tot loss = 4.838678565679812, l1: 0.0001044543944361994, l2: 0.0003794134604772005   Iteration 52 of 100, tot loss = 4.837139056279109, l1: 0.00010438571574083583, l2: 0.00037932818836102693   Iteration 53 of 100, tot loss = 4.839950003713931, l1: 0.00010430050514394611, l2: 0.0003796944939443525   Iteration 54 of 100, tot loss = 4.859959505222462, l1: 0.0001050564458435272, l2: 0.00038093950273900256   Iteration 55 of 100, tot loss = 4.852337897907604, l1: 0.00010475963466557336, l2: 0.00038047415384260766   Iteration 56 of 100, tot loss = 4.8445196236882895, l1: 0.00010498375058887177, l2: 0.0003794682104073997   Iteration 57 of 100, tot loss = 4.816000566147921, l1: 0.00010465448161866924, l2: 0.000376945573901921   Iteration 58 of 100, tot loss = 4.79782013235421, l1: 0.00010455644819714883, l2: 0.00037522556396715084   Iteration 59 of 100, tot loss = 4.8416159112574695, l1: 0.00010528125235627373, l2: 0.0003788803370622129   Iteration 60 of 100, tot loss = 4.814458270867665, l1: 0.00010479829464505504, l2: 0.0003766475308414859   Iteration 61 of 100, tot loss = 4.845812637297834, l1: 0.00010494393582568412, l2: 0.0003796373260184573   Iteration 62 of 100, tot loss = 4.81118724038524, l1: 0.00010407470806732758, l2: 0.00037704401432664224   Iteration 63 of 100, tot loss = 4.798055735845415, l1: 0.0001039208602580604, l2: 0.00037588471189207794   Iteration 64 of 100, tot loss = 4.7716921009123325, l1: 0.0001034845154777031, l2: 0.0003736846931587934   Iteration 65 of 100, tot loss = 4.819209263874934, l1: 0.00010390418605171502, l2: 0.0003780167390896867   Iteration 66 of 100, tot loss = 4.814813696976864, l1: 0.0001038563448012362, l2: 0.0003776250239091248   Iteration 67 of 100, tot loss = 4.823312734490011, l1: 0.00010408061779575048, l2: 0.0003782506546078583   Iteration 68 of 100, tot loss = 4.799750212360831, l1: 0.00010377885759868043, l2: 0.0003761961627734469   Iteration 69 of 100, tot loss = 4.785886215127033, l1: 0.00010344726093614514, l2: 0.00037514135950967074   Iteration 70 of 100, tot loss = 4.850319211823599, l1: 0.00010452801699492348, l2: 0.0003805039028520696   Iteration 71 of 100, tot loss = 4.853945997399344, l1: 0.00010458432621868576, l2: 0.0003808102725011388   Iteration 72 of 100, tot loss = 4.857188764545652, l1: 0.00010465568544128069, l2: 0.00038106318990887504   Iteration 73 of 100, tot loss = 4.849683288025529, l1: 0.0001046116610256196, l2: 0.0003803566671120105   Iteration 74 of 100, tot loss = 4.831458591126107, l1: 0.00010437928048722333, l2: 0.0003787665779315363   Iteration 75 of 100, tot loss = 4.800753981272379, l1: 0.00010357214196119458, l2: 0.0003765032554899032   Iteration 76 of 100, tot loss = 4.805544470485888, l1: 0.00010379792308269411, l2: 0.00037675652339729773   Iteration 77 of 100, tot loss = 4.793684538308677, l1: 0.00010325688996893511, l2: 0.0003761115633994462   Iteration 78 of 100, tot loss = 4.840599341270251, l1: 0.0001040376135698842, l2: 0.0003800223201277475   Iteration 79 of 100, tot loss = 4.842907875399046, l1: 0.00010435619489864529, l2: 0.0003799345925276439   Iteration 80 of 100, tot loss = 4.82480408847332, l1: 0.00010415559081593529, l2: 0.0003783248181207455   Iteration 81 of 100, tot loss = 4.840224698737815, l1: 0.00010402171783088879, l2: 0.0003800007518607962   Iteration 82 of 100, tot loss = 4.851636825538263, l1: 0.00010423639948229964, l2: 0.0003809272828043393   Iteration 83 of 100, tot loss = 4.847229880022716, l1: 0.00010391020036165053, l2: 0.0003808127873624866   Iteration 84 of 100, tot loss = 4.832768979526701, l1: 0.00010370778606253832, l2: 0.0003795691114819853   Iteration 85 of 100, tot loss = 4.832095123739803, l1: 0.0001038233405458467, l2: 0.00037938617152737125   Iteration 86 of 100, tot loss = 4.82667987291203, l1: 0.00010386311556050164, l2: 0.00037880487172176715   Iteration 87 of 100, tot loss = 4.84306822700062, l1: 0.00010406010016135273, l2: 0.0003802467222210427   Iteration 88 of 100, tot loss = 4.8394864201545715, l1: 0.00010403974705365148, l2: 0.00037990889491497495   Iteration 89 of 100, tot loss = 4.856722097718314, l1: 0.00010429307023126088, l2: 0.00038137913925515677   Iteration 90 of 100, tot loss = 4.857401768366496, l1: 0.0001042999460575326, l2: 0.00038144023063877184   Iteration 91 of 100, tot loss = 4.881778088244763, l1: 0.00010464346069256662, l2: 0.0003835343482973218   Iteration 92 of 100, tot loss = 4.889158850130827, l1: 0.00010432905045213967, l2: 0.0003845868352231185   Iteration 93 of 100, tot loss = 4.899658305670625, l1: 0.00010445129385428323, l2: 0.0003855145370189641   Iteration 94 of 100, tot loss = 4.8839025116981345, l1: 0.00010425219543902402, l2: 0.00038413805599160926   Iteration 95 of 100, tot loss = 4.863311225489566, l1: 0.00010374373477565026, l2: 0.0003825873880965733   Iteration 96 of 100, tot loss = 4.856991817553838, l1: 0.00010388299938313139, l2: 0.00038181618250140065   Iteration 97 of 100, tot loss = 4.848624553877054, l1: 0.00010372401560481503, l2: 0.00038113843976068744   Iteration 98 of 100, tot loss = 4.840632881437029, l1: 0.00010368692996312998, l2: 0.00038037635831456935   Iteration 99 of 100, tot loss = 4.815677019080731, l1: 0.00010328215226175903, l2: 0.0003782855499667501   Iteration 100 of 100, tot loss = 4.81721214056015, l1: 0.00010321727066184394, l2: 0.00037850394379347564
   End of epoch 1342; saving model... 

Epoch 1343 of 2000
   Iteration 1 of 100, tot loss = 7.581718921661377, l1: 0.00014420700608752668, l2: 0.0006139648612588644   Iteration 2 of 100, tot loss = 6.759180068969727, l1: 0.00013781220332020894, l2: 0.0005381057999329641   Iteration 3 of 100, tot loss = 6.44410769144694, l1: 0.00012522517014682913, l2: 0.0005191855986292163   Iteration 4 of 100, tot loss = 6.343005418777466, l1: 0.00012972744843864348, l2: 0.0005045730868005194   Iteration 5 of 100, tot loss = 5.797610664367676, l1: 0.00011902735568583011, l2: 0.0004607337061315775   Iteration 6 of 100, tot loss = 5.353598674138387, l1: 0.00011615990903616573, l2: 0.0004191999541944824   Iteration 7 of 100, tot loss = 5.783997876303537, l1: 0.00012359792058954814, l2: 0.0004548018685974447   Iteration 8 of 100, tot loss = 5.56764829158783, l1: 0.0001174568351416383, l2: 0.00043930799984082114   Iteration 9 of 100, tot loss = 5.354819456736247, l1: 0.00011098605520803378, l2: 0.000424495896570281   Iteration 10 of 100, tot loss = 5.260684823989868, l1: 0.00011095541231043171, l2: 0.00041511307790642604   Iteration 11 of 100, tot loss = 5.165311553261497, l1: 0.00010949612062011676, l2: 0.00040703504319853067   Iteration 12 of 100, tot loss = 5.177876869837443, l1: 0.00010873370926371233, l2: 0.0004090539817601287   Iteration 13 of 100, tot loss = 4.997726110311655, l1: 0.00010403189186875422, l2: 0.0003957407237836518   Iteration 14 of 100, tot loss = 4.863230637141636, l1: 0.0001024704587117802, l2: 0.0003838526089176802   Iteration 15 of 100, tot loss = 4.952108383178711, l1: 0.00010516272668610327, l2: 0.0003900481155142188   Iteration 16 of 100, tot loss = 5.0128651559352875, l1: 0.00010557009022704733, l2: 0.0003957164281018777   Iteration 17 of 100, tot loss = 4.87794755486881, l1: 0.00010434277454110356, l2: 0.00038345198417493305   Iteration 18 of 100, tot loss = 4.772733489672343, l1: 0.00010349597727731129, l2: 0.0003737773739784542   Iteration 19 of 100, tot loss = 4.804632801758616, l1: 0.00010361709485694423, l2: 0.00037684618636328534   Iteration 20 of 100, tot loss = 4.7098299860954285, l1: 0.0001028189019052661, l2: 0.00036816409847233443   Iteration 21 of 100, tot loss = 4.641523088727679, l1: 0.00010242463221878833, l2: 0.00036172767882124477   Iteration 22 of 100, tot loss = 4.648623293096369, l1: 0.00010214603042351717, l2: 0.0003627162996880625   Iteration 23 of 100, tot loss = 4.692654506019924, l1: 0.00010287858252843802, l2: 0.0003663868682570351   Iteration 24 of 100, tot loss = 4.7551123102506, l1: 0.00010421873306161918, l2: 0.00037129249843322515   Iteration 25 of 100, tot loss = 4.7062383556365965, l1: 0.0001035731732554268, l2: 0.0003670506627531722   Iteration 26 of 100, tot loss = 4.626187241994417, l1: 0.00010156785650066852, l2: 0.0003610508675382544   Iteration 27 of 100, tot loss = 4.6538576108438, l1: 0.00010139494339387898, l2: 0.0003639908175467065   Iteration 28 of 100, tot loss = 4.596017794949668, l1: 0.00010072753619689527, l2: 0.0003588742422704984   Iteration 29 of 100, tot loss = 4.585655269951656, l1: 0.00010098965453678453, l2: 0.0003575758721502819   Iteration 30 of 100, tot loss = 4.674206201235453, l1: 0.00010247996700248525, l2: 0.00036494065328345946   Iteration 31 of 100, tot loss = 4.640151992920907, l1: 0.00010178112690130459, l2: 0.0003622340727522368   Iteration 32 of 100, tot loss = 4.716360881924629, l1: 0.00010249705303522205, l2: 0.0003691390347739798   Iteration 33 of 100, tot loss = 4.704962383617055, l1: 0.00010228283678192052, l2: 0.00036821340097848213   Iteration 34 of 100, tot loss = 4.768599103478825, l1: 0.00010320792213914341, l2: 0.00037365198744844426   Iteration 35 of 100, tot loss = 4.8040012223379955, l1: 0.0001038577735016588, l2: 0.0003765423474500754   Iteration 36 of 100, tot loss = 4.821566038661533, l1: 0.00010441696809316959, l2: 0.0003777396357488922   Iteration 37 of 100, tot loss = 4.8288077921480745, l1: 0.00010487308217970831, l2: 0.0003780076961423195   Iteration 38 of 100, tot loss = 4.870107111177947, l1: 0.00010586118198836247, l2: 0.00038114952851748586   Iteration 39 of 100, tot loss = 4.801270222052549, l1: 0.00010451232204407764, l2: 0.0003756146996658152   Iteration 40 of 100, tot loss = 4.776205742359162, l1: 0.00010412649999125279, l2: 0.00037349407393776345   Iteration 41 of 100, tot loss = 4.806220182558385, l1: 0.00010447382347348782, l2: 0.0003761481944465919   Iteration 42 of 100, tot loss = 4.842377004169283, l1: 0.000104898984881445, l2: 0.00037933871464615335   Iteration 43 of 100, tot loss = 4.837902612464372, l1: 0.0001047537982738631, l2: 0.00037903646189925194   Iteration 44 of 100, tot loss = 4.789332194761797, l1: 0.00010412895252639746, l2: 0.00037480426610934296   Iteration 45 of 100, tot loss = 4.8868126763237845, l1: 0.00010577035516487539, l2: 0.0003829109113818655   Iteration 46 of 100, tot loss = 4.884853259376857, l1: 0.00010605587182078085, l2: 0.00038242945384294927   Iteration 47 of 100, tot loss = 4.893424713865239, l1: 0.00010590213965537879, l2: 0.00038344033082273414   Iteration 48 of 100, tot loss = 4.882448573907216, l1: 0.00010559728578603729, l2: 0.00038264757070768   Iteration 49 of 100, tot loss = 4.893373742395518, l1: 0.00010567339075128168, l2: 0.0003836639819438664   Iteration 50 of 100, tot loss = 4.864132752418518, l1: 0.00010461143276188522, l2: 0.00038180184084922074   Iteration 51 of 100, tot loss = 4.853480455922146, l1: 0.00010459266095673301, l2: 0.0003807553833158796   Iteration 52 of 100, tot loss = 4.816551176401285, l1: 0.00010396682242012153, l2: 0.00037768829413555347   Iteration 53 of 100, tot loss = 4.792727101523921, l1: 0.00010316212371075174, l2: 0.000376110585386654   Iteration 54 of 100, tot loss = 4.766961737915322, l1: 0.00010274789289084126, l2: 0.0003739482800247734   Iteration 55 of 100, tot loss = 4.777088238976218, l1: 0.00010328833159292116, l2: 0.00037442049122711816   Iteration 56 of 100, tot loss = 4.754873854773385, l1: 0.00010274284483914795, l2: 0.0003727445398128891   Iteration 57 of 100, tot loss = 4.70986166753267, l1: 0.00010192449368779515, l2: 0.00036906167198985553   Iteration 58 of 100, tot loss = 4.717809566136064, l1: 0.0001021885054797167, l2: 0.0003695924498405757   Iteration 59 of 100, tot loss = 4.744017839431763, l1: 0.00010262303562663196, l2: 0.00037177874740228944   Iteration 60 of 100, tot loss = 4.7030445019404095, l1: 0.00010159392077184748, l2: 0.00036871052846739377   Iteration 61 of 100, tot loss = 4.736827248432597, l1: 0.00010194931669279048, l2: 0.00037173340722086425   Iteration 62 of 100, tot loss = 4.7348218579446115, l1: 0.00010166716005656898, l2: 0.00037181502486947145   Iteration 63 of 100, tot loss = 4.729233575245691, l1: 0.0001014749420666328, l2: 0.0003714484145535925   Iteration 64 of 100, tot loss = 4.730729296803474, l1: 0.00010120096499122155, l2: 0.0003718719638072798   Iteration 65 of 100, tot loss = 4.710132668568538, l1: 0.00010077309964869458, l2: 0.000370240166487817   Iteration 66 of 100, tot loss = 4.67322737159151, l1: 0.0001002460918574467, l2: 0.00036707664482740006   Iteration 67 of 100, tot loss = 4.689578323221919, l1: 0.00010097756951008298, l2: 0.00036798026295441357   Iteration 68 of 100, tot loss = 4.703177848282983, l1: 0.00010104338723655982, l2: 0.0003692743979199269   Iteration 69 of 100, tot loss = 4.743562273357226, l1: 0.00010136182306492971, l2: 0.00037299440494583297   Iteration 70 of 100, tot loss = 4.755398733275277, l1: 0.0001014789321093953, l2: 0.00037406094218437957   Iteration 71 of 100, tot loss = 4.747663494566797, l1: 0.00010149462642902377, l2: 0.0003732717237320506   Iteration 72 of 100, tot loss = 4.741724971267912, l1: 0.00010171377471124288, l2: 0.00037245872287409537   Iteration 73 of 100, tot loss = 4.740645366172268, l1: 0.00010172132807719994, l2: 0.0003723432088656708   Iteration 74 of 100, tot loss = 4.716652828293878, l1: 0.00010154054935146878, l2: 0.00037012473370162517   Iteration 75 of 100, tot loss = 4.736107428868611, l1: 0.00010191869475723554, l2: 0.0003716920483081291   Iteration 76 of 100, tot loss = 4.756535627340016, l1: 0.00010213070144297825, l2: 0.0003735228609385606   Iteration 77 of 100, tot loss = 4.763371018620281, l1: 0.00010237636510660727, l2: 0.00037396073646770865   Iteration 78 of 100, tot loss = 4.734803480979724, l1: 0.00010189245557949807, l2: 0.0003715878923778804   Iteration 79 of 100, tot loss = 4.718292308759086, l1: 0.00010120496485394108, l2: 0.00037062426574404434   Iteration 80 of 100, tot loss = 4.734396958351136, l1: 0.00010147873999812873, l2: 0.00037196095599938416   Iteration 81 of 100, tot loss = 4.745626738042008, l1: 0.00010182828043104598, l2: 0.0003727343934803718   Iteration 82 of 100, tot loss = 4.7501220179767145, l1: 0.00010219727491636238, l2: 0.0003728149268881236   Iteration 83 of 100, tot loss = 4.767507507140378, l1: 0.00010259457476751273, l2: 0.00037415617574090474   Iteration 84 of 100, tot loss = 4.771091171673366, l1: 0.00010285348564918553, l2: 0.0003742556310455603   Iteration 85 of 100, tot loss = 4.775352691201602, l1: 0.00010303682733244975, l2: 0.00037449844102786084   Iteration 86 of 100, tot loss = 4.800223084383233, l1: 0.00010343868915549327, l2: 0.00037658361864931406   Iteration 87 of 100, tot loss = 4.820271946918005, l1: 0.00010371889003108244, l2: 0.00037830830407837386   Iteration 88 of 100, tot loss = 4.79569768092849, l1: 0.00010327923858064672, l2: 0.0003762905287823576   Iteration 89 of 100, tot loss = 4.79233784889907, l1: 0.00010334231954177821, l2: 0.00037589146454133134   Iteration 90 of 100, tot loss = 4.799394747945997, l1: 0.00010351483813590474, l2: 0.0003764246355987982   Iteration 91 of 100, tot loss = 4.789376460588896, l1: 0.00010356264741218823, l2: 0.00037537499763977276   Iteration 92 of 100, tot loss = 4.784901144711868, l1: 0.0001033813678868287, l2: 0.0003751087454514598   Iteration 93 of 100, tot loss = 4.768007463024508, l1: 0.000102862486137285, l2: 0.0003739382590665432   Iteration 94 of 100, tot loss = 4.78505949263877, l1: 0.00010308525239413435, l2: 0.0003754206958779213   Iteration 95 of 100, tot loss = 4.78746795152363, l1: 0.00010277708401860621, l2: 0.00037596970995716557   Iteration 96 of 100, tot loss = 4.79397584994634, l1: 0.000103041347188082, l2: 0.0003763562370598568   Iteration 97 of 100, tot loss = 4.7963049534669855, l1: 0.000103062239093598, l2: 0.0003765682555127831   Iteration 98 of 100, tot loss = 4.799113015739286, l1: 0.00010301752460078688, l2: 0.00037689377599495595   Iteration 99 of 100, tot loss = 4.793772263960405, l1: 0.00010290440743597403, l2: 0.00037647281803374153   Iteration 100 of 100, tot loss = 4.806330318450928, l1: 0.00010300442711013601, l2: 0.00037762860345537776
   End of epoch 1343; saving model... 

Epoch 1344 of 2000
   Iteration 1 of 100, tot loss = 3.932100772857666, l1: 8.723625796847045e-05, l2: 0.00030597380828112364   Iteration 2 of 100, tot loss = 3.7605780363082886, l1: 9.06602836039383e-05, l2: 0.00028539751656353474   Iteration 3 of 100, tot loss = 4.220052321751912, l1: 0.00010770816152216867, l2: 0.0003142970672342926   Iteration 4 of 100, tot loss = 4.264338672161102, l1: 0.00010411379844299518, l2: 0.0003223200692445971   Iteration 5 of 100, tot loss = 4.611052751541138, l1: 0.00010415503493277356, l2: 0.0003569502325262874   Iteration 6 of 100, tot loss = 4.344330191612244, l1: 9.753087336624351e-05, l2: 0.00033690213846663636   Iteration 7 of 100, tot loss = 4.348493950707572, l1: 9.997300886815148e-05, l2: 0.00033487638159255894   Iteration 8 of 100, tot loss = 4.549416095018387, l1: 0.00010411010771349538, l2: 0.0003508315021463204   Iteration 9 of 100, tot loss = 4.517092201444838, l1: 0.00010558399743685086, l2: 0.00034612522318234877   Iteration 10 of 100, tot loss = 4.665982747077942, l1: 0.00010716575998230838, l2: 0.00035943251277785747   Iteration 11 of 100, tot loss = 4.7779457569122314, l1: 0.00010822980468881063, l2: 0.0003695647703187371   Iteration 12 of 100, tot loss = 4.744285762310028, l1: 0.0001082613889593631, l2: 0.0003661671847415467   Iteration 13 of 100, tot loss = 4.658943359668438, l1: 0.00010637078734222226, l2: 0.00035952354441038694   Iteration 14 of 100, tot loss = 4.84349114554269, l1: 0.00010740194660944066, l2: 0.00037694716177481623   Iteration 15 of 100, tot loss = 4.823813724517822, l1: 0.0001076644635759294, l2: 0.0003747169025397549   Iteration 16 of 100, tot loss = 4.816271007061005, l1: 0.00010948824092338327, l2: 0.000372138852981152   Iteration 17 of 100, tot loss = 4.829484799328973, l1: 0.00010978925488683778, l2: 0.00037315921851105113   Iteration 18 of 100, tot loss = 4.686934947967529, l1: 0.00010665439610117271, l2: 0.00036203909177048545   Iteration 19 of 100, tot loss = 4.717837785419665, l1: 0.00010622000742219633, l2: 0.0003655637671104877   Iteration 20 of 100, tot loss = 4.893547487258911, l1: 0.00010874236195377308, l2: 0.0003806123837421183   Iteration 21 of 100, tot loss = 4.861208461579823, l1: 0.00010804411067510955, l2: 0.0003780767320200712   Iteration 22 of 100, tot loss = 4.799026380885731, l1: 0.00010759173379814125, l2: 0.000372310901267073   Iteration 23 of 100, tot loss = 4.7078379755434785, l1: 0.00010590682121919994, l2: 0.000364876974186779   Iteration 24 of 100, tot loss = 4.69776185353597, l1: 0.00010475063527337625, l2: 0.00036502554758044425   Iteration 25 of 100, tot loss = 4.639895753860474, l1: 0.00010393864227808081, l2: 0.00036005093075800685   Iteration 26 of 100, tot loss = 4.655278196701636, l1: 0.00010492466544047392, l2: 0.00036060315268024657   Iteration 27 of 100, tot loss = 4.655478768878513, l1: 0.00010480063240349948, l2: 0.0003607472436097278   Iteration 28 of 100, tot loss = 4.684841845716749, l1: 0.00010609181884189769, l2: 0.00036239236370810043   Iteration 29 of 100, tot loss = 4.65349398810288, l1: 0.0001056898014714283, l2: 0.00035965959551535417   Iteration 30 of 100, tot loss = 4.617226092020671, l1: 0.00010477519354026298, l2: 0.0003569474138203077   Iteration 31 of 100, tot loss = 4.569329869362615, l1: 0.00010414554156839938, l2: 0.00035278744349676754   Iteration 32 of 100, tot loss = 4.602520518004894, l1: 0.00010532178964695049, l2: 0.00035493025916366605   Iteration 33 of 100, tot loss = 4.529594508084384, l1: 0.00010409524188479267, l2: 0.00034886420612703216   Iteration 34 of 100, tot loss = 4.537465263815487, l1: 0.00010480590796813725, l2: 0.0003489406149396125   Iteration 35 of 100, tot loss = 4.524860804421561, l1: 0.00010422497445168639, l2: 0.00034826110349968077   Iteration 36 of 100, tot loss = 4.5607600874370995, l1: 0.00010558054853188676, l2: 0.00035049545840593055   Iteration 37 of 100, tot loss = 4.683275493415627, l1: 0.00010741439935471233, l2: 0.00036091314757721047   Iteration 38 of 100, tot loss = 4.618621625398335, l1: 0.00010595080809588054, l2: 0.0003559113523040555   Iteration 39 of 100, tot loss = 4.597427239784827, l1: 0.00010564793158734504, l2: 0.00035409478975149494   Iteration 40 of 100, tot loss = 4.561852437257767, l1: 0.00010465496452525259, l2: 0.00035153027638443744   Iteration 41 of 100, tot loss = 4.5630227123818745, l1: 0.00010439396282890812, l2: 0.00035190830580753886   Iteration 42 of 100, tot loss = 4.576986307189578, l1: 0.00010473379647702954, l2: 0.00035296483214811554   Iteration 43 of 100, tot loss = 4.5774923091711, l1: 0.00010478317342985551, l2: 0.00035296605566928035   Iteration 44 of 100, tot loss = 4.589363547888669, l1: 0.00010451022800142792, l2: 0.00035442612517561594   Iteration 45 of 100, tot loss = 4.628742021984524, l1: 0.00010508027755551868, l2: 0.0003577939228206459   Iteration 46 of 100, tot loss = 4.6570044652275415, l1: 0.00010521874608694698, l2: 0.00036048169852659595   Iteration 47 of 100, tot loss = 4.716810231513166, l1: 0.0001062577153169649, l2: 0.0003654233054901929   Iteration 48 of 100, tot loss = 4.7035019646088285, l1: 0.00010637031482474413, l2: 0.0003639798796939431   Iteration 49 of 100, tot loss = 4.683409024258049, l1: 0.00010601394810970417, l2: 0.00036232695292339335   Iteration 50 of 100, tot loss = 4.706410574913025, l1: 0.00010615542269079014, l2: 0.00036448563332669437   Iteration 51 of 100, tot loss = 4.695033536237829, l1: 0.0001063268881647245, l2: 0.00036317646428120923   Iteration 52 of 100, tot loss = 4.674004806922032, l1: 0.00010584764011302748, l2: 0.00036155283921219123   Iteration 53 of 100, tot loss = 4.6789636926830935, l1: 0.00010569557316386017, l2: 0.000362200794833765   Iteration 54 of 100, tot loss = 4.694589469167921, l1: 0.00010610775899732727, l2: 0.000363351187034924   Iteration 55 of 100, tot loss = 4.710989704999057, l1: 0.00010620798804500902, l2: 0.00036489098195240577   Iteration 56 of 100, tot loss = 4.6882264869553705, l1: 0.00010623402444512717, l2: 0.000362588623569796   Iteration 57 of 100, tot loss = 4.709345658620198, l1: 0.00010684671532975412, l2: 0.00036408785008621125   Iteration 58 of 100, tot loss = 4.692684461330545, l1: 0.00010653462771282145, l2: 0.0003627338178218985   Iteration 59 of 100, tot loss = 4.6835322137606346, l1: 0.0001063978258062596, l2: 0.00036195539489538426   Iteration 60 of 100, tot loss = 4.674851322174073, l1: 0.00010589522959586854, l2: 0.0003615899018768687   Iteration 61 of 100, tot loss = 4.683148970369433, l1: 0.00010601498583068338, l2: 0.000362299910611015   Iteration 62 of 100, tot loss = 4.641927942152946, l1: 0.00010546816338319331, l2: 0.00035872463007352405   Iteration 63 of 100, tot loss = 4.635802412789966, l1: 0.00010569407667000113, l2: 0.00035788616376175055   Iteration 64 of 100, tot loss = 4.660179562866688, l1: 0.000106081341641584, l2: 0.0003599366136768367   Iteration 65 of 100, tot loss = 4.654367696321928, l1: 0.00010588777256019127, l2: 0.00035954899593399696   Iteration 66 of 100, tot loss = 4.619593002579429, l1: 0.00010523085460811149, l2: 0.00035672844453735956   Iteration 67 of 100, tot loss = 4.623162451075085, l1: 0.00010526097484311756, l2: 0.0003570552692494925   Iteration 68 of 100, tot loss = 4.613703524365144, l1: 0.00010518876286583049, l2: 0.00035618158867959315   Iteration 69 of 100, tot loss = 4.602262334547181, l1: 0.00010510038966616696, l2: 0.00035512584292809004   Iteration 70 of 100, tot loss = 4.621783014706203, l1: 0.0001052838505919291, l2: 0.00035689445025387353   Iteration 71 of 100, tot loss = 4.597194584322647, l1: 0.00010443371830263805, l2: 0.00035528573933103395   Iteration 72 of 100, tot loss = 4.5653455555438995, l1: 0.00010396522283271124, l2: 0.00035256933167854894   Iteration 73 of 100, tot loss = 4.5559025855913555, l1: 0.00010324888129093468, l2: 0.0003523413765272891   Iteration 74 of 100, tot loss = 4.536459597381386, l1: 0.00010289745979527062, l2: 0.0003507484991034191   Iteration 75 of 100, tot loss = 4.54346256573995, l1: 0.00010292195870230595, l2: 0.00035142429716264207   Iteration 76 of 100, tot loss = 4.565948558481116, l1: 0.0001033845370617009, l2: 0.0003532103177061991   Iteration 77 of 100, tot loss = 4.560651045340996, l1: 0.00010344918955997383, l2: 0.00035261591388420624   Iteration 78 of 100, tot loss = 4.553738297560276, l1: 0.0001029312828531823, l2: 0.0003524425458939125   Iteration 79 of 100, tot loss = 4.5124400356147865, l1: 0.00010202504743091007, l2: 0.00034921895509992454   Iteration 80 of 100, tot loss = 4.510409206151962, l1: 0.00010232262502540834, l2: 0.0003487182946628309   Iteration 81 of 100, tot loss = 4.510364420620012, l1: 0.00010238451445561454, l2: 0.0003486519270176988   Iteration 82 of 100, tot loss = 4.515875037123517, l1: 0.00010262379705303961, l2: 0.0003489637060627546   Iteration 83 of 100, tot loss = 4.5525445133806715, l1: 0.00010311648832404066, l2: 0.00035213796279522173   Iteration 84 of 100, tot loss = 4.589385515167599, l1: 0.00010349867997796363, l2: 0.0003554398712627257   Iteration 85 of 100, tot loss = 4.5914787516874425, l1: 0.00010371113106991876, l2: 0.00035543674346306085   Iteration 86 of 100, tot loss = 4.5847054913986565, l1: 0.0001035055528370512, l2: 0.0003549649952409984   Iteration 87 of 100, tot loss = 4.591380064514862, l1: 0.00010390742849183921, l2: 0.00035523057660360915   Iteration 88 of 100, tot loss = 4.625478251413866, l1: 0.00010448563543005466, l2: 0.00035806218825614036   Iteration 89 of 100, tot loss = 4.658138393016344, l1: 0.00010484295489732176, l2: 0.00036097088291733577   Iteration 90 of 100, tot loss = 4.681792402267456, l1: 0.00010526044497964904, l2: 0.0003629187941745234   Iteration 91 of 100, tot loss = 4.665365918652042, l1: 0.00010465872465219646, l2: 0.0003618778659933973   Iteration 92 of 100, tot loss = 4.722015772176825, l1: 0.00010545389068283839, l2: 0.0003667476850513713   Iteration 93 of 100, tot loss = 4.721757137647239, l1: 0.00010533515923006851, l2: 0.00036684055302557483   Iteration 94 of 100, tot loss = 4.731365622358119, l1: 0.00010548677903685491, l2: 0.00036764978211338415   Iteration 95 of 100, tot loss = 4.713494883085552, l1: 0.00010518356510529932, l2: 0.0003661659221458984   Iteration 96 of 100, tot loss = 4.775128573179245, l1: 0.00010613020924665761, l2: 0.0003713826469417351   Iteration 97 of 100, tot loss = 4.7679871333014106, l1: 0.00010599925334429724, l2: 0.0003707994591113487   Iteration 98 of 100, tot loss = 4.782003256739403, l1: 0.00010626793631004426, l2: 0.0003719323885517803   Iteration 99 of 100, tot loss = 4.773372633288605, l1: 0.00010609665020243171, l2: 0.00037124061238519247   Iteration 100 of 100, tot loss = 4.775054094791412, l1: 0.00010637369141477392, l2: 0.00037113171769306066
   End of epoch 1344; saving model... 

Epoch 1345 of 2000
   Iteration 1 of 100, tot loss = 4.269809722900391, l1: 0.00011629467917373404, l2: 0.0003106862714048475   Iteration 2 of 100, tot loss = 5.249774932861328, l1: 0.00013090926586301066, l2: 0.00039406822179444134   Iteration 3 of 100, tot loss = 4.805065631866455, l1: 0.00011848582168264936, l2: 0.0003620207426138222   Iteration 4 of 100, tot loss = 4.725533127784729, l1: 0.00010811305219249334, l2: 0.0003644402604550123   Iteration 5 of 100, tot loss = 4.68311824798584, l1: 0.0001072129569365643, l2: 0.000361098867142573   Iteration 6 of 100, tot loss = 4.6823015213012695, l1: 0.0001061603870766703, l2: 0.0003620697680162266   Iteration 7 of 100, tot loss = 4.7898028918675015, l1: 0.00010703022936857971, l2: 0.0003719500590315355   Iteration 8 of 100, tot loss = 4.698097109794617, l1: 0.00010369378560426412, l2: 0.0003661159244074952   Iteration 9 of 100, tot loss = 4.829793400234646, l1: 0.00010782542408883778, l2: 0.00037515391401636106   Iteration 10 of 100, tot loss = 4.869039154052734, l1: 0.00011053417911170982, l2: 0.00037636973429471253   Iteration 11 of 100, tot loss = 4.905897053805265, l1: 0.00010900257753250612, l2: 0.00038158712612295693   Iteration 12 of 100, tot loss = 4.693808754285176, l1: 0.0001054138289570498, l2: 0.00036396704429838184   Iteration 13 of 100, tot loss = 4.6844814740694485, l1: 0.00010557650686958089, l2: 0.0003628716401111048   Iteration 14 of 100, tot loss = 4.896982942308698, l1: 0.00011024733352574653, l2: 0.000379450960151319   Iteration 15 of 100, tot loss = 4.942281723022461, l1: 0.00011229410495919487, l2: 0.000381934066535905   Iteration 16 of 100, tot loss = 4.845718324184418, l1: 0.00011059405187552329, l2: 0.00037397778032755014   Iteration 17 of 100, tot loss = 4.819635699777042, l1: 0.00011003319035380093, l2: 0.00037193037946160664   Iteration 18 of 100, tot loss = 4.781517055299547, l1: 0.00010828198679114899, l2: 0.0003698697190783504   Iteration 19 of 100, tot loss = 4.944846981450131, l1: 0.00011123614776328108, l2: 0.00038324855268001556   Iteration 20 of 100, tot loss = 4.85654845237732, l1: 0.00010974550059472676, l2: 0.0003759093458938878   Iteration 21 of 100, tot loss = 4.745678981145223, l1: 0.00010794738600019454, l2: 0.0003666205136271726   Iteration 22 of 100, tot loss = 4.729981433261525, l1: 0.00010783093867675316, l2: 0.0003651672060104002   Iteration 23 of 100, tot loss = 4.680644968281621, l1: 0.00010688939389934683, l2: 0.0003611751048795312   Iteration 24 of 100, tot loss = 4.677395204703013, l1: 0.0001066988209762106, l2: 0.000361040700833352   Iteration 25 of 100, tot loss = 4.701887950897217, l1: 0.00010657440288923681, l2: 0.0003636143921175972   Iteration 26 of 100, tot loss = 4.658989832951472, l1: 0.00010568001884026811, l2: 0.0003602189642081682   Iteration 27 of 100, tot loss = 4.6482039910775645, l1: 0.00010559060427584443, l2: 0.00035922979360394593   Iteration 28 of 100, tot loss = 4.677295225007193, l1: 0.00010608198291655364, l2: 0.0003616475395184742   Iteration 29 of 100, tot loss = 4.675599114648227, l1: 0.00010660468878682125, l2: 0.0003609552227787609   Iteration 30 of 100, tot loss = 4.70812824567159, l1: 0.00010687250032788142, l2: 0.0003639403240716395   Iteration 31 of 100, tot loss = 4.698964303539645, l1: 0.00010641218203442892, l2: 0.00036348424891325374   Iteration 32 of 100, tot loss = 4.663887657225132, l1: 0.00010576902923276066, l2: 0.0003606197365115804   Iteration 33 of 100, tot loss = 4.707544146162091, l1: 0.00010622678382787853, l2: 0.0003645276314769448   Iteration 34 of 100, tot loss = 4.752843597356011, l1: 0.0001071116592570701, l2: 0.00036817270130781894   Iteration 35 of 100, tot loss = 4.737224531173706, l1: 0.00010745759874615552, l2: 0.00036626485484053515   Iteration 36 of 100, tot loss = 4.75356611278322, l1: 0.0001077130689534695, l2: 0.0003676435416208632   Iteration 37 of 100, tot loss = 4.744524511131081, l1: 0.00010685706640927222, l2: 0.00036759538351042147   Iteration 38 of 100, tot loss = 4.721310766119706, l1: 0.00010675499097009091, l2: 0.00036537608454972014   Iteration 39 of 100, tot loss = 4.6973856412447414, l1: 0.00010609407148037392, l2: 0.00036364449120950524   Iteration 40 of 100, tot loss = 4.6774060249328615, l1: 0.00010542034942773171, l2: 0.00036232025195204187   Iteration 41 of 100, tot loss = 4.7497693038568265, l1: 0.00010683860455173999, l2: 0.00036813832412008196   Iteration 42 of 100, tot loss = 4.782856861750285, l1: 0.00010736427724450117, l2: 0.00037092140735781173   Iteration 43 of 100, tot loss = 4.852376826973849, l1: 0.00010828868056874896, l2: 0.00037694899981224173   Iteration 44 of 100, tot loss = 4.849948861382225, l1: 0.00010806530818544243, l2: 0.0003769295756435233   Iteration 45 of 100, tot loss = 4.819927162594265, l1: 0.00010731595814124577, l2: 0.000374676756392647   Iteration 46 of 100, tot loss = 4.82881091988605, l1: 0.00010725640965139736, l2: 0.00037562468060059473   Iteration 47 of 100, tot loss = 4.829289507358633, l1: 0.00010743991693511526, l2: 0.00037548903213625656   Iteration 48 of 100, tot loss = 4.831787993510564, l1: 0.00010806861306870512, l2: 0.0003751101852079349   Iteration 49 of 100, tot loss = 4.838230901834916, l1: 0.00010778313358338094, l2: 0.00037603995528746854   Iteration 50 of 100, tot loss = 4.868748426437378, l1: 0.00010813565153512172, l2: 0.0003787391897640191   Iteration 51 of 100, tot loss = 4.8280355369343475, l1: 0.00010725873360962735, l2: 0.00037554481897649226   Iteration 52 of 100, tot loss = 4.8635302644509535, l1: 0.0001081501089244669, l2: 0.0003782029167408697   Iteration 53 of 100, tot loss = 4.904770819646008, l1: 0.00010818163856077622, l2: 0.0003822954428680944   Iteration 54 of 100, tot loss = 4.8958922006465775, l1: 0.00010833001956429884, l2: 0.00038125919969321265   Iteration 55 of 100, tot loss = 4.88082669431513, l1: 0.00010812953379089859, l2: 0.00037995313442396847   Iteration 56 of 100, tot loss = 4.868612063782556, l1: 0.00010796866990858689, l2: 0.000378892535601543   Iteration 57 of 100, tot loss = 4.832222629011723, l1: 0.0001076044002394682, l2: 0.0003756178614965297   Iteration 58 of 100, tot loss = 4.861809911399052, l1: 0.0001078790681213863, l2: 0.00037830192239449265   Iteration 59 of 100, tot loss = 4.850365889274467, l1: 0.00010732221673637376, l2: 0.0003777143712672484   Iteration 60 of 100, tot loss = 4.833940732479095, l1: 0.00010669614603102673, l2: 0.000376697926306709   Iteration 61 of 100, tot loss = 4.862351335462977, l1: 0.00010669392452014183, l2: 0.000379541208232814   Iteration 62 of 100, tot loss = 4.846939813706182, l1: 0.00010619670701412213, l2: 0.0003784972737306699   Iteration 63 of 100, tot loss = 4.826384457330855, l1: 0.00010575968028408372, l2: 0.00037687876477046677   Iteration 64 of 100, tot loss = 4.819498438388109, l1: 0.00010595282287795271, l2: 0.0003759970202281693   Iteration 65 of 100, tot loss = 4.791037387114304, l1: 0.00010565825460407023, l2: 0.00037344548349770213   Iteration 66 of 100, tot loss = 4.828784686146361, l1: 0.00010625657578753872, l2: 0.0003766218927039793   Iteration 67 of 100, tot loss = 4.790861944654095, l1: 0.00010578411543540267, l2: 0.0003733020788406147   Iteration 68 of 100, tot loss = 4.783001321203568, l1: 0.00010599976196638582, l2: 0.0003723003700382405   Iteration 69 of 100, tot loss = 4.8219340400419375, l1: 0.00010676774895662253, l2: 0.0003754256552159516   Iteration 70 of 100, tot loss = 4.844888186454773, l1: 0.00010739863667237971, l2: 0.00037709018173960173   Iteration 71 of 100, tot loss = 4.840584724721774, l1: 0.00010735559669523304, l2: 0.00037670287534817526   Iteration 72 of 100, tot loss = 4.825981871949302, l1: 0.0001072088523667642, l2: 0.0003753893344967057   Iteration 73 of 100, tot loss = 4.849709612049469, l1: 0.0001074082209339544, l2: 0.0003775627402531315   Iteration 74 of 100, tot loss = 4.834536813400887, l1: 0.00010742872935323346, l2: 0.0003760249516769388   Iteration 75 of 100, tot loss = 4.8279238859812414, l1: 0.00010719383979449049, l2: 0.0003755985483682404   Iteration 76 of 100, tot loss = 4.8436169341990825, l1: 0.00010759293400042225, l2: 0.0003767687590688614   Iteration 77 of 100, tot loss = 4.855447276846155, l1: 0.00010777872432606875, l2: 0.0003777660025803784   Iteration 78 of 100, tot loss = 4.868242126244765, l1: 0.00010802917499975779, l2: 0.0003787950371565202   Iteration 79 of 100, tot loss = 4.872598548478718, l1: 0.0001080658943856524, l2: 0.000379193960276397   Iteration 80 of 100, tot loss = 4.885038384795189, l1: 0.00010805779429574613, l2: 0.0003804460444371216   Iteration 81 of 100, tot loss = 4.895752115014159, l1: 0.0001080586355076068, l2: 0.0003815165761840978   Iteration 82 of 100, tot loss = 4.872588649028685, l1: 0.00010738637639605724, l2: 0.0003798724888261726   Iteration 83 of 100, tot loss = 4.887726372983082, l1: 0.00010769910554097213, l2: 0.0003810735326120069   Iteration 84 of 100, tot loss = 4.873332588445573, l1: 0.00010753909585522099, l2: 0.00037979416395371245   Iteration 85 of 100, tot loss = 4.882559195686789, l1: 0.00010770149670219433, l2: 0.0003805544241057599   Iteration 86 of 100, tot loss = 4.885547463283983, l1: 0.00010759151324279183, l2: 0.00038096323463156126   Iteration 87 of 100, tot loss = 4.870460315682422, l1: 0.00010753805695092252, l2: 0.00037950797584550137   Iteration 88 of 100, tot loss = 4.854541125622663, l1: 0.00010747401612124204, l2: 0.00037798009758592923   Iteration 89 of 100, tot loss = 4.853594675492705, l1: 0.00010755067117142051, l2: 0.0003778087972595325   Iteration 90 of 100, tot loss = 4.844986504978603, l1: 0.0001074884267533586, l2: 0.00037701022455520516   Iteration 91 of 100, tot loss = 4.823146288211529, l1: 0.00010724398457019201, l2: 0.00037507064497241605   Iteration 92 of 100, tot loss = 4.808850231377975, l1: 0.00010697716357833048, l2: 0.0003739078605102132   Iteration 93 of 100, tot loss = 4.824976921081543, l1: 0.00010736793387144233, l2: 0.0003751297590888596   Iteration 94 of 100, tot loss = 4.8012683391571045, l1: 0.00010676019678128873, l2: 0.0003733666380867362   Iteration 95 of 100, tot loss = 4.821149078168367, l1: 0.00010730524721255182, l2: 0.0003748096617575931   Iteration 96 of 100, tot loss = 4.810812513033549, l1: 0.00010726721457861761, l2: 0.0003738140379330919   Iteration 97 of 100, tot loss = 4.8004908905815835, l1: 0.00010712259354102233, l2: 0.0003729264966581862   Iteration 98 of 100, tot loss = 4.789227110998971, l1: 0.00010711397471266314, l2: 0.0003718087373643505   Iteration 99 of 100, tot loss = 4.776758104863793, l1: 0.0001071123793239663, l2: 0.0003705634321015554   Iteration 100 of 100, tot loss = 4.776024277210236, l1: 0.00010708413337852107, l2: 0.00037051829509437083
   End of epoch 1345; saving model... 

Epoch 1346 of 2000
   Iteration 1 of 100, tot loss = 5.326137065887451, l1: 0.0001226174645125866, l2: 0.0004099962825421244   Iteration 2 of 100, tot loss = 4.75260066986084, l1: 0.00011390013969503343, l2: 0.0003613599401433021   Iteration 3 of 100, tot loss = 4.317936340967814, l1: 9.324982966063544e-05, l2: 0.00033854381763376296   Iteration 4 of 100, tot loss = 4.052992939949036, l1: 9.229741772287525e-05, l2: 0.00031300188493332826   Iteration 5 of 100, tot loss = 3.958927536010742, l1: 9.221660147886723e-05, l2: 0.00030367615690920504   Iteration 6 of 100, tot loss = 4.294944445292155, l1: 9.574682189850137e-05, l2: 0.0003337476300657727   Iteration 7 of 100, tot loss = 4.464785575866699, l1: 0.00010136233010728444, l2: 0.00034511622964471044   Iteration 8 of 100, tot loss = 4.566702723503113, l1: 0.00010177040439884877, l2: 0.0003548998738551745   Iteration 9 of 100, tot loss = 4.746480941772461, l1: 0.00010337349764692287, l2: 0.00037127459816272475   Iteration 10 of 100, tot loss = 4.832700967788696, l1: 0.00010309085482731461, l2: 0.0003801792438025586   Iteration 11 of 100, tot loss = 4.720898389816284, l1: 0.00010156721691600978, l2: 0.0003705226236806166   Iteration 12 of 100, tot loss = 5.001183450222015, l1: 0.00010517439295654185, l2: 0.00039494395241490565   Iteration 13 of 100, tot loss = 4.949016736103938, l1: 0.00010534057778736147, l2: 0.0003895610940302364   Iteration 14 of 100, tot loss = 4.855419210025242, l1: 0.00010412795121997728, l2: 0.0003814139688204575   Iteration 15 of 100, tot loss = 4.8057626247406, l1: 0.00010135866614291444, l2: 0.0003792175955216711   Iteration 16 of 100, tot loss = 4.861333683133125, l1: 0.00010230297129965038, l2: 0.0003838303955490119   Iteration 17 of 100, tot loss = 4.835666586371029, l1: 0.00010189143957926289, l2: 0.00038167521929340987   Iteration 18 of 100, tot loss = 4.859386881192525, l1: 0.00010302742965804, l2: 0.0003829112594960154   Iteration 19 of 100, tot loss = 4.871153718546817, l1: 0.00010329154879161108, l2: 0.0003838238243540553   Iteration 20 of 100, tot loss = 4.992456471920013, l1: 0.00010624241731420625, l2: 0.00039300323041970844   Iteration 21 of 100, tot loss = 5.073390903927031, l1: 0.00010817101428983733, l2: 0.00039916807561114965   Iteration 22 of 100, tot loss = 4.962354790080678, l1: 0.00010568852617738726, l2: 0.0003905469513318332   Iteration 23 of 100, tot loss = 4.914185555084892, l1: 0.00010468660959462716, l2: 0.0003867319452262524   Iteration 24 of 100, tot loss = 4.904366721709569, l1: 0.0001048444329777946, l2: 0.00038559223867196124   Iteration 25 of 100, tot loss = 4.947843046188354, l1: 0.0001059819913643878, l2: 0.0003888023126637563   Iteration 26 of 100, tot loss = 5.045539590028616, l1: 0.00010736020773877569, l2: 0.0003971937509889428   Iteration 27 of 100, tot loss = 5.047185853675559, l1: 0.00010677517585227852, l2: 0.0003979434081379117   Iteration 28 of 100, tot loss = 5.13753833941051, l1: 0.00010895151185000682, l2: 0.000404802320969923   Iteration 29 of 100, tot loss = 5.207307034525378, l1: 0.00010989464762729015, l2: 0.0004108360550037168   Iteration 30 of 100, tot loss = 5.139546529452006, l1: 0.00010842280159219323, l2: 0.0004055318505076381   Iteration 31 of 100, tot loss = 5.158308359884447, l1: 0.00010847154501994561, l2: 0.0004073592900831793   Iteration 32 of 100, tot loss = 5.1095224022865295, l1: 0.00010748258102921682, l2: 0.00040346965897697373   Iteration 33 of 100, tot loss = 5.1328024719700664, l1: 0.00010812803884618916, l2: 0.000405152207928371   Iteration 34 of 100, tot loss = 5.157421771217795, l1: 0.00010862457491063705, l2: 0.00040711760169977103   Iteration 35 of 100, tot loss = 5.163691970280238, l1: 0.00010844793359865434, l2: 0.0004079212621685916   Iteration 36 of 100, tot loss = 5.125080234474606, l1: 0.00010835001527690717, l2: 0.0004041580068587791   Iteration 37 of 100, tot loss = 5.134845972061157, l1: 0.00010871431663257976, l2: 0.00040477027957692643   Iteration 38 of 100, tot loss = 5.117267489433289, l1: 0.00010880928130063694, l2: 0.00040291746699949726   Iteration 39 of 100, tot loss = 5.076108180559599, l1: 0.00010843296788665705, l2: 0.00039917784959424095   Iteration 40 of 100, tot loss = 5.017694532871246, l1: 0.00010747528313004295, l2: 0.00039429416974599005   Iteration 41 of 100, tot loss = 4.945713287446557, l1: 0.0001061611685322643, l2: 0.0003884101599574135   Iteration 42 of 100, tot loss = 4.974157129015241, l1: 0.00010650945495442665, l2: 0.000390906257269394   Iteration 43 of 100, tot loss = 4.96790025400561, l1: 0.00010579382120171388, l2: 0.00039099620318116054   Iteration 44 of 100, tot loss = 4.992240060459483, l1: 0.00010613891324531075, l2: 0.0003930850914250848   Iteration 45 of 100, tot loss = 4.988462469312879, l1: 0.00010670977272740048, l2: 0.00039213647283354983   Iteration 46 of 100, tot loss = 4.963455744411634, l1: 0.00010624187938367908, l2: 0.0003901036938971273   Iteration 47 of 100, tot loss = 4.937994175768913, l1: 0.000106244450162581, l2: 0.0003875549662823571   Iteration 48 of 100, tot loss = 4.890613327423732, l1: 0.0001052730281874877, l2: 0.000383788303527884   Iteration 49 of 100, tot loss = 4.8323705634292295, l1: 0.00010429627077335643, l2: 0.0003789407842583498   Iteration 50 of 100, tot loss = 4.844039964675903, l1: 0.00010366551752667874, l2: 0.00038073847710620615   Iteration 51 of 100, tot loss = 4.8697238622927195, l1: 0.00010429880029394054, l2: 0.0003826735848241358   Iteration 52 of 100, tot loss = 4.867624805523799, l1: 0.00010441494976895719, l2: 0.0003823475294316617   Iteration 53 of 100, tot loss = 4.869031438287699, l1: 0.0001043296130242283, l2: 0.00038257352886985073   Iteration 54 of 100, tot loss = 4.8472314145829944, l1: 0.00010380827316785817, l2: 0.00038091486634652094   Iteration 55 of 100, tot loss = 4.85972917730158, l1: 0.00010407074719709768, l2: 0.0003819021687377244   Iteration 56 of 100, tot loss = 4.832149471555438, l1: 0.00010377035999096864, l2: 0.00037944458537302647   Iteration 57 of 100, tot loss = 4.781645864771123, l1: 0.00010259286411159688, l2: 0.0003755717206921984   Iteration 58 of 100, tot loss = 4.749161352371347, l1: 0.00010207132624506405, l2: 0.00037284480715933226   Iteration 59 of 100, tot loss = 4.737895951432697, l1: 0.00010167110007911352, l2: 0.0003721184929402671   Iteration 60 of 100, tot loss = 4.748674116532007, l1: 0.0001019703862160289, l2: 0.000372897022801529   Iteration 61 of 100, tot loss = 4.723832980531161, l1: 0.00010158299047644937, l2: 0.00037080030501858316   Iteration 62 of 100, tot loss = 4.729829778594356, l1: 0.00010161105467798939, l2: 0.0003713719199896009   Iteration 63 of 100, tot loss = 4.785099050355336, l1: 0.00010222090754218193, l2: 0.00037628899458684914   Iteration 64 of 100, tot loss = 4.761734100058675, l1: 0.00010180382139424182, l2: 0.00037436958587022673   Iteration 65 of 100, tot loss = 4.7681991742207455, l1: 0.00010183242478748211, l2: 0.0003749874897319107   Iteration 66 of 100, tot loss = 4.724972177635539, l1: 0.0001008939939448751, l2: 0.00037160322090406254   Iteration 67 of 100, tot loss = 4.684877610918301, l1: 0.00010029280807131401, l2: 0.0003681949504004764   Iteration 68 of 100, tot loss = 4.697486556628171, l1: 0.00010089930230392115, l2: 0.0003688493508140228   Iteration 69 of 100, tot loss = 4.6886779553648354, l1: 0.0001005484138158204, l2: 0.000368319378838456   Iteration 70 of 100, tot loss = 4.687654306207384, l1: 0.00010055978553802041, l2: 0.0003682056425272354   Iteration 71 of 100, tot loss = 4.6796255665765685, l1: 0.00010072545246401487, l2: 0.00036723710166972496   Iteration 72 of 100, tot loss = 4.670780170294973, l1: 0.00010062358685697998, l2: 0.00036645442742155865   Iteration 73 of 100, tot loss = 4.684353405482148, l1: 0.00010096040464242384, l2: 0.0003674749336014055   Iteration 74 of 100, tot loss = 4.69689643866307, l1: 0.00010122687868912376, l2: 0.0003684627633963435   Iteration 75 of 100, tot loss = 4.7133623266220095, l1: 0.0001012850496529912, l2: 0.0003700511813318978   Iteration 76 of 100, tot loss = 4.695669571035786, l1: 0.00010103255475482193, l2: 0.0003685344004464385   Iteration 77 of 100, tot loss = 4.695846150447796, l1: 0.00010124083182036683, l2: 0.0003683437814772758   Iteration 78 of 100, tot loss = 4.68426092618551, l1: 0.0001007263248865433, l2: 0.00036769976596145006   Iteration 79 of 100, tot loss = 4.703159732154653, l1: 0.0001011595705227891, l2: 0.0003691564009766554   Iteration 80 of 100, tot loss = 4.721272899210453, l1: 0.00010164248096771189, l2: 0.00037048480735393243   Iteration 81 of 100, tot loss = 4.707633334913371, l1: 0.0001016950898050664, l2: 0.0003690682419857447   Iteration 82 of 100, tot loss = 4.7132268577087215, l1: 0.00010164223558825963, l2: 0.00036968044913941766   Iteration 83 of 100, tot loss = 4.7158993396414335, l1: 0.00010182269814168944, l2: 0.0003697672348289964   Iteration 84 of 100, tot loss = 4.722004037527811, l1: 0.0001021146673294771, l2: 0.000370085735907889   Iteration 85 of 100, tot loss = 4.698756241798401, l1: 0.00010164247034802375, l2: 0.00036823315330205815   Iteration 86 of 100, tot loss = 4.701921913512917, l1: 0.00010170779865421896, l2: 0.00036848439218416853   Iteration 87 of 100, tot loss = 4.711360960171141, l1: 0.00010165533812946758, l2: 0.00036948075703339885   Iteration 88 of 100, tot loss = 4.700130125338381, l1: 0.00010145383401488124, l2: 0.0003685591779106868   Iteration 89 of 100, tot loss = 4.678709693169326, l1: 0.00010079749053954983, l2: 0.000367073478076471   Iteration 90 of 100, tot loss = 4.652270143561893, l1: 0.00010038956546244056, l2: 0.00036483744819027685   Iteration 91 of 100, tot loss = 4.660422824241302, l1: 0.00010050294000833234, l2: 0.00036553934134624815   Iteration 92 of 100, tot loss = 4.64789203327635, l1: 0.00010059341947572133, l2: 0.000364195782901294   Iteration 93 of 100, tot loss = 4.627644081269541, l1: 0.00010030858221955808, l2: 0.00036245582485070794   Iteration 94 of 100, tot loss = 4.643161823140814, l1: 0.00010041911039653759, l2: 0.0003638970708979809   Iteration 95 of 100, tot loss = 4.638902708103783, l1: 0.00010041571762198337, l2: 0.00036347455185789025   Iteration 96 of 100, tot loss = 4.624398697167635, l1: 9.997632575201958e-05, l2: 0.0003624635428423062   Iteration 97 of 100, tot loss = 4.607460264078121, l1: 9.979933833933829e-05, l2: 0.0003609466872036399   Iteration 98 of 100, tot loss = 4.595029236102591, l1: 9.933181406397904e-05, l2: 0.00036017110880716153   Iteration 99 of 100, tot loss = 4.585724959469805, l1: 9.932301195063205e-05, l2: 0.0003592494830801951   Iteration 100 of 100, tot loss = 4.57424576163292, l1: 9.938420131220482e-05, l2: 0.00035804037397610953
   End of epoch 1346; saving model... 

Epoch 1347 of 2000
   Iteration 1 of 100, tot loss = 6.930454254150391, l1: 0.00013288606714922935, l2: 0.0005601593875326216   Iteration 2 of 100, tot loss = 5.009987711906433, l1: 0.00010070629286929034, l2: 0.00040029249794315547   Iteration 3 of 100, tot loss = 5.444896141688029, l1: 0.00011145728300713624, l2: 0.0004330323329971482   Iteration 4 of 100, tot loss = 5.305153787136078, l1: 0.00010652576020220295, l2: 0.00042398961522849277   Iteration 5 of 100, tot loss = 5.667572736740112, l1: 0.00011008446454070508, l2: 0.0004566728079225868   Iteration 6 of 100, tot loss = 5.290796756744385, l1: 0.00010443160135764629, l2: 0.0004246480724153419   Iteration 7 of 100, tot loss = 5.161961896078927, l1: 0.00010735539087493504, l2: 0.000408840793949951   Iteration 8 of 100, tot loss = 5.089046835899353, l1: 0.00010807418948388658, l2: 0.0004008304895251058   Iteration 9 of 100, tot loss = 5.307886547512478, l1: 0.00011206929856497381, l2: 0.00041871935698307224   Iteration 10 of 100, tot loss = 5.210382890701294, l1: 0.00011038612356060185, l2: 0.00041065216646529734   Iteration 11 of 100, tot loss = 5.404931588606401, l1: 0.00011457586383171888, l2: 0.0004259172975170341   Iteration 12 of 100, tot loss = 5.509998639424642, l1: 0.00011563036423467565, l2: 0.00043536950640069943   Iteration 13 of 100, tot loss = 5.505288674281194, l1: 0.00011409299790662211, l2: 0.00043643587447989447   Iteration 14 of 100, tot loss = 5.407794645854405, l1: 0.00011293390655607385, l2: 0.0004278455619766776   Iteration 15 of 100, tot loss = 5.166655826568603, l1: 0.00010888774146830352, l2: 0.0004077778450058152   Iteration 16 of 100, tot loss = 5.087460786104202, l1: 0.00010727831499934837, l2: 0.0004014677670056699   Iteration 17 of 100, tot loss = 4.975276428110459, l1: 0.00010577294746968512, l2: 0.0003917546975357896   Iteration 18 of 100, tot loss = 4.885269443194072, l1: 0.00010403954871031197, l2: 0.0003844873984538329   Iteration 19 of 100, tot loss = 5.020136318708721, l1: 0.00010692723586352689, l2: 0.0003950863970921522   Iteration 20 of 100, tot loss = 5.03964022397995, l1: 0.00010675710382201942, l2: 0.00039720691929687744   Iteration 21 of 100, tot loss = 4.903810784930275, l1: 0.00010445341502504779, l2: 0.0003859276642158095   Iteration 22 of 100, tot loss = 4.888261935927651, l1: 0.00010411909633231434, l2: 0.0003847070977578617   Iteration 23 of 100, tot loss = 4.773937536322552, l1: 0.00010168666536312388, l2: 0.00037570708841287893   Iteration 24 of 100, tot loss = 4.871763984362285, l1: 0.00010410642183463399, l2: 0.0003830699776396311   Iteration 25 of 100, tot loss = 4.82676103591919, l1: 0.00010292511928128078, l2: 0.00037975098588503896   Iteration 26 of 100, tot loss = 4.7207277187934285, l1: 0.00010056815979344089, l2: 0.0003715046130729696   Iteration 27 of 100, tot loss = 4.671487207765932, l1: 0.00010029210089770559, l2: 0.0003668566210695577   Iteration 28 of 100, tot loss = 4.6627660138266425, l1: 0.00010007759673109311, l2: 0.00036619900596893525   Iteration 29 of 100, tot loss = 4.8464790541550204, l1: 0.00010284096123510169, l2: 0.00038180694261019855   Iteration 30 of 100, tot loss = 4.762206768989563, l1: 0.0001017441862738148, l2: 0.00037447648937813935   Iteration 31 of 100, tot loss = 4.858367127756918, l1: 0.00010367315377563148, l2: 0.0003821635562475891   Iteration 32 of 100, tot loss = 4.8553305342793465, l1: 0.00010454134883275401, l2: 0.00038099170160421636   Iteration 33 of 100, tot loss = 4.865591822248517, l1: 0.00010545470297184886, l2: 0.0003811044761360708   Iteration 34 of 100, tot loss = 4.866485224050634, l1: 0.00010496696278791847, l2: 0.0003816815562244943   Iteration 35 of 100, tot loss = 4.932659046990531, l1: 0.00010504286734171078, l2: 0.00038822303426318933   Iteration 36 of 100, tot loss = 4.8956303464041815, l1: 0.00010466438602129024, l2: 0.0003848986452794634   Iteration 37 of 100, tot loss = 4.962406506409517, l1: 0.00010619594173181827, l2: 0.0003900447053318793   Iteration 38 of 100, tot loss = 4.9582704995807845, l1: 0.00010640027281433025, l2: 0.00038942677354881247   Iteration 39 of 100, tot loss = 4.988119834508652, l1: 0.00010672674425922406, l2: 0.00039208523529128003   Iteration 40 of 100, tot loss = 4.959299844503403, l1: 0.00010661251990313758, l2: 0.00038931746094021945   Iteration 41 of 100, tot loss = 4.975876023129719, l1: 0.00010710992368020503, l2: 0.00039047767513249887   Iteration 42 of 100, tot loss = 4.995289002146039, l1: 0.00010706811380736152, l2: 0.0003924607832838471   Iteration 43 of 100, tot loss = 4.98589471883552, l1: 0.00010745737626864359, l2: 0.0003911320920113128   Iteration 44 of 100, tot loss = 5.019632951779799, l1: 0.00010814173244646305, l2: 0.0003938215595553629   Iteration 45 of 100, tot loss = 4.992240487204658, l1: 0.0001074793469645859, l2: 0.00039174469873412614   Iteration 46 of 100, tot loss = 5.045243589774422, l1: 0.0001085520978686754, l2: 0.00039597225828480947   Iteration 47 of 100, tot loss = 5.013369123986426, l1: 0.00010786000092729352, l2: 0.0003934769087441345   Iteration 48 of 100, tot loss = 4.995846440394719, l1: 0.00010719816395976522, l2: 0.00039238647756671224   Iteration 49 of 100, tot loss = 4.993250428413858, l1: 0.00010692035015826874, l2: 0.00039240469020430225   Iteration 50 of 100, tot loss = 4.997189226150513, l1: 0.00010688979928090703, l2: 0.00039282912097405643   Iteration 51 of 100, tot loss = 5.041997189615287, l1: 0.00010772217965317323, l2: 0.000396477537316398   Iteration 52 of 100, tot loss = 5.035925525885362, l1: 0.00010761397474058098, l2: 0.0003959785753977485   Iteration 53 of 100, tot loss = 5.0049382605642645, l1: 0.00010661989770871412, l2: 0.00039387392572056996   Iteration 54 of 100, tot loss = 5.026995491098474, l1: 0.00010679747497811654, l2: 0.00039590207085927467   Iteration 55 of 100, tot loss = 5.041464623537931, l1: 0.00010667582080879417, l2: 0.0003974706379019401   Iteration 56 of 100, tot loss = 5.082321345806122, l1: 0.00010718678933179555, l2: 0.00040104534231691754   Iteration 57 of 100, tot loss = 5.090244025514837, l1: 0.00010770526945666541, l2: 0.00040131913020128484   Iteration 58 of 100, tot loss = 5.097309614049977, l1: 0.00010784929220570685, l2: 0.000401881666942339   Iteration 59 of 100, tot loss = 5.090963840484619, l1: 0.00010807811911760482, l2: 0.00040101826287632395   Iteration 60 of 100, tot loss = 5.079142427444458, l1: 0.00010765630728807689, l2: 0.0004002579332639774   Iteration 61 of 100, tot loss = 5.071132190891953, l1: 0.00010755270728831882, l2: 0.00039956050965034203   Iteration 62 of 100, tot loss = 5.131290312736265, l1: 0.00010865430352080898, l2: 0.000404474725890454   Iteration 63 of 100, tot loss = 5.118257499876476, l1: 0.0001085694346993622, l2: 0.0004032563135427024   Iteration 64 of 100, tot loss = 5.111355394124985, l1: 0.00010855923000008261, l2: 0.0004025763073514099   Iteration 65 of 100, tot loss = 5.096685079427866, l1: 0.00010803234567552303, l2: 0.00040163616022954765   Iteration 66 of 100, tot loss = 5.105294393770622, l1: 0.00010790077535740707, l2: 0.0004026286624139175   Iteration 67 of 100, tot loss = 5.110267809967496, l1: 0.00010826000853644948, l2: 0.0004027667711662657   Iteration 68 of 100, tot loss = 5.0726560634725235, l1: 0.00010766993069838089, l2: 0.000399595674346475   Iteration 69 of 100, tot loss = 5.071369260981463, l1: 0.00010788229053065626, l2: 0.0003992546348920281   Iteration 70 of 100, tot loss = 5.070125130244664, l1: 0.00010774981892609503, l2: 0.0003992626934112715   Iteration 71 of 100, tot loss = 5.0593592482553404, l1: 0.00010739418688848366, l2: 0.00039854173755115815   Iteration 72 of 100, tot loss = 5.057899435361226, l1: 0.00010726071200729671, l2: 0.000398529231056778   Iteration 73 of 100, tot loss = 5.045382140433952, l1: 0.00010707164634767065, l2: 0.0003974665672365219   Iteration 74 of 100, tot loss = 5.0265490525477645, l1: 0.00010712466605213735, l2: 0.0003955302385588151   Iteration 75 of 100, tot loss = 5.020269664128621, l1: 0.00010717662162884759, l2: 0.00039485034416429697   Iteration 76 of 100, tot loss = 4.994429039327722, l1: 0.0001069473297129183, l2: 0.00039249557353047607   Iteration 77 of 100, tot loss = 4.998621151044771, l1: 0.00010695002237735497, l2: 0.00039291209159509656   Iteration 78 of 100, tot loss = 4.981750971231705, l1: 0.00010668029944309428, l2: 0.0003914947967412165   Iteration 79 of 100, tot loss = 4.958954515336435, l1: 0.00010645680240601525, l2: 0.0003894386484060743   Iteration 80 of 100, tot loss = 4.961670601367951, l1: 0.00010620840771480289, l2: 0.0003899586516126874   Iteration 81 of 100, tot loss = 4.96371027275368, l1: 0.00010646777562574299, l2: 0.00038990325148556565   Iteration 82 of 100, tot loss = 4.945682400610389, l1: 0.00010615324940661638, l2: 0.00038841499030595737   Iteration 83 of 100, tot loss = 4.948204899408731, l1: 0.0001060948177245698, l2: 0.00038872567178675885   Iteration 84 of 100, tot loss = 4.941202189241137, l1: 0.00010586641166723676, l2: 0.0003882538070943805   Iteration 85 of 100, tot loss = 4.939639077467077, l1: 0.00010607396299533053, l2: 0.0003878899447633611   Iteration 86 of 100, tot loss = 4.925773986550265, l1: 0.00010613578871375784, l2: 0.000386441609893958   Iteration 87 of 100, tot loss = 4.908149776787593, l1: 0.00010588692798748901, l2: 0.0003849280495986064   Iteration 88 of 100, tot loss = 4.910163237289949, l1: 0.0001062604618792565, l2: 0.00038475586172187997   Iteration 89 of 100, tot loss = 4.899658982673388, l1: 0.00010607138175871799, l2: 0.0003838945162846194   Iteration 90 of 100, tot loss = 4.896876968277825, l1: 0.00010567340897169844, l2: 0.0003840142875560559   Iteration 91 of 100, tot loss = 4.908952107796302, l1: 0.00010574636158148329, l2: 0.00038514884875109933   Iteration 92 of 100, tot loss = 4.903684675693512, l1: 0.00010557615373925174, l2: 0.00038479231364402477   Iteration 93 of 100, tot loss = 4.925747381743564, l1: 0.00010593006523014347, l2: 0.0003866446725535958   Iteration 94 of 100, tot loss = 4.917938437867672, l1: 0.00010571081699534726, l2: 0.0003860830265887458   Iteration 95 of 100, tot loss = 4.914515286997745, l1: 0.0001054686885158605, l2: 0.00038598283993594934   Iteration 96 of 100, tot loss = 4.912046668430169, l1: 0.00010548003952711345, l2: 0.00038572462699448806   Iteration 97 of 100, tot loss = 4.907398314820123, l1: 0.00010526961581415458, l2: 0.0003854702153307933   Iteration 98 of 100, tot loss = 4.897059798240662, l1: 0.00010505824235335648, l2: 0.0003846477372963837   Iteration 99 of 100, tot loss = 4.905246852624296, l1: 0.00010532931036773288, l2: 0.0003851953747792809   Iteration 100 of 100, tot loss = 4.899136636257172, l1: 0.00010555667762673693, l2: 0.000384356986178318
   End of epoch 1347; saving model... 

Epoch 1348 of 2000
   Iteration 1 of 100, tot loss = 6.742821216583252, l1: 0.00012694514589384198, l2: 0.0005473369965329766   Iteration 2 of 100, tot loss = 7.12064528465271, l1: 0.00014763410581508651, l2: 0.0005644304328598082   Iteration 3 of 100, tot loss = 6.849141438802083, l1: 0.00014111188162739077, l2: 0.0005438022587137917   Iteration 4 of 100, tot loss = 5.936255872249603, l1: 0.00013139111069904175, l2: 0.0004622344713425264   Iteration 5 of 100, tot loss = 5.785688257217407, l1: 0.00013198388187447564, l2: 0.00044658494298346343   Iteration 6 of 100, tot loss = 5.688778758049011, l1: 0.0001299191632521494, l2: 0.00043895870718794566   Iteration 7 of 100, tot loss = 5.767551388059344, l1: 0.000127914003054944, l2: 0.0004488411276335163   Iteration 8 of 100, tot loss = 5.63926437497139, l1: 0.00012581629562191665, l2: 0.0004381101352919359   Iteration 9 of 100, tot loss = 5.564241223865086, l1: 0.0001232268566834844, l2: 0.00043319725793682865   Iteration 10 of 100, tot loss = 5.467868161201477, l1: 0.00012145943255745806, l2: 0.0004253273771610111   Iteration 11 of 100, tot loss = 5.4272826151414355, l1: 0.00012172803624046288, l2: 0.0004210002159445801   Iteration 12 of 100, tot loss = 5.3239709337552386, l1: 0.00011941655733001728, l2: 0.00041298052868417773   Iteration 13 of 100, tot loss = 5.251478286889883, l1: 0.00011858383424203986, l2: 0.00040656398722113896   Iteration 14 of 100, tot loss = 5.210700699261257, l1: 0.00011543429978441313, l2: 0.0004056357637247337   Iteration 15 of 100, tot loss = 5.099746672312419, l1: 0.00011149433727647799, l2: 0.0003984803236865749   Iteration 16 of 100, tot loss = 5.046065002679825, l1: 0.00011105704084002355, l2: 0.0003935494532925077   Iteration 17 of 100, tot loss = 5.081622376161463, l1: 0.00011042887897073182, l2: 0.0003977333510513691   Iteration 18 of 100, tot loss = 5.021414160728455, l1: 0.00011064635388417325, l2: 0.0003914950551309933   Iteration 19 of 100, tot loss = 5.118542959815578, l1: 0.00011177616416764642, l2: 0.0004000781244892431   Iteration 20 of 100, tot loss = 5.005714285373688, l1: 0.00011034551844204543, l2: 0.00039022590208332987   Iteration 21 of 100, tot loss = 4.951526108242216, l1: 0.00011008979535766965, l2: 0.0003850628078604738   Iteration 22 of 100, tot loss = 4.885214469649575, l1: 0.0001091123824814779, l2: 0.00037940905762794006   Iteration 23 of 100, tot loss = 4.935804854268613, l1: 0.00011088370181665675, l2: 0.00038269677695453816   Iteration 24 of 100, tot loss = 4.8461812535921736, l1: 0.00010884277874841548, l2: 0.000375775339610603   Iteration 25 of 100, tot loss = 4.776137475967407, l1: 0.00010707244728109799, l2: 0.0003705412935232744   Iteration 26 of 100, tot loss = 4.7815395960441, l1: 0.00010768514137173322, l2: 0.00037046881278421584   Iteration 27 of 100, tot loss = 4.867497735553318, l1: 0.00010816967241461734, l2: 0.00037858009616482176   Iteration 28 of 100, tot loss = 4.808876761368343, l1: 0.0001060872452528981, l2: 0.00037480042634082826   Iteration 29 of 100, tot loss = 4.781187756308194, l1: 0.00010569550394048465, l2: 0.0003724232670281018   Iteration 30 of 100, tot loss = 4.704804205894471, l1: 0.00010455829227187982, l2: 0.00036592212418327106   Iteration 31 of 100, tot loss = 4.657206466121059, l1: 0.00010368838247227212, l2: 0.00036203225975060056   Iteration 32 of 100, tot loss = 4.676122672855854, l1: 0.00010453701543156058, l2: 0.0003630752476055932   Iteration 33 of 100, tot loss = 4.704062165636005, l1: 0.00010527262356569709, l2: 0.00036513358910187065   Iteration 34 of 100, tot loss = 4.733842071364908, l1: 0.00010585789748481201, l2: 0.0003675263069453649   Iteration 35 of 100, tot loss = 4.808341877801078, l1: 0.00010686097292429102, l2: 0.0003739732113899663   Iteration 36 of 100, tot loss = 4.7804460591740074, l1: 0.00010656673920392577, l2: 0.0003714778629526134   Iteration 37 of 100, tot loss = 4.777082488343522, l1: 0.00010702916542131051, l2: 0.0003706790806530547   Iteration 38 of 100, tot loss = 4.770242320863824, l1: 0.00010742321434295981, l2: 0.000369601014857548   Iteration 39 of 100, tot loss = 4.806385462100689, l1: 0.00010807411700415496, l2: 0.00037256442644716933   Iteration 40 of 100, tot loss = 4.802258211374283, l1: 0.00010808032220666064, l2: 0.0003721454962942516   Iteration 41 of 100, tot loss = 4.775222330558591, l1: 0.00010786670765544219, l2: 0.0003696555231283305   Iteration 42 of 100, tot loss = 4.765689072154817, l1: 0.00010788091319790554, l2: 0.0003686879918816322   Iteration 43 of 100, tot loss = 4.703477853952452, l1: 0.00010642882972565848, l2: 0.0003639189536591212   Iteration 44 of 100, tot loss = 4.685701776634563, l1: 0.000106380009542177, l2: 0.0003621901662633966   Iteration 45 of 100, tot loss = 4.625104835298326, l1: 0.00010519931949804433, l2: 0.0003573111620628171   Iteration 46 of 100, tot loss = 4.655939542728921, l1: 0.00010586478073610519, l2: 0.0003597291709055476   Iteration 47 of 100, tot loss = 4.692549690287164, l1: 0.00010697163924441099, l2: 0.00036228332789912026   Iteration 48 of 100, tot loss = 4.718163087964058, l1: 0.00010660685681311104, l2: 0.00036520944983446196   Iteration 49 of 100, tot loss = 4.739928989994283, l1: 0.00010698071350723657, l2: 0.0003670121837711456   Iteration 50 of 100, tot loss = 4.73618622303009, l1: 0.00010736830532550811, l2: 0.00036625031556468456   Iteration 51 of 100, tot loss = 4.737275633157468, l1: 0.00010749349951534076, l2: 0.00036623406191995625   Iteration 52 of 100, tot loss = 4.729099424985739, l1: 0.00010756367527602169, l2: 0.0003653462654955757   Iteration 53 of 100, tot loss = 4.739614860066828, l1: 0.0001079417989734244, l2: 0.0003660196846983624   Iteration 54 of 100, tot loss = 4.7147598619814275, l1: 0.00010759078941191547, l2: 0.0003638851943563808   Iteration 55 of 100, tot loss = 4.7147410652854225, l1: 0.00010794757730433379, l2: 0.00036352652664804324   Iteration 56 of 100, tot loss = 4.704712271690369, l1: 0.0001077311500401785, l2: 0.00036274007444652464   Iteration 57 of 100, tot loss = 4.663630142546537, l1: 0.00010661847492181615, l2: 0.0003597445366291427   Iteration 58 of 100, tot loss = 4.615839185385869, l1: 0.00010558674091890711, l2: 0.0003559971750103708   Iteration 59 of 100, tot loss = 4.6569929122924805, l1: 0.00010652323223855662, l2: 0.0003591760562859097   Iteration 60 of 100, tot loss = 4.697714940706889, l1: 0.00010686511426077535, l2: 0.000362906377025259   Iteration 61 of 100, tot loss = 4.702609234168881, l1: 0.00010646443580731474, l2: 0.0003637964846795333   Iteration 62 of 100, tot loss = 4.678593485586105, l1: 0.00010608673361392932, l2: 0.0003617726118048473   Iteration 63 of 100, tot loss = 4.702073630832491, l1: 0.00010650043120785129, l2: 0.00036370692892916616   Iteration 64 of 100, tot loss = 4.711555358022451, l1: 0.00010638942148943897, l2: 0.00036476611103353207   Iteration 65 of 100, tot loss = 4.710665567104633, l1: 0.00010620249869624296, l2: 0.00036486405476282997   Iteration 66 of 100, tot loss = 4.674997185215806, l1: 0.00010524536981282643, l2: 0.000362254345611401   Iteration 67 of 100, tot loss = 4.662238679715057, l1: 0.00010505622689813765, l2: 0.0003611676378867158   Iteration 68 of 100, tot loss = 4.777070343494415, l1: 0.00010647164804943125, l2: 0.0003712353832002811   Iteration 69 of 100, tot loss = 4.756085547848024, l1: 0.0001062699141228855, l2: 0.00036933863737141257   Iteration 70 of 100, tot loss = 4.753699043818883, l1: 0.0001064445505367725, l2: 0.0003689253505269465   Iteration 71 of 100, tot loss = 4.739586870435258, l1: 0.00010622296364269627, l2: 0.0003677357197657201   Iteration 72 of 100, tot loss = 4.730672531657749, l1: 0.00010621415508972455, l2: 0.00036685309431858413   Iteration 73 of 100, tot loss = 4.70770233624602, l1: 0.00010584426789834764, l2: 0.00036492596212168516   Iteration 74 of 100, tot loss = 4.70035161843171, l1: 0.00010573579778350461, l2: 0.00036429936035592864   Iteration 75 of 100, tot loss = 4.692282206217448, l1: 0.00010545758336472014, l2: 0.00036377063350907215   Iteration 76 of 100, tot loss = 4.686688316495795, l1: 0.00010541752501586943, l2: 0.00036325130292281854   Iteration 77 of 100, tot loss = 4.686462767712482, l1: 0.0001054803997485342, l2: 0.0003631658734551135   Iteration 78 of 100, tot loss = 4.70298405793997, l1: 0.00010585807532875035, l2: 0.0003644403266848829   Iteration 79 of 100, tot loss = 4.6991476831556875, l1: 0.0001056472147531996, l2: 0.00036426755003601645   Iteration 80 of 100, tot loss = 4.739061564207077, l1: 0.00010601585381664336, l2: 0.00036789029891224343   Iteration 81 of 100, tot loss = 4.717544075883465, l1: 0.00010589090649133411, l2: 0.0003658634974092337   Iteration 82 of 100, tot loss = 4.69425328475673, l1: 0.0001052784018389591, l2: 0.0003641469231253013   Iteration 83 of 100, tot loss = 4.677917397165873, l1: 0.00010471681131334717, l2: 0.00036307492490481947   Iteration 84 of 100, tot loss = 4.660100820518675, l1: 0.00010405218368500105, l2: 0.00036195789479755885   Iteration 85 of 100, tot loss = 4.640385835310992, l1: 0.00010376134771778357, l2: 0.00036027723228257587   Iteration 86 of 100, tot loss = 4.637061806612237, l1: 0.0001037055565173559, l2: 0.0003600006208681978   Iteration 87 of 100, tot loss = 4.6324448530701385, l1: 0.0001035803418734309, l2: 0.0003596641400617537   Iteration 88 of 100, tot loss = 4.668092294172808, l1: 0.00010430774913898362, l2: 0.0003625014765434158   Iteration 89 of 100, tot loss = 4.6922137013981855, l1: 0.00010451916951046513, l2: 0.00036470219705468356   Iteration 90 of 100, tot loss = 4.68976575533549, l1: 0.00010432652347339576, l2: 0.0003646500487876539   Iteration 91 of 100, tot loss = 4.6806391307285855, l1: 0.00010421928263795152, l2: 0.0003638446273775976   Iteration 92 of 100, tot loss = 4.678299162698829, l1: 0.00010418889395742039, l2: 0.00036364101944617806   Iteration 93 of 100, tot loss = 4.654968320682484, l1: 0.00010378981280292043, l2: 0.0003617070161522196   Iteration 94 of 100, tot loss = 4.6519731131005795, l1: 0.00010386119673909134, l2: 0.0003613361115369232   Iteration 95 of 100, tot loss = 4.630961049230475, l1: 0.0001034693639667239, l2: 0.00035962673795630076   Iteration 96 of 100, tot loss = 4.594802266607682, l1: 0.00010269518694864625, l2: 0.00035678503680477053   Iteration 97 of 100, tot loss = 4.596147228762047, l1: 0.00010282867736622364, l2: 0.00035678604268168875   Iteration 98 of 100, tot loss = 4.587231841622566, l1: 0.000102740264918218, l2: 0.00035598291651695035   Iteration 99 of 100, tot loss = 4.5876141762492635, l1: 0.00010279294230088573, l2: 0.0003559684725284266   Iteration 100 of 100, tot loss = 4.612665740251541, l1: 0.0001030756321415538, l2: 0.00035819093893223907
   End of epoch 1348; saving model... 

Epoch 1349 of 2000
   Iteration 1 of 100, tot loss = 3.1619956493377686, l1: 9.389488695887849e-05, l2: 0.00022230466129258275   Iteration 2 of 100, tot loss = 4.225822329521179, l1: 9.219609273714013e-05, l2: 0.0003303861303720623   Iteration 3 of 100, tot loss = 4.504703442255656, l1: 9.731255462005113e-05, l2: 0.0003531577919299404   Iteration 4 of 100, tot loss = 4.762272536754608, l1: 0.0001023837521643145, l2: 0.0003738435043487698   Iteration 5 of 100, tot loss = 4.928325510025024, l1: 0.00010517925984458998, l2: 0.0003876532951835543   Iteration 6 of 100, tot loss = 5.094179749488831, l1: 0.00010751212660882932, l2: 0.0004019058493819709   Iteration 7 of 100, tot loss = 4.958394629614694, l1: 0.00010543520121635603, l2: 0.0003904042615821319   Iteration 8 of 100, tot loss = 5.015239328145981, l1: 0.00010595020194159588, l2: 0.0003955737302021589   Iteration 9 of 100, tot loss = 4.747523148854573, l1: 0.00010272837890725996, l2: 0.00037202393610237376   Iteration 10 of 100, tot loss = 4.6828631401062015, l1: 0.00010459544137120247, l2: 0.00036369087174534796   Iteration 11 of 100, tot loss = 4.491858785802668, l1: 0.0001020718727059747, l2: 0.0003471140053906393   Iteration 12 of 100, tot loss = 4.641616980234782, l1: 0.00010309627881118406, l2: 0.00036106542029301636   Iteration 13 of 100, tot loss = 4.777906711284931, l1: 0.00010344683071777511, l2: 0.0003743438428500667   Iteration 14 of 100, tot loss = 4.701084818158831, l1: 0.00010406212926942057, l2: 0.00036604635429934466   Iteration 15 of 100, tot loss = 4.5850944995880125, l1: 0.00010298325069015845, l2: 0.0003555262015045931   Iteration 16 of 100, tot loss = 4.762367591261864, l1: 0.00010609112496240414, l2: 0.0003701456353155663   Iteration 17 of 100, tot loss = 4.806221947950475, l1: 0.00010482129565549686, l2: 0.0003758008995860377   Iteration 18 of 100, tot loss = 4.798585401640998, l1: 0.00010599563393043354, l2: 0.0003738629069024076   Iteration 19 of 100, tot loss = 4.964326971455624, l1: 0.00010856214947508354, l2: 0.00038787054731265495   Iteration 20 of 100, tot loss = 5.0052586674690245, l1: 0.00011011090464307927, l2: 0.0003904149605659768   Iteration 21 of 100, tot loss = 5.008250293277559, l1: 0.00010888729593716562, l2: 0.00039193773270762035   Iteration 22 of 100, tot loss = 4.839213067835027, l1: 0.0001054488779614489, l2: 0.00037847242823442105   Iteration 23 of 100, tot loss = 4.746643781661987, l1: 0.00010422320891571555, l2: 0.00037044116976650673   Iteration 24 of 100, tot loss = 4.7295341193675995, l1: 0.00010412836324273182, l2: 0.00036882504900859203   Iteration 25 of 100, tot loss = 4.8460799503326415, l1: 0.00010645591843058355, l2: 0.0003781520767370239   Iteration 26 of 100, tot loss = 4.817630410194397, l1: 0.00010620614907216244, l2: 0.00037555689218257053   Iteration 27 of 100, tot loss = 4.7852848194263595, l1: 0.00010568411447888206, l2: 0.00037284436823231063   Iteration 28 of 100, tot loss = 4.712436641965594, l1: 0.00010397535074194561, l2: 0.0003672683139614362   Iteration 29 of 100, tot loss = 4.676554515443999, l1: 0.00010256973448498496, l2: 0.00036508571818181926   Iteration 30 of 100, tot loss = 4.665533749262492, l1: 0.00010235924176716556, l2: 0.0003641941342114781   Iteration 31 of 100, tot loss = 4.671285752327211, l1: 0.00010293844646977021, l2: 0.0003641901289924018   Iteration 32 of 100, tot loss = 4.5863525830209255, l1: 0.00010119887247128645, l2: 0.0003574363863663166   Iteration 33 of 100, tot loss = 4.548929203640331, l1: 0.00010033742221298091, l2: 0.00035455549897795373   Iteration 34 of 100, tot loss = 4.607181426356821, l1: 0.00010143364403013359, l2: 0.00035928449976970167   Iteration 35 of 100, tot loss = 4.562120774814061, l1: 0.00010017829938858215, l2: 0.0003560337795144213   Iteration 36 of 100, tot loss = 4.561081088251537, l1: 0.00010022526233418223, l2: 0.00035588284825078316   Iteration 37 of 100, tot loss = 4.5831799217172575, l1: 0.0001003784841363872, l2: 0.0003579395085359244   Iteration 38 of 100, tot loss = 4.575437417155818, l1: 0.00010030387688618105, l2: 0.0003572398648094876   Iteration 39 of 100, tot loss = 4.626838522079663, l1: 0.00010134004566143864, l2: 0.0003613438061737002   Iteration 40 of 100, tot loss = 4.623714885115623, l1: 0.00010156648168049287, l2: 0.00036080500576645135   Iteration 41 of 100, tot loss = 4.585932170472494, l1: 0.00010087038745263183, l2: 0.0003577228280624784   Iteration 42 of 100, tot loss = 4.583391777106693, l1: 0.00010122510173546506, l2: 0.00035711407448009896   Iteration 43 of 100, tot loss = 4.6002790733825325, l1: 0.00010218076119578422, l2: 0.00035784714435617074   Iteration 44 of 100, tot loss = 4.604779370806434, l1: 0.00010168248379407238, l2: 0.00035879545149245217   Iteration 45 of 100, tot loss = 4.619180274009705, l1: 0.000101898261184235, l2: 0.00036001976501817505   Iteration 46 of 100, tot loss = 4.557190775871277, l1: 0.0001007724684946563, l2: 0.0003549466077263629   Iteration 47 of 100, tot loss = 4.541298227107271, l1: 0.00010020937228778456, l2: 0.0003539204490346279   Iteration 48 of 100, tot loss = 4.507126808166504, l1: 9.954303777703899e-05, l2: 0.00035116964166566805   Iteration 49 of 100, tot loss = 4.511276877656275, l1: 9.995859950229677e-05, l2: 0.0003511690869048352   Iteration 50 of 100, tot loss = 4.509144716262817, l1: 0.00010026108240708709, l2: 0.000350653387431521   Iteration 51 of 100, tot loss = 4.534241835276286, l1: 0.00010032133657517203, l2: 0.0003531028453246964   Iteration 52 of 100, tot loss = 4.5359484782585735, l1: 0.00010005674364667636, l2: 0.0003535381024098919   Iteration 53 of 100, tot loss = 4.536294091422603, l1: 9.958550813366255e-05, l2: 0.000354043898592211   Iteration 54 of 100, tot loss = 4.518866375640586, l1: 9.931752464143317e-05, l2: 0.00035256911009869156   Iteration 55 of 100, tot loss = 4.512503489581022, l1: 9.95770366650752e-05, l2: 0.00035167330934200437   Iteration 56 of 100, tot loss = 4.478096331868853, l1: 9.870541069696108e-05, l2: 0.0003491042196921106   Iteration 57 of 100, tot loss = 4.454513031139708, l1: 9.867687613382658e-05, l2: 0.00034677442428737617   Iteration 58 of 100, tot loss = 4.455279539371359, l1: 9.866509615927373e-05, l2: 0.0003468628550261452   Iteration 59 of 100, tot loss = 4.425659115031614, l1: 9.776305277058381e-05, l2: 0.000344802856088174   Iteration 60 of 100, tot loss = 4.419163107872009, l1: 9.778645702075058e-05, l2: 0.0003441298513886674   Iteration 61 of 100, tot loss = 4.42703136850576, l1: 9.813578191049183e-05, l2: 0.00034456735273601764   Iteration 62 of 100, tot loss = 4.431346308800482, l1: 9.812934454643138e-05, l2: 0.0003450052841329917   Iteration 63 of 100, tot loss = 4.395456995282855, l1: 9.74629103207946e-05, l2: 0.0003420827870575031   Iteration 64 of 100, tot loss = 4.454394549131393, l1: 9.825625477333233e-05, l2: 0.00034718319830062683   Iteration 65 of 100, tot loss = 4.423157545236441, l1: 9.753247801339827e-05, l2: 0.00034478327478819457   Iteration 66 of 100, tot loss = 4.4287170280109756, l1: 9.784441477456835e-05, l2: 0.0003450272861025014   Iteration 67 of 100, tot loss = 4.4395077121791555, l1: 9.816367261254215e-05, l2: 0.0003457870970761626   Iteration 68 of 100, tot loss = 4.474122966037077, l1: 9.853269330051262e-05, l2: 0.0003488796015410438   Iteration 69 of 100, tot loss = 4.447357347046119, l1: 9.842406702312249e-05, l2: 0.00034631166583520996   Iteration 70 of 100, tot loss = 4.454691290855408, l1: 9.872423324850388e-05, l2: 0.0003467448937174465   Iteration 71 of 100, tot loss = 4.4583729858129795, l1: 9.866370073467179e-05, l2: 0.0003471735960797367   Iteration 72 of 100, tot loss = 4.47079622414377, l1: 9.849939791214031e-05, l2: 0.0003485802230392841   Iteration 73 of 100, tot loss = 4.458537901917549, l1: 9.815908710500991e-05, l2: 0.0003476947018149475   Iteration 74 of 100, tot loss = 4.50591217182778, l1: 9.890690115925454e-05, l2: 0.00035168431507348006   Iteration 75 of 100, tot loss = 4.517633250554403, l1: 9.93427633269069e-05, l2: 0.0003524205608603855   Iteration 76 of 100, tot loss = 4.5627439304402, l1: 9.997694503813124e-05, l2: 0.00035629744678245564   Iteration 77 of 100, tot loss = 4.583241236674321, l1: 0.00010056916256541048, l2: 0.0003577549594103709   Iteration 78 of 100, tot loss = 4.603916935431651, l1: 0.00010095021748267568, l2: 0.0003594414745935072   Iteration 79 of 100, tot loss = 4.591365301156346, l1: 0.00010081849122107548, l2: 0.00035831803725619763   Iteration 80 of 100, tot loss = 4.608487921953201, l1: 0.00010089697689181776, l2: 0.0003599518142436864   Iteration 81 of 100, tot loss = 4.616633544733495, l1: 0.00010112252017773435, l2: 0.00036054083367487714   Iteration 82 of 100, tot loss = 4.615496490059829, l1: 0.00010080602752589962, l2: 0.0003607436207480865   Iteration 83 of 100, tot loss = 4.618782681154918, l1: 0.00010086362778225981, l2: 0.0003610146396754152   Iteration 84 of 100, tot loss = 4.634059111277263, l1: 0.00010118673613067672, l2: 0.00036221917495519545   Iteration 85 of 100, tot loss = 4.621125372718362, l1: 0.00010093045155586236, l2: 0.000361182085608187   Iteration 86 of 100, tot loss = 4.621953060460645, l1: 0.0001011166229524628, l2: 0.000361078682756268   Iteration 87 of 100, tot loss = 4.60158126381622, l1: 0.00010090967942692938, l2: 0.00035924844637154547   Iteration 88 of 100, tot loss = 4.587449981407686, l1: 0.0001006079430060874, l2: 0.00035813705455273686   Iteration 89 of 100, tot loss = 4.558323976698886, l1: 0.00010014213494082725, l2: 0.00035569026217314553   Iteration 90 of 100, tot loss = 4.542745702796513, l1: 9.982713923414444e-05, l2: 0.0003544474303554226   Iteration 91 of 100, tot loss = 4.524565519867362, l1: 9.947641252778e-05, l2: 0.0003529801388440019   Iteration 92 of 100, tot loss = 4.519156053014424, l1: 9.959713046871754e-05, l2: 0.0003523184739448049   Iteration 93 of 100, tot loss = 4.5360982148878035, l1: 9.981900137806592e-05, l2: 0.00035379081901903915   Iteration 94 of 100, tot loss = 4.526220114941292, l1: 9.979680103145817e-05, l2: 0.0003528252093522671   Iteration 95 of 100, tot loss = 4.51302105125628, l1: 9.937338652793857e-05, l2: 0.00035192871735872407   Iteration 96 of 100, tot loss = 4.561485284318526, l1: 9.999390852044598e-05, l2: 0.0003561546188090385   Iteration 97 of 100, tot loss = 4.5508662044387505, l1: 9.972793362901507e-05, l2: 0.0003553586858972792   Iteration 98 of 100, tot loss = 4.55699249068085, l1: 9.982873030376326e-05, l2: 0.00035587051789494876   Iteration 99 of 100, tot loss = 4.568903454626449, l1: 9.995067511676961e-05, l2: 0.00035693966948210893   Iteration 100 of 100, tot loss = 4.561268612146377, l1: 9.987912500946549e-05, l2: 0.0003562477353261784
   End of epoch 1349; saving model... 

Epoch 1350 of 2000
   Iteration 1 of 100, tot loss = 4.833610534667969, l1: 9.043847967404872e-05, l2: 0.0003929225786123425   Iteration 2 of 100, tot loss = 5.184175968170166, l1: 0.00010638254752848297, l2: 0.00041203503496944904   Iteration 3 of 100, tot loss = 4.715589682261149, l1: 9.430695111708094e-05, l2: 0.00037725199945271015   Iteration 4 of 100, tot loss = 4.489169657230377, l1: 8.966843233793043e-05, l2: 0.0003592485227272846   Iteration 5 of 100, tot loss = 4.138726377487183, l1: 8.519717230228707e-05, l2: 0.00032867545960471036   Iteration 6 of 100, tot loss = 4.0923681656519575, l1: 8.593758320785128e-05, l2: 0.00032329923124052584   Iteration 7 of 100, tot loss = 4.0267458983830045, l1: 8.93633031020207e-05, l2: 0.0003133112830775125   Iteration 8 of 100, tot loss = 3.8746816515922546, l1: 8.75135001479066e-05, l2: 0.0002999546595674474   Iteration 9 of 100, tot loss = 3.9299968083699546, l1: 8.550941210059036e-05, l2: 0.0003074902666008307   Iteration 10 of 100, tot loss = 3.870478796958923, l1: 8.518025351804681e-05, l2: 0.00030186762451194226   Iteration 11 of 100, tot loss = 3.8208425261757593, l1: 8.322765246372332e-05, l2: 0.00029885659883306784   Iteration 12 of 100, tot loss = 3.8292907079060874, l1: 8.322650986277343e-05, l2: 0.00029970256097537157   Iteration 13 of 100, tot loss = 4.05355167388916, l1: 8.709885710581708e-05, l2: 0.0003182563120320153   Iteration 14 of 100, tot loss = 4.0256896529878885, l1: 8.603408891108952e-05, l2: 0.0003165348773888711   Iteration 15 of 100, tot loss = 4.00817445119222, l1: 8.797363746756066e-05, l2: 0.0003128438082057983   Iteration 16 of 100, tot loss = 3.9490681141614914, l1: 8.736000427234103e-05, l2: 0.00030754680756217567   Iteration 17 of 100, tot loss = 3.991076707839966, l1: 8.894095363383017e-05, l2: 0.000310166717527489   Iteration 18 of 100, tot loss = 3.9487197399139404, l1: 8.89513624618606e-05, l2: 0.0003059206113296871   Iteration 19 of 100, tot loss = 3.844193414637917, l1: 8.742404660504115e-05, l2: 0.00029699529420069764   Iteration 20 of 100, tot loss = 3.8379941284656525, l1: 8.682358820806257e-05, l2: 0.00029697582431253977   Iteration 21 of 100, tot loss = 3.7658398775827315, l1: 8.455964866914742e-05, l2: 0.00029202433887846944   Iteration 22 of 100, tot loss = 3.9300652904943987, l1: 8.71069476653991e-05, l2: 0.0003058995822687972   Iteration 23 of 100, tot loss = 3.952584085257157, l1: 8.756254955306244e-05, l2: 0.00030769586105041367   Iteration 24 of 100, tot loss = 3.93666639427344, l1: 8.75214488284352e-05, l2: 0.0003061451928563959   Iteration 25 of 100, tot loss = 3.8920181512832643, l1: 8.667874906677753e-05, l2: 0.00030252306780312213   Iteration 26 of 100, tot loss = 3.9233431861950803, l1: 8.753495086029243e-05, l2: 0.00030479936965723307   Iteration 27 of 100, tot loss = 3.944492468127498, l1: 8.78022897436663e-05, l2: 0.00030664695778223513   Iteration 28 of 100, tot loss = 3.9249390406267985, l1: 8.869917681814903e-05, l2: 0.00030379472809727304   Iteration 29 of 100, tot loss = 3.9981213808059692, l1: 8.990493596025647e-05, l2: 0.0003099072035121057   Iteration 30 of 100, tot loss = 3.9456853826840717, l1: 8.884331800800283e-05, l2: 0.00030572522179378816   Iteration 31 of 100, tot loss = 3.9401063496066677, l1: 8.908251660496688e-05, l2: 0.00030492811985764534   Iteration 32 of 100, tot loss = 4.004929978400469, l1: 9.049609195699304e-05, l2: 0.0003099969080722076   Iteration 33 of 100, tot loss = 4.051088452339172, l1: 9.112493992716102e-05, l2: 0.00031398390825470966   Iteration 34 of 100, tot loss = 4.032797816921683, l1: 9.061344237981008e-05, l2: 0.00031266634195002127   Iteration 35 of 100, tot loss = 4.085702368191311, l1: 9.159900894571495e-05, l2: 0.0003169712294558329   Iteration 36 of 100, tot loss = 4.117246048318015, l1: 9.234060912098762e-05, l2: 0.0003193839972179073   Iteration 37 of 100, tot loss = 4.09573884590252, l1: 9.268156370396625e-05, l2: 0.0003168923226562706   Iteration 38 of 100, tot loss = 4.121508055611661, l1: 9.333291851170973e-05, l2: 0.0003188178885274714   Iteration 39 of 100, tot loss = 4.194222434973105, l1: 9.446343205919943e-05, l2: 0.0003249588137408957   Iteration 40 of 100, tot loss = 4.209608945250511, l1: 9.466086039537913e-05, l2: 0.00032630003697704525   Iteration 41 of 100, tot loss = 4.234761188669903, l1: 9.514264208364028e-05, l2: 0.0003283334790910708   Iteration 42 of 100, tot loss = 4.2524480904851645, l1: 9.529383593568734e-05, l2: 0.00032995097613560835   Iteration 43 of 100, tot loss = 4.238296627998352, l1: 9.530944386591896e-05, l2: 0.0003285202222096539   Iteration 44 of 100, tot loss = 4.257729213346135, l1: 9.617711566534126e-05, l2: 0.0003295958085800521   Iteration 45 of 100, tot loss = 4.232304427358839, l1: 9.533428819850088e-05, l2: 0.0003278961573313508   Iteration 46 of 100, tot loss = 4.277037612769915, l1: 9.5637637454986e-05, l2: 0.0003320661262082665   Iteration 47 of 100, tot loss = 4.2807248222067, l1: 9.537863738938889e-05, l2: 0.00033269384739662895   Iteration 48 of 100, tot loss = 4.266480651994546, l1: 9.474088301431038e-05, l2: 0.0003319071850758822   Iteration 49 of 100, tot loss = 4.233114322837518, l1: 9.426867131414652e-05, l2: 0.0003290427634872648   Iteration 50 of 100, tot loss = 4.27683739900589, l1: 9.46447481692303e-05, l2: 0.000333038994576782   Iteration 51 of 100, tot loss = 4.246540067242641, l1: 9.376066515744919e-05, l2: 0.0003308933447959705   Iteration 52 of 100, tot loss = 4.241123114640896, l1: 9.312684456884969e-05, l2: 0.0003309854697503257   Iteration 53 of 100, tot loss = 4.271479748330027, l1: 9.361655590836099e-05, l2: 0.00033353142223962285   Iteration 54 of 100, tot loss = 4.285645562189597, l1: 9.397083812035378e-05, l2: 0.00033459372099497596   Iteration 55 of 100, tot loss = 4.299784857576544, l1: 9.434789323925295e-05, l2: 0.0003356305953771384   Iteration 56 of 100, tot loss = 4.301323803407805, l1: 9.407857747386775e-05, l2: 0.00033605380589766094   Iteration 57 of 100, tot loss = 4.328281718387938, l1: 9.43510495473451e-05, l2: 0.0003384771251850843   Iteration 58 of 100, tot loss = 4.315396796012747, l1: 9.403305601404884e-05, l2: 0.0003375066266142249   Iteration 59 of 100, tot loss = 4.325409648782116, l1: 9.434460354002066e-05, l2: 0.00033819636402028155   Iteration 60 of 100, tot loss = 4.326121054093043, l1: 9.443988310522399e-05, l2: 0.00033817222453459787   Iteration 61 of 100, tot loss = 4.414708428695554, l1: 9.573779410499408e-05, l2: 0.000345733051797085   Iteration 62 of 100, tot loss = 4.408594948630179, l1: 9.590218389583301e-05, l2: 0.0003449573139791497   Iteration 63 of 100, tot loss = 4.423758251326425, l1: 9.662025510647615e-05, l2: 0.0003457555733528727   Iteration 64 of 100, tot loss = 4.396398665383458, l1: 9.622957236388174e-05, l2: 0.00034341029731876915   Iteration 65 of 100, tot loss = 4.390271885578449, l1: 9.635051067631978e-05, l2: 0.0003426766811081996   Iteration 66 of 100, tot loss = 4.376706027623379, l1: 9.610041579986556e-05, l2: 0.0003415701900270175   Iteration 67 of 100, tot loss = 4.363102772342625, l1: 9.574899263158719e-05, l2: 0.0003405612877764697   Iteration 68 of 100, tot loss = 4.346544530461816, l1: 9.565075732312193e-05, l2: 0.0003390036986007437   Iteration 69 of 100, tot loss = 4.336612927740899, l1: 9.559314998298667e-05, l2: 0.00033806814568326473   Iteration 70 of 100, tot loss = 4.335429121766771, l1: 9.584127682111492e-05, l2: 0.0003377016378051069   Iteration 71 of 100, tot loss = 4.328898636388107, l1: 9.588716343835547e-05, l2: 0.00033700270279028624   Iteration 72 of 100, tot loss = 4.334637103809251, l1: 9.57006689835301e-05, l2: 0.00033776304371713195   Iteration 73 of 100, tot loss = 4.33402302983689, l1: 9.569697032606051e-05, l2: 0.00033770533488291534   Iteration 74 of 100, tot loss = 4.345013036921218, l1: 9.604060166590923e-05, l2: 0.00033846070397664115   Iteration 75 of 100, tot loss = 4.35069503625234, l1: 9.624321130104363e-05, l2: 0.00033882629417348654   Iteration 76 of 100, tot loss = 4.34526403169883, l1: 9.579699067935028e-05, l2: 0.0003387294141008935   Iteration 77 of 100, tot loss = 4.322657168685616, l1: 9.549457928341173e-05, l2: 0.0003367711392279666   Iteration 78 of 100, tot loss = 4.310423843371562, l1: 9.509166780984603e-05, l2: 0.00033595071820458636   Iteration 79 of 100, tot loss = 4.290289780761622, l1: 9.450042802476489e-05, l2: 0.0003345285516889483   Iteration 80 of 100, tot loss = 4.300660912692547, l1: 9.485178493378044e-05, l2: 0.0003352143079609959   Iteration 81 of 100, tot loss = 4.3117949447514095, l1: 9.531736968374052e-05, l2: 0.000335862126199954   Iteration 82 of 100, tot loss = 4.31387985479541, l1: 9.527132666645652e-05, l2: 0.0003361166598596156   Iteration 83 of 100, tot loss = 4.348905263176883, l1: 9.560698136814914e-05, l2: 0.0003392835456600233   Iteration 84 of 100, tot loss = 4.3561410662673765, l1: 9.596123687688738e-05, l2: 0.0003396528705447314   Iteration 85 of 100, tot loss = 4.33720764132107, l1: 9.568825048250694e-05, l2: 0.00033803251452113993   Iteration 86 of 100, tot loss = 4.325059601040774, l1: 9.559124166201421e-05, l2: 0.00033691471921778175   Iteration 87 of 100, tot loss = 4.324380900668002, l1: 9.594416673510635e-05, l2: 0.00033649392412632873   Iteration 88 of 100, tot loss = 4.334790537303144, l1: 9.608575291250335e-05, l2: 0.00033739330154441467   Iteration 89 of 100, tot loss = 4.3347848171598455, l1: 9.581701326598623e-05, l2: 0.00033766146928459155   Iteration 90 of 100, tot loss = 4.332683975166745, l1: 9.552946636783114e-05, l2: 0.00033773893212330424   Iteration 91 of 100, tot loss = 4.330050063657237, l1: 9.541151846680985e-05, l2: 0.00033759348865874765   Iteration 92 of 100, tot loss = 4.311396185470664, l1: 9.518156457488658e-05, l2: 0.0003359580547586792   Iteration 93 of 100, tot loss = 4.2929355316264655, l1: 9.490010451108608e-05, l2: 0.0003343934491489543   Iteration 94 of 100, tot loss = 4.285256196843817, l1: 9.50684765136449e-05, l2: 0.00033345714380513837   Iteration 95 of 100, tot loss = 4.295166425955923, l1: 9.513943433902521e-05, l2: 0.0003343772089513215   Iteration 96 of 100, tot loss = 4.32172766700387, l1: 9.558991204509464e-05, l2: 0.000336582855652523   Iteration 97 of 100, tot loss = 4.345821344975343, l1: 9.618317791634028e-05, l2: 0.0003383989578182569   Iteration 98 of 100, tot loss = 4.344336109501975, l1: 9.606340590464093e-05, l2: 0.0003383702063061088   Iteration 99 of 100, tot loss = 4.342308982454165, l1: 9.596682712347528e-05, l2: 0.00033826407235035803   Iteration 100 of 100, tot loss = 4.321664429903031, l1: 9.572337537974818e-05, l2: 0.000336443068808876
   End of epoch 1350; saving model... 

Epoch 1351 of 2000
   Iteration 1 of 100, tot loss = 5.893246173858643, l1: 0.00010293970262864605, l2: 0.0004863849317189306   Iteration 2 of 100, tot loss = 5.871757745742798, l1: 0.00010871196718653664, l2: 0.0004784638003911823   Iteration 3 of 100, tot loss = 5.800914605458577, l1: 0.00011464994167909026, l2: 0.00046544150488140684   Iteration 4 of 100, tot loss = 5.51738977432251, l1: 0.00011646910570561886, l2: 0.00043526985973585397   Iteration 5 of 100, tot loss = 5.145373106002808, l1: 0.00010450501722516491, l2: 0.00041003228398039937   Iteration 6 of 100, tot loss = 5.014377156893413, l1: 0.00010583982899940263, l2: 0.00039559788031813997   Iteration 7 of 100, tot loss = 4.848151854106358, l1: 0.0001050764590867662, l2: 0.00037973871803842485   Iteration 8 of 100, tot loss = 4.716746002435684, l1: 0.00010503284465812612, l2: 0.0003666417469503358   Iteration 9 of 100, tot loss = 4.8085907035403785, l1: 0.0001072912919981819, l2: 0.0003735677681915048   Iteration 10 of 100, tot loss = 4.823787951469422, l1: 0.0001073118852218613, l2: 0.0003750669042346999   Iteration 11 of 100, tot loss = 4.993361017920754, l1: 0.00011049281006721272, l2: 0.00038884328519502145   Iteration 12 of 100, tot loss = 4.965919554233551, l1: 0.00010924405978585128, l2: 0.0003873478902581458   Iteration 13 of 100, tot loss = 5.0740721225738525, l1: 0.00011218336910063114, l2: 0.0003952238386353621   Iteration 14 of 100, tot loss = 5.1423830815723965, l1: 0.00011342026690337142, l2: 0.0004008180382827829   Iteration 15 of 100, tot loss = 5.147609853744507, l1: 0.00011332738019215564, l2: 0.0004014336038380861   Iteration 16 of 100, tot loss = 5.1895072013139725, l1: 0.00011534688746905886, l2: 0.00040360382990911603   Iteration 17 of 100, tot loss = 5.094721429488239, l1: 0.0001143494895693627, l2: 0.00039512265185989877   Iteration 18 of 100, tot loss = 4.999783939785427, l1: 0.00011250561308568447, l2: 0.0003874727796452741   Iteration 19 of 100, tot loss = 4.997228898500142, l1: 0.00011112221316005544, l2: 0.00038860067510732303   Iteration 20 of 100, tot loss = 4.839963668584824, l1: 0.00010792169523483608, l2: 0.0003760746702027973   Iteration 21 of 100, tot loss = 4.746287567274911, l1: 0.00010631415387338383, l2: 0.00036831460117052   Iteration 22 of 100, tot loss = 4.847806708379225, l1: 0.00010779044367320074, l2: 0.00037699022637256843   Iteration 23 of 100, tot loss = 4.880580689596093, l1: 0.00010809076364081272, l2: 0.00037996730458436775   Iteration 24 of 100, tot loss = 5.018155013521512, l1: 0.00011104234939314968, l2: 0.0003907731503810889   Iteration 25 of 100, tot loss = 5.046478476524353, l1: 0.00011097913840785623, l2: 0.00039366870652884244   Iteration 26 of 100, tot loss = 4.924380352863898, l1: 0.00010822618409852461, l2: 0.000384211848610833   Iteration 27 of 100, tot loss = 4.80798562809273, l1: 0.00010597447032557318, l2: 0.0003748240900070717   Iteration 28 of 100, tot loss = 4.714528547866004, l1: 0.00010454652406873979, l2: 0.0003669063283138842   Iteration 29 of 100, tot loss = 4.75761670490791, l1: 0.00010601946971668252, l2: 0.0003697421970769185   Iteration 30 of 100, tot loss = 4.731901721159617, l1: 0.00010554274267633445, l2: 0.00036764742544619365   Iteration 31 of 100, tot loss = 4.675914429849194, l1: 0.00010511137506616632, l2: 0.0003624800644296732   Iteration 32 of 100, tot loss = 4.674705807119608, l1: 0.00010437090372761304, l2: 0.0003630996739047987   Iteration 33 of 100, tot loss = 4.681820193926494, l1: 0.00010489909883207557, l2: 0.0003632829170921027   Iteration 34 of 100, tot loss = 4.649783677914563, l1: 0.00010362581408972961, l2: 0.0003613525499103536   Iteration 35 of 100, tot loss = 4.718870486531939, l1: 0.00010462230441459854, l2: 0.0003672647410504786   Iteration 36 of 100, tot loss = 4.738839543528027, l1: 0.00010484412996447645, l2: 0.00036903982214021706   Iteration 37 of 100, tot loss = 4.7148209552507145, l1: 0.00010501347207684523, l2: 0.0003664686216349789   Iteration 38 of 100, tot loss = 4.702723230186262, l1: 0.00010482715247474659, l2: 0.00036544516938432167   Iteration 39 of 100, tot loss = 4.663737935897632, l1: 0.00010430221057807405, l2: 0.0003620715817818657   Iteration 40 of 100, tot loss = 4.720508959889412, l1: 0.00010510682659514714, l2: 0.0003669440688099712   Iteration 41 of 100, tot loss = 4.736522049438663, l1: 0.00010446447537638383, l2: 0.0003691877283882804   Iteration 42 of 100, tot loss = 4.687615738028572, l1: 0.00010318528715981215, l2: 0.0003655762851676194   Iteration 43 of 100, tot loss = 4.642698518065519, l1: 0.00010272984590576312, l2: 0.00036154000418867137   Iteration 44 of 100, tot loss = 4.631866105578163, l1: 0.00010267428412837695, l2: 0.00036051232415378433   Iteration 45 of 100, tot loss = 4.6353882339265615, l1: 0.00010268399847619649, l2: 0.00036085482246966824   Iteration 46 of 100, tot loss = 4.590074800926706, l1: 0.00010187981941904265, l2: 0.00035712765827117   Iteration 47 of 100, tot loss = 4.568574705022447, l1: 0.00010161509132384263, l2: 0.0003552423772139554   Iteration 48 of 100, tot loss = 4.538979671895504, l1: 0.00010058098519039049, l2: 0.0003533169804844268   Iteration 49 of 100, tot loss = 4.559811497221188, l1: 0.00010071578933037722, l2: 0.0003552653583845276   Iteration 50 of 100, tot loss = 4.580458567142487, l1: 0.00010130288508662488, l2: 0.0003567429693066515   Iteration 51 of 100, tot loss = 4.598827925382876, l1: 0.00010170852464416028, l2: 0.00035817426596266016   Iteration 52 of 100, tot loss = 4.542476509626095, l1: 0.00010075656274029125, l2: 0.00035349108643108944   Iteration 53 of 100, tot loss = 4.554065490668675, l1: 0.00010106730042025447, l2: 0.0003543392465974837   Iteration 54 of 100, tot loss = 4.564792679415809, l1: 0.00010112440321353022, l2: 0.00035535486259269183   Iteration 55 of 100, tot loss = 4.5584049940109255, l1: 0.00010121524286329408, l2: 0.00035462525437734174   Iteration 56 of 100, tot loss = 4.545705503651074, l1: 0.00010071498127217637, l2: 0.0003538555672223863   Iteration 57 of 100, tot loss = 4.5050709728609055, l1: 0.00010017921886174008, l2: 0.00035032787676308335   Iteration 58 of 100, tot loss = 4.576854543439273, l1: 0.00010119659381071587, l2: 0.0003564888595127709   Iteration 59 of 100, tot loss = 4.600460668741646, l1: 0.00010072985383824019, l2: 0.0003593162118242185   Iteration 60 of 100, tot loss = 4.59765250881513, l1: 0.00010082266865841423, l2: 0.0003589425812000021   Iteration 61 of 100, tot loss = 4.5773086371969, l1: 0.00010014494641762242, l2: 0.00035758591607713033   Iteration 62 of 100, tot loss = 4.596157448907053, l1: 0.0001002616715977051, l2: 0.00035935407164112304   Iteration 63 of 100, tot loss = 4.586423644943843, l1: 0.00010000050996683375, l2: 0.0003586418529194484   Iteration 64 of 100, tot loss = 4.589971309527755, l1: 0.00010031571849822285, l2: 0.0003586814112850334   Iteration 65 of 100, tot loss = 4.600011317546551, l1: 0.00010100021267694851, l2: 0.0003590009178599128   Iteration 66 of 100, tot loss = 4.622997556671952, l1: 0.0001012115680527076, l2: 0.00036108818654390757   Iteration 67 of 100, tot loss = 4.641361967841191, l1: 0.00010153113093545119, l2: 0.00036260506445316096   Iteration 68 of 100, tot loss = 4.637212660382776, l1: 0.00010157483757211594, l2: 0.00036214642739865026   Iteration 69 of 100, tot loss = 4.623020128927369, l1: 0.00010100290814264461, l2: 0.0003612991037527167   Iteration 70 of 100, tot loss = 4.596042423588889, l1: 0.0001002729262316799, l2: 0.0003593313153292651   Iteration 71 of 100, tot loss = 4.586550455697825, l1: 0.00010009725622399906, l2: 0.0003585577888117487   Iteration 72 of 100, tot loss = 4.593799768222703, l1: 0.00010055403941401487, l2: 0.00035882593662487733   Iteration 73 of 100, tot loss = 4.622000117824502, l1: 0.00010126462419140982, l2: 0.0003609353873188199   Iteration 74 of 100, tot loss = 4.5937652893968535, l1: 0.00010079111932616404, l2: 0.0003585854093536319   Iteration 75 of 100, tot loss = 4.614896194140116, l1: 0.0001008238395055135, l2: 0.00036066577954140184   Iteration 76 of 100, tot loss = 4.600479591833918, l1: 0.0001008364068088428, l2: 0.00035921155195913005   Iteration 77 of 100, tot loss = 4.59485451122383, l1: 0.00010069148227959484, l2: 0.0003587939683310143   Iteration 78 of 100, tot loss = 4.567062617876591, l1: 0.00010023772539660477, l2: 0.0003564685357559938   Iteration 79 of 100, tot loss = 4.538671905481363, l1: 9.974556644609245e-05, l2: 0.0003541216234524758   Iteration 80 of 100, tot loss = 4.544398249685765, l1: 9.97393579382333e-05, l2: 0.0003547004668689624   Iteration 81 of 100, tot loss = 4.555021475862573, l1: 0.00010009082307303209, l2: 0.0003554113242483445   Iteration 82 of 100, tot loss = 4.574525631055599, l1: 0.00010008888651699213, l2: 0.00035736367659055536   Iteration 83 of 100, tot loss = 4.586767669183662, l1: 0.00010020943328384759, l2: 0.0003584673336646744   Iteration 84 of 100, tot loss = 4.590490632113957, l1: 0.00010036682085358604, l2: 0.000358682242655223   Iteration 85 of 100, tot loss = 4.592225620325874, l1: 0.00010045332096256864, l2: 0.0003587692415321191   Iteration 86 of 100, tot loss = 4.573805596939353, l1: 0.00010011490413426356, l2: 0.0003572656559997487   Iteration 87 of 100, tot loss = 4.557572308628038, l1: 0.00010016727265928625, l2: 0.0003555899584798367   Iteration 88 of 100, tot loss = 4.569799996235154, l1: 0.00010031671982605158, l2: 0.0003566632796553842   Iteration 89 of 100, tot loss = 4.5731704141316785, l1: 0.00010055286291165257, l2: 0.0003567641783928174   Iteration 90 of 100, tot loss = 4.598458145724402, l1: 0.00010089630814036354, l2: 0.00035894950617350535   Iteration 91 of 100, tot loss = 4.606480597139715, l1: 0.00010058147913346496, l2: 0.000360066579910031   Iteration 92 of 100, tot loss = 4.6105499980242355, l1: 0.00010052470869318906, l2: 0.00036053029077226273   Iteration 93 of 100, tot loss = 4.606102244828337, l1: 0.00010047742335866856, l2: 0.00036013280083763323   Iteration 94 of 100, tot loss = 4.598608763928109, l1: 0.00010034207073832921, l2: 0.0003595188054339118   Iteration 95 of 100, tot loss = 4.628187988933764, l1: 0.0001007090693364214, l2: 0.0003621097294901694   Iteration 96 of 100, tot loss = 4.630420864870151, l1: 0.00010067460289064911, l2: 0.0003623674834519382   Iteration 97 of 100, tot loss = 4.603945875905223, l1: 0.00010006614222932971, l2: 0.00036032844510963956   Iteration 98 of 100, tot loss = 4.638652973029078, l1: 0.00010053574872899288, l2: 0.00036332954803175216   Iteration 99 of 100, tot loss = 4.6463463824204725, l1: 0.00010078476624024769, l2: 0.00036384987183599594   Iteration 100 of 100, tot loss = 4.638802617788315, l1: 0.00010077399987494573, l2: 0.0003631062618660508
   End of epoch 1351; saving model... 

Epoch 1352 of 2000
   Iteration 1 of 100, tot loss = 5.56516695022583, l1: 0.0001320143637713045, l2: 0.000424502301029861   Iteration 2 of 100, tot loss = 5.213860273361206, l1: 0.00011152592196594924, l2: 0.0004098600911675021   Iteration 3 of 100, tot loss = 4.562171220779419, l1: 0.0001077918471613278, l2: 0.0003484252617151166   Iteration 4 of 100, tot loss = 4.5956626534461975, l1: 0.00010502186341909692, l2: 0.00035454439421300776   Iteration 5 of 100, tot loss = 4.146402597427368, l1: 9.701962844701483e-05, l2: 0.0003176206286298111   Iteration 6 of 100, tot loss = 4.221119840939839, l1: 9.931938378334355e-05, l2: 0.00032279259903589264   Iteration 7 of 100, tot loss = 4.272895642689297, l1: 9.958457667380571e-05, l2: 0.0003277049878046715   Iteration 8 of 100, tot loss = 4.056194573640823, l1: 9.723891525936779e-05, l2: 0.000308380544083775   Iteration 9 of 100, tot loss = 4.207240290111965, l1: 9.909626694732449e-05, l2: 0.0003216277643029268   Iteration 10 of 100, tot loss = 4.092090821266174, l1: 9.359775649500079e-05, l2: 0.00031561132782371716   Iteration 11 of 100, tot loss = 4.016374024477872, l1: 9.231371090705083e-05, l2: 0.00030932369488503105   Iteration 12 of 100, tot loss = 4.117071787516276, l1: 9.441426179061334e-05, l2: 0.000317292917316081   Iteration 13 of 100, tot loss = 4.0805747509002686, l1: 9.43511477089487e-05, l2: 0.0003137063277580847   Iteration 14 of 100, tot loss = 4.055514182363238, l1: 9.327423530131844e-05, l2: 0.00031227718344390657   Iteration 15 of 100, tot loss = 4.115281216303507, l1: 9.564193314872682e-05, l2: 0.0003158861897342528   Iteration 16 of 100, tot loss = 4.158689871430397, l1: 9.629058740756591e-05, l2: 0.0003195784020135761   Iteration 17 of 100, tot loss = 4.184479306725895, l1: 9.7218099707628e-05, l2: 0.00032122983333572527   Iteration 18 of 100, tot loss = 4.252609690030416, l1: 9.909675281960517e-05, l2: 0.0003261642195866443   Iteration 19 of 100, tot loss = 4.375638622986643, l1: 0.0001013398508994693, l2: 0.00033622401487694954   Iteration 20 of 100, tot loss = 4.31975679397583, l1: 0.00010054645172203891, l2: 0.00033142923202831296   Iteration 21 of 100, tot loss = 4.3382806323823475, l1: 0.00010166375224278974, l2: 0.0003321643148194111   Iteration 22 of 100, tot loss = 4.397321679375389, l1: 0.00010266580449586564, l2: 0.0003370663669722324   Iteration 23 of 100, tot loss = 4.401252953902535, l1: 0.0001036326303501087, l2: 0.00033649266697466373   Iteration 24 of 100, tot loss = 4.5260123411814375, l1: 0.00010498148427965741, l2: 0.0003476197525742464   Iteration 25 of 100, tot loss = 4.55745038986206, l1: 0.00010610888246446848, l2: 0.0003496361593715847   Iteration 26 of 100, tot loss = 4.559952332423284, l1: 0.00010688492665604617, l2: 0.0003491103091689113   Iteration 27 of 100, tot loss = 4.536601243195711, l1: 0.00010633144389700007, l2: 0.00034732868274052936   Iteration 28 of 100, tot loss = 4.5519760847091675, l1: 0.00010637745825598748, l2: 0.00034882015253450457   Iteration 29 of 100, tot loss = 4.506711614542994, l1: 0.00010477447132620125, l2: 0.0003458966926575221   Iteration 30 of 100, tot loss = 4.5109570026397705, l1: 0.00010405312981068468, l2: 0.0003470425736547137   Iteration 31 of 100, tot loss = 4.5177851953814105, l1: 0.00010385052202412138, l2: 0.00034792800039623774   Iteration 32 of 100, tot loss = 4.687769770622253, l1: 0.00010609714695419825, l2: 0.0003626798325058189   Iteration 33 of 100, tot loss = 4.610142686150291, l1: 0.00010468399010609242, l2: 0.0003563302809859372   Iteration 34 of 100, tot loss = 4.586895045112161, l1: 0.00010370710376317969, l2: 0.0003549824027954053   Iteration 35 of 100, tot loss = 4.565220178876604, l1: 0.0001023635476095868, l2: 0.00035415847212009663   Iteration 36 of 100, tot loss = 4.58344304561615, l1: 0.00010217299131909385, l2: 0.00035617131612121337   Iteration 37 of 100, tot loss = 4.597165043289597, l1: 0.00010253662362260543, l2: 0.0003571798832224984   Iteration 38 of 100, tot loss = 4.589316330457988, l1: 0.00010208440117390924, l2: 0.0003568472346064243   Iteration 39 of 100, tot loss = 4.602129630553416, l1: 0.00010217604516909864, l2: 0.0003580369203775309   Iteration 40 of 100, tot loss = 4.579018491506576, l1: 0.00010142151531908894, l2: 0.00035648033626785037   Iteration 41 of 100, tot loss = 4.5770395848809216, l1: 0.00010179885820678731, l2: 0.0003559051025460088   Iteration 42 of 100, tot loss = 4.553829210145133, l1: 0.00010115824781158673, l2: 0.00035422467494971075   Iteration 43 of 100, tot loss = 4.587347357772117, l1: 0.00010201430116065367, l2: 0.00035672043592311703   Iteration 44 of 100, tot loss = 4.551507527177984, l1: 0.00010108816737060393, l2: 0.00035406258691430344   Iteration 45 of 100, tot loss = 4.588434749179417, l1: 0.00010189850112914832, l2: 0.00035694497540437927   Iteration 46 of 100, tot loss = 4.593017743981403, l1: 0.0001020641330282872, l2: 0.00035723764291184995   Iteration 47 of 100, tot loss = 4.5721231876535615, l1: 0.00010192785795996005, l2: 0.00035528446259125673   Iteration 48 of 100, tot loss = 4.551333660880725, l1: 0.00010089659447961215, l2: 0.0003542367736978728   Iteration 49 of 100, tot loss = 4.583310754931703, l1: 0.00010083020784730586, l2: 0.0003575008699304557   Iteration 50 of 100, tot loss = 4.570489649772644, l1: 0.00010124801061465404, l2: 0.00035580095689510924   Iteration 51 of 100, tot loss = 4.56616083313437, l1: 0.00010100635999526061, l2: 0.00035560972601695755   Iteration 52 of 100, tot loss = 4.55024857704456, l1: 0.00010078850405429526, l2: 0.0003542363559133194   Iteration 53 of 100, tot loss = 4.57198400317498, l1: 0.00010119139517213762, l2: 0.00035600700741853425   Iteration 54 of 100, tot loss = 4.554855514455725, l1: 0.00010098165135045915, l2: 0.00035450390202674533   Iteration 55 of 100, tot loss = 4.54369626045227, l1: 0.0001009406234846789, l2: 0.00035342900479339403   Iteration 56 of 100, tot loss = 4.5155028104782104, l1: 0.00010093457889784727, l2: 0.00035061570451944135   Iteration 57 of 100, tot loss = 4.53242988753737, l1: 0.00010105227007890087, l2: 0.00035219072066084074   Iteration 58 of 100, tot loss = 4.5676397126296475, l1: 0.00010171631282869855, l2: 0.0003550476605345591   Iteration 59 of 100, tot loss = 4.551506070767418, l1: 0.00010195002712060745, l2: 0.0003532005821244191   Iteration 60 of 100, tot loss = 4.524798723061879, l1: 0.00010194131051927494, l2: 0.0003505385638466881   Iteration 61 of 100, tot loss = 4.522152005649004, l1: 0.00010125472163352123, l2: 0.00035096048095958215   Iteration 62 of 100, tot loss = 4.556974068764718, l1: 0.00010159559426556957, l2: 0.00035410181496239775   Iteration 63 of 100, tot loss = 4.544572535015288, l1: 0.00010081250597039119, l2: 0.00035364475006067624   Iteration 64 of 100, tot loss = 4.524473529309034, l1: 0.00010009733523475006, l2: 0.0003523500201936258   Iteration 65 of 100, tot loss = 4.53827470265902, l1: 0.00010063793362440685, l2: 0.00035318953920907985   Iteration 66 of 100, tot loss = 4.512051296956612, l1: 0.0001002550329069367, l2: 0.000350950099042772   Iteration 67 of 100, tot loss = 4.520138544822807, l1: 0.00010026295546880131, l2: 0.0003517509011096502   Iteration 68 of 100, tot loss = 4.520955222494462, l1: 0.00010038250055753023, l2: 0.0003517130242150469   Iteration 69 of 100, tot loss = 4.501597801844279, l1: 0.00010037177454923157, l2: 0.00034978800787307   Iteration 70 of 100, tot loss = 4.529040503501892, l1: 0.00010095139192084649, l2: 0.0003519526607955673   Iteration 71 of 100, tot loss = 4.594175301807027, l1: 0.00010173089603867225, l2: 0.0003576866370743849   Iteration 72 of 100, tot loss = 4.623836593495475, l1: 0.00010209714997068254, l2: 0.00036028651245740347   Iteration 73 of 100, tot loss = 4.640271529759446, l1: 0.00010210980441140598, l2: 0.000361917351455466   Iteration 74 of 100, tot loss = 4.622522151147997, l1: 0.00010165927891943579, l2: 0.00036059293897533034   Iteration 75 of 100, tot loss = 4.6142600154876705, l1: 0.00010159502029030895, l2: 0.00035983098399204513   Iteration 76 of 100, tot loss = 4.6436724756893355, l1: 0.00010231532695271246, l2: 0.0003620519231557601   Iteration 77 of 100, tot loss = 4.642935836469972, l1: 0.00010242179363464041, l2: 0.0003618717926112911   Iteration 78 of 100, tot loss = 4.6381466908332625, l1: 0.00010209202856714957, l2: 0.0003617226428991088   Iteration 79 of 100, tot loss = 4.633920612214487, l1: 0.00010180567832195047, l2: 0.00036158638525919255   Iteration 80 of 100, tot loss = 4.64458096921444, l1: 0.00010226298645648058, l2: 0.00036219511275703553   Iteration 81 of 100, tot loss = 4.680433129086906, l1: 0.00010272710597398403, l2: 0.00036531620921946513   Iteration 82 of 100, tot loss = 4.682139731035, l1: 0.00010285862769772549, l2: 0.00036535534770625484   Iteration 83 of 100, tot loss = 4.693448086819017, l1: 0.00010302301979461033, l2: 0.00036632179083437267   Iteration 84 of 100, tot loss = 4.6746151248614, l1: 0.0001028717696109587, l2: 0.00036458974465772157   Iteration 85 of 100, tot loss = 4.682770518695607, l1: 0.00010311553864191999, l2: 0.00036516151500537116   Iteration 86 of 100, tot loss = 4.695229777069979, l1: 0.00010308114276064538, l2: 0.00036644183646151145   Iteration 87 of 100, tot loss = 4.71131210491575, l1: 0.00010340387359911832, l2: 0.0003677273383926086   Iteration 88 of 100, tot loss = 4.690774939276955, l1: 0.0001027701191147075, l2: 0.00036630737634947184   Iteration 89 of 100, tot loss = 4.7028760481416505, l1: 0.00010270197653267898, l2: 0.0003675856301168587   Iteration 90 of 100, tot loss = 4.690419234169854, l1: 0.00010250541207723371, l2: 0.00036653651299679445   Iteration 91 of 100, tot loss = 4.692669019594297, l1: 0.00010268442177790688, l2: 0.00036658248147712304   Iteration 92 of 100, tot loss = 4.6973463193230005, l1: 0.00010297202270917099, l2: 0.0003667626107293789   Iteration 93 of 100, tot loss = 4.702156154058313, l1: 0.00010320504557595448, l2: 0.00036701057118774264   Iteration 94 of 100, tot loss = 4.680178540818234, l1: 0.00010283287930236683, l2: 0.000365184976080274   Iteration 95 of 100, tot loss = 4.700008196579782, l1: 0.00010335733696640665, l2: 0.000366643484142658   Iteration 96 of 100, tot loss = 4.710794289906819, l1: 0.00010349775148673264, l2: 0.00036758167880179826   Iteration 97 of 100, tot loss = 4.696171689279301, l1: 0.00010323552667558164, l2: 0.0003663816434672083   Iteration 98 of 100, tot loss = 4.676483750343323, l1: 0.00010298057238788734, l2: 0.0003646678038533036   Iteration 99 of 100, tot loss = 4.694494558103157, l1: 0.00010305757009197318, l2: 0.0003663918870381277   Iteration 100 of 100, tot loss = 4.691766703128815, l1: 0.00010318762226233957, l2: 0.0003659890491690021
   End of epoch 1352; saving model... 

Epoch 1353 of 2000
   Iteration 1 of 100, tot loss = 4.866872787475586, l1: 9.138081804849207e-05, l2: 0.00039530647336505353   Iteration 2 of 100, tot loss = 4.978827238082886, l1: 9.852662333287299e-05, l2: 0.0003993560967501253   Iteration 3 of 100, tot loss = 4.006681203842163, l1: 7.909790535146992e-05, l2: 0.000321570216328837   Iteration 4 of 100, tot loss = 4.461442530155182, l1: 9.406465687789023e-05, l2: 0.000352079590811627   Iteration 5 of 100, tot loss = 4.824120283126831, l1: 0.00010615124483592808, l2: 0.00037626078410539775   Iteration 6 of 100, tot loss = 4.517643928527832, l1: 0.00010232514493206206, l2: 0.0003494392512948252   Iteration 7 of 100, tot loss = 4.37898986680167, l1: 9.963779510664088e-05, l2: 0.00033826119241504263   Iteration 8 of 100, tot loss = 4.505654603242874, l1: 0.0001010858341032872, l2: 0.00034947962740261573   Iteration 9 of 100, tot loss = 4.310599591996935, l1: 9.946511676793711e-05, l2: 0.0003315948447885199   Iteration 10 of 100, tot loss = 4.384998416900634, l1: 0.00010011895064963028, l2: 0.0003383808914804831   Iteration 11 of 100, tot loss = 4.483688701282848, l1: 0.00010194452492710711, l2: 0.0003464243464722213   Iteration 12 of 100, tot loss = 4.592718323071797, l1: 0.00010222752886572077, l2: 0.00035704430532253656   Iteration 13 of 100, tot loss = 4.58937483567458, l1: 0.00010202624197028435, l2: 0.00035691124279625143   Iteration 14 of 100, tot loss = 4.597129549298968, l1: 0.00010261788026712435, l2: 0.00035709507522239747   Iteration 15 of 100, tot loss = 4.719435850779216, l1: 0.00010622131570319956, l2: 0.00036572227254509925   Iteration 16 of 100, tot loss = 4.513510920107365, l1: 0.00010165177377530199, l2: 0.0003496993213047972   Iteration 17 of 100, tot loss = 4.614811707945431, l1: 0.00010250335424857708, l2: 0.00035897781901225885   Iteration 18 of 100, tot loss = 4.520603305763668, l1: 0.00010147146066527866, l2: 0.00035058887200041983   Iteration 19 of 100, tot loss = 4.469125051247446, l1: 9.99987090548721e-05, l2: 0.0003469137981913886   Iteration 20 of 100, tot loss = 4.504728370904923, l1: 0.00010118034206243464, l2: 0.0003492924974125344   Iteration 21 of 100, tot loss = 4.573287787891569, l1: 0.00010249021111771331, l2: 0.00035483856932149225   Iteration 22 of 100, tot loss = 4.472976429895922, l1: 9.993581740921151e-05, l2: 0.0003473618271527812   Iteration 23 of 100, tot loss = 4.641434881998145, l1: 0.00010267297756913847, l2: 0.00036147051445289475   Iteration 24 of 100, tot loss = 4.55327083170414, l1: 0.00010009256478345681, l2: 0.00035523452243069187   Iteration 25 of 100, tot loss = 4.598736357688904, l1: 9.992179009714164e-05, l2: 0.00035995184909552334   Iteration 26 of 100, tot loss = 4.554061674154722, l1: 9.922133722284343e-05, l2: 0.0003561848335756132   Iteration 27 of 100, tot loss = 4.508317572099191, l1: 9.834108924325038e-05, l2: 0.0003524906706944522   Iteration 28 of 100, tot loss = 4.493107331650598, l1: 9.827831971571348e-05, l2: 0.00035103241680189967   Iteration 29 of 100, tot loss = 4.544931654272409, l1: 9.879570543175917e-05, l2: 0.0003556974617571666   Iteration 30 of 100, tot loss = 4.566598252455393, l1: 9.977427313666946e-05, l2: 0.0003568855541137358   Iteration 31 of 100, tot loss = 4.608300512836825, l1: 0.000101368742522731, l2: 0.00035946131139362773   Iteration 32 of 100, tot loss = 4.592251565307379, l1: 0.00010169401559778635, l2: 0.0003575311438908102   Iteration 33 of 100, tot loss = 4.604918331810922, l1: 0.00010169573821864712, l2: 0.00035879609640687704   Iteration 34 of 100, tot loss = 4.5835373086087845, l1: 0.0001010532215492704, l2: 0.00035730051047935644   Iteration 35 of 100, tot loss = 4.573368491445269, l1: 0.00010098598566920763, l2: 0.00035635086491570944   Iteration 36 of 100, tot loss = 4.661953366465038, l1: 0.00010173089291735475, l2: 0.0003644644449296821   Iteration 37 of 100, tot loss = 4.6320031559145125, l1: 0.00010208751130424364, l2: 0.00036111280575356876   Iteration 38 of 100, tot loss = 4.647455595041576, l1: 0.00010221835471288374, l2: 0.0003625272055567046   Iteration 39 of 100, tot loss = 4.653195200822292, l1: 0.00010271023095880922, l2: 0.00036260928960146906   Iteration 40 of 100, tot loss = 4.6377214103937145, l1: 0.00010275710583300679, l2: 0.00036101503610552754   Iteration 41 of 100, tot loss = 4.67200965706895, l1: 0.00010312380485318987, l2: 0.0003640771621310084   Iteration 42 of 100, tot loss = 4.654986571697962, l1: 0.00010284002660011451, l2: 0.0003626586317079186   Iteration 43 of 100, tot loss = 4.632787308027578, l1: 0.0001024832027026203, l2: 0.000360795529220838   Iteration 44 of 100, tot loss = 4.6830089661208065, l1: 0.00010297836367747301, l2: 0.0003653225348071746   Iteration 45 of 100, tot loss = 4.721401990784539, l1: 0.00010421119958563294, l2: 0.00036792900161041565   Iteration 46 of 100, tot loss = 4.7059114005254665, l1: 0.00010388164280258302, l2: 0.00036670949971860114   Iteration 47 of 100, tot loss = 4.676557416611529, l1: 0.0001039844830031259, l2: 0.0003636712610890019   Iteration 48 of 100, tot loss = 4.6606849655508995, l1: 0.00010378928944495176, l2: 0.00036227920948779985   Iteration 49 of 100, tot loss = 4.686153930060717, l1: 0.00010447406377203819, l2: 0.0003641413320426126   Iteration 50 of 100, tot loss = 4.686525709629059, l1: 0.00010447948567161802, l2: 0.0003641730884555727   Iteration 51 of 100, tot loss = 4.665377109658484, l1: 0.00010420425457471306, l2: 0.00036233345927743645   Iteration 52 of 100, tot loss = 4.661050766706467, l1: 0.0001037550823289516, l2: 0.00036234999760591355   Iteration 53 of 100, tot loss = 4.658540102670777, l1: 0.0001038345492489961, l2: 0.0003620194642417976   Iteration 54 of 100, tot loss = 4.636826919184791, l1: 0.00010346510255961523, l2: 0.00036021759270052253   Iteration 55 of 100, tot loss = 4.604246146028692, l1: 0.00010307770102041435, l2: 0.00035734691667709163   Iteration 56 of 100, tot loss = 4.610988061342921, l1: 0.00010367271462590517, l2: 0.0003574260943943435   Iteration 57 of 100, tot loss = 4.598178811240614, l1: 0.0001033794301691702, l2: 0.0003564384538198315   Iteration 58 of 100, tot loss = 4.600859319341594, l1: 0.00010321572643016256, l2: 0.0003568702081322734   Iteration 59 of 100, tot loss = 4.583386960676161, l1: 0.00010296414645489353, l2: 0.00035537455242790154   Iteration 60 of 100, tot loss = 4.5982516745726265, l1: 0.00010335583932222411, l2: 0.0003564693315032249   Iteration 61 of 100, tot loss = 4.575835808378751, l1: 0.00010292656195099985, l2: 0.00035465702216034054   Iteration 62 of 100, tot loss = 4.58285681855294, l1: 0.00010307194251283598, l2: 0.00035521374206103746   Iteration 63 of 100, tot loss = 4.641477094756232, l1: 0.00010395530914557019, l2: 0.00036019240256531963   Iteration 64 of 100, tot loss = 4.659285956993699, l1: 0.00010426118871009749, l2: 0.0003616674089244043   Iteration 65 of 100, tot loss = 4.654732089776259, l1: 0.00010446875625907873, l2: 0.0003610044547643226   Iteration 66 of 100, tot loss = 4.638901535308722, l1: 0.00010416266019147412, l2: 0.0003597274952688057   Iteration 67 of 100, tot loss = 4.657020118699145, l1: 0.00010471712149994156, l2: 0.00036098489254615755   Iteration 68 of 100, tot loss = 4.624660535770304, l1: 0.0001041588388141142, l2: 0.00035830721689682617   Iteration 69 of 100, tot loss = 4.607631857844367, l1: 0.0001041754870696594, l2: 0.0003565877007043826   Iteration 70 of 100, tot loss = 4.594481357506344, l1: 0.00010380917181984322, l2: 0.0003556389656815944   Iteration 71 of 100, tot loss = 4.5571871626545, l1: 0.00010304575063317725, l2: 0.00035267296734161046   Iteration 72 of 100, tot loss = 4.571505515111817, l1: 0.00010304879530546411, l2: 0.0003541017576935701   Iteration 73 of 100, tot loss = 4.5461120785099185, l1: 0.00010251005912560026, l2: 0.0003521011501218971   Iteration 74 of 100, tot loss = 4.575029590645352, l1: 0.00010304510106908383, l2: 0.000354457858987656   Iteration 75 of 100, tot loss = 4.578070562680562, l1: 0.00010323831093652795, l2: 0.00035456874640658496   Iteration 76 of 100, tot loss = 4.598032579610222, l1: 0.00010364770921449618, l2: 0.0003561555495252833   Iteration 77 of 100, tot loss = 4.592979968368233, l1: 0.00010362071846536203, l2: 0.0003556772789089372   Iteration 78 of 100, tot loss = 4.6008712741044855, l1: 0.00010385142731814622, l2: 0.00035623570035498304   Iteration 79 of 100, tot loss = 4.594834205470508, l1: 0.0001036661382892782, l2: 0.0003558172825193386   Iteration 80 of 100, tot loss = 4.589530335366726, l1: 0.00010325607399863657, l2: 0.00035569695974118075   Iteration 81 of 100, tot loss = 4.581191844410366, l1: 0.00010337309091190957, l2: 0.0003547460935940897   Iteration 82 of 100, tot loss = 4.573429088766982, l1: 0.00010343124245637005, l2: 0.0003539116665573291   Iteration 83 of 100, tot loss = 4.549842459609709, l1: 0.00010268135086236331, l2: 0.00035230289525436573   Iteration 84 of 100, tot loss = 4.563098213502339, l1: 0.00010279260455025083, l2: 0.0003535172171333605   Iteration 85 of 100, tot loss = 4.559666794889114, l1: 0.00010264644828138819, l2: 0.00035332023142152193   Iteration 86 of 100, tot loss = 4.58192637770675, l1: 0.00010326494322175994, l2: 0.00035492769529810175   Iteration 87 of 100, tot loss = 4.574538243227992, l1: 0.0001032989759927471, l2: 0.00035415484885091027   Iteration 88 of 100, tot loss = 4.566842259331183, l1: 0.00010291466982049646, l2: 0.0003537695564856139   Iteration 89 of 100, tot loss = 4.583794723735767, l1: 0.00010344052023948903, l2: 0.00035493895218555887   Iteration 90 of 100, tot loss = 4.567474212911394, l1: 0.00010315929018058038, l2: 0.00035358813086835045   Iteration 91 of 100, tot loss = 4.574375871773604, l1: 0.00010303020238867388, l2: 0.0003544073846129762   Iteration 92 of 100, tot loss = 4.600789558628331, l1: 0.00010372972377334246, l2: 0.000356349231720821   Iteration 93 of 100, tot loss = 4.642821723415006, l1: 0.0001042238424512713, l2: 0.0003600583301596744   Iteration 94 of 100, tot loss = 4.647463835300283, l1: 0.00010427042608616249, l2: 0.0003604759574258462   Iteration 95 of 100, tot loss = 4.651441198901126, l1: 0.00010436509776474467, l2: 0.0003607790221729757   Iteration 96 of 100, tot loss = 4.6601993553340435, l1: 0.00010427680858053161, l2: 0.00036174312693522853   Iteration 97 of 100, tot loss = 4.670050243741458, l1: 0.00010461149817199407, l2: 0.0003623935263771948   Iteration 98 of 100, tot loss = 4.690553218734507, l1: 0.00010511552492823043, l2: 0.00036393979722301343   Iteration 99 of 100, tot loss = 4.675783837684477, l1: 0.00010488844694037019, l2: 0.0003626899372767469   Iteration 100 of 100, tot loss = 4.648661079406739, l1: 0.00010442554081237176, l2: 0.000360440567455953
   End of epoch 1353; saving model... 

Epoch 1354 of 2000
   Iteration 1 of 100, tot loss = 4.237384796142578, l1: 0.00012150860129622743, l2: 0.0003022298915311694   Iteration 2 of 100, tot loss = 4.583593368530273, l1: 0.00012681852604146115, l2: 0.000331540810293518   Iteration 3 of 100, tot loss = 5.562600612640381, l1: 0.00013296234101289883, l2: 0.0004232977031885336   Iteration 4 of 100, tot loss = 5.807570219039917, l1: 0.00012904716277262196, l2: 0.0004517098350333981   Iteration 5 of 100, tot loss = 4.956261682510376, l1: 0.0001118728847359307, l2: 0.0003837532625766471   Iteration 6 of 100, tot loss = 4.960210045178731, l1: 0.00011252180775045417, l2: 0.0003834991778906745   Iteration 7 of 100, tot loss = 4.77740216255188, l1: 0.00010840046577089067, l2: 0.000369339734399026   Iteration 8 of 100, tot loss = 5.044291943311691, l1: 0.00011105537305411417, l2: 0.0003933738134946907   Iteration 9 of 100, tot loss = 5.062795135709974, l1: 0.00010927128621713362, l2: 0.0003970082244551223   Iteration 10 of 100, tot loss = 5.104775309562683, l1: 0.0001099691267882008, l2: 0.00040050840034382417   Iteration 11 of 100, tot loss = 5.1678480018268935, l1: 0.00011191804514965042, l2: 0.00040486675226764584   Iteration 12 of 100, tot loss = 4.994405766328176, l1: 0.00010912963504476163, l2: 0.00039031094032300945   Iteration 13 of 100, tot loss = 5.076075975711529, l1: 0.00011065178506004695, l2: 0.0003969558109439766   Iteration 14 of 100, tot loss = 5.008478420121329, l1: 0.00011081405110806892, l2: 0.0003900337895694455   Iteration 15 of 100, tot loss = 4.9249717712402346, l1: 0.00010940868572409576, l2: 0.0003830884903436527   Iteration 16 of 100, tot loss = 4.78209513425827, l1: 0.00010700816119424417, l2: 0.0003712013522090274   Iteration 17 of 100, tot loss = 4.814748567693374, l1: 0.00010802880955024568, l2: 0.00037344604617614737   Iteration 18 of 100, tot loss = 4.759348816341824, l1: 0.00010627593858064049, l2: 0.0003696589418622251   Iteration 19 of 100, tot loss = 4.715654423362331, l1: 0.00010503081476168805, l2: 0.00036653462647288844   Iteration 20 of 100, tot loss = 4.820214438438415, l1: 0.00010621228284435347, l2: 0.00037580915886792354   Iteration 21 of 100, tot loss = 4.736992007210141, l1: 0.00010347858222389949, l2: 0.0003702206172636666   Iteration 22 of 100, tot loss = 4.660190755670721, l1: 0.00010283532041781159, l2: 0.00036318375416819686   Iteration 23 of 100, tot loss = 4.653750357420548, l1: 0.000102657164224302, l2: 0.0003627178717262881   Iteration 24 of 100, tot loss = 4.59362272421519, l1: 0.00010157294339781704, l2: 0.0003577893294277601   Iteration 25 of 100, tot loss = 4.5550510025024415, l1: 0.00010052479192381724, l2: 0.00035498030949383976   Iteration 26 of 100, tot loss = 4.50503211755019, l1: 0.00010089798245014838, l2: 0.00034960523043992   Iteration 27 of 100, tot loss = 4.5382021444815175, l1: 0.00010145385426262187, l2: 0.0003523663601501741   Iteration 28 of 100, tot loss = 4.510980597564152, l1: 9.989362206397345e-05, l2: 0.0003512044375903705   Iteration 29 of 100, tot loss = 4.4525060982539735, l1: 9.880474292380125e-05, l2: 0.00034644586738632544   Iteration 30 of 100, tot loss = 4.452396217981974, l1: 9.836805850985305e-05, l2: 0.00034687156391252454   Iteration 31 of 100, tot loss = 4.430553643934188, l1: 9.806259150800836e-05, l2: 0.0003449927734759366   Iteration 32 of 100, tot loss = 4.470982767641544, l1: 9.896438916712214e-05, l2: 0.00034813388947441126   Iteration 33 of 100, tot loss = 4.546401161136049, l1: 0.0001001519414068863, l2: 0.0003544881769674689   Iteration 34 of 100, tot loss = 4.558739528936498, l1: 0.00010058331516869229, l2: 0.00035529063975178255   Iteration 35 of 100, tot loss = 4.54718005997794, l1: 0.00010080145345585022, l2: 0.0003539165544290362   Iteration 36 of 100, tot loss = 4.548425402906206, l1: 0.0001007127762527994, l2: 0.0003541297664924059   Iteration 37 of 100, tot loss = 4.557250931456283, l1: 0.00010077578927676596, l2: 0.0003549493058754534   Iteration 38 of 100, tot loss = 4.5373294604452035, l1: 0.00010100386603689433, l2: 0.00035272908165720045   Iteration 39 of 100, tot loss = 4.522914103972606, l1: 0.00010059523321195648, l2: 0.0003516961792812277   Iteration 40 of 100, tot loss = 4.53731552362442, l1: 0.00010128804597115959, l2: 0.00035244350765424316   Iteration 41 of 100, tot loss = 4.569973864206454, l1: 0.00010182736390028944, l2: 0.00035517002284106593   Iteration 42 of 100, tot loss = 4.567013922191801, l1: 0.00010222872662693372, l2: 0.00035447266522429084   Iteration 43 of 100, tot loss = 4.573478521302689, l1: 0.0001022894642563526, l2: 0.00035505838723322595   Iteration 44 of 100, tot loss = 4.568360740488226, l1: 0.00010233291744622296, l2: 0.00035450315606960265   Iteration 45 of 100, tot loss = 4.572926182217068, l1: 0.00010267191011937232, l2: 0.0003546207072860044   Iteration 46 of 100, tot loss = 4.582443144010461, l1: 0.000102969867878963, l2: 0.00035527444529377493   Iteration 47 of 100, tot loss = 4.5573512036749655, l1: 0.00010225290153609054, l2: 0.0003534822177979104   Iteration 48 of 100, tot loss = 4.553326040506363, l1: 0.00010228036391405719, l2: 0.00035305223870333674   Iteration 49 of 100, tot loss = 4.622549504649882, l1: 0.00010381491888345371, l2: 0.0003584400297508442   Iteration 50 of 100, tot loss = 4.6081770372390745, l1: 0.00010318664375517983, l2: 0.0003576310581411235   Iteration 51 of 100, tot loss = 4.5916925364849615, l1: 0.00010276684235363705, l2: 0.0003564024099272073   Iteration 52 of 100, tot loss = 4.579206709678356, l1: 0.00010245092443465882, l2: 0.00035546974546517816   Iteration 53 of 100, tot loss = 4.5701242887748865, l1: 0.00010276187115452112, l2: 0.00035425055720448   Iteration 54 of 100, tot loss = 4.541408997994882, l1: 0.00010226941763162527, l2: 0.00035187148158136687   Iteration 55 of 100, tot loss = 4.54140341498635, l1: 0.0001022269946009725, l2: 0.00035191334662323985   Iteration 56 of 100, tot loss = 4.597549932343619, l1: 0.00010319751936549437, l2: 0.0003565574736837464   Iteration 57 of 100, tot loss = 4.570451757364106, l1: 0.0001031131116114761, l2: 0.0003539320638529924   Iteration 58 of 100, tot loss = 4.556616080218348, l1: 0.00010327142815091568, l2: 0.0003523901794888561   Iteration 59 of 100, tot loss = 4.6026652263382735, l1: 0.00010371722348312595, l2: 0.00035654929919381453   Iteration 60 of 100, tot loss = 4.565910053253174, l1: 0.00010322405681411813, l2: 0.0003533669485477731   Iteration 61 of 100, tot loss = 4.547471191062302, l1: 0.0001028869069085006, l2: 0.0003518602122804608   Iteration 62 of 100, tot loss = 4.5496110262409335, l1: 0.00010296352792788492, l2: 0.0003519975749265042   Iteration 63 of 100, tot loss = 4.522417541534182, l1: 0.00010256177963114284, l2: 0.000349679974735611   Iteration 64 of 100, tot loss = 4.494850751012564, l1: 0.00010198075295875242, l2: 0.00034750432246255514   Iteration 65 of 100, tot loss = 4.460527122937716, l1: 0.00010143727474944451, l2: 0.00034461543769933857   Iteration 66 of 100, tot loss = 4.469251383434642, l1: 0.0001019690633697654, l2: 0.00034495607454146284   Iteration 67 of 100, tot loss = 4.481335935307972, l1: 0.00010226558835928523, l2: 0.0003458680046388685   Iteration 68 of 100, tot loss = 4.463473772301393, l1: 0.00010202940933702528, l2: 0.0003443179670926731   Iteration 69 of 100, tot loss = 4.439233928486921, l1: 0.00010172789278431742, l2: 0.00034219549928346407   Iteration 70 of 100, tot loss = 4.434386570113046, l1: 0.00010169643516876801, l2: 0.0003417422209167853   Iteration 71 of 100, tot loss = 4.436994649994541, l1: 0.00010212830141388392, l2: 0.00034157116241543227   Iteration 72 of 100, tot loss = 4.428210420740975, l1: 0.00010194651728549313, l2: 0.00034087452351943485   Iteration 73 of 100, tot loss = 4.42332639759534, l1: 0.00010137179037572, l2: 0.0003409608480941555   Iteration 74 of 100, tot loss = 4.412121611672479, l1: 0.00010109593613133011, l2: 0.0003401162236538791   Iteration 75 of 100, tot loss = 4.385377858479818, l1: 0.00010068635436861466, l2: 0.00033785143013422686   Iteration 76 of 100, tot loss = 4.374501786733928, l1: 0.00010052621243965787, l2: 0.00033692396468004997   Iteration 77 of 100, tot loss = 4.349552002820102, l1: 0.00010006132406370061, l2: 0.0003348938748845226   Iteration 78 of 100, tot loss = 4.347537593963819, l1: 9.98961041682961e-05, l2: 0.0003348576540772158   Iteration 79 of 100, tot loss = 4.3463659014882925, l1: 9.95795910771276e-05, l2: 0.0003350569978592139   Iteration 80 of 100, tot loss = 4.355817773938179, l1: 9.947380322046228e-05, l2: 0.0003361079729074845   Iteration 81 of 100, tot loss = 4.3420934471083275, l1: 9.918599154147675e-05, l2: 0.0003350233518300049   Iteration 82 of 100, tot loss = 4.3289088185240585, l1: 9.874931492959149e-05, l2: 0.0003341415654953069   Iteration 83 of 100, tot loss = 4.312860655497356, l1: 9.815239484095105e-05, l2: 0.00033313366942141337   Iteration 84 of 100, tot loss = 4.299052161829812, l1: 9.778953863999396e-05, l2: 0.0003321156760821829   Iteration 85 of 100, tot loss = 4.284068020652322, l1: 9.718363302561235e-05, l2: 0.00033122316749273416   Iteration 86 of 100, tot loss = 4.26680859854055, l1: 9.692886513136706e-05, l2: 0.00032975199293253164   Iteration 87 of 100, tot loss = 4.277970324987653, l1: 9.74790973173342e-05, l2: 0.0003303179332395031   Iteration 88 of 100, tot loss = 4.276986994526603, l1: 9.759314736418838e-05, l2: 0.00033010554976830133   Iteration 89 of 100, tot loss = 4.280936203645856, l1: 9.779606082017554e-05, l2: 0.0003302975571151886   Iteration 90 of 100, tot loss = 4.27129975689782, l1: 9.737496608674216e-05, l2: 0.00032975500717940224   Iteration 91 of 100, tot loss = 4.283993252031096, l1: 9.728796934583582e-05, l2: 0.0003311113535316538   Iteration 92 of 100, tot loss = 4.288502130819404, l1: 9.709069490781985e-05, l2: 0.0003317595159308479   Iteration 93 of 100, tot loss = 4.324767038386355, l1: 9.768995283026328e-05, l2: 0.0003347867486124698   Iteration 94 of 100, tot loss = 4.300406747675956, l1: 9.738562035404067e-05, l2: 0.00033265505214543777   Iteration 95 of 100, tot loss = 4.27429676808809, l1: 9.695141270640306e-05, l2: 0.0003304782619172903   Iteration 96 of 100, tot loss = 4.279854965706666, l1: 9.661597380272724e-05, l2: 0.00033136952045727713   Iteration 97 of 100, tot loss = 4.290865556480958, l1: 9.689529509056772e-05, l2: 0.0003321912581012248   Iteration 98 of 100, tot loss = 4.268864283756334, l1: 9.630018442350069e-05, l2: 0.0003305862416282809   Iteration 99 of 100, tot loss = 4.274662598214968, l1: 9.640462634375912e-05, l2: 0.0003310616310119083   Iteration 100 of 100, tot loss = 4.297352330684662, l1: 9.650721895013704e-05, l2: 0.00033322801158647053
   End of epoch 1354; saving model... 

Epoch 1355 of 2000
   Iteration 1 of 100, tot loss = 5.0087890625, l1: 0.00010669926996342838, l2: 0.0003941796603612602   Iteration 2 of 100, tot loss = 5.924234390258789, l1: 0.0001200880651595071, l2: 0.00047233537770807743   Iteration 3 of 100, tot loss = 5.237809816996257, l1: 0.00011046648432966322, l2: 0.00041331449756398797   Iteration 4 of 100, tot loss = 5.774149179458618, l1: 0.00011798430205089971, l2: 0.0004594306228682399   Iteration 5 of 100, tot loss = 5.283433341979981, l1: 0.00011081108823418617, l2: 0.0004175322537776083   Iteration 6 of 100, tot loss = 4.907461524009705, l1: 0.00010717080537384997, l2: 0.00038357535474157584   Iteration 7 of 100, tot loss = 5.470623050417219, l1: 0.00011566827847023628, l2: 0.0004313940257166645   Iteration 8 of 100, tot loss = 5.588911265134811, l1: 0.00011887167784152552, l2: 0.0004400194484333042   Iteration 9 of 100, tot loss = 5.633720318476359, l1: 0.00011820629636834686, l2: 0.00044516573931711417   Iteration 10 of 100, tot loss = 5.359433197975159, l1: 0.00011120882190880365, l2: 0.00042473450012039395   Iteration 11 of 100, tot loss = 5.4582058949904, l1: 0.0001155922132470137, l2: 0.00043022837648591536   Iteration 12 of 100, tot loss = 5.591652015844981, l1: 0.00011664286709371179, l2: 0.0004425223378348164   Iteration 13 of 100, tot loss = 5.4953397787534275, l1: 0.00011489687536735661, l2: 0.0004346371032834913   Iteration 14 of 100, tot loss = 5.609238675662449, l1: 0.00011625613011087157, l2: 0.0004446677360517372   Iteration 15 of 100, tot loss = 5.599996836980184, l1: 0.00011726374699113269, l2: 0.00044273593618224065   Iteration 16 of 100, tot loss = 5.560292676091194, l1: 0.00011653905403363751, l2: 0.00043949021528533194   Iteration 17 of 100, tot loss = 5.4155815629398125, l1: 0.00011381850173861226, l2: 0.0004277396561947706   Iteration 18 of 100, tot loss = 5.5428933832380505, l1: 0.00011681155996888669, l2: 0.0004374777806030276   Iteration 19 of 100, tot loss = 5.497787350102475, l1: 0.00011577380677121447, l2: 0.00043400493067517683   Iteration 20 of 100, tot loss = 5.3711827278137205, l1: 0.00011268555226706667, l2: 0.0004244327232299838   Iteration 21 of 100, tot loss = 5.309625852675665, l1: 0.00011257298406451896, l2: 0.0004183896048094279   Iteration 22 of 100, tot loss = 5.297217997637662, l1: 0.00011290689028762493, l2: 0.0004168149138091725   Iteration 23 of 100, tot loss = 5.239106385604195, l1: 0.00011161065349765325, l2: 0.0004122999892823155   Iteration 24 of 100, tot loss = 5.305720408757527, l1: 0.00011321842991189139, l2: 0.00041735361507259466   Iteration 25 of 100, tot loss = 5.224809150695801, l1: 0.00011306732994853519, l2: 0.0004094135883497074   Iteration 26 of 100, tot loss = 5.15711679825416, l1: 0.00011215916352879364, l2: 0.0004035525204157099   Iteration 27 of 100, tot loss = 5.187600859889278, l1: 0.00011334483470246023, l2: 0.00040541525608946187   Iteration 28 of 100, tot loss = 5.075825248445783, l1: 0.00011080397013886665, l2: 0.00039677855914175907   Iteration 29 of 100, tot loss = 5.10113707904158, l1: 0.00011127186095610999, l2: 0.0003988418516357718   Iteration 30 of 100, tot loss = 5.163329935073852, l1: 0.00011191044844357141, l2: 0.00040442255024875827   Iteration 31 of 100, tot loss = 5.230493376331944, l1: 0.00011320640283708101, l2: 0.0004098429405618639   Iteration 32 of 100, tot loss = 5.249095007777214, l1: 0.00011334017915487493, l2: 0.00041156932775265886   Iteration 33 of 100, tot loss = 5.230837287324848, l1: 0.00011319330124851231, l2: 0.0004098904332336546   Iteration 34 of 100, tot loss = 5.217852578443639, l1: 0.00011255621628900852, l2: 0.0004092290469356265   Iteration 35 of 100, tot loss = 5.181339972359794, l1: 0.00011250580771177608, l2: 0.0004056281946499699   Iteration 36 of 100, tot loss = 5.236908753712972, l1: 0.000113484482931704, l2: 0.0004102063980503266   Iteration 37 of 100, tot loss = 5.289454511694006, l1: 0.00011430433536392033, l2: 0.000414641121342602   Iteration 38 of 100, tot loss = 5.253817771610461, l1: 0.00011333910371927471, l2: 0.0004120426789373404   Iteration 39 of 100, tot loss = 5.25316246961936, l1: 0.000113342997139672, l2: 0.0004119732553730361   Iteration 40 of 100, tot loss = 5.209586882591248, l1: 0.00011226289343539975, l2: 0.00040869580006983595   Iteration 41 of 100, tot loss = 5.15070616326681, l1: 0.00011144730728474537, l2: 0.0004036233138588325   Iteration 42 of 100, tot loss = 5.126182351793561, l1: 0.00011071571905285098, l2: 0.0004019025211510736   Iteration 43 of 100, tot loss = 5.121938084447106, l1: 0.00011104721632490995, l2: 0.0004011465975646536   Iteration 44 of 100, tot loss = 5.108751513741233, l1: 0.00011071565040765563, l2: 0.00040015950600553134   Iteration 45 of 100, tot loss = 5.0482535786098905, l1: 0.00010985877120725086, l2: 0.00039496659139533425   Iteration 46 of 100, tot loss = 5.035709018292635, l1: 0.00011010043034848817, l2: 0.0003934704762524115   Iteration 47 of 100, tot loss = 5.046668539655969, l1: 0.00011030655576255509, l2: 0.00039436030319267687   Iteration 48 of 100, tot loss = 5.041484047969182, l1: 0.00010995283332704275, l2: 0.0003941955762153763   Iteration 49 of 100, tot loss = 4.986370913836421, l1: 0.00010921628719578231, l2: 0.00038942080899854477   Iteration 50 of 100, tot loss = 4.9694071292877195, l1: 0.00010859377267479431, l2: 0.0003883469451102428   Iteration 51 of 100, tot loss = 5.017996965670118, l1: 0.00010908276717778862, l2: 0.00039271693351432025   Iteration 52 of 100, tot loss = 5.018145203590393, l1: 0.00010944445763855653, l2: 0.00039237006729728397   Iteration 53 of 100, tot loss = 5.032059363599093, l1: 0.00010957935365643967, l2: 0.0003936265868174334   Iteration 54 of 100, tot loss = 5.001955266352053, l1: 0.00010879529354160359, l2: 0.0003914002372766845   Iteration 55 of 100, tot loss = 5.003525460850109, l1: 0.00010883048082839443, l2: 0.0003915220686890693   Iteration 56 of 100, tot loss = 5.012890402759824, l1: 0.00010862452700166614, l2: 0.0003926645172863833   Iteration 57 of 100, tot loss = 4.971878570422792, l1: 0.00010794133841205111, l2: 0.0003892465222937365   Iteration 58 of 100, tot loss = 4.950316885422016, l1: 0.00010773085278774018, l2: 0.0003873008390655741   Iteration 59 of 100, tot loss = 4.96131097260168, l1: 0.00010821769183937665, l2: 0.00038791340933639116   Iteration 60 of 100, tot loss = 4.9436811765035, l1: 0.00010791673709415287, l2: 0.00038645138459590574   Iteration 61 of 100, tot loss = 4.963783264160156, l1: 0.00010775173013527939, l2: 0.00038862659989047   Iteration 62 of 100, tot loss = 4.957417057406518, l1: 0.00010754771110735038, l2: 0.0003881939980689616   Iteration 63 of 100, tot loss = 4.985172604757642, l1: 0.00010794028172053263, l2: 0.00039057698265265023   Iteration 64 of 100, tot loss = 4.961445722728968, l1: 0.00010737030305563167, l2: 0.0003887742727783916   Iteration 65 of 100, tot loss = 4.965375082309429, l1: 0.00010757971853868535, l2: 0.0003889577933408033   Iteration 66 of 100, tot loss = 5.01282206809882, l1: 0.00010840262670667884, l2: 0.00039287958387638247   Iteration 67 of 100, tot loss = 5.0016799435686705, l1: 0.00010821867610176733, l2: 0.00039194932176189414   Iteration 68 of 100, tot loss = 4.999973300625296, l1: 0.00010809757671214517, l2: 0.00039189975659075356   Iteration 69 of 100, tot loss = 5.028946790142336, l1: 0.00010877198903893958, l2: 0.0003941226931983956   Iteration 70 of 100, tot loss = 5.02570172377995, l1: 0.00010857143045411379, l2: 0.00039399874513037505   Iteration 71 of 100, tot loss = 5.0092198445763385, l1: 0.000108255774533281, l2: 0.00039266621321767676   Iteration 72 of 100, tot loss = 4.981766684187783, l1: 0.00010808891546629538, l2: 0.0003900877561843825   Iteration 73 of 100, tot loss = 4.965711204972986, l1: 0.00010779028082041594, l2: 0.0003887808429992643   Iteration 74 of 100, tot loss = 4.924486456690608, l1: 0.00010711038246242695, l2: 0.00038533826650441244   Iteration 75 of 100, tot loss = 4.892058121363322, l1: 0.00010661486764244425, l2: 0.0003825909475563094   Iteration 76 of 100, tot loss = 4.855631323237168, l1: 0.00010596798605738993, l2: 0.0003795951492257269   Iteration 77 of 100, tot loss = 4.855767723801848, l1: 0.00010620460066152132, l2: 0.0003793721749483571   Iteration 78 of 100, tot loss = 4.84958819548289, l1: 0.00010636800708688007, l2: 0.0003785908155782053   Iteration 79 of 100, tot loss = 4.871686570252044, l1: 0.00010680960507140955, l2: 0.0003803590555009255   Iteration 80 of 100, tot loss = 4.840525671839714, l1: 0.00010607336066641438, l2: 0.0003779792099521728   Iteration 81 of 100, tot loss = 4.8207158895186435, l1: 0.00010553651664838666, l2: 0.00037653507545509914   Iteration 82 of 100, tot loss = 4.8161818486888235, l1: 0.0001053756650657217, l2: 0.000376242523155425   Iteration 83 of 100, tot loss = 4.823796751987503, l1: 0.00010545082560898904, l2: 0.000376928853311475   Iteration 84 of 100, tot loss = 4.822870700132279, l1: 0.00010558047865959539, l2: 0.0003767065950558477   Iteration 85 of 100, tot loss = 4.831976865319644, l1: 0.00010555941275122356, l2: 0.0003776382771320641   Iteration 86 of 100, tot loss = 4.8232160862102065, l1: 0.00010554324724707416, l2: 0.00037677836439269053   Iteration 87 of 100, tot loss = 4.8183515126677765, l1: 0.00010521604773025135, l2: 0.00037661910655201766   Iteration 88 of 100, tot loss = 4.824433464895595, l1: 0.00010533721892484623, l2: 0.0003771061303930103   Iteration 89 of 100, tot loss = 4.81318449438288, l1: 0.00010515604348195288, l2: 0.0003761624086130243   Iteration 90 of 100, tot loss = 4.834146269162496, l1: 0.00010519516925139922, l2: 0.00037821946025360373   Iteration 91 of 100, tot loss = 4.83983020730071, l1: 0.00010541480482820739, l2: 0.00037856821858600436   Iteration 92 of 100, tot loss = 4.8287714356961455, l1: 0.00010542664087710284, l2: 0.00037745050529180014   Iteration 93 of 100, tot loss = 4.815098667657503, l1: 0.0001051576900178872, l2: 0.000376352179427481   Iteration 94 of 100, tot loss = 4.804144676695478, l1: 0.00010515677852109025, l2: 0.0003752576916706451   Iteration 95 of 100, tot loss = 4.801241844578793, l1: 0.00010505973558266353, l2: 0.0003750644513945046   Iteration 96 of 100, tot loss = 4.813762570420901, l1: 0.00010527736886463875, l2: 0.0003760988905317693   Iteration 97 of 100, tot loss = 4.799498248346073, l1: 0.00010512971865180064, l2: 0.00037482010858306256   Iteration 98 of 100, tot loss = 4.783273852601344, l1: 0.00010494450436119817, l2: 0.00037338288322124366   Iteration 99 of 100, tot loss = 4.7952905616374935, l1: 0.00010532266447865028, l2: 0.00037420639398418406   Iteration 100 of 100, tot loss = 4.80169753074646, l1: 0.00010553354844887508, l2: 0.0003746362066885922
   End of epoch 1355; saving model... 

Epoch 1356 of 2000
   Iteration 1 of 100, tot loss = 5.217915058135986, l1: 0.0001228108158102259, l2: 0.00039898071554489434   Iteration 2 of 100, tot loss = 4.393449544906616, l1: 0.00011031553367502056, l2: 0.00032902944076340646   Iteration 3 of 100, tot loss = 4.728874683380127, l1: 0.00011017790044813107, l2: 0.0003627095914756258   Iteration 4 of 100, tot loss = 5.3209404945373535, l1: 0.00011867383727803826, l2: 0.0004134202317800373   Iteration 5 of 100, tot loss = 5.084129905700683, l1: 0.0001123221853049472, l2: 0.0003960908215958625   Iteration 6 of 100, tot loss = 5.157489061355591, l1: 0.00011257433046315175, l2: 0.0004031745872149865   Iteration 7 of 100, tot loss = 4.988737753459385, l1: 0.00011006168330953057, l2: 0.0003888121032754758   Iteration 8 of 100, tot loss = 5.086537152528763, l1: 0.00011164102579641622, l2: 0.0003970126999774948   Iteration 9 of 100, tot loss = 5.14442425303989, l1: 0.00010958374413247738, l2: 0.00040485869329940114   Iteration 10 of 100, tot loss = 4.912796187400818, l1: 0.00010590936508378946, l2: 0.0003853702641208656   Iteration 11 of 100, tot loss = 4.811445561322299, l1: 0.00010664128521139818, l2: 0.00037450328157071703   Iteration 12 of 100, tot loss = 4.836691478888194, l1: 0.0001084061580816827, l2: 0.0003752629963855725   Iteration 13 of 100, tot loss = 4.777115950217614, l1: 0.00010809945412732374, l2: 0.00036961214657192334   Iteration 14 of 100, tot loss = 4.833023701395307, l1: 0.00010887832883911739, l2: 0.00037442404988853795   Iteration 15 of 100, tot loss = 4.704192129770915, l1: 0.0001070284978292572, l2: 0.00036339072297172   Iteration 16 of 100, tot loss = 4.700989007949829, l1: 0.00010778403066069586, l2: 0.0003623148777478491   Iteration 17 of 100, tot loss = 4.8138508796691895, l1: 0.00011105622551814817, l2: 0.0003703288667191587   Iteration 18 of 100, tot loss = 4.881620274649726, l1: 0.00011243915004241798, l2: 0.0003757228833920736   Iteration 19 of 100, tot loss = 4.7645743269669385, l1: 0.00011046717363091088, l2: 0.000365990263832684   Iteration 20 of 100, tot loss = 4.719216597080231, l1: 0.00011070734544773586, l2: 0.00036121431840001604   Iteration 21 of 100, tot loss = 4.758909600121634, l1: 0.00011086445634386369, l2: 0.0003650265079202308   Iteration 22 of 100, tot loss = 4.764017809521068, l1: 0.00011067510819306004, l2: 0.0003657266768806783   Iteration 23 of 100, tot loss = 4.900549650192261, l1: 0.00011268591555088516, l2: 0.00037736905316073125   Iteration 24 of 100, tot loss = 4.985930730899175, l1: 0.00011326479246539141, l2: 0.0003853282839069531   Iteration 25 of 100, tot loss = 4.926768217086792, l1: 0.00011206467257579789, l2: 0.00038061215134803204   Iteration 26 of 100, tot loss = 4.942516702872056, l1: 0.00011197546113040656, l2: 0.00038227620993543847   Iteration 27 of 100, tot loss = 4.903691168184634, l1: 0.00011129600493909998, l2: 0.0003790731128969195   Iteration 28 of 100, tot loss = 4.999993664877755, l1: 0.00011273539348621853, l2: 0.00038726397334747683   Iteration 29 of 100, tot loss = 4.9769784335432385, l1: 0.00011286322190024858, l2: 0.0003848346216194653   Iteration 30 of 100, tot loss = 4.98077163696289, l1: 0.00011157223343616351, l2: 0.00038650493127837156   Iteration 31 of 100, tot loss = 4.944139349845148, l1: 0.00011164511529533493, l2: 0.00038276882062951526   Iteration 32 of 100, tot loss = 4.932484723627567, l1: 0.00011192616352673213, l2: 0.0003813223088400264   Iteration 33 of 100, tot loss = 4.899784131483599, l1: 0.00011076117119624872, l2: 0.000379217242218808   Iteration 34 of 100, tot loss = 4.87086046443266, l1: 0.00011004548415418386, l2: 0.0003770405622644295   Iteration 35 of 100, tot loss = 4.881204850333077, l1: 0.00010995694881005745, l2: 0.00037816353682761214   Iteration 36 of 100, tot loss = 4.898545847998725, l1: 0.0001096593829667351, l2: 0.0003801952023220818   Iteration 37 of 100, tot loss = 4.92075810561309, l1: 0.00011034752281596632, l2: 0.00038172828774534265   Iteration 38 of 100, tot loss = 4.981989320955779, l1: 0.00011145808323482542, l2: 0.0003867408480513596   Iteration 39 of 100, tot loss = 4.963253889328394, l1: 0.000110810861186996, l2: 0.0003855145265473626   Iteration 40 of 100, tot loss = 4.918407481908798, l1: 0.00010980019796988927, l2: 0.00038204054908419494   Iteration 41 of 100, tot loss = 4.898794668476756, l1: 0.00010998959623975679, l2: 0.0003798898687819019   Iteration 42 of 100, tot loss = 4.885399755977449, l1: 0.00011014499666219178, l2: 0.0003783949765160547   Iteration 43 of 100, tot loss = 4.848274796508079, l1: 0.0001096365767596073, l2: 0.0003751909003598442   Iteration 44 of 100, tot loss = 4.838675585660067, l1: 0.00010995523619137451, l2: 0.0003739123200251594   Iteration 45 of 100, tot loss = 4.870271449618869, l1: 0.00011038600706443604, l2: 0.00037664113559811893   Iteration 46 of 100, tot loss = 4.90941249805948, l1: 0.00011113376082073006, l2: 0.00037980748754268024   Iteration 47 of 100, tot loss = 4.884262495852531, l1: 0.0001102750687463645, l2: 0.0003781511793235079   Iteration 48 of 100, tot loss = 4.849369570612907, l1: 0.0001096848318411503, l2: 0.00037525212367957767   Iteration 49 of 100, tot loss = 4.814230310673616, l1: 0.00010917643744412961, l2: 0.0003722465920205019   Iteration 50 of 100, tot loss = 4.802905898094178, l1: 0.00010924137037363834, l2: 0.0003710492176469415   Iteration 51 of 100, tot loss = 4.808705278471405, l1: 0.00010984317886337237, l2: 0.00037102734809284845   Iteration 52 of 100, tot loss = 4.802652106835292, l1: 0.00010971383464773401, l2: 0.00037055137475433113   Iteration 53 of 100, tot loss = 4.7659719845034045, l1: 0.00010929772350827302, l2: 0.000367299473475454   Iteration 54 of 100, tot loss = 4.7880021466149225, l1: 0.00010969841255077713, l2: 0.0003691018010377539   Iteration 55 of 100, tot loss = 4.784578358043324, l1: 0.00010944876266876236, l2: 0.0003690090723632073   Iteration 56 of 100, tot loss = 4.762688360043934, l1: 0.00010861745535554032, l2: 0.0003676513796173302   Iteration 57 of 100, tot loss = 4.77138958897507, l1: 0.00010890957721705108, l2: 0.000368229380559601   Iteration 58 of 100, tot loss = 4.777360048787347, l1: 0.00010900897516343133, l2: 0.00036872702887570804   Iteration 59 of 100, tot loss = 4.771951695620003, l1: 0.00010874938571004797, l2: 0.0003684457826578074   Iteration 60 of 100, tot loss = 4.761491572856903, l1: 0.00010823557834858851, l2: 0.00036791357803546513   Iteration 61 of 100, tot loss = 4.735424585029727, l1: 0.00010779790199060207, l2: 0.00036574455601216647   Iteration 62 of 100, tot loss = 4.7252098475733115, l1: 0.00010734897824154506, l2: 0.00036517200609406214   Iteration 63 of 100, tot loss = 4.742012686199612, l1: 0.00010721805232432893, l2: 0.0003669832157255668   Iteration 64 of 100, tot loss = 4.770332094281912, l1: 0.00010781806611248612, l2: 0.00036921514333698724   Iteration 65 of 100, tot loss = 4.769635233512291, l1: 0.0001080763966176444, l2: 0.000368887126723376   Iteration 66 of 100, tot loss = 4.7601287401083745, l1: 0.00010829868165963103, l2: 0.0003677141920144839   Iteration 67 of 100, tot loss = 4.771331691030246, l1: 0.00010835749067630229, l2: 0.00036877567802535006   Iteration 68 of 100, tot loss = 4.746871453874252, l1: 0.00010753723130191622, l2: 0.0003671499137400293   Iteration 69 of 100, tot loss = 4.766587122626927, l1: 0.00010788424043457253, l2: 0.00036877447185005343   Iteration 70 of 100, tot loss = 4.7731244053159445, l1: 0.00010796337654548033, l2: 0.0003693490636318789   Iteration 71 of 100, tot loss = 4.776783315228744, l1: 0.00010801990474262734, l2: 0.00036965842620292547   Iteration 72 of 100, tot loss = 4.771224369605382, l1: 0.00010780615795940523, l2: 0.00036931627881939575   Iteration 73 of 100, tot loss = 4.7562519393555105, l1: 0.00010771396046038717, l2: 0.0003679112333099498   Iteration 74 of 100, tot loss = 4.764609849130785, l1: 0.00010784982431859307, l2: 0.0003686111604848348   Iteration 75 of 100, tot loss = 4.745938558578491, l1: 0.00010760813602246344, l2: 0.0003669857196897889   Iteration 76 of 100, tot loss = 4.739157441415284, l1: 0.00010745528839163996, l2: 0.000366460455552442   Iteration 77 of 100, tot loss = 4.745635698368023, l1: 0.00010765834294589139, l2: 0.0003669052269512352   Iteration 78 of 100, tot loss = 4.752047285055503, l1: 0.00010793838000375754, l2: 0.00036726634891984315   Iteration 79 of 100, tot loss = 4.74623606174807, l1: 0.00010738717675757229, l2: 0.00036723642957049035   Iteration 80 of 100, tot loss = 4.719014883041382, l1: 0.00010674202908376174, l2: 0.00036515945939754604   Iteration 81 of 100, tot loss = 4.710774209764269, l1: 0.00010689744436761283, l2: 0.00036417997699449367   Iteration 82 of 100, tot loss = 4.689501300090697, l1: 0.00010660037190884795, l2: 0.0003623497587551999   Iteration 83 of 100, tot loss = 4.715174695095384, l1: 0.00010630314259233054, l2: 0.0003652143278453746   Iteration 84 of 100, tot loss = 4.7079998056093855, l1: 0.00010612626729631475, l2: 0.0003646737145789389   Iteration 85 of 100, tot loss = 4.71916034081403, l1: 0.000106452945371583, l2: 0.0003654630904373548   Iteration 86 of 100, tot loss = 4.747649982918141, l1: 0.00010686644113881156, l2: 0.0003678985584735719   Iteration 87 of 100, tot loss = 4.729201426451233, l1: 0.00010655640214500981, l2: 0.0003663637418093845   Iteration 88 of 100, tot loss = 4.7086077386682685, l1: 0.00010618420976077852, l2: 0.0003646765656991523   Iteration 89 of 100, tot loss = 4.675485079208117, l1: 0.00010541238626000847, l2: 0.000362136123278126   Iteration 90 of 100, tot loss = 4.658255604902903, l1: 0.00010497044103330053, l2: 0.00036085512094561836   Iteration 91 of 100, tot loss = 4.658213470008347, l1: 0.00010489552285155066, l2: 0.00036092582580909787   Iteration 92 of 100, tot loss = 4.668432496164156, l1: 0.00010501780568871334, l2: 0.0003618254454720401   Iteration 93 of 100, tot loss = 4.646105734250879, l1: 0.00010444793885546205, l2: 0.0003601626360974204   Iteration 94 of 100, tot loss = 4.639333089615437, l1: 0.00010426283590780432, l2: 0.00035967047424105173   Iteration 95 of 100, tot loss = 4.646723650631151, l1: 0.00010422807819467332, l2: 0.00036044428829642896   Iteration 96 of 100, tot loss = 4.6628603128095465, l1: 0.00010453378437584131, l2: 0.00036175224856075755   Iteration 97 of 100, tot loss = 4.639144827410118, l1: 0.00010398316912541464, l2: 0.00035993131516698135   Iteration 98 of 100, tot loss = 4.633310258388519, l1: 0.00010378639226659362, l2: 0.0003595446351955987   Iteration 99 of 100, tot loss = 4.627184476515259, l1: 0.00010384151880656407, l2: 0.0003588769305321021   Iteration 100 of 100, tot loss = 4.614123772382737, l1: 0.00010363672980020056, l2: 0.00035777564902673476
   End of epoch 1356; saving model... 

Epoch 1357 of 2000
   Iteration 1 of 100, tot loss = 3.5239059925079346, l1: 7.406817167066038e-05, l2: 0.0002783224335871637   Iteration 2 of 100, tot loss = 3.4284173250198364, l1: 7.378762529697269e-05, l2: 0.00026905411505140364   Iteration 3 of 100, tot loss = 4.714221874872844, l1: 8.897307755736013e-05, l2: 0.0003824491092624764   Iteration 4 of 100, tot loss = 4.532959878444672, l1: 8.026804698602064e-05, l2: 0.00037302794225979596   Iteration 5 of 100, tot loss = 4.502597284317017, l1: 8.007105861906894e-05, l2: 0.00037018866860307755   Iteration 6 of 100, tot loss = 4.294855833053589, l1: 7.820461117565476e-05, l2: 0.00035128096836463857   Iteration 7 of 100, tot loss = 4.253044128417969, l1: 8.205119073474114e-05, l2: 0.0003432532206975988   Iteration 8 of 100, tot loss = 4.095150738954544, l1: 7.908923771537957e-05, l2: 0.0003304258334537735   Iteration 9 of 100, tot loss = 4.085803853140937, l1: 7.889814838159105e-05, l2: 0.00032968223644679203   Iteration 10 of 100, tot loss = 4.163174366950988, l1: 8.2831779945991e-05, l2: 0.00033348565484629943   Iteration 11 of 100, tot loss = 4.035186659206044, l1: 8.098365155588412e-05, l2: 0.00032253501227718186   Iteration 12 of 100, tot loss = 4.119660476843516, l1: 8.375689776585205e-05, l2: 0.0003282091493019834   Iteration 13 of 100, tot loss = 4.200810927611131, l1: 8.613122050659373e-05, l2: 0.0003339498697851713   Iteration 14 of 100, tot loss = 4.192626799855914, l1: 8.678913608101928e-05, l2: 0.0003324735423250656   Iteration 15 of 100, tot loss = 4.173412974675497, l1: 8.696160730323755e-05, l2: 0.00033037968872425455   Iteration 16 of 100, tot loss = 4.233530357480049, l1: 8.703702064849494e-05, l2: 0.000336316013999749   Iteration 17 of 100, tot loss = 4.293181741938872, l1: 8.999369974309743e-05, l2: 0.000339324474814074   Iteration 18 of 100, tot loss = 4.244885537359449, l1: 8.998963979441517e-05, l2: 0.0003344989139198636   Iteration 19 of 100, tot loss = 4.327383455477263, l1: 9.06503791108997e-05, l2: 0.00034208796569146216   Iteration 20 of 100, tot loss = 4.225506353378296, l1: 8.983794195955852e-05, l2: 0.0003327126927615609   Iteration 21 of 100, tot loss = 4.283458028520856, l1: 9.142832953200144e-05, l2: 0.0003369174727205453   Iteration 22 of 100, tot loss = 4.2227466973391445, l1: 8.9660208686837e-05, l2: 0.000332614461182278   Iteration 23 of 100, tot loss = 4.253802900728972, l1: 9.104023063074514e-05, l2: 0.00033434006036259234   Iteration 24 of 100, tot loss = 4.175396233797073, l1: 9.000013884967e-05, l2: 0.000327539485321419   Iteration 25 of 100, tot loss = 4.210804204940796, l1: 9.1245509829605e-05, l2: 0.00032983491255436095   Iteration 26 of 100, tot loss = 4.1341412250812235, l1: 9.020077120042585e-05, l2: 0.00032321335255311657   Iteration 27 of 100, tot loss = 4.04742811344288, l1: 8.864599617987147e-05, l2: 0.00031609681631110745   Iteration 28 of 100, tot loss = 3.9911686522620067, l1: 8.811024352845769e-05, l2: 0.00031100662246379737   Iteration 29 of 100, tot loss = 3.9795022421869737, l1: 8.786065669043856e-05, l2: 0.00031008956761195743   Iteration 30 of 100, tot loss = 4.073208610216777, l1: 9.001325537004353e-05, l2: 0.00031730760626184445   Iteration 31 of 100, tot loss = 4.089431278167233, l1: 9.027963671776947e-05, l2: 0.00031866349144688537   Iteration 32 of 100, tot loss = 4.087102375924587, l1: 8.955576834068779e-05, l2: 0.00031915447016217513   Iteration 33 of 100, tot loss = 4.134972984140569, l1: 9.109314046465002e-05, l2: 0.00032240415792771137   Iteration 34 of 100, tot loss = 4.205316143877366, l1: 9.260934647874605e-05, l2: 0.0003279222677092013   Iteration 35 of 100, tot loss = 4.22340133530753, l1: 9.309278237716561e-05, l2: 0.0003292473498731852   Iteration 36 of 100, tot loss = 4.267757462130652, l1: 9.411292163955902e-05, l2: 0.00033266282359060523   Iteration 37 of 100, tot loss = 4.267688963864301, l1: 9.489546239303702e-05, l2: 0.0003318734320723829   Iteration 38 of 100, tot loss = 4.212624970235322, l1: 9.354410844695706e-05, l2: 0.0003277183863438519   Iteration 39 of 100, tot loss = 4.227180279218233, l1: 9.355043920759972e-05, l2: 0.00032916758694530773   Iteration 40 of 100, tot loss = 4.260180252790451, l1: 9.464185786782764e-05, l2: 0.0003313761651952518   Iteration 41 of 100, tot loss = 4.299699405344521, l1: 9.478053337860307e-05, l2: 0.00033518940561008074   Iteration 42 of 100, tot loss = 4.286644345238095, l1: 9.445655901106962e-05, l2: 0.0003342078738829254   Iteration 43 of 100, tot loss = 4.261901151302249, l1: 9.410609531779449e-05, l2: 0.00033208401806024445   Iteration 44 of 100, tot loss = 4.244207815690474, l1: 9.359736048447138e-05, l2: 0.0003308234193933789   Iteration 45 of 100, tot loss = 4.252931382921007, l1: 9.337300531721363e-05, l2: 0.00033192013118726513   Iteration 46 of 100, tot loss = 4.217662920122561, l1: 9.31526897740854e-05, l2: 0.0003286136001723049   Iteration 47 of 100, tot loss = 4.253050433828475, l1: 9.38736907010799e-05, l2: 0.000331431350477555   Iteration 48 of 100, tot loss = 4.243567064404488, l1: 9.371949575627998e-05, l2: 0.00033063720851108275   Iteration 49 of 100, tot loss = 4.31566106543249, l1: 9.460637389862796e-05, l2: 0.0003369597305914349   Iteration 50 of 100, tot loss = 4.334232296943664, l1: 9.494157857261598e-05, l2: 0.00033848164894152434   Iteration 51 of 100, tot loss = 4.361499903248806, l1: 9.496552343525942e-05, l2: 0.0003411844646667733   Iteration 52 of 100, tot loss = 4.374709216447977, l1: 9.513054940795813e-05, l2: 0.00034234037034016533   Iteration 53 of 100, tot loss = 4.385924784642346, l1: 9.536722187339417e-05, l2: 0.00034322525497596217   Iteration 54 of 100, tot loss = 4.386203443562543, l1: 9.537047986264547e-05, l2: 0.0003432498628239113   Iteration 55 of 100, tot loss = 4.375558233261108, l1: 9.555211489152332e-05, l2: 0.0003420037069273266   Iteration 56 of 100, tot loss = 4.379324372325625, l1: 9.557092783195133e-05, l2: 0.0003423615079165237   Iteration 57 of 100, tot loss = 4.385478567658809, l1: 9.554214691314356e-05, l2: 0.0003430057080103117   Iteration 58 of 100, tot loss = 4.353403288742592, l1: 9.520613055993353e-05, l2: 0.0003401341966999663   Iteration 59 of 100, tot loss = 4.36986620951507, l1: 9.523098289441727e-05, l2: 0.00034175563669739826   Iteration 60 of 100, tot loss = 4.387016105651855, l1: 9.544098041563605e-05, l2: 0.000343260629112289   Iteration 61 of 100, tot loss = 4.369991451013283, l1: 9.528056111999741e-05, l2: 0.0003417185829765331   Iteration 62 of 100, tot loss = 4.408296115936771, l1: 9.606785979779107e-05, l2: 0.0003447617512598123   Iteration 63 of 100, tot loss = 4.4172802198500865, l1: 9.618241338536055e-05, l2: 0.00034554560815671547   Iteration 64 of 100, tot loss = 4.406467378139496, l1: 9.63019359687678e-05, l2: 0.00034434480107847776   Iteration 65 of 100, tot loss = 4.406792890108549, l1: 9.634354691325615e-05, l2: 0.00034433574160525147   Iteration 66 of 100, tot loss = 4.43806554331924, l1: 9.718404456207557e-05, l2: 0.0003466225089417121   Iteration 67 of 100, tot loss = 4.39777577279219, l1: 9.623893596138111e-05, l2: 0.00034353864047356617   Iteration 68 of 100, tot loss = 4.471676638897727, l1: 9.75437731754295e-05, l2: 0.0003496238894192228   Iteration 69 of 100, tot loss = 4.498869427736254, l1: 9.771264719176173e-05, l2: 0.0003521742946232089   Iteration 70 of 100, tot loss = 4.535730676991599, l1: 9.8598883986207e-05, l2: 0.00035497418263860577   Iteration 71 of 100, tot loss = 4.545606957355016, l1: 9.909581821772571e-05, l2: 0.0003554648768917566   Iteration 72 of 100, tot loss = 4.548161230153507, l1: 9.91215968800437e-05, l2: 0.00035569452585251484   Iteration 73 of 100, tot loss = 4.528569406026031, l1: 9.898244453536678e-05, l2: 0.000353874495748928   Iteration 74 of 100, tot loss = 4.546640030435614, l1: 9.925084078851207e-05, l2: 0.0003554131617420353   Iteration 75 of 100, tot loss = 4.566769868532816, l1: 9.962416224880144e-05, l2: 0.000357052824110724   Iteration 76 of 100, tot loss = 4.542859602915613, l1: 9.906134164339164e-05, l2: 0.00035522461812912586   Iteration 77 of 100, tot loss = 4.565881919551205, l1: 9.966044861168673e-05, l2: 0.0003569277429381724   Iteration 78 of 100, tot loss = 4.6012803209133635, l1: 0.00010047703663309594, l2: 0.00035965099512115837   Iteration 79 of 100, tot loss = 4.593681213221973, l1: 0.00010022023357296232, l2: 0.0003591478871155082   Iteration 80 of 100, tot loss = 4.58909225910902, l1: 9.983982786252455e-05, l2: 0.00035906939756387147   Iteration 81 of 100, tot loss = 4.581176721019509, l1: 9.95766102637446e-05, l2: 0.0003585410615167707   Iteration 82 of 100, tot loss = 4.579369072507068, l1: 9.938354422757118e-05, l2: 0.0003585533628299855   Iteration 83 of 100, tot loss = 4.578124044889427, l1: 9.940958835909716e-05, l2: 0.0003584028158199424   Iteration 84 of 100, tot loss = 4.55658995395615, l1: 9.920898476896885e-05, l2: 0.0003564500101284856   Iteration 85 of 100, tot loss = 4.5635656847673305, l1: 9.96515317049617e-05, l2: 0.0003567050363498685   Iteration 86 of 100, tot loss = 4.5519205983295, l1: 9.941322546077318e-05, l2: 0.0003557788341124759   Iteration 87 of 100, tot loss = 4.555609194711707, l1: 9.960646363244199e-05, l2: 0.0003559544557724255   Iteration 88 of 100, tot loss = 4.553350444544446, l1: 9.961870120440091e-05, l2: 0.0003557163431237727   Iteration 89 of 100, tot loss = 4.579573279016473, l1: 9.996332685179287e-05, l2: 0.0003579940008891621   Iteration 90 of 100, tot loss = 4.607764107651181, l1: 0.00010062287114528266, l2: 0.0003601535399664297   Iteration 91 of 100, tot loss = 4.59824424130576, l1: 0.00010048488164122298, l2: 0.0003593395427982374   Iteration 92 of 100, tot loss = 4.592048980619596, l1: 0.00010045323850373161, l2: 0.00035875166007747833   Iteration 93 of 100, tot loss = 4.582982549103358, l1: 0.0001005349875667498, l2: 0.00035776326796465544   Iteration 94 of 100, tot loss = 4.556746570353813, l1: 0.00010018844605315764, l2: 0.00035548621142763605   Iteration 95 of 100, tot loss = 4.552069072974356, l1: 0.00010029791090729353, l2: 0.0003549089969295126   Iteration 96 of 100, tot loss = 4.551480463395516, l1: 0.00010034711654801261, l2: 0.0003548009306844809   Iteration 97 of 100, tot loss = 4.5529596547490545, l1: 0.00010042866377955658, l2: 0.00035486730237457826   Iteration 98 of 100, tot loss = 4.544437767291556, l1: 9.997389033556755e-05, l2: 0.0003544698869963937   Iteration 99 of 100, tot loss = 4.548670088401948, l1: 0.0001003397529079894, l2: 0.0003545272566502763   Iteration 100 of 100, tot loss = 4.554503034353257, l1: 0.00010007995220803423, l2: 0.0003553703518991824
   End of epoch 1357; saving model... 

Epoch 1358 of 2000
   Iteration 1 of 100, tot loss = 6.444394588470459, l1: 0.00014692118566017598, l2: 0.0004975182819180191   Iteration 2 of 100, tot loss = 5.223257064819336, l1: 0.00012684637113125063, l2: 0.0003954793355660513   Iteration 3 of 100, tot loss = 5.1611701647440595, l1: 0.00012509570418236157, l2: 0.00039102131268009543   Iteration 4 of 100, tot loss = 5.191996097564697, l1: 0.00012079190128133632, l2: 0.00039840771205490455   Iteration 5 of 100, tot loss = 5.126582431793213, l1: 0.00012189898989163339, l2: 0.00039075925596989693   Iteration 6 of 100, tot loss = 4.746093948682149, l1: 0.0001129737647715956, l2: 0.0003616356334532611   Iteration 7 of 100, tot loss = 4.513588496616909, l1: 0.00010526905368481363, l2: 0.00034608979850807894   Iteration 8 of 100, tot loss = 4.475615084171295, l1: 0.00010631283839757089, l2: 0.00034124866760976147   Iteration 9 of 100, tot loss = 4.458068900638157, l1: 0.00010458113037101511, l2: 0.00034122575986354303   Iteration 10 of 100, tot loss = 4.410986804962159, l1: 0.00010098602942889556, l2: 0.00034011265233857557   Iteration 11 of 100, tot loss = 4.39173377643932, l1: 9.857101196592504e-05, l2: 0.0003406023672803051   Iteration 12 of 100, tot loss = 4.3504494825998945, l1: 9.776598441627964e-05, l2: 0.0003372789639494537   Iteration 13 of 100, tot loss = 4.4295836962186375, l1: 9.741483518155292e-05, l2: 0.00034554353400241013   Iteration 14 of 100, tot loss = 4.38611194065639, l1: 9.67220044653264e-05, l2: 0.0003418891896477102   Iteration 15 of 100, tot loss = 4.191780742009481, l1: 9.24346735700965e-05, l2: 0.00032674340036464855   Iteration 16 of 100, tot loss = 4.113292574882507, l1: 9.222779135598103e-05, l2: 0.00031910146526570315   Iteration 17 of 100, tot loss = 4.095691372366512, l1: 9.187500108964741e-05, l2: 0.0003176941360887962   Iteration 18 of 100, tot loss = 4.185911628935072, l1: 9.256551978372347e-05, l2: 0.0003260256437190239   Iteration 19 of 100, tot loss = 4.047956786657634, l1: 8.943579584746131e-05, l2: 0.0003153598835955276   Iteration 20 of 100, tot loss = 4.037205213308335, l1: 9.051256402017315e-05, l2: 0.0003132079578790581   Iteration 21 of 100, tot loss = 4.071547729628427, l1: 9.182196091777379e-05, l2: 0.0003153328140061127   Iteration 22 of 100, tot loss = 4.133988548408855, l1: 9.362154262569014e-05, l2: 0.00031977731420986606   Iteration 23 of 100, tot loss = 4.162332508874976, l1: 9.365430251700278e-05, l2: 0.0003225789503472776   Iteration 24 of 100, tot loss = 4.152190372347832, l1: 9.335345369739419e-05, l2: 0.00032186558595033904   Iteration 25 of 100, tot loss = 4.236565804481506, l1: 9.352768116514198e-05, l2: 0.0003301289022783749   Iteration 26 of 100, tot loss = 4.2615370612878065, l1: 9.428741773612834e-05, l2: 0.00033186629046283016   Iteration 27 of 100, tot loss = 4.294172397366276, l1: 9.485591928953202e-05, l2: 0.0003345613214251999   Iteration 28 of 100, tot loss = 4.345336322273527, l1: 9.561435022728151e-05, l2: 0.0003389192827723621   Iteration 29 of 100, tot loss = 4.341660832536632, l1: 9.631370743716553e-05, l2: 0.0003378523776111   Iteration 30 of 100, tot loss = 4.364584974447886, l1: 9.674952319376947e-05, l2: 0.0003397089766319065   Iteration 31 of 100, tot loss = 4.2869973336496665, l1: 9.518193333098034e-05, l2: 0.0003335178022802387   Iteration 32 of 100, tot loss = 4.295885533094406, l1: 9.56534976239709e-05, l2: 0.0003339350571422983   Iteration 33 of 100, tot loss = 4.306019205035585, l1: 9.473137635557742e-05, l2: 0.0003358705463611069   Iteration 34 of 100, tot loss = 4.27800062824698, l1: 9.497025412736762e-05, l2: 0.00033282981059849537   Iteration 35 of 100, tot loss = 4.287518126623971, l1: 9.459013350091742e-05, l2: 0.00033416168069899346   Iteration 36 of 100, tot loss = 4.284879465897878, l1: 9.510670204488431e-05, l2: 0.0003333812461884615   Iteration 37 of 100, tot loss = 4.3016196779302645, l1: 9.521077093243523e-05, l2: 0.0003349511985229706   Iteration 38 of 100, tot loss = 4.301928601766887, l1: 9.585747169407306e-05, l2: 0.00033433539015516686   Iteration 39 of 100, tot loss = 4.282159083928818, l1: 9.562455232252176e-05, l2: 0.00033259135764241457   Iteration 40 of 100, tot loss = 4.226556038856506, l1: 9.472068322793347e-05, l2: 0.0003279349222793826   Iteration 41 of 100, tot loss = 4.210950659542549, l1: 9.462612513566344e-05, l2: 0.0003264689423703225   Iteration 42 of 100, tot loss = 4.302082373982384, l1: 9.647887435719549e-05, l2: 0.00033372936446713063   Iteration 43 of 100, tot loss = 4.301401787025984, l1: 9.670250454841745e-05, l2: 0.00033343767596349163   Iteration 44 of 100, tot loss = 4.29699949242852, l1: 9.685091952798592e-05, l2: 0.0003328490309451643   Iteration 45 of 100, tot loss = 4.313075685501099, l1: 9.671766698981325e-05, l2: 0.0003345899033269638   Iteration 46 of 100, tot loss = 4.2561817350594895, l1: 9.55416644896816e-05, l2: 0.0003300765106953053   Iteration 47 of 100, tot loss = 4.2558753718721105, l1: 9.580830583024393e-05, l2: 0.00032977923330419914   Iteration 48 of 100, tot loss = 4.288802502055963, l1: 9.641160257463828e-05, l2: 0.0003324686484423485   Iteration 49 of 100, tot loss = 4.309605547360012, l1: 9.716981144358252e-05, l2: 0.00033379074412383784   Iteration 50 of 100, tot loss = 4.302149403095245, l1: 9.698111687612255e-05, l2: 0.00033323382449452766   Iteration 51 of 100, tot loss = 4.279259922457676, l1: 9.666232404461149e-05, l2: 0.0003312636692106596   Iteration 52 of 100, tot loss = 4.258507226522152, l1: 9.576075612690496e-05, l2: 0.0003300899676945007   Iteration 53 of 100, tot loss = 4.296611959079526, l1: 9.618225933836755e-05, l2: 0.0003334789380117074   Iteration 54 of 100, tot loss = 4.281092924100381, l1: 9.576792237769243e-05, l2: 0.0003323413711095108   Iteration 55 of 100, tot loss = 4.3779447013681585, l1: 9.696705164969898e-05, l2: 0.00034082741913152856   Iteration 56 of 100, tot loss = 4.367785621966634, l1: 9.626947413770332e-05, l2: 0.000340509088735936   Iteration 57 of 100, tot loss = 4.403604681031746, l1: 9.703942902106384e-05, l2: 0.00034332103997146256   Iteration 58 of 100, tot loss = 4.397643555854929, l1: 9.650085704724288e-05, l2: 0.000343263499636468   Iteration 59 of 100, tot loss = 4.399805454884545, l1: 9.653839977977056e-05, l2: 0.00034344214682123953   Iteration 60 of 100, tot loss = 4.385848091046015, l1: 9.67028271285623e-05, l2: 0.00034188198296760677   Iteration 61 of 100, tot loss = 4.388386857314188, l1: 9.678755406162426e-05, l2: 0.0003420511322891813   Iteration 62 of 100, tot loss = 4.392182255944898, l1: 9.69777151875596e-05, l2: 0.0003422405107196556   Iteration 63 of 100, tot loss = 4.366044701091827, l1: 9.675973030555594e-05, l2: 0.00033984474003008225   Iteration 64 of 100, tot loss = 4.344382485374808, l1: 9.6403446889326e-05, l2: 0.0003380348019845769   Iteration 65 of 100, tot loss = 4.358277656481817, l1: 9.669503807134998e-05, l2: 0.0003391327272625998   Iteration 66 of 100, tot loss = 4.359332350167361, l1: 9.66063704033855e-05, l2: 0.000339326864460335   Iteration 67 of 100, tot loss = 4.36577737687239, l1: 9.705358698100561e-05, l2: 0.0003395241505158857   Iteration 68 of 100, tot loss = 4.340003779705833, l1: 9.633264712379862e-05, l2: 0.0003376677303843762   Iteration 69 of 100, tot loss = 4.368168970812922, l1: 9.713833497843935e-05, l2: 0.0003396785609041125   Iteration 70 of 100, tot loss = 4.359570576463427, l1: 9.722487499987307e-05, l2: 0.00033873218147034223   Iteration 71 of 100, tot loss = 4.334317088127136, l1: 9.6400785276299e-05, l2: 0.0003370309223356972   Iteration 72 of 100, tot loss = 4.324779051873419, l1: 9.630206255274566e-05, l2: 0.0003361758411604872   Iteration 73 of 100, tot loss = 4.356049242084974, l1: 9.676079068026722e-05, l2: 0.00033884413218626447   Iteration 74 of 100, tot loss = 4.379662518565719, l1: 9.716291301506194e-05, l2: 0.0003408033376586359   Iteration 75 of 100, tot loss = 4.355670631726583, l1: 9.676420867132644e-05, l2: 0.0003388028532693473   Iteration 76 of 100, tot loss = 4.343809995212053, l1: 9.670895920671203e-05, l2: 0.00033767203910285487   Iteration 77 of 100, tot loss = 4.3601699510178005, l1: 9.688773712554536e-05, l2: 0.00033912925620778017   Iteration 78 of 100, tot loss = 4.346296342519613, l1: 9.657859444791355e-05, l2: 0.0003380510378263306   Iteration 79 of 100, tot loss = 4.348335104652598, l1: 9.629241215902133e-05, l2: 0.000338541096517716   Iteration 80 of 100, tot loss = 4.361624182760716, l1: 9.646973676353809e-05, l2: 0.00033969267997235875   Iteration 81 of 100, tot loss = 4.363995529987194, l1: 9.655072071724629e-05, l2: 0.00033984883114577695   Iteration 82 of 100, tot loss = 4.365893632900424, l1: 9.661781933107528e-05, l2: 0.00033997154286026206   Iteration 83 of 100, tot loss = 4.357099319078836, l1: 9.650823920376105e-05, l2: 0.00033920169175049875   Iteration 84 of 100, tot loss = 4.34986277847063, l1: 9.599132941470348e-05, l2: 0.0003389949477076741   Iteration 85 of 100, tot loss = 4.370572963882895, l1: 9.617818098323529e-05, l2: 0.00034087911472477784   Iteration 86 of 100, tot loss = 4.341347051221271, l1: 9.549331454848492e-05, l2: 0.00033864139000205646   Iteration 87 of 100, tot loss = 4.326471512345062, l1: 9.524920832696264e-05, l2: 0.00033739794220644234   Iteration 88 of 100, tot loss = 4.338423216884786, l1: 9.524590086584794e-05, l2: 0.00033859641974810404   Iteration 89 of 100, tot loss = 4.3560086394963635, l1: 9.534228564497722e-05, l2: 0.000340258577309439   Iteration 90 of 100, tot loss = 4.378575581974453, l1: 9.559405746889146e-05, l2: 0.0003422634995432519   Iteration 91 of 100, tot loss = 4.350513777890048, l1: 9.520506696739736e-05, l2: 0.00033984630975352886   Iteration 92 of 100, tot loss = 4.344899509264075, l1: 9.526441056590082e-05, l2: 0.0003392255391942023   Iteration 93 of 100, tot loss = 4.339334321278398, l1: 9.534353301459824e-05, l2: 0.00033858989815100006   Iteration 94 of 100, tot loss = 4.352527169471092, l1: 9.560264438635947e-05, l2: 0.0003396500716452794   Iteration 95 of 100, tot loss = 4.357913034840634, l1: 9.576157838302223e-05, l2: 0.00034002972419042827   Iteration 96 of 100, tot loss = 4.348750047385693, l1: 9.556280300178817e-05, l2: 0.00033931220097353315   Iteration 97 of 100, tot loss = 4.381969149579707, l1: 9.62534261244522e-05, l2: 0.00034194348835129505   Iteration 98 of 100, tot loss = 4.371518804102528, l1: 9.590816440248425e-05, l2: 0.0003412437153183047   Iteration 99 of 100, tot loss = 4.369530068503486, l1: 9.584609285935365e-05, l2: 0.0003411069132694551   Iteration 100 of 100, tot loss = 4.379778897762298, l1: 9.584312385413796e-05, l2: 0.0003421347653056728
   End of epoch 1358; saving model... 

Epoch 1359 of 2000
   Iteration 1 of 100, tot loss = 2.7295751571655273, l1: 7.492809527320787e-05, l2: 0.00019802941824309528   Iteration 2 of 100, tot loss = 3.974092483520508, l1: 9.105812205234542e-05, l2: 0.00030635111033916473   Iteration 3 of 100, tot loss = 4.072146097819011, l1: 9.923115188333516e-05, l2: 0.0003079834471767147   Iteration 4 of 100, tot loss = 4.050993084907532, l1: 9.795570804271847e-05, l2: 0.00030714359309058636   Iteration 5 of 100, tot loss = 4.178085803985596, l1: 0.00010350445518270135, l2: 0.0003143041161820292   Iteration 6 of 100, tot loss = 3.977119962374369, l1: 9.749144495193225e-05, l2: 0.0003002205436738829   Iteration 7 of 100, tot loss = 3.9471771035875594, l1: 9.528725786367431e-05, l2: 0.000299430443971817   Iteration 8 of 100, tot loss = 4.175773411989212, l1: 9.939837491401704e-05, l2: 0.0003181789579684846   Iteration 9 of 100, tot loss = 4.282384104198879, l1: 9.883557747040565e-05, l2: 0.00032940282188873325   Iteration 10 of 100, tot loss = 4.358774971961975, l1: 0.00010089842035085895, l2: 0.0003349790669744834   Iteration 11 of 100, tot loss = 4.272551103071733, l1: 9.814861021533778e-05, l2: 0.00032910649050873786   Iteration 12 of 100, tot loss = 4.242945671081543, l1: 9.792266973818187e-05, l2: 0.0003263718875435491   Iteration 13 of 100, tot loss = 4.161941436620859, l1: 9.759200307039115e-05, l2: 0.00031860213270052697   Iteration 14 of 100, tot loss = 4.210044741630554, l1: 0.00010004928065297593, l2: 0.00032095518586824516   Iteration 15 of 100, tot loss = 4.385222578048706, l1: 0.0001027813763357699, l2: 0.00033574087428860365   Iteration 16 of 100, tot loss = 4.356949046254158, l1: 0.00010057826693810057, l2: 0.00033511663059471175   Iteration 17 of 100, tot loss = 4.328454606673297, l1: 0.00010003927052500384, l2: 0.0003328061823541408   Iteration 18 of 100, tot loss = 4.346153948042128, l1: 9.959782983059995e-05, l2: 0.0003350175587305178   Iteration 19 of 100, tot loss = 4.499331675077739, l1: 0.00010098043352818901, l2: 0.0003489527275393668   Iteration 20 of 100, tot loss = 4.393390727043152, l1: 9.82957439191523e-05, l2: 0.0003410433229873888   Iteration 21 of 100, tot loss = 4.358695881707328, l1: 9.633389882288785e-05, l2: 0.0003395356831052119   Iteration 22 of 100, tot loss = 4.31437856500799, l1: 9.55879702153404e-05, l2: 0.0003358498797751963   Iteration 23 of 100, tot loss = 4.313989038052767, l1: 9.6355697441259e-05, l2: 0.0003350432000487395   Iteration 24 of 100, tot loss = 4.371829926967621, l1: 9.870359614675787e-05, l2: 0.00033847939024174895   Iteration 25 of 100, tot loss = 4.305778846740723, l1: 9.778450039448217e-05, l2: 0.0003327933786204085   Iteration 26 of 100, tot loss = 4.4141519619868355, l1: 0.00010001468959112222, l2: 0.0003414005013137984   Iteration 27 of 100, tot loss = 4.418596426645915, l1: 0.00010074956258500202, l2: 0.00034111007429331677   Iteration 28 of 100, tot loss = 4.531812804085868, l1: 0.00010181366243549357, l2: 0.0003513676123735162   Iteration 29 of 100, tot loss = 4.557912875866068, l1: 0.00010276695910102592, l2: 0.0003530243225997828   Iteration 30 of 100, tot loss = 4.528541763623555, l1: 0.00010111085017949032, l2: 0.00035174332006135953   Iteration 31 of 100, tot loss = 4.538739027515534, l1: 0.00010209005995420739, l2: 0.0003517838368650466   Iteration 32 of 100, tot loss = 4.624471507966518, l1: 0.00010364976901655609, l2: 0.0003587973756111751   Iteration 33 of 100, tot loss = 4.656890601822824, l1: 0.00010406891162733011, l2: 0.0003616201438214348   Iteration 34 of 100, tot loss = 4.662022948265076, l1: 0.00010438108588192229, l2: 0.00036182120461222334   Iteration 35 of 100, tot loss = 4.664952870777675, l1: 0.00010496638153979022, l2: 0.0003615289007679426   Iteration 36 of 100, tot loss = 4.633793890476227, l1: 0.00010435850931066347, l2: 0.00035902087458655133   Iteration 37 of 100, tot loss = 4.65465083637753, l1: 0.0001045026650457215, l2: 0.0003609624136761586   Iteration 38 of 100, tot loss = 4.60794356622194, l1: 0.00010435641122871617, l2: 0.00035643794092864384   Iteration 39 of 100, tot loss = 4.6280805086478205, l1: 0.0001046146200492214, l2: 0.000358193426225812   Iteration 40 of 100, tot loss = 4.619787234067917, l1: 0.00010448204084241297, l2: 0.00035749667731579395   Iteration 41 of 100, tot loss = 4.604952248131356, l1: 0.00010471898588925464, l2: 0.0003557762339790692   Iteration 42 of 100, tot loss = 4.58979792821975, l1: 0.0001042570837030542, l2: 0.0003547227042006506   Iteration 43 of 100, tot loss = 4.557808598806692, l1: 0.00010388547982370784, l2: 0.0003518953754940222   Iteration 44 of 100, tot loss = 4.604887171225115, l1: 0.00010498871911708689, l2: 0.000355499994012379   Iteration 45 of 100, tot loss = 4.6056147045559355, l1: 0.0001051226545112311, l2: 0.00035543881207963246   Iteration 46 of 100, tot loss = 4.580364781877269, l1: 0.00010462639977998586, l2: 0.0003534100747935514   Iteration 47 of 100, tot loss = 4.598942213870109, l1: 0.00010545671303840751, l2: 0.00035443750520960054   Iteration 48 of 100, tot loss = 4.556810403863589, l1: 0.00010469824458899286, l2: 0.00035098279264881665   Iteration 49 of 100, tot loss = 4.527448094620997, l1: 0.00010420512589174608, l2: 0.00034853968055377125   Iteration 50 of 100, tot loss = 4.503775696754456, l1: 0.00010377293408964761, l2: 0.00034660463250475005   Iteration 51 of 100, tot loss = 4.505808563793407, l1: 0.00010394369975056536, l2: 0.0003466371531876754   Iteration 52 of 100, tot loss = 4.471413465646597, l1: 0.0001034894195562139, l2: 0.0003436519232659171   Iteration 53 of 100, tot loss = 4.485148222941272, l1: 0.00010360056056118272, l2: 0.00034491425843575515   Iteration 54 of 100, tot loss = 4.45139424889176, l1: 0.00010346148041275296, l2: 0.00034167794095723094   Iteration 55 of 100, tot loss = 4.434699717434969, l1: 0.0001035004343562336, l2: 0.00033996953412001446   Iteration 56 of 100, tot loss = 4.396785374198641, l1: 0.00010280615294634896, l2: 0.00033687238127250955   Iteration 57 of 100, tot loss = 4.371641815754405, l1: 0.00010244713906247757, l2: 0.00033471703947023405   Iteration 58 of 100, tot loss = 4.413496259985299, l1: 0.00010325881634469561, l2: 0.0003380908067277954   Iteration 59 of 100, tot loss = 4.431231438103369, l1: 0.00010337550030287228, l2: 0.0003397476401128727   Iteration 60 of 100, tot loss = 4.42443365653356, l1: 0.00010364636424734878, l2: 0.0003387969979182041   Iteration 61 of 100, tot loss = 4.401435754338249, l1: 0.0001027423086419671, l2: 0.0003374012635131443   Iteration 62 of 100, tot loss = 4.431287807803, l1: 0.00010321779184103493, l2: 0.0003399109857949248   Iteration 63 of 100, tot loss = 4.4202597292642745, l1: 0.0001030279064065008, l2: 0.000338998063521031   Iteration 64 of 100, tot loss = 4.431131441146135, l1: 0.00010317982844298967, l2: 0.00033993331294368545   Iteration 65 of 100, tot loss = 4.445746286098774, l1: 0.00010309187167037566, l2: 0.0003414827544474974   Iteration 66 of 100, tot loss = 4.446331439596234, l1: 0.00010336045295733845, l2: 0.0003412726882467696   Iteration 67 of 100, tot loss = 4.487974547628147, l1: 0.00010382860126634087, l2: 0.0003449688502674255   Iteration 68 of 100, tot loss = 4.514885057421291, l1: 0.00010437399888520732, l2: 0.0003471145038715536   Iteration 69 of 100, tot loss = 4.523497177206951, l1: 0.00010469384943607493, l2: 0.0003476558652608588   Iteration 70 of 100, tot loss = 4.5643016644886565, l1: 0.00010539296781644225, l2: 0.00035103719544297615   Iteration 71 of 100, tot loss = 4.6232242147687455, l1: 0.00010597630089122645, l2: 0.0003563461168980698   Iteration 72 of 100, tot loss = 4.61280679040485, l1: 0.00010578110009697007, l2: 0.00035549957525896997   Iteration 73 of 100, tot loss = 4.569974028900878, l1: 0.00010480756937384236, l2: 0.000352189829991652   Iteration 74 of 100, tot loss = 4.565451736385758, l1: 0.00010496587851407271, l2: 0.00035157929124653565   Iteration 75 of 100, tot loss = 4.592444132169088, l1: 0.00010559637802847041, l2: 0.00035364803169310715   Iteration 76 of 100, tot loss = 4.584788019719877, l1: 0.00010552523010693748, l2: 0.0003529535680220426   Iteration 77 of 100, tot loss = 4.6199316653338345, l1: 0.00010625404906164334, l2: 0.0003557391136819678   Iteration 78 of 100, tot loss = 4.601783781479567, l1: 0.00010571269344020552, l2: 0.00035446568085134437   Iteration 79 of 100, tot loss = 4.577452845211271, l1: 0.00010509122535328475, l2: 0.0003526540554336367   Iteration 80 of 100, tot loss = 4.589290879666805, l1: 0.00010507498286642658, l2: 0.00035385410128583315   Iteration 81 of 100, tot loss = 4.604273941781786, l1: 0.00010550931157338733, l2: 0.00035491807899728166   Iteration 82 of 100, tot loss = 4.611397887148509, l1: 0.00010552522521387933, l2: 0.0003556145605149895   Iteration 83 of 100, tot loss = 4.609153947198247, l1: 0.00010544322801898346, l2: 0.00035547216344661227   Iteration 84 of 100, tot loss = 4.584118719611849, l1: 0.00010502237312591336, l2: 0.00035338949557973097   Iteration 85 of 100, tot loss = 4.593444864890155, l1: 0.00010532690712887629, l2: 0.00035401757623356605   Iteration 86 of 100, tot loss = 4.62985035014707, l1: 0.00010602723514873719, l2: 0.0003569577970348439   Iteration 87 of 100, tot loss = 4.642079535572008, l1: 0.00010615485425451074, l2: 0.0003580530967741359   Iteration 88 of 100, tot loss = 4.662683444944295, l1: 0.00010632322792761525, l2: 0.0003599451140623635   Iteration 89 of 100, tot loss = 4.660238717379195, l1: 0.00010617721317829093, l2: 0.0003598466558215794   Iteration 90 of 100, tot loss = 4.656216283639272, l1: 0.00010596715102211521, l2: 0.0003596544748385592   Iteration 91 of 100, tot loss = 4.675026397128682, l1: 0.00010600185755270554, l2: 0.00036150077973546175   Iteration 92 of 100, tot loss = 4.6668988090494405, l1: 0.0001059734899884105, l2: 0.0003607163887310257   Iteration 93 of 100, tot loss = 4.63446981035253, l1: 0.00010538547490228978, l2: 0.0003580615039907336   Iteration 94 of 100, tot loss = 4.627449361567802, l1: 0.000105349529707034, l2: 0.00035739540443829995   Iteration 95 of 100, tot loss = 4.613258902650131, l1: 0.00010502135482965968, l2: 0.00035630453356517186   Iteration 96 of 100, tot loss = 4.607920069247484, l1: 0.00010472810189791441, l2: 0.00035606390323058196   Iteration 97 of 100, tot loss = 4.610135013295203, l1: 0.00010469816210313661, l2: 0.00035631533735551744   Iteration 98 of 100, tot loss = 4.615620203164159, l1: 0.00010482975322906432, l2: 0.0003567322649415677   Iteration 99 of 100, tot loss = 4.622577381856514, l1: 0.00010505672874231353, l2: 0.00035720100732769517   Iteration 100 of 100, tot loss = 4.616266759634018, l1: 0.00010502666929824045, l2: 0.0003566000047430862
   End of epoch 1359; saving model... 

Epoch 1360 of 2000
   Iteration 1 of 100, tot loss = 3.683823347091675, l1: 5.5397766118403524e-05, l2: 0.0003129845717921853   Iteration 2 of 100, tot loss = 2.767330050468445, l1: 4.720585457107518e-05, l2: 0.00022952714789425954   Iteration 3 of 100, tot loss = 3.0766711235046387, l1: 6.35772266832646e-05, l2: 0.00024408988247159868   Iteration 4 of 100, tot loss = 3.4736403226852417, l1: 8.027904641494388e-05, l2: 0.0002670849826245103   Iteration 5 of 100, tot loss = 3.914539909362793, l1: 8.718803073861636e-05, l2: 0.0003042659518541768   Iteration 6 of 100, tot loss = 3.7162625789642334, l1: 8.658666229166556e-05, l2: 0.000285039587955301   Iteration 7 of 100, tot loss = 3.6882163797106062, l1: 9.067938922921062e-05, l2: 0.0002781422413785809   Iteration 8 of 100, tot loss = 3.7846986949443817, l1: 9.530612305752584e-05, l2: 0.0002831637375493301   Iteration 9 of 100, tot loss = 3.742845058441162, l1: 9.406232816723382e-05, l2: 0.00028022217156831175   Iteration 10 of 100, tot loss = 3.8013699531555174, l1: 9.118462876358535e-05, l2: 0.00028895235882373525   Iteration 11 of 100, tot loss = 3.8485988703641025, l1: 9.213698814908804e-05, l2: 0.00029272289380473507   Iteration 12 of 100, tot loss = 3.9354397455851235, l1: 9.319242872152245e-05, l2: 0.000300351538802109   Iteration 13 of 100, tot loss = 3.9063236163212705, l1: 9.235615428993836e-05, l2: 0.00029827620220692974   Iteration 14 of 100, tot loss = 3.993545021329607, l1: 9.39374068756089e-05, l2: 0.0003054170904631194   Iteration 15 of 100, tot loss = 4.078639698028565, l1: 9.60017459874507e-05, l2: 0.0003118622184653456   Iteration 16 of 100, tot loss = 4.1678227186203, l1: 9.56602591486444e-05, l2: 0.00032112200733536156   Iteration 17 of 100, tot loss = 4.253090269425336, l1: 9.70920739157865e-05, l2: 0.0003282169491285458   Iteration 18 of 100, tot loss = 4.315751420127021, l1: 9.794935188741268e-05, l2: 0.0003336257852626861   Iteration 19 of 100, tot loss = 4.408424277054636, l1: 9.923407675447187e-05, l2: 0.0003416083460538893   Iteration 20 of 100, tot loss = 4.388865613937378, l1: 9.98018005702761e-05, l2: 0.0003390847567061428   Iteration 21 of 100, tot loss = 4.420146419888451, l1: 0.00010067810754186386, l2: 0.00034133652904226136   Iteration 22 of 100, tot loss = 4.341969945214012, l1: 0.0001002254183814247, l2: 0.00033397157015067273   Iteration 23 of 100, tot loss = 4.379854513251263, l1: 0.00010043642380522847, l2: 0.0003375490208703053   Iteration 24 of 100, tot loss = 4.363921453555425, l1: 0.0001009393519476968, l2: 0.00033545278711244464   Iteration 25 of 100, tot loss = 4.341072521209717, l1: 0.0001013322344806511, l2: 0.00033277501235716043   Iteration 26 of 100, tot loss = 4.287346628996042, l1: 0.00010069912171345025, l2: 0.0003280355365579733   Iteration 27 of 100, tot loss = 4.284365291948672, l1: 0.00010031572862687797, l2: 0.000328120795462315   Iteration 28 of 100, tot loss = 4.280466599123819, l1: 0.00010016178449794617, l2: 0.0003278848697456332   Iteration 29 of 100, tot loss = 4.241936708318776, l1: 9.875789039832507e-05, l2: 0.000325435775047136   Iteration 30 of 100, tot loss = 4.289197357495626, l1: 9.983455965993926e-05, l2: 0.0003290851712032842   Iteration 31 of 100, tot loss = 4.272828371294083, l1: 9.96206710565715e-05, l2: 0.0003276621606463807   Iteration 32 of 100, tot loss = 4.236581072211266, l1: 9.879282470137696e-05, l2: 0.0003248652774345828   Iteration 33 of 100, tot loss = 4.2221550941467285, l1: 9.923615641927233e-05, l2: 0.00032297934751223886   Iteration 34 of 100, tot loss = 4.214662671089172, l1: 9.906255581881851e-05, l2: 0.00032240370552137713   Iteration 35 of 100, tot loss = 4.220287125451224, l1: 9.917855100606435e-05, l2: 0.0003228501566419644   Iteration 36 of 100, tot loss = 4.282241735193464, l1: 0.00010015721454692539, l2: 0.0003280669544538897   Iteration 37 of 100, tot loss = 4.337417918282586, l1: 9.999643028808154e-05, l2: 0.00033374535819716955   Iteration 38 of 100, tot loss = 4.282145374699643, l1: 9.878277725013169e-05, l2: 0.00032943175718078   Iteration 39 of 100, tot loss = 4.395880454625839, l1: 0.00010096087890605514, l2: 0.00033862716391479643   Iteration 40 of 100, tot loss = 4.42277958393097, l1: 0.00010161279606109019, l2: 0.00034066515981976406   Iteration 41 of 100, tot loss = 4.468712446166248, l1: 0.00010200592326483031, l2: 0.0003448653197421397   Iteration 42 of 100, tot loss = 4.4941649323418025, l1: 0.00010251453360459501, l2: 0.0003469019578742085   Iteration 43 of 100, tot loss = 4.455088016598723, l1: 0.00010215376671620234, l2: 0.00034335503343839284   Iteration 44 of 100, tot loss = 4.5028873898766255, l1: 0.00010255454733189914, l2: 0.00034773419081995434   Iteration 45 of 100, tot loss = 4.457979636722141, l1: 0.00010182491953148403, l2: 0.0003439730431030815   Iteration 46 of 100, tot loss = 4.465405256851859, l1: 0.00010227703545672779, l2: 0.00034426348935559634   Iteration 47 of 100, tot loss = 4.453884652320375, l1: 0.00010211470269380098, l2: 0.0003432737613230587   Iteration 48 of 100, tot loss = 4.5313218633333845, l1: 0.0001035044324453338, l2: 0.0003496277528635498   Iteration 49 of 100, tot loss = 4.50138081336508, l1: 0.0001032763543569635, l2: 0.0003468617262337737   Iteration 50 of 100, tot loss = 4.458247537612915, l1: 0.0001024778415740002, l2: 0.00034334691154072063   Iteration 51 of 100, tot loss = 4.449521560294955, l1: 0.00010250257679110612, l2: 0.0003424495787875654   Iteration 52 of 100, tot loss = 4.464759514882014, l1: 0.00010315980552359878, l2: 0.0003433161448181356   Iteration 53 of 100, tot loss = 4.4979803337241115, l1: 0.00010323613983104533, l2: 0.0003465618924479404   Iteration 54 of 100, tot loss = 4.4818082871260465, l1: 0.0001028924691329141, l2: 0.00034528835843688013   Iteration 55 of 100, tot loss = 4.484150570089167, l1: 0.00010277266582389447, l2: 0.0003456423897825351   Iteration 56 of 100, tot loss = 4.4958114836897165, l1: 0.0001029799316581505, l2: 0.00034660121543441037   Iteration 57 of 100, tot loss = 4.522326431776348, l1: 0.00010314413278441255, l2: 0.0003490885089073951   Iteration 58 of 100, tot loss = 4.523005366325378, l1: 0.00010311250436392709, l2: 0.00034918803054285395   Iteration 59 of 100, tot loss = 4.538494542493659, l1: 0.00010348605662900008, l2: 0.0003503633964455576   Iteration 60 of 100, tot loss = 4.569325951735179, l1: 0.00010400916823224785, l2: 0.00035292342630176184   Iteration 61 of 100, tot loss = 4.5579481554813075, l1: 0.00010376603528189275, l2: 0.0003520287795194623   Iteration 62 of 100, tot loss = 4.573118213684328, l1: 0.00010324841587721641, l2: 0.0003540634049474442   Iteration 63 of 100, tot loss = 4.552028084558154, l1: 0.00010239792119670984, l2: 0.0003528048865042538   Iteration 64 of 100, tot loss = 4.5723348967731, l1: 0.00010230203099581558, l2: 0.00035493145719556196   Iteration 65 of 100, tot loss = 4.595309121792133, l1: 0.00010236949859133277, l2: 0.0003571614123827133   Iteration 66 of 100, tot loss = 4.582974769852378, l1: 0.00010254841408547664, l2: 0.0003557490618373832   Iteration 67 of 100, tot loss = 4.568367698299351, l1: 0.00010186821274865946, l2: 0.0003549685558815028   Iteration 68 of 100, tot loss = 4.553279094836292, l1: 0.00010113056449721197, l2: 0.0003541973438747364   Iteration 69 of 100, tot loss = 4.5775366831516875, l1: 0.0001016161158327452, l2: 0.0003561375511260163   Iteration 70 of 100, tot loss = 4.575050623076303, l1: 0.0001016233332068493, l2: 0.0003558817280073916   Iteration 71 of 100, tot loss = 4.54987100480308, l1: 0.00010127457211074472, l2: 0.00035371252747160763   Iteration 72 of 100, tot loss = 4.522280226151149, l1: 0.00010086521630607119, l2: 0.00035136280570845376   Iteration 73 of 100, tot loss = 4.501006273374165, l1: 0.00010071721816816597, l2: 0.00034938340851947767   Iteration 74 of 100, tot loss = 4.493375262698612, l1: 0.00010007734355796758, l2: 0.0003492601819153925   Iteration 75 of 100, tot loss = 4.53036558787028, l1: 0.00010065967944683507, l2: 0.00035237687833917635   Iteration 76 of 100, tot loss = 4.567446708679199, l1: 0.00010103573530566217, l2: 0.00035570893478028377   Iteration 77 of 100, tot loss = 4.544967134277542, l1: 0.00010074842742012282, l2: 0.00035374828534164504   Iteration 78 of 100, tot loss = 4.5259814995985765, l1: 0.00010047852787792157, l2: 0.0003521196215446346   Iteration 79 of 100, tot loss = 4.513882205456118, l1: 0.00010013727250559066, l2: 0.00035125094733371907   Iteration 80 of 100, tot loss = 4.544010469317437, l1: 0.00010071903061543707, l2: 0.00035368201624805804   Iteration 81 of 100, tot loss = 4.538368852050216, l1: 0.00010067895947693974, l2: 0.00035315792541232326   Iteration 82 of 100, tot loss = 4.515950618720636, l1: 0.00010050349604769996, l2: 0.0003510915655458187   Iteration 83 of 100, tot loss = 4.492389437663986, l1: 0.00010004010307812973, l2: 0.0003491988402429452   Iteration 84 of 100, tot loss = 4.461589953729084, l1: 9.95358985684496e-05, l2: 0.00034662309640441977   Iteration 85 of 100, tot loss = 4.441159353536718, l1: 9.936070055118761e-05, l2: 0.00034475523451178827   Iteration 86 of 100, tot loss = 4.432707505170689, l1: 9.878891657091688e-05, l2: 0.00034448183358235413   Iteration 87 of 100, tot loss = 4.437654452762384, l1: 9.857594792767235e-05, l2: 0.00034518949700862   Iteration 88 of 100, tot loss = 4.426348254084587, l1: 9.824434474384444e-05, l2: 0.00034439048018115994   Iteration 89 of 100, tot loss = 4.4370515011669545, l1: 9.84785091010932e-05, l2: 0.0003452266404912755   Iteration 90 of 100, tot loss = 4.450775635242462, l1: 9.890883694525756e-05, l2: 0.000346168725971236   Iteration 91 of 100, tot loss = 4.428352221027835, l1: 9.847477669789261e-05, l2: 0.00034436044474464973   Iteration 92 of 100, tot loss = 4.425084338240001, l1: 9.872412564341768e-05, l2: 0.00034378430759913857   Iteration 93 of 100, tot loss = 4.417402548174704, l1: 9.865523707146646e-05, l2: 0.00034308501707655325   Iteration 94 of 100, tot loss = 4.424294455254332, l1: 9.891978744293583e-05, l2: 0.00034350965731217467   Iteration 95 of 100, tot loss = 4.457335089382372, l1: 9.939117161066909e-05, l2: 0.0003463423368214679   Iteration 96 of 100, tot loss = 4.4470448307693005, l1: 9.912641015337915e-05, l2: 0.00034557807233189425   Iteration 97 of 100, tot loss = 4.4629711871294635, l1: 9.958822057839272e-05, l2: 0.00034670889741841935   Iteration 98 of 100, tot loss = 4.480783750816268, l1: 9.9640535841083e-05, l2: 0.00034843783867806764   Iteration 99 of 100, tot loss = 4.475214294712953, l1: 9.964519267668948e-05, l2: 0.0003478762361801916   Iteration 100 of 100, tot loss = 4.466221624612809, l1: 9.946675912942737e-05, l2: 0.00034715540256001986
   End of epoch 1360; saving model... 

Epoch 1361 of 2000
   Iteration 1 of 100, tot loss = 4.491950511932373, l1: 0.00011283875937806442, l2: 0.00033635631552897394   Iteration 2 of 100, tot loss = 4.48287034034729, l1: 8.841387534630485e-05, l2: 0.00035987317096441984   Iteration 3 of 100, tot loss = 5.081450780232747, l1: 9.632164680321391e-05, l2: 0.0004118234501220286   Iteration 4 of 100, tot loss = 4.859328389167786, l1: 9.614021473680623e-05, l2: 0.0003897926362697035   Iteration 5 of 100, tot loss = 4.725570201873779, l1: 9.689816070022062e-05, l2: 0.00037565886741504074   Iteration 6 of 100, tot loss = 4.600795229276021, l1: 9.789373022310126e-05, l2: 0.0003621858001376192   Iteration 7 of 100, tot loss = 4.715282542364938, l1: 9.967697509897075e-05, l2: 0.00037185128478865536   Iteration 8 of 100, tot loss = 4.601485788822174, l1: 9.805176523514092e-05, l2: 0.0003620968163886573   Iteration 9 of 100, tot loss = 4.490830130047268, l1: 9.649624068212385e-05, l2: 0.0003525867747763793   Iteration 10 of 100, tot loss = 4.70498902797699, l1: 0.00010018154498538933, l2: 0.0003703173599205911   Iteration 11 of 100, tot loss = 4.852954192595049, l1: 0.00010307998970595442, l2: 0.00038221543019806796   Iteration 12 of 100, tot loss = 4.897360384464264, l1: 0.00010500453026907053, l2: 0.000384731509257108   Iteration 13 of 100, tot loss = 4.825306213819063, l1: 0.00010423264869202215, l2: 0.00037829797320927563   Iteration 14 of 100, tot loss = 4.836770721844265, l1: 0.0001042833698323063, l2: 0.0003793937004437404   Iteration 15 of 100, tot loss = 4.660291957855224, l1: 0.0001014842462609522, l2: 0.00036454494790329287   Iteration 16 of 100, tot loss = 4.6317324340343475, l1: 0.00010121144759978051, l2: 0.0003619617937147268   Iteration 17 of 100, tot loss = 4.563114096136654, l1: 9.984524299130392e-05, l2: 0.00035646616443580785   Iteration 18 of 100, tot loss = 4.543291793929206, l1: 0.00010002542613720935, l2: 0.0003543037512119756   Iteration 19 of 100, tot loss = 4.569445120660882, l1: 0.0001008984480377924, l2: 0.0003560460624468856   Iteration 20 of 100, tot loss = 4.502055132389069, l1: 9.89018384643714e-05, l2: 0.0003513036725053098   Iteration 21 of 100, tot loss = 4.473802714120774, l1: 9.778972352727405e-05, l2: 0.0003495905458250837   Iteration 22 of 100, tot loss = 4.433564825491472, l1: 9.684110591479111e-05, l2: 0.000346515374655031   Iteration 23 of 100, tot loss = 4.350189727285634, l1: 9.468307373184791e-05, l2: 0.0003403358973825917   Iteration 24 of 100, tot loss = 4.4241629640261335, l1: 9.599721094370277e-05, l2: 0.000346419082537371   Iteration 25 of 100, tot loss = 4.455388164520263, l1: 9.62754277861677e-05, l2: 0.0003492633847054094   Iteration 26 of 100, tot loss = 4.533684968948364, l1: 9.786407379299187e-05, l2: 0.0003555044207202557   Iteration 27 of 100, tot loss = 4.566677358415392, l1: 9.910894597716699e-05, l2: 0.00035755878801595556   Iteration 28 of 100, tot loss = 4.542528467518943, l1: 9.943532027786464e-05, l2: 0.0003548175248267528   Iteration 29 of 100, tot loss = 4.465640109160851, l1: 9.75611787751028e-05, l2: 0.00034900283107758853   Iteration 30 of 100, tot loss = 4.456276551882426, l1: 9.781058470252902e-05, l2: 0.0003478170692687854   Iteration 31 of 100, tot loss = 4.471383379351709, l1: 9.781530811338715e-05, l2: 0.0003493230286685209   Iteration 32 of 100, tot loss = 4.4951314851641655, l1: 9.855567532213172e-05, l2: 0.0003509574726194842   Iteration 33 of 100, tot loss = 4.504022706638683, l1: 9.870046580379659e-05, l2: 0.00035170180490240455   Iteration 34 of 100, tot loss = 4.599670936079586, l1: 0.0001004106379696168, l2: 0.00035955645623342953   Iteration 35 of 100, tot loss = 4.631764364242554, l1: 0.00010113427706528454, l2: 0.0003620421597068863   Iteration 36 of 100, tot loss = 4.612514952818553, l1: 0.00010117307505829053, l2: 0.0003600784200696378   Iteration 37 of 100, tot loss = 4.563543538789491, l1: 0.00010064356477389968, l2: 0.00035571078832789853   Iteration 38 of 100, tot loss = 4.53809318416997, l1: 0.0001000175484238609, l2: 0.00035379176934886917   Iteration 39 of 100, tot loss = 4.58216974674127, l1: 0.0001002787134777277, l2: 0.00035793826025791274   Iteration 40 of 100, tot loss = 4.5409403324127195, l1: 9.956525045708985e-05, l2: 0.000354528782190755   Iteration 41 of 100, tot loss = 4.536157526621005, l1: 9.99302746339065e-05, l2: 0.00035368547726581553   Iteration 42 of 100, tot loss = 4.576284544808524, l1: 0.00010078657578560524, l2: 0.00035684187674806233   Iteration 43 of 100, tot loss = 4.564584510270939, l1: 0.00010071782416225476, l2: 0.00035574062535171074   Iteration 44 of 100, tot loss = 4.572164123708552, l1: 0.00010046134577185677, l2: 0.0003567550652819178   Iteration 45 of 100, tot loss = 4.541758198208279, l1: 9.996757661509846e-05, l2: 0.00035420824230338136   Iteration 46 of 100, tot loss = 4.524501194124636, l1: 9.97392663797971e-05, l2: 0.00035271085241971457   Iteration 47 of 100, tot loss = 4.496412180839701, l1: 9.94119629390201e-05, l2: 0.0003502292544927765   Iteration 48 of 100, tot loss = 4.475411514441173, l1: 9.879833032755414e-05, l2: 0.00034874282027885783   Iteration 49 of 100, tot loss = 4.472680228097098, l1: 9.86462926411317e-05, l2: 0.0003486217293004524   Iteration 50 of 100, tot loss = 4.491255512237549, l1: 9.873895716737024e-05, l2: 0.0003503865929087624   Iteration 51 of 100, tot loss = 4.458012029236438, l1: 9.840603146870967e-05, l2: 0.0003473951701558761   Iteration 52 of 100, tot loss = 4.458984961876502, l1: 9.853927552355274e-05, l2: 0.00034735922012119915   Iteration 53 of 100, tot loss = 4.418487089984822, l1: 9.761933166751164e-05, l2: 0.00034422937693856303   Iteration 54 of 100, tot loss = 4.384853411603857, l1: 9.703702926344704e-05, l2: 0.0003414483114654474   Iteration 55 of 100, tot loss = 4.356945835460316, l1: 9.652136405665343e-05, l2: 0.00033917321892328224   Iteration 56 of 100, tot loss = 4.3708378076553345, l1: 9.668875236457097e-05, l2: 0.00034039502783603633   Iteration 57 of 100, tot loss = 4.347740495414064, l1: 9.65371331595203e-05, l2: 0.0003382369156992226   Iteration 58 of 100, tot loss = 4.343660588922171, l1: 9.651864626181938e-05, l2: 0.0003378474116445808   Iteration 59 of 100, tot loss = 4.413582143137011, l1: 9.708775616087162e-05, l2: 0.00034427045700456   Iteration 60 of 100, tot loss = 4.460230696201324, l1: 9.77604609336898e-05, l2: 0.00034826260768265155   Iteration 61 of 100, tot loss = 4.436179852876507, l1: 9.746080963179606e-05, l2: 0.00034615717500020735   Iteration 62 of 100, tot loss = 4.512498628708624, l1: 9.850409796560979e-05, l2: 0.000352745764933887   Iteration 63 of 100, tot loss = 4.532254442336067, l1: 9.920319159088346e-05, l2: 0.0003540222523807888   Iteration 64 of 100, tot loss = 4.531139273196459, l1: 9.902593529886872e-05, l2: 0.00035408799158176407   Iteration 65 of 100, tot loss = 4.561824919627263, l1: 9.958885611222985e-05, l2: 0.00035659363493323326   Iteration 66 of 100, tot loss = 4.5370540835640645, l1: 9.902449743425346e-05, l2: 0.00035468091024967816   Iteration 67 of 100, tot loss = 4.5347491093535925, l1: 9.898711059984418e-05, l2: 0.00035448779969705517   Iteration 68 of 100, tot loss = 4.530068551792818, l1: 9.917770239553647e-05, l2: 0.0003538291517436258   Iteration 69 of 100, tot loss = 4.5092804742896035, l1: 9.891705865258068e-05, l2: 0.0003520109877580831   Iteration 70 of 100, tot loss = 4.502614198412214, l1: 9.87618799367088e-05, l2: 0.00035149953930938086   Iteration 71 of 100, tot loss = 4.524435829108869, l1: 9.882787364890838e-05, l2: 0.0003536157090705491   Iteration 72 of 100, tot loss = 4.523631036281586, l1: 9.89826004721787e-05, l2: 0.0003533805027271026   Iteration 73 of 100, tot loss = 4.530899727181213, l1: 9.880050318514966e-05, l2: 0.0003542894693266932   Iteration 74 of 100, tot loss = 4.5145591014140365, l1: 9.847707420900602e-05, l2: 0.0003529788359505678   Iteration 75 of 100, tot loss = 4.513368555704752, l1: 9.884014507406391e-05, l2: 0.0003524967102566734   Iteration 76 of 100, tot loss = 4.508583627249065, l1: 9.869217190402866e-05, l2: 0.00035216619079093156   Iteration 77 of 100, tot loss = 4.518195641505254, l1: 9.887946565301375e-05, l2: 0.0003529400982389033   Iteration 78 of 100, tot loss = 4.523686763567802, l1: 9.89942087844844e-05, l2: 0.00035337446714924555   Iteration 79 of 100, tot loss = 4.539690144454377, l1: 9.909390022976714e-05, l2: 0.00035487511361023715   Iteration 80 of 100, tot loss = 4.547791683673859, l1: 9.887145274660725e-05, l2: 0.00035590771494753426   Iteration 81 of 100, tot loss = 4.551044052029833, l1: 9.894174074338761e-05, l2: 0.0003561626634356439   Iteration 82 of 100, tot loss = 4.531171827781491, l1: 9.858635783574555e-05, l2: 0.0003545308237534766   Iteration 83 of 100, tot loss = 4.5489669478083234, l1: 9.89849709240153e-05, l2: 0.0003559117228934452   Iteration 84 of 100, tot loss = 4.5587527127493, l1: 9.913240851996566e-05, l2: 0.0003567428615323955   Iteration 85 of 100, tot loss = 4.544315669115852, l1: 9.851557902841117e-05, l2: 0.00035591598665204777   Iteration 86 of 100, tot loss = 4.5237745074338696, l1: 9.826794391914899e-05, l2: 0.0003541095055731793   Iteration 87 of 100, tot loss = 4.52237483276718, l1: 9.824213213568297e-05, l2: 0.00035399534965694035   Iteration 88 of 100, tot loss = 4.529532129114324, l1: 9.866065267322648e-05, l2: 0.0003542925584373403   Iteration 89 of 100, tot loss = 4.529303979337885, l1: 9.875308162744648e-05, l2: 0.00035417731491629075   Iteration 90 of 100, tot loss = 4.525469382603963, l1: 9.857278702030372e-05, l2: 0.00035397415017036514   Iteration 91 of 100, tot loss = 4.550570456536262, l1: 9.877027668363343e-05, l2: 0.0003562867676629429   Iteration 92 of 100, tot loss = 4.555206630540931, l1: 9.906871863400421e-05, l2: 0.0003564519432510006   Iteration 93 of 100, tot loss = 4.5642342413625405, l1: 9.947553408765284e-05, l2: 0.00035694788894816354   Iteration 94 of 100, tot loss = 4.550743843646759, l1: 9.926075963835985e-05, l2: 0.0003558136234827776   Iteration 95 of 100, tot loss = 4.521849288438496, l1: 9.864228302735444e-05, l2: 0.00035354264454278897   Iteration 96 of 100, tot loss = 4.516789577901363, l1: 9.8648069600434e-05, l2: 0.000353030887102553   Iteration 97 of 100, tot loss = 4.508515063020372, l1: 9.848397399464469e-05, l2: 0.0003523675313209827   Iteration 98 of 100, tot loss = 4.491460316035212, l1: 9.828848205684989e-05, l2: 0.00035085754866393437   Iteration 99 of 100, tot loss = 4.4976883392141325, l1: 9.841620281216397e-05, l2: 0.00035135263022660943   Iteration 100 of 100, tot loss = 4.493954513072968, l1: 9.842878214840312e-05, l2: 0.000350966668484034
   End of epoch 1361; saving model... 

Epoch 1362 of 2000
   Iteration 1 of 100, tot loss = 7.684122085571289, l1: 0.00015507936768699437, l2: 0.000613332842476666   Iteration 2 of 100, tot loss = 6.596487522125244, l1: 0.00011861138045787811, l2: 0.00054103737056721   Iteration 3 of 100, tot loss = 6.488137404123942, l1: 0.0001107369292488632, l2: 0.0005380768076671908   Iteration 4 of 100, tot loss = 5.616672456264496, l1: 0.00010059696433017962, l2: 0.00046107027446851134   Iteration 5 of 100, tot loss = 5.333790922164917, l1: 9.585133520886301e-05, l2: 0.00043752775527536867   Iteration 6 of 100, tot loss = 5.181413292884827, l1: 0.00010119604121427983, l2: 0.0004169452828743185   Iteration 7 of 100, tot loss = 5.014582599912371, l1: 0.00010270982810262857, l2: 0.000398748429558639   Iteration 8 of 100, tot loss = 5.1076968014240265, l1: 0.0001048624208124238, l2: 0.000405907256208593   Iteration 9 of 100, tot loss = 4.900849713219537, l1: 0.00010187460268045672, l2: 0.000388210364488057   Iteration 10 of 100, tot loss = 4.925412607192993, l1: 0.00010433847419335506, l2: 0.00038820278423372655   Iteration 11 of 100, tot loss = 5.046534538269043, l1: 0.00010461206891870296, l2: 0.0004000413808336651   Iteration 12 of 100, tot loss = 5.1703182856241865, l1: 0.00010834080482406232, l2: 0.0004086910218272048   Iteration 13 of 100, tot loss = 5.0015168740199165, l1: 0.00010362594716403131, l2: 0.000396525738044427   Iteration 14 of 100, tot loss = 4.853305390902928, l1: 0.00010132304279457978, l2: 0.0003840074943062583   Iteration 15 of 100, tot loss = 4.715205065409342, l1: 9.948979592688071e-05, l2: 0.0003720307092104728   Iteration 16 of 100, tot loss = 4.706731200218201, l1: 9.936799665410945e-05, l2: 0.0003713051219165209   Iteration 17 of 100, tot loss = 4.647150502485387, l1: 9.99232406149317e-05, l2: 0.00036479180819108426   Iteration 18 of 100, tot loss = 4.503700256347656, l1: 9.805633110065376e-05, l2: 0.00035231369353520375   Iteration 19 of 100, tot loss = 4.6182456267507455, l1: 9.904555554385297e-05, l2: 0.00036277900768541977   Iteration 20 of 100, tot loss = 4.718523788452148, l1: 0.00010034416645794408, l2: 0.00037150821299292147   Iteration 21 of 100, tot loss = 4.739276817866734, l1: 0.00010044716934013802, l2: 0.00037348051167403656   Iteration 22 of 100, tot loss = 4.7396112138574775, l1: 9.986632969891865e-05, l2: 0.0003740947907896374   Iteration 23 of 100, tot loss = 4.738734597745149, l1: 9.976409969765328e-05, l2: 0.0003741093589823045   Iteration 24 of 100, tot loss = 4.7228701909383135, l1: 9.989014673313552e-05, l2: 0.00037239687056474696   Iteration 25 of 100, tot loss = 4.793344230651855, l1: 0.00010077070954139344, l2: 0.00037856371025554833   Iteration 26 of 100, tot loss = 4.725223798018235, l1: 9.992979973433718e-05, l2: 0.0003725925775003046   Iteration 27 of 100, tot loss = 4.713313597219962, l1: 0.00010002283387519937, l2: 0.0003713085224498408   Iteration 28 of 100, tot loss = 4.66414087159293, l1: 9.915931033382159e-05, l2: 0.0003672547739240274   Iteration 29 of 100, tot loss = 4.593562767423433, l1: 9.78953889741739e-05, l2: 0.00036146088477208055   Iteration 30 of 100, tot loss = 4.640032784144084, l1: 9.918515534081961e-05, l2: 0.0003648181198514067   Iteration 31 of 100, tot loss = 4.6227486671939975, l1: 9.80556960125649e-05, l2: 0.0003642191675739483   Iteration 32 of 100, tot loss = 4.525487873703241, l1: 9.632321643948671e-05, l2: 0.00035622556788439397   Iteration 33 of 100, tot loss = 4.503814072319956, l1: 9.629622174102361e-05, l2: 0.0003540851828416414   Iteration 34 of 100, tot loss = 4.490483589032117, l1: 9.651436609120163e-05, l2: 0.00035253398955854425   Iteration 35 of 100, tot loss = 4.52372362613678, l1: 9.730981754338635e-05, l2: 0.00035506254145210343   Iteration 36 of 100, tot loss = 4.548449195093578, l1: 9.768410442726842e-05, l2: 0.0003571608103811741   Iteration 37 of 100, tot loss = 4.543786999341604, l1: 9.7554613603279e-05, l2: 0.0003568240814225597   Iteration 38 of 100, tot loss = 4.531554407195041, l1: 9.772851645768816e-05, l2: 0.00035542691981199344   Iteration 39 of 100, tot loss = 4.530566554803115, l1: 9.840709184899401e-05, l2: 0.0003546495602513926   Iteration 40 of 100, tot loss = 4.529271075129509, l1: 9.861187663773308e-05, l2: 0.0003543152277416084   Iteration 41 of 100, tot loss = 4.550126561304418, l1: 9.922322049799444e-05, l2: 0.0003557894322112566   Iteration 42 of 100, tot loss = 4.510744404225123, l1: 9.799815580620253e-05, l2: 0.00035307628110915957   Iteration 43 of 100, tot loss = 4.512144479640694, l1: 9.774181214810977e-05, l2: 0.0003534726328812106   Iteration 44 of 100, tot loss = 4.508885830640793, l1: 9.699707896453964e-05, l2: 0.00035389150170059026   Iteration 45 of 100, tot loss = 4.501059312290615, l1: 9.692479814273408e-05, l2: 0.00035318113061495954   Iteration 46 of 100, tot loss = 4.5734920631284295, l1: 9.842670371416834e-05, l2: 0.0003589225006694703   Iteration 47 of 100, tot loss = 4.574541926383972, l1: 9.832937707579596e-05, l2: 0.00035912481426419887   Iteration 48 of 100, tot loss = 4.644936216374238, l1: 9.948528713721316e-05, l2: 0.0003650083338773887   Iteration 49 of 100, tot loss = 4.677342465945652, l1: 0.00010001708899520109, l2: 0.00036771715754091894   Iteration 50 of 100, tot loss = 4.656574332714081, l1: 0.00010021923691965639, l2: 0.0003654381964588538   Iteration 51 of 100, tot loss = 4.632464658980276, l1: 0.00010006292708852675, l2: 0.0003631835387946636   Iteration 52 of 100, tot loss = 4.633564295677038, l1: 0.00010041083512008369, l2: 0.00036294559406367346   Iteration 53 of 100, tot loss = 4.62624551440185, l1: 0.00010022422093414825, l2: 0.0003624003306953764   Iteration 54 of 100, tot loss = 4.663311488098568, l1: 0.00010136126851042111, l2: 0.0003649698806435077   Iteration 55 of 100, tot loss = 4.664717732776295, l1: 0.00010117444498146969, l2: 0.0003652973285749216   Iteration 56 of 100, tot loss = 4.682076375399317, l1: 0.0001014805078999156, l2: 0.0003667271308326495   Iteration 57 of 100, tot loss = 4.680520984164455, l1: 0.00010188205411204283, l2: 0.00036617004508642773   Iteration 58 of 100, tot loss = 4.6767287151566865, l1: 0.0001012739749998657, l2: 0.0003663988970621521   Iteration 59 of 100, tot loss = 4.6678258342258, l1: 0.00010136531599820986, l2: 0.00036541726815088067   Iteration 60 of 100, tot loss = 4.658129539092382, l1: 0.00010162526656737706, l2: 0.00036418768771303195   Iteration 61 of 100, tot loss = 4.689455382159499, l1: 0.00010204396545066361, l2: 0.0003669015733051862   Iteration 62 of 100, tot loss = 4.668562452639303, l1: 0.00010157187963129892, l2: 0.0003652843658909983   Iteration 63 of 100, tot loss = 4.664176254045396, l1: 0.00010166183312235045, l2: 0.00036475579263568514   Iteration 64 of 100, tot loss = 4.675031321123242, l1: 0.00010185788880789914, l2: 0.00036564524361892836   Iteration 65 of 100, tot loss = 4.67007472881904, l1: 0.00010177035079463027, l2: 0.000365237121989664   Iteration 66 of 100, tot loss = 4.69479537190813, l1: 0.00010254751032334752, l2: 0.00036693202679144275   Iteration 67 of 100, tot loss = 4.685628067201643, l1: 0.0001024527733914429, l2: 0.0003661100327822644   Iteration 68 of 100, tot loss = 4.686952971360263, l1: 0.00010253599579942495, l2: 0.0003661593004568096   Iteration 69 of 100, tot loss = 4.680848572565162, l1: 0.00010273117009816907, l2: 0.00036535368670466477   Iteration 70 of 100, tot loss = 4.705124911240169, l1: 0.00010288252659458002, l2: 0.0003676299641873421   Iteration 71 of 100, tot loss = 4.698842856245981, l1: 0.00010290505749035731, l2: 0.00036697922779483277   Iteration 72 of 100, tot loss = 4.65869485007392, l1: 0.00010201985636134243, l2: 0.00036384962833027303   Iteration 73 of 100, tot loss = 4.639233497724141, l1: 0.00010132325849255023, l2: 0.00036260009123720483   Iteration 74 of 100, tot loss = 4.639652967453003, l1: 0.00010075874146226656, l2: 0.000363206555178024   Iteration 75 of 100, tot loss = 4.648031972249349, l1: 0.00010037478006173236, l2: 0.0003644284177183484   Iteration 76 of 100, tot loss = 4.634832341420023, l1: 0.00010034718123183426, l2: 0.0003631360536689355   Iteration 77 of 100, tot loss = 4.659810874369238, l1: 0.00010098896079882677, l2: 0.0003649921267211752   Iteration 78 of 100, tot loss = 4.64628415229993, l1: 0.00010071158999963723, l2: 0.00036391682526580273   Iteration 79 of 100, tot loss = 4.645721538157403, l1: 0.00010084744294717778, l2: 0.0003637247109163812   Iteration 80 of 100, tot loss = 4.638492739200592, l1: 0.0001008844418265653, l2: 0.0003629648317655665   Iteration 81 of 100, tot loss = 4.63614488531042, l1: 0.00010094626113753708, l2: 0.00036266822684814944   Iteration 82 of 100, tot loss = 4.658593009157879, l1: 0.00010162705629914942, l2: 0.00036423224419860804   Iteration 83 of 100, tot loss = 4.630937843437654, l1: 0.00010094332831514808, l2: 0.0003621504555809794   Iteration 84 of 100, tot loss = 4.641326299735478, l1: 0.00010110768627263107, l2: 0.00036302494291227223   Iteration 85 of 100, tot loss = 4.639966911428115, l1: 0.0001011786020706709, l2: 0.00036281808793051714   Iteration 86 of 100, tot loss = 4.642454105754231, l1: 0.00010143403055418424, l2: 0.00036281137930250964   Iteration 87 of 100, tot loss = 4.642851119753958, l1: 0.00010153829485904586, l2: 0.00036274681627836033   Iteration 88 of 100, tot loss = 4.625703380866484, l1: 0.00010133537121377726, l2: 0.00036123496598024343   Iteration 89 of 100, tot loss = 4.641560798280694, l1: 0.00010156168461302453, l2: 0.00036259439382754435   Iteration 90 of 100, tot loss = 4.649183540874057, l1: 0.00010185553178922014, l2: 0.0003630628204619926   Iteration 91 of 100, tot loss = 4.669732154070676, l1: 0.0001021591413738641, l2: 0.0003648140722921548   Iteration 92 of 100, tot loss = 4.679471588653067, l1: 0.00010221604106450484, l2: 0.0003657311159337911   Iteration 93 of 100, tot loss = 4.676828079326178, l1: 0.00010232968825905744, l2: 0.00036535311774450844   Iteration 94 of 100, tot loss = 4.667783546955027, l1: 0.00010181677432828632, l2: 0.0003649615784834555   Iteration 95 of 100, tot loss = 4.668487611569856, l1: 0.00010198161054224903, l2: 0.00036486714870031725   Iteration 96 of 100, tot loss = 4.678797446191311, l1: 0.00010200068940472799, l2: 0.0003658790531820462   Iteration 97 of 100, tot loss = 4.675534585087569, l1: 0.00010187832729983355, l2: 0.00036567512938042276   Iteration 98 of 100, tot loss = 4.703911506399816, l1: 0.00010252298689382009, l2: 0.0003678681618743101   Iteration 99 of 100, tot loss = 4.763858657894713, l1: 0.00010352069412792017, l2: 0.0003728651702686241   Iteration 100 of 100, tot loss = 4.768185822963715, l1: 0.00010324256250896723, l2: 0.00037357601861003786
   End of epoch 1362; saving model... 

Epoch 1363 of 2000
   Iteration 1 of 100, tot loss = 5.26637601852417, l1: 0.00010719238343881443, l2: 0.00041944519034586847   Iteration 2 of 100, tot loss = 5.266069412231445, l1: 0.00011709386672009714, l2: 0.00040951307164505124   Iteration 3 of 100, tot loss = 5.197220325469971, l1: 0.00011567275457006569, l2: 0.0004040492737355332   Iteration 4 of 100, tot loss = 4.926385045051575, l1: 0.00011098315553681459, l2: 0.00038165535079315305   Iteration 5 of 100, tot loss = 4.655707597732544, l1: 0.00010342702444177121, l2: 0.0003621437354013324   Iteration 6 of 100, tot loss = 4.633340080579122, l1: 0.0001050937232018138, l2: 0.0003582402811540912   Iteration 7 of 100, tot loss = 4.729489292417254, l1: 0.0001053772191101286, l2: 0.0003675717078814549   Iteration 8 of 100, tot loss = 4.839600294828415, l1: 0.00010693534750316758, l2: 0.000377024680346949   Iteration 9 of 100, tot loss = 4.5928416517045765, l1: 0.00010313269174528412, l2: 0.00035615147337213985   Iteration 10 of 100, tot loss = 4.6540639162063595, l1: 0.00010379313534940593, l2: 0.00036161325115244833   Iteration 11 of 100, tot loss = 4.603757273067128, l1: 0.00010322395114185797, l2: 0.0003571517696731131   Iteration 12 of 100, tot loss = 4.456202407677968, l1: 0.0001011337474968362, l2: 0.00034448648875695653   Iteration 13 of 100, tot loss = 4.464194059371948, l1: 0.00010065068804229108, l2: 0.0003457687148161662   Iteration 14 of 100, tot loss = 4.329930356570652, l1: 9.847561107432869e-05, l2: 0.0003345174211842407   Iteration 15 of 100, tot loss = 4.500405136744181, l1: 0.00010074810464478408, l2: 0.00034929240549293655   Iteration 16 of 100, tot loss = 4.4094027280807495, l1: 9.944402063410962e-05, l2: 0.0003414962484384887   Iteration 17 of 100, tot loss = 4.523599961224725, l1: 0.0001006519712117391, l2: 0.0003517080194262021   Iteration 18 of 100, tot loss = 4.4962792661454944, l1: 9.84510823198232e-05, l2: 0.0003511768397099028   Iteration 19 of 100, tot loss = 4.6332721961172005, l1: 0.00010148919835429344, l2: 0.00036183801651197046   Iteration 20 of 100, tot loss = 4.6413352489471436, l1: 0.00010176709656661842, l2: 0.0003623664233600721   Iteration 21 of 100, tot loss = 4.675896531059628, l1: 0.00010349720659654676, l2: 0.0003640924419631206   Iteration 22 of 100, tot loss = 4.699682105671275, l1: 0.00010417484174302609, l2: 0.0003657933650009165   Iteration 23 of 100, tot loss = 4.743071410966956, l1: 0.00010549929087890474, l2: 0.00036880784445320785   Iteration 24 of 100, tot loss = 4.690633376439412, l1: 0.00010482673618146994, l2: 0.0003642365954874549   Iteration 25 of 100, tot loss = 4.71868673324585, l1: 0.00010583096649497747, l2: 0.00036603769985958933   Iteration 26 of 100, tot loss = 4.646282196044922, l1: 0.0001042429771937, l2: 0.0003603852355109456   Iteration 27 of 100, tot loss = 4.7393040303830745, l1: 0.0001059287886299124, l2: 0.00036800160833755163   Iteration 28 of 100, tot loss = 4.693219457353864, l1: 0.0001051066769183048, l2: 0.00036421526341915263   Iteration 29 of 100, tot loss = 4.6782026290893555, l1: 0.00010464881679267976, l2: 0.00036317144133198747   Iteration 30 of 100, tot loss = 4.678034480412801, l1: 0.00010408633194553355, l2: 0.0003637171122439516   Iteration 31 of 100, tot loss = 4.694981544248519, l1: 0.00010354544558081656, l2: 0.0003659527062536067   Iteration 32 of 100, tot loss = 4.774102255702019, l1: 0.00010420584931125632, l2: 0.00037320437377275084   Iteration 33 of 100, tot loss = 4.729446093241374, l1: 0.0001029946859379186, l2: 0.0003699499211506918   Iteration 34 of 100, tot loss = 4.711145274779376, l1: 0.00010279133451095892, l2: 0.00036832319122314565   Iteration 35 of 100, tot loss = 4.68875800541469, l1: 0.0001031313037466524, l2: 0.000365744494982729   Iteration 36 of 100, tot loss = 4.694949799113804, l1: 0.00010318940985598601, l2: 0.0003663055690089499   Iteration 37 of 100, tot loss = 4.696916116250528, l1: 0.00010325397955524307, l2: 0.00036643763077508256   Iteration 38 of 100, tot loss = 4.691753600773058, l1: 0.00010325820173755729, l2: 0.0003659171569253024   Iteration 39 of 100, tot loss = 4.754675217163869, l1: 0.00010395213048934985, l2: 0.0003715153893258852   Iteration 40 of 100, tot loss = 4.816905725002289, l1: 0.00010544331689743558, l2: 0.0003762472548260121   Iteration 41 of 100, tot loss = 4.861299049563524, l1: 0.00010590319839027915, l2: 0.0003802267058678653   Iteration 42 of 100, tot loss = 4.845350333622524, l1: 0.00010591453194917579, l2: 0.00037862050001941886   Iteration 43 of 100, tot loss = 4.8398358766422716, l1: 0.0001059270502463373, l2: 0.0003780565359277737   Iteration 44 of 100, tot loss = 4.809234624559229, l1: 0.0001049078986067219, l2: 0.000376015562572072   Iteration 45 of 100, tot loss = 4.827127748065525, l1: 0.00010493422871352069, l2: 0.0003777785449832057   Iteration 46 of 100, tot loss = 4.864019005194955, l1: 0.00010503802458634195, l2: 0.00038136387371670696   Iteration 47 of 100, tot loss = 4.849121220568393, l1: 0.00010491716285345164, l2: 0.00037999495714060726   Iteration 48 of 100, tot loss = 4.843367223938306, l1: 0.00010513737940224625, l2: 0.0003791993406897139   Iteration 49 of 100, tot loss = 4.821700412399915, l1: 0.00010480518584681333, l2: 0.00037736485311904046   Iteration 50 of 100, tot loss = 4.756393804550171, l1: 0.00010346530856622849, l2: 0.00037217406948911955   Iteration 51 of 100, tot loss = 4.7764423875247735, l1: 0.00010395450918426207, l2: 0.0003736897275552574   Iteration 52 of 100, tot loss = 4.739628695524656, l1: 0.00010292119028375825, l2: 0.00037104167705155513   Iteration 53 of 100, tot loss = 4.701835969709, l1: 0.00010236604025264551, l2: 0.0003678175544680342   Iteration 54 of 100, tot loss = 4.695117522169043, l1: 0.00010234227449001521, l2: 0.0003671694752551115   Iteration 55 of 100, tot loss = 4.709466955878518, l1: 0.00010291167358941907, l2: 0.0003680350194124251   Iteration 56 of 100, tot loss = 4.685285231896809, l1: 0.00010248999719156668, l2: 0.0003660385231601789   Iteration 57 of 100, tot loss = 4.696504555250469, l1: 0.0001026261901036681, l2: 0.00036702426280642514   Iteration 58 of 100, tot loss = 4.667311540965376, l1: 0.00010185291272325136, l2: 0.00036487823885620636   Iteration 59 of 100, tot loss = 4.716128975658093, l1: 0.00010260491084625108, l2: 0.00036900798418713037   Iteration 60 of 100, tot loss = 4.706094205379486, l1: 0.00010276971327887926, l2: 0.0003678397046314785   Iteration 61 of 100, tot loss = 4.676254198199413, l1: 0.0001020979898534219, l2: 0.00036552742748143397   Iteration 62 of 100, tot loss = 4.677732186932718, l1: 0.00010225223559245933, l2: 0.0003655209806193823   Iteration 63 of 100, tot loss = 4.676589553318326, l1: 0.00010246159697075315, l2: 0.00036519735560490055   Iteration 64 of 100, tot loss = 4.644515749067068, l1: 0.00010196613794732912, l2: 0.00036248543426609103   Iteration 65 of 100, tot loss = 4.686933917265672, l1: 0.00010248539133149629, l2: 0.0003662079974534348   Iteration 66 of 100, tot loss = 4.6797540585199995, l1: 0.0001022234551011493, l2: 0.0003657519479350991   Iteration 67 of 100, tot loss = 4.693492017575164, l1: 0.00010217088807865964, l2: 0.0003671783109242444   Iteration 68 of 100, tot loss = 4.687474191188812, l1: 0.000102350565184144, l2: 0.0003663968509499682   Iteration 69 of 100, tot loss = 4.67968762093696, l1: 0.00010261501982155175, l2: 0.0003653537393235467   Iteration 70 of 100, tot loss = 4.706578176362174, l1: 0.0001028696495528233, l2: 0.00036778816543768956   Iteration 71 of 100, tot loss = 4.686035727111387, l1: 0.0001029335139143038, l2: 0.00036567005603065924   Iteration 72 of 100, tot loss = 4.6639856232537165, l1: 0.00010249590695821098, l2: 0.00036390265257877117   Iteration 73 of 100, tot loss = 4.685362214911474, l1: 0.00010294878048214692, l2: 0.0003655874386648265   Iteration 74 of 100, tot loss = 4.727413306365142, l1: 0.0001034257733211947, l2: 0.0003693155556755153   Iteration 75 of 100, tot loss = 4.7246832784016926, l1: 0.00010350140951535043, l2: 0.0003689669170610917   Iteration 76 of 100, tot loss = 4.729726728640105, l1: 0.00010370411133695406, l2: 0.00036926855998325135   Iteration 77 of 100, tot loss = 4.71183121668828, l1: 0.00010335933616083998, l2: 0.0003678237838109701   Iteration 78 of 100, tot loss = 4.690366867261055, l1: 0.0001031776124037629, l2: 0.0003658590724696226   Iteration 79 of 100, tot loss = 4.679926947702335, l1: 0.00010281423578529524, l2: 0.00036517845728667453   Iteration 80 of 100, tot loss = 4.675837555527687, l1: 0.00010305097052878409, l2: 0.0003645327832600742   Iteration 81 of 100, tot loss = 4.664322673538585, l1: 0.00010271259533409547, l2: 0.0003637196703628568   Iteration 82 of 100, tot loss = 4.6561167530897185, l1: 0.00010262558866887092, l2: 0.0003629860849930371   Iteration 83 of 100, tot loss = 4.668427065194371, l1: 0.00010311159112242454, l2: 0.0003637311139799015   Iteration 84 of 100, tot loss = 4.669752399126689, l1: 0.00010303384316925768, l2: 0.00036394139507527664   Iteration 85 of 100, tot loss = 4.680985461964327, l1: 0.00010302362271057277, l2: 0.0003650749214370187   Iteration 86 of 100, tot loss = 4.689649360124455, l1: 0.00010320991523892803, l2: 0.0003657550188045418   Iteration 87 of 100, tot loss = 4.710643181855652, l1: 0.00010374792691436596, l2: 0.0003673163895467136   Iteration 88 of 100, tot loss = 4.716600434346632, l1: 0.00010385501028312402, l2: 0.0003678050312704396   Iteration 89 of 100, tot loss = 4.714103746949957, l1: 0.00010382044123350956, l2: 0.000367589931515846   Iteration 90 of 100, tot loss = 4.720098521974352, l1: 0.00010415521735719974, l2: 0.0003678546331583574   Iteration 91 of 100, tot loss = 4.725831817794632, l1: 0.00010415547992554391, l2: 0.00036842770038804574   Iteration 92 of 100, tot loss = 4.717179777829544, l1: 0.00010410469712951453, l2: 0.0003676132792347546   Iteration 93 of 100, tot loss = 4.730689507658764, l1: 0.00010443927487201979, l2: 0.00036862967415369786   Iteration 94 of 100, tot loss = 4.712735457623259, l1: 0.00010400794057436832, l2: 0.0003672656035787634   Iteration 95 of 100, tot loss = 4.688227216820968, l1: 0.00010361072755155251, l2: 0.00036521199255826344   Iteration 96 of 100, tot loss = 4.6859041800101595, l1: 0.00010333051261568471, l2: 0.00036525990367408667   Iteration 97 of 100, tot loss = 4.672114315721178, l1: 0.00010323146129052213, l2: 0.0003639799685993942   Iteration 98 of 100, tot loss = 4.673332192459885, l1: 0.00010338745306974172, l2: 0.00036394576462192406   Iteration 99 of 100, tot loss = 4.661625854896776, l1: 0.00010343653738887914, l2: 0.00036272604667847376   Iteration 100 of 100, tot loss = 4.656079590320587, l1: 0.0001032011925781262, l2: 0.00036240676483430436
   End of epoch 1363; saving model... 

Epoch 1364 of 2000
   Iteration 1 of 100, tot loss = 5.332141876220703, l1: 0.00012913066893815994, l2: 0.00040408354834653437   Iteration 2 of 100, tot loss = 6.391080856323242, l1: 0.00013483131624525413, l2: 0.000504276787978597   Iteration 3 of 100, tot loss = 5.3410905202229815, l1: 0.00011557208199519664, l2: 0.0004185369859139125   Iteration 4 of 100, tot loss = 5.767947554588318, l1: 0.00012451879592845216, l2: 0.0004522759700194001   Iteration 5 of 100, tot loss = 5.444475555419922, l1: 0.00011677210277412086, l2: 0.0004276754625607282   Iteration 6 of 100, tot loss = 5.465747992197673, l1: 0.00011810935150909548, l2: 0.0004284654520840074   Iteration 7 of 100, tot loss = 5.566262926374163, l1: 0.00012090084367498224, l2: 0.0004357254531766687   Iteration 8 of 100, tot loss = 5.455671548843384, l1: 0.00011913894013559911, l2: 0.00042642821790650487   Iteration 9 of 100, tot loss = 5.008823209338718, l1: 0.00011055923176981095, l2: 0.00039032309157644503   Iteration 10 of 100, tot loss = 4.954679703712463, l1: 0.00011157520639244467, l2: 0.00038389276232919656   Iteration 11 of 100, tot loss = 4.964358568191528, l1: 0.00011265832555099306, l2: 0.0003837775326577354   Iteration 12 of 100, tot loss = 4.903634647528331, l1: 0.00011228565987645804, l2: 0.0003780778054836749   Iteration 13 of 100, tot loss = 4.881177993921133, l1: 0.00011172603206852308, l2: 0.00037639177025994286   Iteration 14 of 100, tot loss = 4.931784885270255, l1: 0.00011231438319165525, l2: 0.00038086411067134965   Iteration 15 of 100, tot loss = 4.863089609146118, l1: 0.0001110326416286019, l2: 0.0003752763242421982   Iteration 16 of 100, tot loss = 4.8699986189603806, l1: 0.00011136390821775422, l2: 0.00037563595788014936   Iteration 17 of 100, tot loss = 4.891826082678402, l1: 0.00011282533438027124, l2: 0.00037635727879687156   Iteration 18 of 100, tot loss = 4.783560832341512, l1: 0.00011145253221103404, l2: 0.00036690355571206764   Iteration 19 of 100, tot loss = 4.780215940977397, l1: 0.0001104401078344764, l2: 0.000367581490871472   Iteration 20 of 100, tot loss = 4.883229041099549, l1: 0.00011076806040364318, l2: 0.00037755484991066626   Iteration 21 of 100, tot loss = 4.995713006882441, l1: 0.00011257876619874012, l2: 0.00038699254045717506   Iteration 22 of 100, tot loss = 4.98685546354814, l1: 0.0001120386583682954, l2: 0.0003866468935236546   Iteration 23 of 100, tot loss = 4.866434325342593, l1: 0.0001091760083078913, l2: 0.00037746743011824867   Iteration 24 of 100, tot loss = 4.829757471879323, l1: 0.00010772076393550378, l2: 0.00037525498828472337   Iteration 25 of 100, tot loss = 4.860256996154785, l1: 0.00010797351729706862, l2: 0.0003780521886073984   Iteration 26 of 100, tot loss = 4.939474197534414, l1: 0.00010966222306216459, l2: 0.0003842852034162766   Iteration 27 of 100, tot loss = 4.906836792274758, l1: 0.00010975333740740704, l2: 0.00038093034877588423   Iteration 28 of 100, tot loss = 4.908539193017142, l1: 0.00010990789724019123, l2: 0.0003809460293788496   Iteration 29 of 100, tot loss = 4.861539298090442, l1: 0.00010884026728333616, l2: 0.0003773136697167627   Iteration 30 of 100, tot loss = 4.992849493026734, l1: 0.00011107781526031128, l2: 0.00038820714131967786   Iteration 31 of 100, tot loss = 5.074980028213993, l1: 0.00011168001935490588, l2: 0.00039581799031903725   Iteration 32 of 100, tot loss = 5.178121984004974, l1: 0.00011346124495048571, l2: 0.0004043509600251127   Iteration 33 of 100, tot loss = 5.133736465916489, l1: 0.00011322555600958312, l2: 0.00040014809686302516   Iteration 34 of 100, tot loss = 5.184208926032571, l1: 0.0001140758295685587, l2: 0.00040434506958905226   Iteration 35 of 100, tot loss = 5.182683985573905, l1: 0.00011462448611772353, l2: 0.00040364391856460965   Iteration 36 of 100, tot loss = 5.230632053481208, l1: 0.00011515031362958123, l2: 0.0004079128979760248   Iteration 37 of 100, tot loss = 5.193528168910259, l1: 0.00011506865327671246, l2: 0.00040428416989901026   Iteration 38 of 100, tot loss = 5.121925046569423, l1: 0.00011362723987411683, l2: 0.00039856527081084145   Iteration 39 of 100, tot loss = 5.101391835090442, l1: 0.00011321878292409774, l2: 0.000396920406320789   Iteration 40 of 100, tot loss = 5.048637133836746, l1: 0.00011211748378627817, l2: 0.00039274623504752525   Iteration 41 of 100, tot loss = 5.048338000367328, l1: 0.0001122876086943817, l2: 0.000392546196696746   Iteration 42 of 100, tot loss = 5.025656058674767, l1: 0.00011181347465372666, l2: 0.0003907521363760766   Iteration 43 of 100, tot loss = 4.997222018796344, l1: 0.00011169018170073476, l2: 0.00038803202525826256   Iteration 44 of 100, tot loss = 4.9809258796951985, l1: 0.0001113004212708223, l2: 0.00038679217240397435   Iteration 45 of 100, tot loss = 4.9844761689503985, l1: 0.0001117245091235317, l2: 0.00038672311364078065   Iteration 46 of 100, tot loss = 4.946191715157551, l1: 0.00011102430613865853, l2: 0.0003835948710215946   Iteration 47 of 100, tot loss = 4.9422026390725, l1: 0.00011090206688559773, l2: 0.00038331820232610396   Iteration 48 of 100, tot loss = 4.923570861419042, l1: 0.00011065286260721526, l2: 0.0003817042280995035   Iteration 49 of 100, tot loss = 4.936508937757843, l1: 0.00011038017550979892, l2: 0.000383270722522866   Iteration 50 of 100, tot loss = 4.896425085067749, l1: 0.00010952474964142312, l2: 0.00038011776327039114   Iteration 51 of 100, tot loss = 4.855372349421184, l1: 0.00010878449214871187, l2: 0.0003767527473517511   Iteration 52 of 100, tot loss = 4.832363293721126, l1: 0.00010811360901313422, l2: 0.00037512272473787353   Iteration 53 of 100, tot loss = 4.798492692551523, l1: 0.00010749726150709837, l2: 0.00037235201197513817   Iteration 54 of 100, tot loss = 4.750112220093056, l1: 0.0001067715794407478, l2: 0.00036823964681523784   Iteration 55 of 100, tot loss = 4.744631754268299, l1: 0.00010667605893104337, l2: 0.0003677871203928424   Iteration 56 of 100, tot loss = 4.741672647850854, l1: 0.00010659272987010939, l2: 0.00036757453861875026   Iteration 57 of 100, tot loss = 4.740343407580727, l1: 0.00010700414630966342, l2: 0.0003670301979454643   Iteration 58 of 100, tot loss = 4.750544412382718, l1: 0.00010658469288720316, l2: 0.0003684697513908132   Iteration 59 of 100, tot loss = 4.756276199373148, l1: 0.00010664157106460199, l2: 0.00036898605223109756   Iteration 60 of 100, tot loss = 4.715162432193756, l1: 0.000105647509432553, l2: 0.0003658687372080749   Iteration 61 of 100, tot loss = 4.713267048851389, l1: 0.00010558270353568168, l2: 0.0003657440048285577   Iteration 62 of 100, tot loss = 4.723564536340775, l1: 0.00010616761609262246, l2: 0.0003661888407687882   Iteration 63 of 100, tot loss = 4.714189555909899, l1: 0.00010597422621527036, l2: 0.00036544473215823045   Iteration 64 of 100, tot loss = 4.712255459278822, l1: 0.00010581725712199841, l2: 0.0003654082912589729   Iteration 65 of 100, tot loss = 4.714236571238591, l1: 0.00010562983549719389, l2: 0.0003657938242003393   Iteration 66 of 100, tot loss = 4.688426881125479, l1: 0.00010520906454820016, l2: 0.00036363362629675646   Iteration 67 of 100, tot loss = 4.728398618413441, l1: 0.00010593803864451988, l2: 0.00036690182562086924   Iteration 68 of 100, tot loss = 4.716048387920155, l1: 0.00010595791162118344, l2: 0.00036564692934060823   Iteration 69 of 100, tot loss = 4.698091282360796, l1: 0.00010566326934655291, l2: 0.0003641458609743732   Iteration 70 of 100, tot loss = 4.667740300723485, l1: 0.00010492803017509037, l2: 0.00036184600186451075   Iteration 71 of 100, tot loss = 4.682401888807055, l1: 0.00010534228029837964, l2: 0.00036289790982600224   Iteration 72 of 100, tot loss = 4.667265226443608, l1: 0.00010500088076999721, l2: 0.00036172564341541147   Iteration 73 of 100, tot loss = 4.630862382993306, l1: 0.00010431857832646865, l2: 0.00035876766146616755   Iteration 74 of 100, tot loss = 4.61797566349442, l1: 0.0001041151093935745, l2: 0.00035768245829417166   Iteration 75 of 100, tot loss = 4.605852565765381, l1: 0.0001034991238945319, l2: 0.0003570861341237711   Iteration 76 of 100, tot loss = 4.609447190636082, l1: 0.00010378724748175285, l2: 0.00035715747330150886   Iteration 77 of 100, tot loss = 4.618572804834936, l1: 0.00010364764758768226, l2: 0.00035820963447414603   Iteration 78 of 100, tot loss = 4.62701451472747, l1: 0.00010384869281738364, l2: 0.00035885275997899065   Iteration 79 of 100, tot loss = 4.624163180966921, l1: 0.00010416331170296261, l2: 0.00035825300815476907   Iteration 80 of 100, tot loss = 4.615568545460701, l1: 0.00010428230330035148, l2: 0.000357274552789022   Iteration 81 of 100, tot loss = 4.6152550173394475, l1: 0.00010420489217475184, l2: 0.0003573206108010282   Iteration 82 of 100, tot loss = 4.611407224724933, l1: 0.00010404160705703118, l2: 0.0003570991169154451   Iteration 83 of 100, tot loss = 4.62934563246118, l1: 0.00010440606138765733, l2: 0.00035852850342929613   Iteration 84 of 100, tot loss = 4.620230314277467, l1: 0.00010421497630713497, l2: 0.00035780805661661796   Iteration 85 of 100, tot loss = 4.611506523805506, l1: 0.00010404993154224939, l2: 0.00035710072252721363   Iteration 86 of 100, tot loss = 4.616162305654481, l1: 0.00010418478695798915, l2: 0.0003574314454623206   Iteration 87 of 100, tot loss = 4.6044915160913575, l1: 0.00010398921054813358, l2: 0.0003564599428918673   Iteration 88 of 100, tot loss = 4.600237957455895, l1: 0.00010421036077704312, l2: 0.0003558134365613858   Iteration 89 of 100, tot loss = 4.611802473496855, l1: 0.00010428140413250219, l2: 0.000356898845086434   Iteration 90 of 100, tot loss = 4.606367405255636, l1: 0.00010409144900525765, l2: 0.00035654529339000066   Iteration 91 of 100, tot loss = 4.584830525157216, l1: 0.00010371174133839444, l2: 0.0003547713132210557   Iteration 92 of 100, tot loss = 4.593588575072911, l1: 0.00010371799935851216, l2: 0.00035564086022571683   Iteration 93 of 100, tot loss = 4.599848849799043, l1: 0.00010390325789108262, l2: 0.0003560816296294922   Iteration 94 of 100, tot loss = 4.591421107028393, l1: 0.00010391382815253158, l2: 0.0003552282849999214   Iteration 95 of 100, tot loss = 4.5853197850679095, l1: 0.00010373284653484773, l2: 0.00035479913464146913   Iteration 96 of 100, tot loss = 4.575483384231727, l1: 0.000103641247392261, l2: 0.00035390709361612   Iteration 97 of 100, tot loss = 4.57580789339911, l1: 0.00010357146783997112, l2: 0.00035400932422807903   Iteration 98 of 100, tot loss = 4.5776760991738765, l1: 0.00010382108826443258, l2: 0.0003539465244696713   Iteration 99 of 100, tot loss = 4.567082513462413, l1: 0.00010362521076023419, l2: 0.00035308304322457807   Iteration 100 of 100, tot loss = 4.565607612133026, l1: 0.0001034179736234364, l2: 0.0003531427898997208
   End of epoch 1364; saving model... 

Epoch 1365 of 2000
   Iteration 1 of 100, tot loss = 3.2571115493774414, l1: 7.610584725625813e-05, l2: 0.0002496053057257086   Iteration 2 of 100, tot loss = 4.599684238433838, l1: 9.293305993196554e-05, l2: 0.0003670353762572631   Iteration 3 of 100, tot loss = 4.27488907178243, l1: 9.127186785917729e-05, l2: 0.00033621705370023847   Iteration 4 of 100, tot loss = 4.496567666530609, l1: 9.473437967244536e-05, l2: 0.00035492239840095863   Iteration 5 of 100, tot loss = 4.609743452072143, l1: 9.734695922816172e-05, l2: 0.00036362739047035575   Iteration 6 of 100, tot loss = 4.542933026949565, l1: 9.580182571274538e-05, l2: 0.00035849148116540164   Iteration 7 of 100, tot loss = 4.916905641555786, l1: 0.00010536422736809723, l2: 0.00038632634511616616   Iteration 8 of 100, tot loss = 4.993320971727371, l1: 0.00010802763790707104, l2: 0.0003913044711225666   Iteration 9 of 100, tot loss = 5.138079775704278, l1: 0.00011135322468665738, l2: 0.000402454765410059   Iteration 10 of 100, tot loss = 5.242573666572571, l1: 0.00011499984393594786, l2: 0.00040925753128249196   Iteration 11 of 100, tot loss = 5.147099776701494, l1: 0.00011111616847549819, l2: 0.0004035938155456361   Iteration 12 of 100, tot loss = 5.177147328853607, l1: 0.00011357969136345976, l2: 0.0004041350499998468   Iteration 13 of 100, tot loss = 5.276119397236751, l1: 0.00011539642302802979, l2: 0.0004122155257321608   Iteration 14 of 100, tot loss = 5.114656329154968, l1: 0.00011327334297155695, l2: 0.00039819229901435653   Iteration 15 of 100, tot loss = 5.169171826044718, l1: 0.0001145642037348201, l2: 0.0004023529860811929   Iteration 16 of 100, tot loss = 5.207233652472496, l1: 0.00011382297225281945, l2: 0.0004069004007760668   Iteration 17 of 100, tot loss = 5.193442667231841, l1: 0.00011328271819579909, l2: 0.0004060615559437257   Iteration 18 of 100, tot loss = 5.178703135914272, l1: 0.00011236415391774951, l2: 0.0004055061662155721   Iteration 19 of 100, tot loss = 5.099355132956254, l1: 0.00011195060735764472, l2: 0.0003979849126680117   Iteration 20 of 100, tot loss = 5.080794680118561, l1: 0.00011187709278601688, l2: 0.00039620238239876924   Iteration 21 of 100, tot loss = 5.125058003834316, l1: 0.00011360059926075683, l2: 0.0003989052091215161   Iteration 22 of 100, tot loss = 4.985954349691218, l1: 0.00011068470104708632, l2: 0.0003879107409209775   Iteration 23 of 100, tot loss = 4.9866884894993, l1: 0.00010986094576250964, l2: 0.0003888079100642758   Iteration 24 of 100, tot loss = 4.960579693317413, l1: 0.00010926971678297075, l2: 0.00038678825937192113   Iteration 25 of 100, tot loss = 4.883746767044068, l1: 0.00010901206711423583, l2: 0.0003793626156402752   Iteration 26 of 100, tot loss = 4.867332577705383, l1: 0.0001089510958711169, l2: 0.0003777821688205362   Iteration 27 of 100, tot loss = 4.7982673909929066, l1: 0.00010823725609009637, l2: 0.0003715894902901103   Iteration 28 of 100, tot loss = 4.709685027599335, l1: 0.00010649494002531615, l2: 0.00036447356978897005   Iteration 29 of 100, tot loss = 4.6000071928418915, l1: 0.00010401527570452187, l2: 0.00035598545043191325   Iteration 30 of 100, tot loss = 4.587855343023936, l1: 0.00010453530837063832, l2: 0.00035425023355249626   Iteration 31 of 100, tot loss = 4.595041094287749, l1: 0.00010402836039335648, l2: 0.0003554757570400984   Iteration 32 of 100, tot loss = 4.636283416301012, l1: 0.00010455183553403913, l2: 0.00035907651295019605   Iteration 33 of 100, tot loss = 4.713844808665189, l1: 0.00010561296144398071, l2: 0.000365771525442445   Iteration 34 of 100, tot loss = 4.728744313997381, l1: 0.00010512728615996072, l2: 0.0003677471517328419   Iteration 35 of 100, tot loss = 4.726382776669094, l1: 0.0001047249545600997, l2: 0.00036791332885124055   Iteration 36 of 100, tot loss = 4.8028514550791845, l1: 0.00010661615093946845, l2: 0.0003736690012172201   Iteration 37 of 100, tot loss = 4.838325181522885, l1: 0.00010769574451055755, l2: 0.00037613678019575626   Iteration 38 of 100, tot loss = 4.867568885025225, l1: 0.00010846742100655224, l2: 0.00037828947461093776   Iteration 39 of 100, tot loss = 4.867991034801189, l1: 0.00010855860557804147, l2: 0.00037824050452446757   Iteration 40 of 100, tot loss = 4.8303412169218065, l1: 0.00010744269793576678, l2: 0.00037559142965619684   Iteration 41 of 100, tot loss = 4.816420796440869, l1: 0.0001072432069787926, l2: 0.00037439887872584753   Iteration 42 of 100, tot loss = 4.812696658429646, l1: 0.00010714634057270207, l2: 0.00037412333092236495   Iteration 43 of 100, tot loss = 4.785762262898822, l1: 0.000106334723758939, l2: 0.0003722415078596515   Iteration 44 of 100, tot loss = 4.761417543346232, l1: 0.0001063991315029191, l2: 0.00036974262796072503   Iteration 45 of 100, tot loss = 4.855942410892911, l1: 0.00010785469890429845, l2: 0.0003777395469822093   Iteration 46 of 100, tot loss = 4.9345374133275905, l1: 0.00010932306418987497, l2: 0.00038413068108164464   Iteration 47 of 100, tot loss = 4.953521112178234, l1: 0.00010918311044348384, l2: 0.00038616900518660414   Iteration 48 of 100, tot loss = 4.9413086399436, l1: 0.00010896998636174733, l2: 0.0003851608824637272   Iteration 49 of 100, tot loss = 4.934474178722927, l1: 0.0001086050648823124, l2: 0.00038484235698647075   Iteration 50 of 100, tot loss = 4.951279556751251, l1: 0.00010915811588347424, l2: 0.00038596984362811783   Iteration 51 of 100, tot loss = 4.930330213378458, l1: 0.00010909303498064058, l2: 0.00038393999021340565   Iteration 52 of 100, tot loss = 4.913441165135457, l1: 0.00010885778598234398, l2: 0.0003824863341343679   Iteration 53 of 100, tot loss = 4.895480823966692, l1: 0.00010850812718414982, l2: 0.0003810399586723608   Iteration 54 of 100, tot loss = 4.848878487392708, l1: 0.00010740592302614391, l2: 0.0003774819292798552   Iteration 55 of 100, tot loss = 4.8497212648391725, l1: 0.00010710915289977988, l2: 0.0003778629765623588   Iteration 56 of 100, tot loss = 4.932499270353999, l1: 0.00010787292291882165, l2: 0.0003853770071405701   Iteration 57 of 100, tot loss = 4.926977682531926, l1: 0.00010760291677382938, l2: 0.0003850948546243129   Iteration 58 of 100, tot loss = 4.909077625850151, l1: 0.00010669649368380839, l2: 0.00038421127214076416   Iteration 59 of 100, tot loss = 4.895610572928089, l1: 0.00010642990636662975, l2: 0.00038313115461062545   Iteration 60 of 100, tot loss = 4.880343208710353, l1: 0.00010635289072524756, l2: 0.0003816814335247424   Iteration 61 of 100, tot loss = 4.86890660934761, l1: 0.00010638144110581364, l2: 0.00038050922322312186   Iteration 62 of 100, tot loss = 4.877603079042127, l1: 0.00010654478002482304, l2: 0.000381215530773437   Iteration 63 of 100, tot loss = 4.8578698464802335, l1: 0.00010642896005366411, l2: 0.00037935802780945477   Iteration 64 of 100, tot loss = 4.86155197955668, l1: 0.00010673775204850244, l2: 0.0003794174493805258   Iteration 65 of 100, tot loss = 4.826750669112572, l1: 0.00010618823737156792, l2: 0.0003764868329968662   Iteration 66 of 100, tot loss = 4.8127923571702205, l1: 0.00010562808485260713, l2: 0.0003756511543928575   Iteration 67 of 100, tot loss = 4.810955743291485, l1: 0.0001056186131370226, l2: 0.0003754769651950754   Iteration 68 of 100, tot loss = 4.812303309931474, l1: 0.00010538485998147414, l2: 0.0003758454747773666   Iteration 69 of 100, tot loss = 4.854900468950686, l1: 0.00010595729891328897, l2: 0.0003795327511793205   Iteration 70 of 100, tot loss = 4.834940445423126, l1: 0.000105694881286971, l2: 0.00037779916653692326   Iteration 71 of 100, tot loss = 4.846692258203533, l1: 0.0001058003327786319, l2: 0.0003788688964690869   Iteration 72 of 100, tot loss = 4.8290316975779, l1: 0.00010549208673182875, l2: 0.0003774110863458999   Iteration 73 of 100, tot loss = 4.811485876775768, l1: 0.00010523801298304907, l2: 0.00037591057809226034   Iteration 74 of 100, tot loss = 4.777645473544662, l1: 0.00010485089011569005, l2: 0.0003729136606617013   Iteration 75 of 100, tot loss = 4.794368249575297, l1: 0.00010485298621157805, l2: 0.000374583842640277   Iteration 76 of 100, tot loss = 4.78395237577589, l1: 0.00010484330423902353, l2: 0.0003735519372927037   Iteration 77 of 100, tot loss = 4.785030539933738, l1: 0.00010466781702246396, l2: 0.00037383524084527215   Iteration 78 of 100, tot loss = 4.7676164996929655, l1: 0.000104519298101644, l2: 0.00037224235571696994   Iteration 79 of 100, tot loss = 4.773543214496178, l1: 0.000104098218821779, l2: 0.0003732561066200002   Iteration 80 of 100, tot loss = 4.80760385543108, l1: 0.0001044196777002071, l2: 0.00037634071240972845   Iteration 81 of 100, tot loss = 4.856924735469582, l1: 0.00010519871706671921, l2: 0.0003804937613169364   Iteration 82 of 100, tot loss = 4.869552799841252, l1: 0.00010530861041710212, l2: 0.0003816466743911605   Iteration 83 of 100, tot loss = 4.8428854152380705, l1: 0.00010471157222938533, l2: 0.00037957697393897224   Iteration 84 of 100, tot loss = 4.839781830708186, l1: 0.00010456700727532042, l2: 0.00037941118032557865   Iteration 85 of 100, tot loss = 4.853324330554289, l1: 0.00010489774679573362, l2: 0.0003804346908616614   Iteration 86 of 100, tot loss = 4.855164013629736, l1: 0.00010496556815057777, l2: 0.0003805508376688014   Iteration 87 of 100, tot loss = 4.836771377201738, l1: 0.00010463410070843075, l2: 0.00037904304140126975   Iteration 88 of 100, tot loss = 4.82219263504852, l1: 0.00010409863418978851, l2: 0.0003781206336167419   Iteration 89 of 100, tot loss = 4.820486854971125, l1: 0.0001041612389538008, l2: 0.00037788745077010346   Iteration 90 of 100, tot loss = 4.807148155901167, l1: 0.00010391210501741928, l2: 0.0003768027148099564   Iteration 91 of 100, tot loss = 4.783364687647138, l1: 0.00010346456262625877, l2: 0.0003748719103456536   Iteration 92 of 100, tot loss = 4.759995925685634, l1: 0.00010312656843819676, l2: 0.000372873028185416   Iteration 93 of 100, tot loss = 4.769443059480318, l1: 0.00010315761599446877, l2: 0.00037378669390642654   Iteration 94 of 100, tot loss = 4.783186601831558, l1: 0.0001034213028299533, l2: 0.0003748973614029695   Iteration 95 of 100, tot loss = 4.797279155881781, l1: 0.0001037925666375821, l2: 0.00037593535311507845   Iteration 96 of 100, tot loss = 4.787081533422072, l1: 0.00010364375119327936, l2: 0.00037506440632265975   Iteration 97 of 100, tot loss = 4.7620909005096275, l1: 0.00010305906681358315, l2: 0.00037315002720282153   Iteration 98 of 100, tot loss = 4.735870333350435, l1: 0.00010265479795634747, l2: 0.0003709322392046024   Iteration 99 of 100, tot loss = 4.721957152540034, l1: 0.00010224132226236786, l2: 0.0003699543967465588   Iteration 100 of 100, tot loss = 4.70783215880394, l1: 0.0001021950384892989, l2: 0.0003685881810815772
   End of epoch 1365; saving model... 

Epoch 1366 of 2000
   Iteration 1 of 100, tot loss = 4.109747886657715, l1: 0.00011083927529398352, l2: 0.00030013552168384194   Iteration 2 of 100, tot loss = 4.3220720291137695, l1: 0.00011065575745305978, l2: 0.0003215514589101076   Iteration 3 of 100, tot loss = 3.590533892313639, l1: 8.849053604838748e-05, l2: 0.0002705628624729191   Iteration 4 of 100, tot loss = 4.321204781532288, l1: 9.666906589700375e-05, l2: 0.00033545141559443437   Iteration 5 of 100, tot loss = 4.2187892436981205, l1: 9.387592435814441e-05, l2: 0.0003280030010500923   Iteration 6 of 100, tot loss = 4.046739498774211, l1: 9.236826250950496e-05, l2: 0.0003123056885669939   Iteration 7 of 100, tot loss = 3.9345652716500417, l1: 9.074931585928425e-05, l2: 0.00030270721306026516   Iteration 8 of 100, tot loss = 3.9709916710853577, l1: 8.88513268364477e-05, l2: 0.00030824783789284993   Iteration 9 of 100, tot loss = 4.209770891401503, l1: 9.474597027292475e-05, l2: 0.00032623111716626835   Iteration 10 of 100, tot loss = 4.1335369348526, l1: 9.35394702537451e-05, l2: 0.0003198142207111232   Iteration 11 of 100, tot loss = 4.50323089686307, l1: 9.922254288209264e-05, l2: 0.0003511005449002947   Iteration 12 of 100, tot loss = 4.5370615522066755, l1: 9.887713895295747e-05, l2: 0.00035482901269764017   Iteration 13 of 100, tot loss = 4.493309589532705, l1: 9.736782689847481e-05, l2: 0.00035196312931545364   Iteration 14 of 100, tot loss = 4.420552287782941, l1: 9.408915034977586e-05, l2: 0.00034796607776245637   Iteration 15 of 100, tot loss = 4.6217198689778645, l1: 9.73544495839936e-05, l2: 0.00036481753437935064   Iteration 16 of 100, tot loss = 4.598228842020035, l1: 9.762477611729992e-05, l2: 0.00036219810590409907   Iteration 17 of 100, tot loss = 4.628696329453412, l1: 9.848955532481127e-05, l2: 0.0003643800780136029   Iteration 18 of 100, tot loss = 4.873276657528347, l1: 0.0001027989290176063, l2: 0.0003845287372112378   Iteration 19 of 100, tot loss = 4.91424728694715, l1: 0.00010367791362927834, l2: 0.0003877468169728098   Iteration 20 of 100, tot loss = 4.807764601707459, l1: 0.0001010918782412773, l2: 0.00037968458418617955   Iteration 21 of 100, tot loss = 4.8364926519848055, l1: 0.00010307188183235536, l2: 0.00038057738441109123   Iteration 22 of 100, tot loss = 4.828493378379128, l1: 0.00010428550112092952, l2: 0.0003785638380742801   Iteration 23 of 100, tot loss = 4.942733743916387, l1: 0.00010547285700854643, l2: 0.0003888005176913398   Iteration 24 of 100, tot loss = 5.075904389222463, l1: 0.00010803128649664966, l2: 0.00039955915114357293   Iteration 25 of 100, tot loss = 5.087325763702393, l1: 0.00010884958523092791, l2: 0.00039988298958633096   Iteration 26 of 100, tot loss = 5.117736577987671, l1: 0.00011040897264208, l2: 0.0004013646836168706   Iteration 27 of 100, tot loss = 5.086353513929579, l1: 0.00010993635159037594, l2: 0.0003986989985504705   Iteration 28 of 100, tot loss = 4.9806601745741705, l1: 0.00010807372875985623, l2: 0.0003899922875072142   Iteration 29 of 100, tot loss = 4.864745399047589, l1: 0.0001056492718785664, l2: 0.0003808252668963617   Iteration 30 of 100, tot loss = 4.791028281052907, l1: 0.00010419923164590727, l2: 0.0003749035948809857   Iteration 31 of 100, tot loss = 4.848730506435517, l1: 0.00010530073620517919, l2: 0.0003795723136018721   Iteration 32 of 100, tot loss = 4.919193837791681, l1: 0.00010687951692034403, l2: 0.00038503986525029177   Iteration 33 of 100, tot loss = 4.887736483053728, l1: 0.00010695721886639313, l2: 0.0003818164281476515   Iteration 34 of 100, tot loss = 4.913006533594692, l1: 0.00010660668457978104, l2: 0.0003846939661718138   Iteration 35 of 100, tot loss = 4.897824808529445, l1: 0.00010631536081616235, l2: 0.00038346711828905555   Iteration 36 of 100, tot loss = 4.8641137778759, l1: 0.0001053905071886321, l2: 0.0003810208686950824   Iteration 37 of 100, tot loss = 4.935733779056652, l1: 0.00010550404555475768, l2: 0.00038806933082161923   Iteration 38 of 100, tot loss = 4.930581528889506, l1: 0.0001054469285966326, l2: 0.00038761122299260214   Iteration 39 of 100, tot loss = 4.991721431414287, l1: 0.00010582665102750373, l2: 0.00039334549085022166   Iteration 40 of 100, tot loss = 4.994370314478874, l1: 0.0001064015429619758, l2: 0.0003930354869225994   Iteration 41 of 100, tot loss = 4.974356206451974, l1: 0.00010615689266492931, l2: 0.00039127872683244146   Iteration 42 of 100, tot loss = 4.951047366573697, l1: 0.00010520274848940538, l2: 0.0003899019869831612   Iteration 43 of 100, tot loss = 4.903729702151099, l1: 0.00010391948155809714, l2: 0.0003864534871333257   Iteration 44 of 100, tot loss = 4.877077148719267, l1: 0.00010393591020891273, l2: 0.0003837718028411142   Iteration 45 of 100, tot loss = 4.893956960572137, l1: 0.00010438741204173614, l2: 0.00038500828248086487   Iteration 46 of 100, tot loss = 4.922508107579273, l1: 0.00010508621009016086, l2: 0.0003871645992900164   Iteration 47 of 100, tot loss = 4.967301766923133, l1: 0.00010592276923992532, l2: 0.00039080740664738847   Iteration 48 of 100, tot loss = 4.956239712735017, l1: 0.000105539871280295, l2: 0.00039008409930829657   Iteration 49 of 100, tot loss = 4.939737818679031, l1: 0.00010516152156478896, l2: 0.0003888122591888532   Iteration 50 of 100, tot loss = 4.983731548786164, l1: 0.00010576092929113656, l2: 0.0003926122243865393   Iteration 51 of 100, tot loss = 4.96236427858764, l1: 0.00010554223305970321, l2: 0.0003906941939003802   Iteration 52 of 100, tot loss = 4.938241131030596, l1: 0.00010571194938025795, l2: 0.00038811216290923767   Iteration 53 of 100, tot loss = 4.980761921630715, l1: 0.00010663224384188652, l2: 0.00039144394724884615   Iteration 54 of 100, tot loss = 5.002768770412162, l1: 0.00010655743433205687, l2: 0.0003937194422986876   Iteration 55 of 100, tot loss = 4.999828397143971, l1: 0.00010653164526130157, l2: 0.00039345119386615063   Iteration 56 of 100, tot loss = 4.981828887547765, l1: 0.00010634896157820808, l2: 0.0003918339261872461   Iteration 57 of 100, tot loss = 4.965562262033162, l1: 0.00010651746344050033, l2: 0.0003900387620026442   Iteration 58 of 100, tot loss = 4.934708145158044, l1: 0.00010615301414810378, l2: 0.00038731779982456295   Iteration 59 of 100, tot loss = 4.919277508380049, l1: 0.00010614535907950347, l2: 0.00038578239160045284   Iteration 60 of 100, tot loss = 4.914442557096481, l1: 0.00010634093972233435, l2: 0.0003851033157843631   Iteration 61 of 100, tot loss = 4.897015933130608, l1: 0.00010640265421987679, l2: 0.0003832989388057718   Iteration 62 of 100, tot loss = 4.886153884472385, l1: 0.00010586750243918129, l2: 0.0003827478858217927   Iteration 63 of 100, tot loss = 4.8861737800022915, l1: 0.00010608119503428628, l2: 0.0003825361830725645   Iteration 64 of 100, tot loss = 4.8665169198066, l1: 0.00010602577469853713, l2: 0.00038062591715970484   Iteration 65 of 100, tot loss = 4.892375463705797, l1: 0.00010657757666194811, l2: 0.00038265996912601765   Iteration 66 of 100, tot loss = 4.885489335565856, l1: 0.0001065509185895225, l2: 0.00038199801422182867   Iteration 67 of 100, tot loss = 4.911479624349679, l1: 0.00010682643100588041, l2: 0.0003843215303304397   Iteration 68 of 100, tot loss = 4.917024487958235, l1: 0.00010678298402875524, l2: 0.00038491946372018633   Iteration 69 of 100, tot loss = 4.922917492147805, l1: 0.00010688401622852932, l2: 0.0003854077319071318   Iteration 70 of 100, tot loss = 4.907996029513223, l1: 0.0001067975172190927, l2: 0.0003840020847357144   Iteration 71 of 100, tot loss = 4.94457205416451, l1: 0.00010734837433375733, l2: 0.0003871088296184841   Iteration 72 of 100, tot loss = 4.921995989150471, l1: 0.00010711490227145582, l2: 0.0003850846953557468   Iteration 73 of 100, tot loss = 4.917547217787129, l1: 0.00010725465078506466, l2: 0.0003845000694177356   Iteration 74 of 100, tot loss = 4.924883428457621, l1: 0.00010743676142049701, l2: 0.0003850515800090884   Iteration 75 of 100, tot loss = 4.937414008776347, l1: 0.00010769075655844063, l2: 0.0003860506428948914   Iteration 76 of 100, tot loss = 4.946251340602574, l1: 0.00010778646011251687, l2: 0.00038683867272706456   Iteration 77 of 100, tot loss = 4.9763832510291754, l1: 0.0001083539692834685, l2: 0.0003892843546905228   Iteration 78 of 100, tot loss = 4.975890268117953, l1: 0.00010851669363686457, l2: 0.00038907233195062965   Iteration 79 of 100, tot loss = 4.987686373010466, l1: 0.00010909408003407165, l2: 0.00038967455595089243   Iteration 80 of 100, tot loss = 4.97484395056963, l1: 0.00010871696713365964, l2: 0.00038876742673892294   Iteration 81 of 100, tot loss = 4.964937576541194, l1: 0.00010834405641358944, l2: 0.0003881497004279223   Iteration 82 of 100, tot loss = 4.976680443054292, l1: 0.00010812569421321358, l2: 0.00038954234918403416   Iteration 83 of 100, tot loss = 4.957699682339128, l1: 0.00010780392750096228, l2: 0.0003879660397209788   Iteration 84 of 100, tot loss = 4.992193214950108, l1: 0.00010833503923398287, l2: 0.0003908842809323687   Iteration 85 of 100, tot loss = 4.980172747724197, l1: 0.00010817792824716033, l2: 0.00038983934500720353   Iteration 86 of 100, tot loss = 4.990005184051602, l1: 0.00010821081885423641, l2: 0.0003907896977981964   Iteration 87 of 100, tot loss = 5.0043871498656, l1: 0.00010840409641400322, l2: 0.0003920346165635227   Iteration 88 of 100, tot loss = 4.98195506632328, l1: 0.00010819442856948378, l2: 0.00039000107608444523   Iteration 89 of 100, tot loss = 4.969396442509769, l1: 0.00010801936395785489, l2: 0.00038892027821909805   Iteration 90 of 100, tot loss = 4.959131528271569, l1: 0.00010812961554620414, l2: 0.00038778353498653613   Iteration 91 of 100, tot loss = 4.970897529151413, l1: 0.00010812159030887575, l2: 0.0003889681604130521   Iteration 92 of 100, tot loss = 4.9741401840811195, l1: 0.00010834420960315543, l2: 0.0003890698067350414   Iteration 93 of 100, tot loss = 4.977656980996491, l1: 0.00010843310281083548, l2: 0.00038933259311697935   Iteration 94 of 100, tot loss = 4.987347198293564, l1: 0.00010860905093339054, l2: 0.00039012566691219925   Iteration 95 of 100, tot loss = 4.998999421220077, l1: 0.0001087725801258593, l2: 0.0003911273598389112   Iteration 96 of 100, tot loss = 4.991122997055451, l1: 0.00010874335566768423, l2: 0.00039036894198337296   Iteration 97 of 100, tot loss = 5.002190099549048, l1: 0.00010899299489857494, l2: 0.00039122601297141544   Iteration 98 of 100, tot loss = 5.00624880133843, l1: 0.00010910749700329057, l2: 0.00039151738078822856   Iteration 99 of 100, tot loss = 5.013510440335129, l1: 0.00010911828385268555, l2: 0.0003922327582999556   Iteration 100 of 100, tot loss = 5.00954607129097, l1: 0.00010910318742389791, l2: 0.00039185141780762934
   End of epoch 1366; saving model... 

Epoch 1367 of 2000
   Iteration 1 of 100, tot loss = 3.2937562465667725, l1: 9.879727440420538e-05, l2: 0.00023057835642248392   Iteration 2 of 100, tot loss = 3.1445807218551636, l1: 7.890946108091157e-05, l2: 0.00023554862127639353   Iteration 3 of 100, tot loss = 3.3981263637542725, l1: 9.29427854619765e-05, l2: 0.00024686986580491066   Iteration 4 of 100, tot loss = 3.760819137096405, l1: 9.912977566273184e-05, l2: 0.0002769521524896845   Iteration 5 of 100, tot loss = 4.138357782363892, l1: 0.00010745880208560266, l2: 0.00030637698946520684   Iteration 6 of 100, tot loss = 4.023301879564921, l1: 0.00010199145071965177, l2: 0.0003003387488812829   Iteration 7 of 100, tot loss = 3.8991882460457936, l1: 9.828630001201028e-05, l2: 0.0002916325382622225   Iteration 8 of 100, tot loss = 3.757088392972946, l1: 9.359763635075069e-05, l2: 0.00028211121752974577   Iteration 9 of 100, tot loss = 3.693487008412679, l1: 9.232009364899973e-05, l2: 0.0002770286195704506   Iteration 10 of 100, tot loss = 3.929982089996338, l1: 9.329155909654219e-05, l2: 0.0002997066592797637   Iteration 11 of 100, tot loss = 3.821445183320479, l1: 8.951269079592417e-05, l2: 0.00029263183585664427   Iteration 12 of 100, tot loss = 3.7867974440256753, l1: 8.880380725410457e-05, l2: 0.0002898759448726196   Iteration 13 of 100, tot loss = 3.9933917339031515, l1: 9.432496265687335e-05, l2: 0.0003050142158127318   Iteration 14 of 100, tot loss = 3.95404509135655, l1: 9.342870751944636e-05, l2: 0.0003019758057364795   Iteration 15 of 100, tot loss = 4.013546498616536, l1: 9.327687682040657e-05, l2: 0.00030807777754186344   Iteration 16 of 100, tot loss = 4.193620383739471, l1: 9.617861951483064e-05, l2: 0.0003231834261896438   Iteration 17 of 100, tot loss = 4.162912537069881, l1: 9.516916778760359e-05, l2: 0.00032112209266736447   Iteration 18 of 100, tot loss = 4.140284564759996, l1: 9.561868379629838e-05, l2: 0.0003184097788309575   Iteration 19 of 100, tot loss = 4.249231112630744, l1: 9.697808127384633e-05, l2: 0.0003279450351041497   Iteration 20 of 100, tot loss = 4.224489104747772, l1: 9.633922236389481e-05, l2: 0.00032610969283268786   Iteration 21 of 100, tot loss = 4.24686971164885, l1: 9.58198236600895e-05, l2: 0.00032886715087529625   Iteration 22 of 100, tot loss = 4.333259484984658, l1: 9.589287947164848e-05, l2: 0.00033743307325163517   Iteration 23 of 100, tot loss = 4.448659969412762, l1: 9.780991999634907e-05, l2: 0.0003470560800253778   Iteration 24 of 100, tot loss = 4.463094383478165, l1: 9.865719979037142e-05, l2: 0.0003476522415439831   Iteration 25 of 100, tot loss = 4.495909070968628, l1: 9.983817784814164e-05, l2: 0.0003497527336003259   Iteration 26 of 100, tot loss = 4.476478998477642, l1: 9.910729344343193e-05, l2: 0.0003485406100830565   Iteration 27 of 100, tot loss = 4.449887187392624, l1: 9.972934451609574e-05, l2: 0.00034525937829770284   Iteration 28 of 100, tot loss = 4.4785590171813965, l1: 0.00010049237711687706, l2: 0.0003473635281677291   Iteration 29 of 100, tot loss = 4.478931295460668, l1: 0.00010130102475221944, l2: 0.00034659210811288833   Iteration 30 of 100, tot loss = 4.443001723289489, l1: 9.969111949127788e-05, l2: 0.0003446090563860101   Iteration 31 of 100, tot loss = 4.474930186425486, l1: 0.00010009476470933746, l2: 0.00034739825669901383   Iteration 32 of 100, tot loss = 4.478938214480877, l1: 9.900466284307186e-05, l2: 0.00034888916115960455   Iteration 33 of 100, tot loss = 4.459685997529463, l1: 9.955422424492568e-05, l2: 0.0003464143784102899   Iteration 34 of 100, tot loss = 4.47772680310642, l1: 0.00010003252121832167, l2: 0.000347740162824801   Iteration 35 of 100, tot loss = 4.5488349710191995, l1: 0.00010087513926139633, l2: 0.0003540083620464429   Iteration 36 of 100, tot loss = 4.554849379592472, l1: 0.00010118753561982885, l2: 0.0003542974061727162   Iteration 37 of 100, tot loss = 4.572151963775222, l1: 0.00010079469172302886, l2: 0.00035642050843799134   Iteration 38 of 100, tot loss = 4.563477986737301, l1: 0.00010111117597644854, l2: 0.0003552366263312778   Iteration 39 of 100, tot loss = 4.530043491950402, l1: 0.0001005870319950657, l2: 0.0003524173207085532   Iteration 40 of 100, tot loss = 4.542793977260589, l1: 0.0001003670426143799, l2: 0.00035391235833230893   Iteration 41 of 100, tot loss = 4.561240859147979, l1: 0.00010077505608134699, l2: 0.00035534903340371006   Iteration 42 of 100, tot loss = 4.573131708871751, l1: 0.00010109675869378927, l2: 0.00035621641637546765   Iteration 43 of 100, tot loss = 4.6408351077589876, l1: 0.00010268032201565802, l2: 0.00036140319215794374   Iteration 44 of 100, tot loss = 4.65970521623438, l1: 0.00010274648560549726, l2: 0.00036322403883042915   Iteration 45 of 100, tot loss = 4.638547759585911, l1: 0.00010201908605975203, l2: 0.00036183569237538095   Iteration 46 of 100, tot loss = 4.687524785166201, l1: 0.00010231915423545338, l2: 0.00036643332587909356   Iteration 47 of 100, tot loss = 4.76127367831291, l1: 0.00010376452608281707, l2: 0.0003723628429492856   Iteration 48 of 100, tot loss = 4.760574559370677, l1: 0.00010435203330416698, l2: 0.0003717054238829102   Iteration 49 of 100, tot loss = 4.7533345222473145, l1: 0.0001041901568711108, l2: 0.0003711432969905626   Iteration 50 of 100, tot loss = 4.789149627685547, l1: 0.00010464317834703252, l2: 0.00037427178613143043   Iteration 51 of 100, tot loss = 4.874658248003791, l1: 0.00010636024942904638, l2: 0.000381105576297177   Iteration 52 of 100, tot loss = 4.83766305905122, l1: 0.00010586404879667008, l2: 0.0003779022576614247   Iteration 53 of 100, tot loss = 4.830563558722442, l1: 0.00010563936405749289, l2: 0.0003774169920877782   Iteration 54 of 100, tot loss = 4.794646029119138, l1: 0.000105077455642728, l2: 0.00037438714773290686   Iteration 55 of 100, tot loss = 4.763966274261475, l1: 0.00010416908996624195, l2: 0.00037222753797488457   Iteration 56 of 100, tot loss = 4.804824258599963, l1: 0.0001044965752693575, l2: 0.0003759858513408939   Iteration 57 of 100, tot loss = 4.84459512275562, l1: 0.00010547409099097804, l2: 0.00037898542206293265   Iteration 58 of 100, tot loss = 4.8675334206942855, l1: 0.00010566637364059591, l2: 0.0003810869685398286   Iteration 59 of 100, tot loss = 4.869390851360256, l1: 0.00010602504754403479, l2: 0.0003809140377551696   Iteration 60 of 100, tot loss = 4.877265397707621, l1: 0.00010612120240693912, l2: 0.0003816053378007685   Iteration 61 of 100, tot loss = 4.876329523618104, l1: 0.00010650098771353054, l2: 0.00038113196468225025   Iteration 62 of 100, tot loss = 4.83741904458692, l1: 0.00010547563186029888, l2: 0.00037826627287300185   Iteration 63 of 100, tot loss = 4.880626530874343, l1: 0.00010597344682162021, l2: 0.00038208920690637556   Iteration 64 of 100, tot loss = 4.851768605411053, l1: 0.00010577581775805811, l2: 0.0003794010435740347   Iteration 65 of 100, tot loss = 4.837999354876005, l1: 0.00010504171973130164, l2: 0.00037875821670660604   Iteration 66 of 100, tot loss = 4.856548182892077, l1: 0.00010534745083670273, l2: 0.0003803073685214093   Iteration 67 of 100, tot loss = 4.856085866244872, l1: 0.00010568617626767494, l2: 0.0003799224109910151   Iteration 68 of 100, tot loss = 4.829044699668884, l1: 0.0001050738958249841, l2: 0.0003778305750422399   Iteration 69 of 100, tot loss = 4.821854591369629, l1: 0.0001051925792125985, l2: 0.0003769928808796449   Iteration 70 of 100, tot loss = 4.791202344213214, l1: 0.00010472694016893261, l2: 0.0003743932950393563   Iteration 71 of 100, tot loss = 4.843769990222555, l1: 0.00010561815284284383, l2: 0.00037875884761017235   Iteration 72 of 100, tot loss = 4.850228564606772, l1: 0.00010562239994113852, l2: 0.0003794004577583918   Iteration 73 of 100, tot loss = 4.814912877670706, l1: 0.00010498357952288543, l2: 0.00037650770965440533   Iteration 74 of 100, tot loss = 4.807963264955057, l1: 0.00010458165490637987, l2: 0.00037621467325496614   Iteration 75 of 100, tot loss = 4.812098137537638, l1: 0.00010484114609425888, l2: 0.0003763686690945178   Iteration 76 of 100, tot loss = 4.815192627279382, l1: 0.0001050339793245688, l2: 0.00037648528507959684   Iteration 77 of 100, tot loss = 4.804589240581958, l1: 0.0001044726609093645, l2: 0.00037598626479482303   Iteration 78 of 100, tot loss = 4.788216859866411, l1: 0.00010432523749141883, l2: 0.0003744964499044447   Iteration 79 of 100, tot loss = 4.75746815717673, l1: 0.00010385539285437781, l2: 0.00037189142423375404   Iteration 80 of 100, tot loss = 4.765205889940262, l1: 0.00010424352676636772, l2: 0.00037227706343401223   Iteration 81 of 100, tot loss = 4.775950266991132, l1: 0.00010437169759016897, l2: 0.0003732233304662607   Iteration 82 of 100, tot loss = 4.793345811890393, l1: 0.00010451363636004716, l2: 0.0003748209462018411   Iteration 83 of 100, tot loss = 4.799341224762331, l1: 0.00010488467103049877, l2: 0.00037504945277146634   Iteration 84 of 100, tot loss = 4.7700425159363515, l1: 0.00010437747412305769, l2: 0.00037262677887199623   Iteration 85 of 100, tot loss = 4.7785770696752214, l1: 0.0001045441214741646, l2: 0.0003733135869397837   Iteration 86 of 100, tot loss = 4.790199978407039, l1: 0.0001048216633302387, l2: 0.0003741983355423652   Iteration 87 of 100, tot loss = 4.778707378212062, l1: 0.00010431892111102633, l2: 0.0003735518177312895   Iteration 88 of 100, tot loss = 4.750943725759333, l1: 0.00010375508980525245, l2: 0.00037133928361237685   Iteration 89 of 100, tot loss = 4.736458012227262, l1: 0.00010375351574908932, l2: 0.00036989228624662154   Iteration 90 of 100, tot loss = 4.754823758867052, l1: 0.00010397630455069398, l2: 0.0003715060722445034   Iteration 91 of 100, tot loss = 4.742855753217425, l1: 0.0001041092759156267, l2: 0.0003701763001787425   Iteration 92 of 100, tot loss = 4.74160217202228, l1: 0.00010429846023053751, l2: 0.0003698617580559348   Iteration 93 of 100, tot loss = 4.771884487521264, l1: 0.00010472782174787504, l2: 0.00037246062770806334   Iteration 94 of 100, tot loss = 4.757133133867954, l1: 0.00010423286430329639, l2: 0.00037148044979635705   Iteration 95 of 100, tot loss = 4.744709180530749, l1: 0.00010410862228845393, l2: 0.0003703622965411724   Iteration 96 of 100, tot loss = 4.7379692395528155, l1: 0.000104210179263949, l2: 0.00036958674566752353   Iteration 97 of 100, tot loss = 4.755947575126727, l1: 0.00010466191106951152, l2: 0.00037093284766708223   Iteration 98 of 100, tot loss = 4.758861133030483, l1: 0.00010457006921606823, l2: 0.00037131604515206146   Iteration 99 of 100, tot loss = 4.7637332424973, l1: 0.00010446250154119398, l2: 0.0003719108239183146   Iteration 100 of 100, tot loss = 4.7957235527038575, l1: 0.00010505637459573336, l2: 0.00037451598211191595
   End of epoch 1367; saving model... 

Epoch 1368 of 2000
   Iteration 1 of 100, tot loss = 3.6741461753845215, l1: 9.440814756089821e-05, l2: 0.000273006473435089   Iteration 2 of 100, tot loss = 4.903246641159058, l1: 0.00010436126831336878, l2: 0.00038596340164076537   Iteration 3 of 100, tot loss = 4.622494220733643, l1: 0.00010147153564806406, l2: 0.00036077789263799787   Iteration 4 of 100, tot loss = 4.508468508720398, l1: 9.357719136460219e-05, l2: 0.0003572696659830399   Iteration 5 of 100, tot loss = 4.420604133605957, l1: 9.209801210090518e-05, l2: 0.0003499624086543918   Iteration 6 of 100, tot loss = 4.585123538970947, l1: 9.476421352398272e-05, l2: 0.00036374814226292074   Iteration 7 of 100, tot loss = 4.8337997027805875, l1: 0.00010391356564858662, l2: 0.0003794664059699114   Iteration 8 of 100, tot loss = 4.764082908630371, l1: 9.792302853384172e-05, l2: 0.0003784852633543778   Iteration 9 of 100, tot loss = 4.7194647789001465, l1: 9.721172399521392e-05, l2: 0.0003747347541826053   Iteration 10 of 100, tot loss = 4.78620457649231, l1: 9.727337419462856e-05, l2: 0.0003813470830209553   Iteration 11 of 100, tot loss = 4.73523473739624, l1: 9.61993047729431e-05, l2: 0.0003773241690148345   Iteration 12 of 100, tot loss = 4.623898526032765, l1: 9.512244196230313e-05, l2: 0.0003672674113962178   Iteration 13 of 100, tot loss = 4.671221311275776, l1: 9.689068051207309e-05, l2: 0.00037023145257710264   Iteration 14 of 100, tot loss = 4.51165200982775, l1: 9.329210193495133e-05, l2: 0.0003578731006460397   Iteration 15 of 100, tot loss = 4.633669424057007, l1: 9.573308343533427e-05, l2: 0.0003676338616060093   Iteration 16 of 100, tot loss = 4.569172576069832, l1: 9.581284530213452e-05, l2: 0.0003611044148783549   Iteration 17 of 100, tot loss = 4.61881563242744, l1: 9.874732666717405e-05, l2: 0.0003631342389031916   Iteration 18 of 100, tot loss = 4.6305405961142645, l1: 9.815749136679288e-05, l2: 0.00036489657051990635   Iteration 19 of 100, tot loss = 4.761776585327952, l1: 0.00010039965318834507, l2: 0.0003757780076212887   Iteration 20 of 100, tot loss = 4.70767332315445, l1: 0.00010012679995270446, l2: 0.00037064053394715303   Iteration 21 of 100, tot loss = 4.6161591325487406, l1: 9.826852813906346e-05, l2: 0.0003633473873681699   Iteration 22 of 100, tot loss = 4.594961545684121, l1: 9.854068081370893e-05, l2: 0.00036095547693548724   Iteration 23 of 100, tot loss = 4.699501047963682, l1: 0.00010090511647792047, l2: 0.00036904499068876487   Iteration 24 of 100, tot loss = 4.640856146812439, l1: 9.985718012709792e-05, l2: 0.0003642284361073204   Iteration 25 of 100, tot loss = 4.575355482101441, l1: 9.778776657185518e-05, l2: 0.0003597477829316631   Iteration 26 of 100, tot loss = 4.48840974844419, l1: 9.573989882483147e-05, l2: 0.0003531010767731529   Iteration 27 of 100, tot loss = 4.4947623411814375, l1: 9.61264223507088e-05, l2: 0.00035334981253577606   Iteration 28 of 100, tot loss = 4.527689448424748, l1: 9.627834983153402e-05, l2: 0.0003564905959397687   Iteration 29 of 100, tot loss = 4.589903773932622, l1: 9.795582345445994e-05, l2: 0.000361034554300894   Iteration 30 of 100, tot loss = 4.6399978240331015, l1: 9.942830656655133e-05, l2: 0.0003645714750746265   Iteration 31 of 100, tot loss = 4.5809027225740495, l1: 9.894793906677214e-05, l2: 0.00035914233247298866   Iteration 32 of 100, tot loss = 4.5398648381233215, l1: 9.90353637462249e-05, l2: 0.0003549511193341459   Iteration 33 of 100, tot loss = 4.526552388162324, l1: 9.91734522165533e-05, l2: 0.00035348178547186154   Iteration 34 of 100, tot loss = 4.479192453272202, l1: 9.904429059722187e-05, l2: 0.0003488749541301171   Iteration 35 of 100, tot loss = 4.524831090654645, l1: 0.00010049720211619777, l2: 0.000351985905685329   Iteration 36 of 100, tot loss = 4.4670756260554, l1: 9.892938861513458e-05, l2: 0.0003477781732120396   Iteration 37 of 100, tot loss = 4.505069578016126, l1: 9.927598502317041e-05, l2: 0.00035123097296565066   Iteration 38 of 100, tot loss = 4.49459991957012, l1: 9.906997940178324e-05, l2: 0.00035039001369640524   Iteration 39 of 100, tot loss = 4.501220201834654, l1: 9.898862014942539e-05, l2: 0.0003511334014966941   Iteration 40 of 100, tot loss = 4.495976316928863, l1: 9.940279414877295e-05, l2: 0.0003501948380289832   Iteration 41 of 100, tot loss = 4.451633005607419, l1: 9.875933276972075e-05, l2: 0.00034640396827343486   Iteration 42 of 100, tot loss = 4.535359195300511, l1: 0.00010029320591933182, l2: 0.0003532427130959972   Iteration 43 of 100, tot loss = 4.518048386241114, l1: 0.00010006327901528282, l2: 0.0003517415590427253   Iteration 44 of 100, tot loss = 4.502521926706487, l1: 9.973061225371731e-05, l2: 0.0003505215800859415   Iteration 45 of 100, tot loss = 4.527180194854736, l1: 0.00010013641052258511, l2: 0.0003525816088464732   Iteration 46 of 100, tot loss = 4.5097285198128745, l1: 9.984610701592274e-05, l2: 0.0003511267451635238   Iteration 47 of 100, tot loss = 4.505369363947118, l1: 9.97188304763961e-05, l2: 0.00035081810641171844   Iteration 48 of 100, tot loss = 4.524681732058525, l1: 0.00010005974293865923, l2: 0.0003524084316571437   Iteration 49 of 100, tot loss = 4.515669642662515, l1: 9.980703685052541e-05, l2: 0.00035175992911253885   Iteration 50 of 100, tot loss = 4.523643536567688, l1: 9.979775844840333e-05, l2: 0.0003525665969937108   Iteration 51 of 100, tot loss = 4.52533527916553, l1: 0.0001001304472618572, l2: 0.0003524030819558082   Iteration 52 of 100, tot loss = 4.5124773887487555, l1: 9.942140940648432e-05, l2: 0.00035182633039837057   Iteration 53 of 100, tot loss = 4.489250192102396, l1: 9.852791776691722e-05, l2: 0.0003503971024227487   Iteration 54 of 100, tot loss = 4.470255326341699, l1: 9.87576752743701e-05, l2: 0.00034826785822924985   Iteration 55 of 100, tot loss = 4.4978991811925715, l1: 9.924188581697474e-05, l2: 0.0003505480330204591   Iteration 56 of 100, tot loss = 4.46773960334914, l1: 9.845628827471436e-05, l2: 0.000348317672822824   Iteration 57 of 100, tot loss = 4.476094852414048, l1: 9.895398812787199e-05, l2: 0.0003486554977778149   Iteration 58 of 100, tot loss = 4.533754254209584, l1: 0.00010009075858447589, l2: 0.0003532846679043924   Iteration 59 of 100, tot loss = 4.526800507206028, l1: 0.00010041654351911255, l2: 0.000352263507843649   Iteration 60 of 100, tot loss = 4.566679720083872, l1: 0.00010105576566274977, l2: 0.0003556122072041035   Iteration 61 of 100, tot loss = 4.564560237478037, l1: 0.00010142957613484736, l2: 0.00035502644805390326   Iteration 62 of 100, tot loss = 4.528102705555577, l1: 0.00010101728883220799, l2: 0.00035179298227435096   Iteration 63 of 100, tot loss = 4.516071160634358, l1: 0.00010048027760358453, l2: 0.00035112683909819533   Iteration 64 of 100, tot loss = 4.4959221966564655, l1: 0.00010037303951548893, l2: 0.00034921918086183723   Iteration 65 of 100, tot loss = 4.538552485979521, l1: 0.00010093311525102203, l2: 0.0003529221347819727   Iteration 66 of 100, tot loss = 4.517664959936431, l1: 0.00010088770040690152, l2: 0.0003508787970834722   Iteration 67 of 100, tot loss = 4.5197603738130026, l1: 0.00010101673723877853, l2: 0.00035095930146400943   Iteration 68 of 100, tot loss = 4.497840194141164, l1: 0.00010079607363877585, l2: 0.00034898794696988156   Iteration 69 of 100, tot loss = 4.498670439789261, l1: 0.00010075940525090066, l2: 0.00034910764024707663   Iteration 70 of 100, tot loss = 4.479458652223859, l1: 0.00010058234843849538, l2: 0.0003473635182932152   Iteration 71 of 100, tot loss = 4.46492208897228, l1: 0.00010007794793329569, l2: 0.0003464142625912053   Iteration 72 of 100, tot loss = 4.4238175137175455, l1: 9.926380360209603e-05, l2: 0.00034311794934183126   Iteration 73 of 100, tot loss = 4.435415952172998, l1: 9.916423847268923e-05, l2: 0.0003443773586122791   Iteration 74 of 100, tot loss = 4.425719581745766, l1: 9.914741732810964e-05, l2: 0.00034342454248847363   Iteration 75 of 100, tot loss = 4.414719754854838, l1: 9.905594852170907e-05, l2: 0.00034241602862797055   Iteration 76 of 100, tot loss = 4.405661986062401, l1: 9.866801218250705e-05, l2: 0.00034189818794211054   Iteration 77 of 100, tot loss = 4.40085181477782, l1: 9.844684392227428e-05, l2: 0.00034163833938632765   Iteration 78 of 100, tot loss = 4.406877800440177, l1: 9.850320748228114e-05, l2: 0.0003421845746440037   Iteration 79 of 100, tot loss = 4.3988417205931265, l1: 9.844690955301476e-05, l2: 0.0003414372647368302   Iteration 80 of 100, tot loss = 4.396146769821644, l1: 9.826442078519904e-05, l2: 0.0003413502584407979   Iteration 81 of 100, tot loss = 4.402986775209874, l1: 9.817220654193439e-05, l2: 0.00034212647320308874   Iteration 82 of 100, tot loss = 4.414861296735158, l1: 9.815790029434294e-05, l2: 0.00034332823137136565   Iteration 83 of 100, tot loss = 4.4000087114701785, l1: 9.79810710228469e-05, l2: 0.0003420198019783304   Iteration 84 of 100, tot loss = 4.410648168552489, l1: 9.840565226546633e-05, l2: 0.00034265916638971596   Iteration 85 of 100, tot loss = 4.413293377090903, l1: 9.825524205499439e-05, l2: 0.00034307409719879026   Iteration 86 of 100, tot loss = 4.408203204010809, l1: 9.836273039941579e-05, l2: 0.0003424575913604078   Iteration 87 of 100, tot loss = 4.394911046685843, l1: 9.812405718465057e-05, l2: 0.00034136704867371174   Iteration 88 of 100, tot loss = 4.405947900631211, l1: 9.800398183043316e-05, l2: 0.00034259080935457445   Iteration 89 of 100, tot loss = 4.404570508539007, l1: 9.826934273293232e-05, l2: 0.000342187709341825   Iteration 90 of 100, tot loss = 4.399744980865055, l1: 9.841921625290222e-05, l2: 0.000341555283052407   Iteration 91 of 100, tot loss = 4.383706542161795, l1: 9.798432902143858e-05, l2: 0.0003403863265303189   Iteration 92 of 100, tot loss = 4.360641737347064, l1: 9.745972285758319e-05, l2: 0.00033860445222197325   Iteration 93 of 100, tot loss = 4.341514991175744, l1: 9.713336663891471e-05, l2: 0.00033701813373882423   Iteration 94 of 100, tot loss = 4.337690382561785, l1: 9.680163357382086e-05, l2: 0.00033696740579313323   Iteration 95 of 100, tot loss = 4.335659979519091, l1: 9.672409467288832e-05, l2: 0.0003368419045919405   Iteration 96 of 100, tot loss = 4.340095056841771, l1: 9.640457430274789e-05, l2: 0.00033760493253491103   Iteration 97 of 100, tot loss = 4.341125118363764, l1: 9.642466803719817e-05, l2: 0.0003376878451012296   Iteration 98 of 100, tot loss = 4.328554057345098, l1: 9.624233733083368e-05, l2: 0.00033661306944701876   Iteration 99 of 100, tot loss = 4.340521442769754, l1: 9.639118166024223e-05, l2: 0.0003376609637405526   Iteration 100 of 100, tot loss = 4.343315631151199, l1: 9.630133376049343e-05, l2: 0.0003380302304140059
   End of epoch 1368; saving model... 

Epoch 1369 of 2000
   Iteration 1 of 100, tot loss = 9.229948043823242, l1: 0.00016501086065545678, l2: 0.000757983943913132   Iteration 2 of 100, tot loss = 6.888887166976929, l1: 0.0001322438365605194, l2: 0.0005566448817262426   Iteration 3 of 100, tot loss = 7.193743387858073, l1: 0.00014328259324732548, l2: 0.0005760917459459355   Iteration 4 of 100, tot loss = 6.10705840587616, l1: 0.00012293812687858008, l2: 0.00048776771291159093   Iteration 5 of 100, tot loss = 5.766799640655518, l1: 0.0001145315676694736, l2: 0.0004621483967639506   Iteration 6 of 100, tot loss = 5.787902116775513, l1: 0.0001164081380314504, l2: 0.0004623820762693261   Iteration 7 of 100, tot loss = 6.017187935965402, l1: 0.00012244290805288723, l2: 0.00047927588041472646   Iteration 8 of 100, tot loss = 5.519978314638138, l1: 0.00011391753105272073, l2: 0.0004380802947707707   Iteration 9 of 100, tot loss = 5.5495785342322455, l1: 0.00011398707202816795, l2: 0.0004409707762533799   Iteration 10 of 100, tot loss = 5.502369236946106, l1: 0.00011506220544106327, l2: 0.0004351747120381333   Iteration 11 of 100, tot loss = 5.591676170175726, l1: 0.00011663722332080148, l2: 0.00044253038967409253   Iteration 12 of 100, tot loss = 5.815324604511261, l1: 0.00012100546822087684, l2: 0.0004605269847767583   Iteration 13 of 100, tot loss = 5.788823512884287, l1: 0.00011996508468515598, l2: 0.00045891726152219164   Iteration 14 of 100, tot loss = 5.516577873911176, l1: 0.0001142484067843595, l2: 0.00043740937586075494   Iteration 15 of 100, tot loss = 5.475188620885214, l1: 0.0001134459247017124, l2: 0.0004340729326941073   Iteration 16 of 100, tot loss = 5.416920974850655, l1: 0.00011393357021916017, l2: 0.0004277585230738623   Iteration 17 of 100, tot loss = 5.251431086484124, l1: 0.00011032921054934645, l2: 0.00041481389352531335   Iteration 18 of 100, tot loss = 5.327712045775519, l1: 0.0001121809736004151, l2: 0.0004205902263897264   Iteration 19 of 100, tot loss = 5.265095397045738, l1: 0.0001104401936141872, l2: 0.0004160693413149075   Iteration 20 of 100, tot loss = 5.292301833629608, l1: 0.00011120338385808281, l2: 0.00041802679552347397   Iteration 21 of 100, tot loss = 5.26102462269011, l1: 0.000111446095356119, l2: 0.0004146563636216645   Iteration 22 of 100, tot loss = 5.306919520551508, l1: 0.00011308303477232006, l2: 0.000417608913928482   Iteration 23 of 100, tot loss = 5.317841125571209, l1: 0.00011362669750309104, l2: 0.00041815741101543057   Iteration 24 of 100, tot loss = 5.282464653253555, l1: 0.00011279567055074342, l2: 0.00041545079087275855   Iteration 25 of 100, tot loss = 5.298114614486694, l1: 0.00011340794037096203, l2: 0.00041640351701062174   Iteration 26 of 100, tot loss = 5.207513928413391, l1: 0.00011090207562329642, l2: 0.00040984931323551934   Iteration 27 of 100, tot loss = 5.118961069318983, l1: 0.00010958233030612098, l2: 0.0004023137724423712   Iteration 28 of 100, tot loss = 5.203275408063616, l1: 0.00011127831736043195, l2: 0.00040904921791349934   Iteration 29 of 100, tot loss = 5.199083163820464, l1: 0.00011173550404331261, l2: 0.00040817280713436677   Iteration 30 of 100, tot loss = 5.220726251602173, l1: 0.00011303492780522599, l2: 0.000409037692588754   Iteration 31 of 100, tot loss = 5.271854062234202, l1: 0.00011349170487054864, l2: 0.0004136936970040082   Iteration 32 of 100, tot loss = 5.2598303854465485, l1: 0.00011247571717376559, l2: 0.00041350731680722674   Iteration 33 of 100, tot loss = 5.253658612569173, l1: 0.00011242074271484108, l2: 0.0004129451135116996   Iteration 34 of 100, tot loss = 5.259872646892772, l1: 0.00011303831803054302, l2: 0.0004129489406135262   Iteration 35 of 100, tot loss = 5.189620992115565, l1: 0.00011190892910235561, l2: 0.0004070531639237223   Iteration 36 of 100, tot loss = 5.177439020739661, l1: 0.0001125210537793464, l2: 0.0004052228416387354   Iteration 37 of 100, tot loss = 5.136368203807521, l1: 0.00011130375954766154, l2: 0.00040233305480796844   Iteration 38 of 100, tot loss = 5.092208441935088, l1: 0.00011124496057546869, l2: 0.00039797587784338057   Iteration 39 of 100, tot loss = 5.084607778451382, l1: 0.00011086761057809688, l2: 0.0003975931616971651   Iteration 40 of 100, tot loss = 5.095910733938217, l1: 0.0001113291725232557, l2: 0.00039826189567975233   Iteration 41 of 100, tot loss = 5.055573946092187, l1: 0.00011120822499633948, l2: 0.0003943491646190878   Iteration 42 of 100, tot loss = 5.055022585959661, l1: 0.00011121493757638659, l2: 0.00039428731543011963   Iteration 43 of 100, tot loss = 5.027877330780029, l1: 0.00011048151941923966, l2: 0.000392306208215264   Iteration 44 of 100, tot loss = 5.033147270029241, l1: 0.00011042519880985228, l2: 0.00039288952393690124   Iteration 45 of 100, tot loss = 5.030323314666748, l1: 0.0001096291707654018, l2: 0.00039340315658288697   Iteration 46 of 100, tot loss = 5.007717355437901, l1: 0.00010885865531631482, l2: 0.00039191307618950856   Iteration 47 of 100, tot loss = 4.944952203872356, l1: 0.00010787240968598548, l2: 0.00038662280681780835   Iteration 48 of 100, tot loss = 4.97026789188385, l1: 0.00010843875823714673, l2: 0.0003885880263017801   Iteration 49 of 100, tot loss = 5.011247342946578, l1: 0.00010903719751751145, l2: 0.0003920875321028336   Iteration 50 of 100, tot loss = 5.000410690307617, l1: 0.00010894717423070687, l2: 0.0003910938900662586   Iteration 51 of 100, tot loss = 4.9903433743645165, l1: 0.00010878867297102333, l2: 0.0003902456596466329   Iteration 52 of 100, tot loss = 4.966811326833872, l1: 0.00010844834811835496, l2: 0.0003882327795368977   Iteration 53 of 100, tot loss = 4.952683709702402, l1: 0.00010853116094400527, l2: 0.0003867372057734514   Iteration 54 of 100, tot loss = 4.948575699770892, l1: 0.00010825919626869209, l2: 0.00038659836961111677   Iteration 55 of 100, tot loss = 4.943976133519953, l1: 0.00010776625217245468, l2: 0.00038663135703907096   Iteration 56 of 100, tot loss = 4.93883387531553, l1: 0.00010725199971212922, l2: 0.0003866313834026057   Iteration 57 of 100, tot loss = 4.927690321939034, l1: 0.00010694253375588328, l2: 0.00038582649441403253   Iteration 58 of 100, tot loss = 4.90320320787101, l1: 0.0001065279665109019, l2: 0.0003837923502252469   Iteration 59 of 100, tot loss = 4.859464423131135, l1: 0.00010570633932760664, l2: 0.000380240099098123   Iteration 60 of 100, tot loss = 4.860616600513458, l1: 0.00010572093869996025, l2: 0.0003803407181597625   Iteration 61 of 100, tot loss = 4.87208589569467, l1: 0.00010601723893234491, l2: 0.0003811913479470694   Iteration 62 of 100, tot loss = 4.877934328971371, l1: 0.00010642323216034715, l2: 0.0003813701985035873   Iteration 63 of 100, tot loss = 4.8782036228785435, l1: 0.00010624652009388979, l2: 0.00038157383974138945   Iteration 64 of 100, tot loss = 4.906124908477068, l1: 0.00010642526950732645, l2: 0.00038418721942434786   Iteration 65 of 100, tot loss = 4.960380029678345, l1: 0.00010765390162794993, l2: 0.00038838409908259144   Iteration 66 of 100, tot loss = 5.00780603018674, l1: 0.00010834197840295295, l2: 0.0003924386224808228   Iteration 67 of 100, tot loss = 5.048850433150334, l1: 0.00010937014852047537, l2: 0.00039551489284512267   Iteration 68 of 100, tot loss = 5.034403054153218, l1: 0.00010935321086649578, l2: 0.00039408709263251   Iteration 69 of 100, tot loss = 4.994430604188339, l1: 0.00010877317016939227, l2: 0.0003906698883561741   Iteration 70 of 100, tot loss = 4.979927144731794, l1: 0.00010850395997944621, l2: 0.00038948875257379507   Iteration 71 of 100, tot loss = 4.972754243394019, l1: 0.00010845551494153178, l2: 0.00038881990775747985   Iteration 72 of 100, tot loss = 5.02456667025884, l1: 0.00010924223245941296, l2: 0.00039321443323893216   Iteration 73 of 100, tot loss = 5.023369723803376, l1: 0.00010940597917085628, l2: 0.0003929309918120988   Iteration 74 of 100, tot loss = 5.0087156038026555, l1: 0.00010897713567632113, l2: 0.0003918944234641999   Iteration 75 of 100, tot loss = 5.013364893595377, l1: 0.00010841214141692035, l2: 0.0003929243467670555   Iteration 76 of 100, tot loss = 5.006524788705926, l1: 0.00010819767842469063, l2: 0.0003924547990640062   Iteration 77 of 100, tot loss = 5.061107325863529, l1: 0.00010887247344135766, l2: 0.0003972382569313654   Iteration 78 of 100, tot loss = 5.072934358547895, l1: 0.00010941277646335762, l2: 0.0003978806577297525   Iteration 79 of 100, tot loss = 5.070406980152372, l1: 0.00010945773152067161, l2: 0.0003975829652207019   Iteration 80 of 100, tot loss = 5.097986769676209, l1: 0.00010962474566440505, l2: 0.00040017392984736944   Iteration 81 of 100, tot loss = 5.11289268658485, l1: 0.00011000264791540376, l2: 0.00040128661903845905   Iteration 82 of 100, tot loss = 5.097260082640299, l1: 0.00010940058950282259, l2: 0.0004003254173922625   Iteration 83 of 100, tot loss = 5.084586864494415, l1: 0.000109437168588096, l2: 0.00039902151648533047   Iteration 84 of 100, tot loss = 5.068353442918687, l1: 0.00010923401233802516, l2: 0.00039760133066530604   Iteration 85 of 100, tot loss = 5.068583886763629, l1: 0.00010908744439160834, l2: 0.00039777094241478204   Iteration 86 of 100, tot loss = 5.123482354851657, l1: 0.00010968655907734416, l2: 0.0004026616741104463   Iteration 87 of 100, tot loss = 5.12565172129664, l1: 0.00010944453414145525, l2: 0.0004031206362401724   Iteration 88 of 100, tot loss = 5.122605692256581, l1: 0.00010949512848541913, l2: 0.0004027654390070397   Iteration 89 of 100, tot loss = 5.112286926655287, l1: 0.00010905000200499047, l2: 0.0004021786890722539   Iteration 90 of 100, tot loss = 5.111440155241224, l1: 0.00010904113049845264, l2: 0.0004021028831226027   Iteration 91 of 100, tot loss = 5.092129544897394, l1: 0.0001087492631456001, l2: 0.0004004636895523028   Iteration 92 of 100, tot loss = 5.077017172523167, l1: 0.0001085833759055316, l2: 0.0003991183394923017   Iteration 93 of 100, tot loss = 5.083569731763614, l1: 0.00010875658613600848, l2: 0.00039960038507111894   Iteration 94 of 100, tot loss = 5.056123900920786, l1: 0.00010839272305174939, l2: 0.00039721966521106064   Iteration 95 of 100, tot loss = 5.060071980325799, l1: 0.00010826857955850611, l2: 0.00039773861690742994   Iteration 96 of 100, tot loss = 5.0572324097156525, l1: 0.00010848192213567624, l2: 0.0003972413170837778   Iteration 97 of 100, tot loss = 5.053809259355683, l1: 0.00010833645055098124, l2: 0.0003970444738593663   Iteration 98 of 100, tot loss = 5.0397453210791765, l1: 0.00010818965830014572, l2: 0.0003957848724783208   Iteration 99 of 100, tot loss = 5.056840000730572, l1: 0.00010852026521619833, l2: 0.0003971637335707048   Iteration 100 of 100, tot loss = 5.047466559410095, l1: 0.00010845491055079037, l2: 0.00039629174410947597
   End of epoch 1369; saving model... 

Epoch 1370 of 2000
   Iteration 1 of 100, tot loss = 5.816941738128662, l1: 8.828138379612938e-05, l2: 0.0004934127791784704   Iteration 2 of 100, tot loss = 6.233523845672607, l1: 0.00011993298176093958, l2: 0.0005034194036852568   Iteration 3 of 100, tot loss = 5.308952331542969, l1: 9.87256680673454e-05, l2: 0.0004321695596445352   Iteration 4 of 100, tot loss = 5.354454278945923, l1: 0.00010492739147593966, l2: 0.0004305180336814374   Iteration 5 of 100, tot loss = 5.623159408569336, l1: 0.0001071084632712882, l2: 0.0004552074824459851   Iteration 6 of 100, tot loss = 5.5680224895477295, l1: 0.0001094648087018868, l2: 0.0004473374428926036   Iteration 7 of 100, tot loss = 5.307483400617327, l1: 0.00010491745498646716, l2: 0.0004258308867325208   Iteration 8 of 100, tot loss = 5.009262949228287, l1: 9.850930791799328e-05, l2: 0.00040241699025500566   Iteration 9 of 100, tot loss = 4.691424608230591, l1: 9.553793441025644e-05, l2: 0.0003736045295631306   Iteration 10 of 100, tot loss = 4.931410717964172, l1: 0.00010198066411248874, l2: 0.0003911604086169973   Iteration 11 of 100, tot loss = 4.847979437221181, l1: 0.00010125213587095708, l2: 0.0003835458086210896   Iteration 12 of 100, tot loss = 4.749914328257243, l1: 9.756058655815043e-05, l2: 0.00037743084734150517   Iteration 13 of 100, tot loss = 4.624606040807871, l1: 9.577283568572826e-05, l2: 0.0003666877679419346   Iteration 14 of 100, tot loss = 4.669318318367004, l1: 9.587071638504443e-05, l2: 0.00037106111579175504   Iteration 15 of 100, tot loss = 4.560658995310465, l1: 9.31098834068204e-05, l2: 0.00036295601748861375   Iteration 16 of 100, tot loss = 4.4789542108774185, l1: 9.253959387933719e-05, l2: 0.00035535582810553024   Iteration 17 of 100, tot loss = 4.526158571243286, l1: 9.429303677547175e-05, l2: 0.0003583228210439248   Iteration 18 of 100, tot loss = 4.433877322408888, l1: 9.323197082267143e-05, l2: 0.0003501557617305985   Iteration 19 of 100, tot loss = 4.4484038227482845, l1: 9.409489231122854e-05, l2: 0.00035074549056221977   Iteration 20 of 100, tot loss = 4.406353509426117, l1: 9.364685065520462e-05, l2: 0.00034698850140557624   Iteration 21 of 100, tot loss = 4.33958481606983, l1: 9.145911601427499e-05, l2: 0.0003424993670445734   Iteration 22 of 100, tot loss = 4.436235048554161, l1: 9.401610540490682e-05, l2: 0.0003496073999981904   Iteration 23 of 100, tot loss = 4.458256856254909, l1: 9.505190625888012e-05, l2: 0.0003507737805510102   Iteration 24 of 100, tot loss = 4.518870383501053, l1: 9.708163694692e-05, l2: 0.00035480540282151196   Iteration 25 of 100, tot loss = 4.5282314586639405, l1: 9.76755251758732e-05, l2: 0.0003551476210122928   Iteration 26 of 100, tot loss = 4.532870503572317, l1: 9.877142488571386e-05, l2: 0.0003545156264758239   Iteration 27 of 100, tot loss = 4.478795740339491, l1: 9.810949295233176e-05, l2: 0.00034977008235054436   Iteration 28 of 100, tot loss = 4.486182587487357, l1: 9.910426134800738e-05, l2: 0.00034951399928624075   Iteration 29 of 100, tot loss = 4.597693657052928, l1: 0.0001010720014415585, l2: 0.0003586973661781642   Iteration 30 of 100, tot loss = 4.58409940401713, l1: 0.00010129668468531842, l2: 0.0003571132586027185   Iteration 31 of 100, tot loss = 4.578958849753103, l1: 0.00010178543415085803, l2: 0.00035611045407882383   Iteration 32 of 100, tot loss = 4.6385326236486435, l1: 0.00010298569850419881, l2: 0.00036086756608710857   Iteration 33 of 100, tot loss = 4.601930553262884, l1: 0.0001023962600315413, l2: 0.0003577967978675257   Iteration 34 of 100, tot loss = 4.529164966414957, l1: 0.00010047704071279012, l2: 0.00035243945860523074   Iteration 35 of 100, tot loss = 4.5291718551090785, l1: 0.0001000533377269416, l2: 0.0003528638493402728   Iteration 36 of 100, tot loss = 4.542313145266639, l1: 0.00010024231940203915, l2: 0.000353988996519345   Iteration 37 of 100, tot loss = 4.675296029529056, l1: 0.0001022262210451142, l2: 0.0003653033833157875   Iteration 38 of 100, tot loss = 4.647675934590791, l1: 0.0001016107633308581, l2: 0.0003631568320806285   Iteration 39 of 100, tot loss = 4.6075235941471195, l1: 0.0001008997644067933, l2: 0.00035985259754129517   Iteration 40 of 100, tot loss = 4.619059985876083, l1: 0.00010057296403829242, l2: 0.00036133303656242787   Iteration 41 of 100, tot loss = 4.659633456206903, l1: 0.00010126354062799137, l2: 0.0003646998069953264   Iteration 42 of 100, tot loss = 4.738171174412682, l1: 0.00010268983706260943, l2: 0.0003711272826573501   Iteration 43 of 100, tot loss = 4.728834168855534, l1: 0.0001022236181090066, l2: 0.00037065980113977785   Iteration 44 of 100, tot loss = 4.703838326714256, l1: 0.00010218626762666231, l2: 0.00036819756761277944   Iteration 45 of 100, tot loss = 4.701632319556342, l1: 0.00010199580445057816, l2: 0.0003681674302141699   Iteration 46 of 100, tot loss = 4.682246317034182, l1: 0.00010224675165575363, l2: 0.00036597788288339   Iteration 47 of 100, tot loss = 4.693510760652258, l1: 0.0001028475085052939, l2: 0.00036650357094216855   Iteration 48 of 100, tot loss = 4.716496184468269, l1: 0.00010330449867979041, l2: 0.0003683451232063817   Iteration 49 of 100, tot loss = 4.741287031952216, l1: 0.00010380409708441406, l2: 0.0003703246089126154   Iteration 50 of 100, tot loss = 4.767192091941833, l1: 0.00010458554184879176, l2: 0.00037213367060758175   Iteration 51 of 100, tot loss = 4.806336173824236, l1: 0.00010519897706199474, l2: 0.0003754346444289766   Iteration 52 of 100, tot loss = 4.789383397652553, l1: 0.00010455467516014603, l2: 0.00037438366864360153   Iteration 53 of 100, tot loss = 4.754232887951833, l1: 0.0001040925759160979, l2: 0.000371330716967899   Iteration 54 of 100, tot loss = 4.750738316112095, l1: 0.0001037324454281824, l2: 0.0003713413903757033   Iteration 55 of 100, tot loss = 4.701679238406094, l1: 0.00010284706626076844, l2: 0.0003673208615509793   Iteration 56 of 100, tot loss = 4.669521455253873, l1: 0.00010255700785169861, l2: 0.0003643951414622799   Iteration 57 of 100, tot loss = 4.703714575683861, l1: 0.0001028027262803159, l2: 0.00036756873560718083   Iteration 58 of 100, tot loss = 4.6929038763046265, l1: 0.00010281895767297226, l2: 0.000366471434094349   Iteration 59 of 100, tot loss = 4.66695554377669, l1: 0.00010184640058916125, l2: 0.0003648491576807258   Iteration 60 of 100, tot loss = 4.668376159667969, l1: 0.00010182008639579484, l2: 0.0003650175332343982   Iteration 61 of 100, tot loss = 4.680361622669658, l1: 0.00010225029276267885, l2: 0.0003657858729549516   Iteration 62 of 100, tot loss = 4.674971557432605, l1: 0.00010215701225998572, l2: 0.00036534014643327666   Iteration 63 of 100, tot loss = 4.643655633169507, l1: 0.00010163223152399979, l2: 0.00036273333465769176   Iteration 64 of 100, tot loss = 4.663540959358215, l1: 0.00010177541935263434, l2: 0.00036457867918215925   Iteration 65 of 100, tot loss = 4.674603410867545, l1: 0.00010193272509450953, l2: 0.0003655276185152336   Iteration 66 of 100, tot loss = 4.668081803755327, l1: 0.00010215667906308998, l2: 0.0003646515035529085   Iteration 67 of 100, tot loss = 4.671457774603545, l1: 0.00010245616996049214, l2: 0.0003646896099115708   Iteration 68 of 100, tot loss = 4.70548872386708, l1: 0.00010267618167477743, l2: 0.00036787269345950335   Iteration 69 of 100, tot loss = 4.708954700525256, l1: 0.00010285770522255072, l2: 0.0003680377680659834   Iteration 70 of 100, tot loss = 4.732351214545114, l1: 0.0001035234034394047, l2: 0.00036971172085031867   Iteration 71 of 100, tot loss = 4.738761955583599, l1: 0.00010338896667440488, l2: 0.00037048723176449645   Iteration 72 of 100, tot loss = 4.713203665282991, l1: 0.00010289243911958895, l2: 0.0003684279302736589   Iteration 73 of 100, tot loss = 4.742515723999232, l1: 0.00010363509659639445, l2: 0.0003706164788040141   Iteration 74 of 100, tot loss = 4.7156446849977645, l1: 0.00010295914438068615, l2: 0.00036860532713330323   Iteration 75 of 100, tot loss = 4.710802971522013, l1: 0.00010272645362420007, l2: 0.0003683538461336866   Iteration 76 of 100, tot loss = 4.701501535741906, l1: 0.00010252343274355793, l2: 0.00036762672301081595   Iteration 77 of 100, tot loss = 4.6789903176295295, l1: 0.00010211208616776464, l2: 0.0003657869477952867   Iteration 78 of 100, tot loss = 4.6500134620911036, l1: 0.00010166755395729501, l2: 0.0003633337943868425   Iteration 79 of 100, tot loss = 4.649537312833568, l1: 0.0001018775273902825, l2: 0.0003630762062878407   Iteration 80 of 100, tot loss = 4.643951740860939, l1: 0.00010196821785939392, l2: 0.0003624269589636242   Iteration 81 of 100, tot loss = 4.646756051499167, l1: 0.00010215542949290579, l2: 0.0003625201787167217   Iteration 82 of 100, tot loss = 4.657937721508305, l1: 0.00010179505687385866, l2: 0.0003639987184348085   Iteration 83 of 100, tot loss = 4.650343352053539, l1: 0.00010195034499987062, l2: 0.00036308399355319517   Iteration 84 of 100, tot loss = 4.628303221293858, l1: 0.00010158130360304356, l2: 0.00036124902198068974   Iteration 85 of 100, tot loss = 4.6145999992595, l1: 0.00010149807597318774, l2: 0.00035996192737537273   Iteration 86 of 100, tot loss = 4.616308996843737, l1: 0.00010161993867409047, l2: 0.0003600109644033838   Iteration 87 of 100, tot loss = 4.65043059984843, l1: 0.00010220248662952291, l2: 0.00036284057644260086   Iteration 88 of 100, tot loss = 4.64539496736093, l1: 0.00010216724612374409, l2: 0.00036237225404792383   Iteration 89 of 100, tot loss = 4.652010706033599, l1: 0.00010234822417423652, l2: 0.0003628528499034014   Iteration 90 of 100, tot loss = 4.661239626672533, l1: 0.0001026562629299911, l2: 0.00036346770325002985   Iteration 91 of 100, tot loss = 4.66156698845245, l1: 0.00010256033899597721, l2: 0.00036359636290868807   Iteration 92 of 100, tot loss = 4.677025289639182, l1: 0.00010288400430315802, l2: 0.00036481852768464824   Iteration 93 of 100, tot loss = 4.6621123590777, l1: 0.00010285832675396695, l2: 0.0003633529120888461   Iteration 94 of 100, tot loss = 4.654600678606236, l1: 0.00010295905505041989, l2: 0.00036250101572690295   Iteration 95 of 100, tot loss = 4.689310538141351, l1: 0.00010342352608476129, l2: 0.00036550753023843036   Iteration 96 of 100, tot loss = 4.678155057132244, l1: 0.00010329716413555919, l2: 0.00036451834406155587   Iteration 97 of 100, tot loss = 4.679435044219813, l1: 0.00010328668247087446, l2: 0.0003646568246077765   Iteration 98 of 100, tot loss = 4.66999448562155, l1: 0.00010311230378610329, l2: 0.0003638871473542947   Iteration 99 of 100, tot loss = 4.669115254373262, l1: 0.00010300293187622564, l2: 0.00036390859623688904   Iteration 100 of 100, tot loss = 4.67390929222107, l1: 0.00010304803050530609, l2: 0.0003643429010116961
   End of epoch 1370; saving model... 

Epoch 1371 of 2000
   Iteration 1 of 100, tot loss = 4.662184715270996, l1: 0.00011020765668945387, l2: 0.00035601083072833717   Iteration 2 of 100, tot loss = 4.44366455078125, l1: 9.852467337623239e-05, l2: 0.0003458417922956869   Iteration 3 of 100, tot loss = 4.252939860026042, l1: 9.709085619154696e-05, l2: 0.0003282031393609941   Iteration 4 of 100, tot loss = 5.163126587867737, l1: 0.00010490458043932449, l2: 0.0004114080948056653   Iteration 5 of 100, tot loss = 5.261660766601563, l1: 0.00011201380257261918, l2: 0.0004141522862482816   Iteration 6 of 100, tot loss = 5.2295347849528, l1: 0.00011503747979683492, l2: 0.0004079160086500148   Iteration 7 of 100, tot loss = 5.192140919821603, l1: 0.00011268628960741418, l2: 0.0004065278127589928   Iteration 8 of 100, tot loss = 5.2688374519348145, l1: 0.0001100650088119437, l2: 0.0004168187406321522   Iteration 9 of 100, tot loss = 5.2337942653232155, l1: 0.00011237733593184708, l2: 0.0004110020947539144   Iteration 10 of 100, tot loss = 5.1387749195098875, l1: 0.0001082469243556261, l2: 0.00040563057409599424   Iteration 11 of 100, tot loss = 5.289393251592463, l1: 0.00011093443688216873, l2: 0.00041800489733842284   Iteration 12 of 100, tot loss = 5.199736952781677, l1: 0.00011025220373994671, l2: 0.00040972149872686714   Iteration 13 of 100, tot loss = 5.12915116090041, l1: 0.00010819038624714057, l2: 0.00040472473707408284   Iteration 14 of 100, tot loss = 5.3157923221588135, l1: 0.00010856488786105598, l2: 0.00042301435314584523   Iteration 15 of 100, tot loss = 5.263450717926025, l1: 0.00010805319179780781, l2: 0.0004182918870355934   Iteration 16 of 100, tot loss = 5.301384061574936, l1: 0.00010866163802347728, l2: 0.00042147677413595375   Iteration 17 of 100, tot loss = 5.299912761239445, l1: 0.00010960929734317367, l2: 0.00042038198615259985   Iteration 18 of 100, tot loss = 5.216368675231934, l1: 0.00010845712596266012, l2: 0.0004131797484458528   Iteration 19 of 100, tot loss = 5.249027377680728, l1: 0.00010953235400129894, l2: 0.0004153703890838905   Iteration 20 of 100, tot loss = 5.311202049255371, l1: 0.00011029556699213572, l2: 0.00042082464206032454   Iteration 21 of 100, tot loss = 5.135343364306858, l1: 0.00010653362464619845, l2: 0.00040700071535649753   Iteration 22 of 100, tot loss = 5.200802515853535, l1: 0.00010878545816178138, l2: 0.0004112947975624014   Iteration 23 of 100, tot loss = 5.192967679189599, l1: 0.00010855533084553747, l2: 0.00041074143930176354   Iteration 24 of 100, tot loss = 5.240178103248279, l1: 0.00010962886957107305, l2: 0.00041438894186285324   Iteration 25 of 100, tot loss = 5.27720769405365, l1: 0.0001100102333293762, l2: 0.0004177105368580669   Iteration 26 of 100, tot loss = 5.2126178328807535, l1: 0.00010990188135250579, l2: 0.00041135990333994135   Iteration 27 of 100, tot loss = 5.145403998869437, l1: 0.00010909073454159725, l2: 0.0004054496670572984   Iteration 28 of 100, tot loss = 5.086290295634951, l1: 0.00010812466968803034, l2: 0.0004005043623952328   Iteration 29 of 100, tot loss = 5.0848825101194715, l1: 0.00010786492667052541, l2: 0.00040062332570006877   Iteration 30 of 100, tot loss = 5.08300287326177, l1: 0.00010761735696481385, l2: 0.0004006829316494986   Iteration 31 of 100, tot loss = 5.127480710706403, l1: 0.00010762252221015402, l2: 0.00040512554997187706   Iteration 32 of 100, tot loss = 5.097125876694918, l1: 0.00010694803438582312, l2: 0.0004027645536552882   Iteration 33 of 100, tot loss = 5.103970935850432, l1: 0.00010719239810097145, l2: 0.0004032046955685611   Iteration 34 of 100, tot loss = 5.115856440628276, l1: 0.0001071121960729429, l2: 0.00040447344808318815   Iteration 35 of 100, tot loss = 5.08625180721283, l1: 0.00010650278553449814, l2: 0.0004021223942150495   Iteration 36 of 100, tot loss = 5.078537533680598, l1: 0.00010619497708022309, l2: 0.00040165877564706735   Iteration 37 of 100, tot loss = 5.0421327481398714, l1: 0.00010607734539253461, l2: 0.00039813592888389687   Iteration 38 of 100, tot loss = 5.054355103718607, l1: 0.00010666999447781372, l2: 0.0003987655141635945   Iteration 39 of 100, tot loss = 5.021178658191975, l1: 0.00010588237814655981, l2: 0.000396235485543282   Iteration 40 of 100, tot loss = 4.967163065075875, l1: 0.00010461952861078317, l2: 0.00039209677597682455   Iteration 41 of 100, tot loss = 4.899832815658756, l1: 0.0001032697918850406, l2: 0.0003867134875646315   Iteration 42 of 100, tot loss = 4.897086912677402, l1: 0.00010370462162730594, l2: 0.0003860040676608194   Iteration 43 of 100, tot loss = 4.877985774084579, l1: 0.00010320317892893759, l2: 0.00038459539639514457   Iteration 44 of 100, tot loss = 4.894870311021805, l1: 0.00010368191132411911, l2: 0.00038580511748229833   Iteration 45 of 100, tot loss = 4.900891598065694, l1: 0.00010433396043178315, l2: 0.0003857551967181886   Iteration 46 of 100, tot loss = 4.913568898387577, l1: 0.00010459184341016226, l2: 0.00038676504482282326   Iteration 47 of 100, tot loss = 4.915184901115742, l1: 0.00010479142641069051, l2: 0.0003867270620019631   Iteration 48 of 100, tot loss = 4.893726356327534, l1: 0.00010439034349474241, l2: 0.00038498229059769074   Iteration 49 of 100, tot loss = 4.89303889323254, l1: 0.00010440316290608892, l2: 0.0003849007245699628   Iteration 50 of 100, tot loss = 4.901462237834931, l1: 0.00010488948129932395, l2: 0.00038525674113770947   Iteration 51 of 100, tot loss = 4.932989127495709, l1: 0.00010492479743655113, l2: 0.0003883741135759206   Iteration 52 of 100, tot loss = 4.9240285868828115, l1: 0.0001044054864066241, l2: 0.00038799737068564776   Iteration 53 of 100, tot loss = 4.945268106910418, l1: 0.0001045077614143561, l2: 0.000390019047871355   Iteration 54 of 100, tot loss = 4.933600856198205, l1: 0.00010439777664335755, l2: 0.00038896230749001177   Iteration 55 of 100, tot loss = 4.926057137142529, l1: 0.00010359160309731537, l2: 0.0003890141090695662   Iteration 56 of 100, tot loss = 4.909345965300288, l1: 0.0001031355369117851, l2: 0.00038779905792450463   Iteration 57 of 100, tot loss = 4.919840417410198, l1: 0.00010353616591218222, l2: 0.00038844787412559975   Iteration 58 of 100, tot loss = 4.920371957894029, l1: 0.00010343760111246756, l2: 0.0003885995932344893   Iteration 59 of 100, tot loss = 4.92481745501696, l1: 0.00010348109658403022, l2: 0.00038900064828339964   Iteration 60 of 100, tot loss = 4.921097371975581, l1: 0.00010343591026564051, l2: 0.00038867382609169   Iteration 61 of 100, tot loss = 4.941592503766545, l1: 0.00010355415182201513, l2: 0.00039060509760795376   Iteration 62 of 100, tot loss = 4.964497741191618, l1: 0.00010403359609030773, l2: 0.00039241617689126984   Iteration 63 of 100, tot loss = 4.943912417169601, l1: 0.00010396391706539333, l2: 0.0003904273235959755   Iteration 64 of 100, tot loss = 4.91020960919559, l1: 0.00010353598685242105, l2: 0.000387484973316532   Iteration 65 of 100, tot loss = 4.8847693424958445, l1: 0.00010336237985309428, l2: 0.0003851145535008982   Iteration 66 of 100, tot loss = 4.866591148304217, l1: 0.00010338172791252322, l2: 0.0003832773859771392   Iteration 67 of 100, tot loss = 4.871879967290964, l1: 0.00010390475605407716, l2: 0.0003832832394504864   Iteration 68 of 100, tot loss = 4.881486491245382, l1: 0.0001042347056521548, l2: 0.00038391394264181145   Iteration 69 of 100, tot loss = 4.845649040263632, l1: 0.00010334118849351543, l2: 0.00038122371483621174   Iteration 70 of 100, tot loss = 4.860739277090345, l1: 0.000103833412140375, l2: 0.00038224051531869917   Iteration 71 of 100, tot loss = 4.8259866590231235, l1: 0.00010301901738144601, l2: 0.00037957964818367184   Iteration 72 of 100, tot loss = 4.839036092162132, l1: 0.00010338406996702866, l2: 0.0003805195388445605   Iteration 73 of 100, tot loss = 4.847733110597689, l1: 0.00010387303620817663, l2: 0.0003809002740388379   Iteration 74 of 100, tot loss = 4.835577595878291, l1: 0.0001036800485717076, l2: 0.0003798777102634336   Iteration 75 of 100, tot loss = 4.8120537837346395, l1: 0.00010305234590002026, l2: 0.0003781530317307139   Iteration 76 of 100, tot loss = 4.819381652693999, l1: 0.0001032565628281047, l2: 0.0003786816012411452   Iteration 77 of 100, tot loss = 4.801248010102805, l1: 0.00010319077853920761, l2: 0.00037693402120637915   Iteration 78 of 100, tot loss = 4.791816055774689, l1: 0.00010305473626477835, l2: 0.00037612686775481474   Iteration 79 of 100, tot loss = 4.76321086249774, l1: 0.00010270606785352472, l2: 0.00037361501699130793   Iteration 80 of 100, tot loss = 4.7727318331599236, l1: 0.0001026401966555568, l2: 0.00037463298485818086   Iteration 81 of 100, tot loss = 4.788863663320188, l1: 0.00010286760543412702, l2: 0.00037601875895088146   Iteration 82 of 100, tot loss = 4.75686975077885, l1: 0.00010240882906279095, l2: 0.0003732781441635225   Iteration 83 of 100, tot loss = 4.765652907900064, l1: 0.00010266101705920265, l2: 0.0003739042719194935   Iteration 84 of 100, tot loss = 4.736604041996456, l1: 0.00010210227728315484, l2: 0.00037155812502273226   Iteration 85 of 100, tot loss = 4.716651463508606, l1: 0.00010174376404714113, l2: 0.0003699213803937549   Iteration 86 of 100, tot loss = 4.717276696548906, l1: 0.00010171516881856341, l2: 0.00037001249912400663   Iteration 87 of 100, tot loss = 4.734652542519844, l1: 0.00010199632567397705, l2: 0.0003714689268361263   Iteration 88 of 100, tot loss = 4.7253656915643, l1: 0.00010184112823646336, l2: 0.0003706954392493406   Iteration 89 of 100, tot loss = 4.740048337518499, l1: 0.00010196507138787413, l2: 0.00037203976077425263   Iteration 90 of 100, tot loss = 4.748864024215274, l1: 0.00010218862577554925, l2: 0.0003726977751486831   Iteration 91 of 100, tot loss = 4.779129729166136, l1: 0.00010282700951882781, l2: 0.00037508596190261647   Iteration 92 of 100, tot loss = 4.783390603635622, l1: 0.00010290134571772322, l2: 0.0003754377128485509   Iteration 93 of 100, tot loss = 4.791556639056052, l1: 0.0001031005903637232, l2: 0.0003760550721919024   Iteration 94 of 100, tot loss = 4.7899991641653346, l1: 0.00010330604643658784, l2: 0.00037569386861039365   Iteration 95 of 100, tot loss = 4.791350318256177, l1: 0.0001032888340552362, l2: 0.00037584619621109023   Iteration 96 of 100, tot loss = 4.784494532893102, l1: 0.0001031527871949341, l2: 0.00037529666466677253   Iteration 97 of 100, tot loss = 4.761281479265272, l1: 0.00010254400424940089, l2: 0.00037358414226322024   Iteration 98 of 100, tot loss = 4.744954412080804, l1: 0.00010237692245305991, l2: 0.0003721185174963095   Iteration 99 of 100, tot loss = 4.7443285096775405, l1: 0.00010231247855552632, l2: 0.00037212037135061403   Iteration 100 of 100, tot loss = 4.7120998573303225, l1: 0.00010162099224544363, l2: 0.00036958899247110824
   End of epoch 1371; saving model... 

Epoch 1372 of 2000
   Iteration 1 of 100, tot loss = 2.9221208095550537, l1: 8.609338692622259e-05, l2: 0.00020611871150322258   Iteration 2 of 100, tot loss = 5.0405110120773315, l1: 9.827587564359419e-05, l2: 0.0004057752521475777   Iteration 3 of 100, tot loss = 4.716418822606404, l1: 8.734252575474481e-05, l2: 0.0003842993755824864   Iteration 4 of 100, tot loss = 4.821469843387604, l1: 9.212768236466218e-05, l2: 0.0003900193187291734   Iteration 5 of 100, tot loss = 5.17673134803772, l1: 0.00010234345536446199, l2: 0.00041532968753017486   Iteration 6 of 100, tot loss = 5.165163954099019, l1: 0.00010690414273994975, l2: 0.0004096122574992478   Iteration 7 of 100, tot loss = 5.778165102005005, l1: 0.00011706993869406038, l2: 0.00046074657335079143   Iteration 8 of 100, tot loss = 5.399292826652527, l1: 0.00010835282500920584, l2: 0.0004315764581406256   Iteration 9 of 100, tot loss = 5.275946935017903, l1: 0.00011016706654724355, l2: 0.0004174276303577547   Iteration 10 of 100, tot loss = 5.314826345443725, l1: 0.00011179906068718992, l2: 0.000419683575455565   Iteration 11 of 100, tot loss = 5.346436023712158, l1: 0.0001147973201692697, l2: 0.00041984628213950515   Iteration 12 of 100, tot loss = 5.055042376120885, l1: 0.00010819866808257454, l2: 0.00039730556939806166   Iteration 13 of 100, tot loss = 4.998795756926904, l1: 0.00010439082297335307, l2: 0.0003954887528939602   Iteration 14 of 100, tot loss = 4.840747024331774, l1: 0.00010105589613626112, l2: 0.0003830188070423901   Iteration 15 of 100, tot loss = 4.8548962036768595, l1: 0.00010088726679290024, l2: 0.0003846023561588178   Iteration 16 of 100, tot loss = 4.719957105815411, l1: 9.922306185217167e-05, l2: 0.0003727726516444818   Iteration 17 of 100, tot loss = 4.871927114094005, l1: 0.00010292151080862181, l2: 0.0003842712039305993   Iteration 18 of 100, tot loss = 4.824773993757036, l1: 0.00010291085365881574, l2: 0.0003795665486702799   Iteration 19 of 100, tot loss = 4.733542875239723, l1: 0.00010170015995850248, l2: 0.0003716541304081482   Iteration 20 of 100, tot loss = 4.714433175325394, l1: 0.0001013066181258182, l2: 0.0003701367022586055   Iteration 21 of 100, tot loss = 4.894692267690386, l1: 0.00010482144283568708, l2: 0.0003846477858522641   Iteration 22 of 100, tot loss = 4.833595833995125, l1: 0.00010512534952960613, l2: 0.0003782342352100055   Iteration 23 of 100, tot loss = 4.8619909856630406, l1: 0.0001064737002509545, l2: 0.000379725401137915   Iteration 24 of 100, tot loss = 4.901972467700641, l1: 0.00010742166417306483, l2: 0.0003827755851186036   Iteration 25 of 100, tot loss = 4.81216670513153, l1: 0.00010605782314087265, l2: 0.000375158850220032   Iteration 26 of 100, tot loss = 4.73732885489097, l1: 0.00010387459755852782, l2: 0.0003698582916583221   Iteration 27 of 100, tot loss = 4.645211842324999, l1: 0.00010157853830605745, l2: 0.0003629426495603251   Iteration 28 of 100, tot loss = 4.604653984308243, l1: 0.00010074935718356366, l2: 0.00035971604562031904   Iteration 29 of 100, tot loss = 4.641931776342721, l1: 0.00010120972528339138, l2: 0.0003629834551749558   Iteration 30 of 100, tot loss = 4.619953803221384, l1: 0.00010127251080120914, l2: 0.0003607228728166471   Iteration 31 of 100, tot loss = 4.544698296054717, l1: 0.00010008197780060132, l2: 0.00035438785468408415   Iteration 32 of 100, tot loss = 4.51513184979558, l1: 0.00010030662042481708, l2: 0.00035120656684739515   Iteration 33 of 100, tot loss = 4.565903407154662, l1: 0.00010155470359181477, l2: 0.00035503563986188084   Iteration 34 of 100, tot loss = 4.579381876132068, l1: 0.00010171291732523754, l2: 0.0003562252728871125   Iteration 35 of 100, tot loss = 4.609399158614022, l1: 0.00010217095072480982, l2: 0.0003587689674792013   Iteration 36 of 100, tot loss = 4.643409679333369, l1: 0.00010254235995590634, l2: 0.0003617986092447407   Iteration 37 of 100, tot loss = 4.61635434627533, l1: 0.00010233234206680208, l2: 0.00035930309378592347   Iteration 38 of 100, tot loss = 4.628121586222398, l1: 0.00010297282853188287, l2: 0.0003598393308685014   Iteration 39 of 100, tot loss = 4.653674269333864, l1: 0.00010298364400230825, l2: 0.00036238378304868745   Iteration 40 of 100, tot loss = 4.66898076236248, l1: 0.00010349312469770666, l2: 0.0003634049520769622   Iteration 41 of 100, tot loss = 4.615100206398383, l1: 0.00010251372459162844, l2: 0.00035899629633271747   Iteration 42 of 100, tot loss = 4.575009468055907, l1: 0.00010150415082101798, l2: 0.0003559967964455219   Iteration 43 of 100, tot loss = 4.5735733425894445, l1: 0.00010133543824046027, l2: 0.00035602189695224337   Iteration 44 of 100, tot loss = 4.599330791018226, l1: 0.00010188151066937611, l2: 0.0003580515688306398   Iteration 45 of 100, tot loss = 4.6077440765168935, l1: 0.00010242697462672367, l2: 0.0003583474336968114   Iteration 46 of 100, tot loss = 4.587185224761134, l1: 0.00010253470685236844, l2: 0.0003561838163042684   Iteration 47 of 100, tot loss = 4.637420874960879, l1: 0.00010341086639760141, l2: 0.0003603312219096784   Iteration 48 of 100, tot loss = 4.630526351432006, l1: 0.00010354330076249123, l2: 0.000359509335491263   Iteration 49 of 100, tot loss = 4.631637230211375, l1: 0.0001035538783869516, l2: 0.00035960984544600454   Iteration 50 of 100, tot loss = 4.623340241909027, l1: 0.00010344834445277228, l2: 0.0003588856809074059   Iteration 51 of 100, tot loss = 4.598688903976889, l1: 0.000103167505157591, l2: 0.00035670138644419756   Iteration 52 of 100, tot loss = 4.588160462104357, l1: 0.00010260768431180622, l2: 0.0003562083629031594   Iteration 53 of 100, tot loss = 4.621975613090227, l1: 0.0001026829548140566, l2: 0.00035951460738016186   Iteration 54 of 100, tot loss = 4.6213759779930115, l1: 0.00010261204213882727, l2: 0.0003595255564303241   Iteration 55 of 100, tot loss = 4.636279905926098, l1: 0.00010283760758201507, l2: 0.00036079038347286934   Iteration 56 of 100, tot loss = 4.632375106215477, l1: 0.0001027685823698578, l2: 0.0003604689284527142   Iteration 57 of 100, tot loss = 4.618699339398167, l1: 0.00010243688928057326, l2: 0.00035943304496948   Iteration 58 of 100, tot loss = 4.620552126703592, l1: 0.00010209585073833547, l2: 0.00035995936195831746   Iteration 59 of 100, tot loss = 4.591732855570519, l1: 0.00010150155028864994, l2: 0.00035767173527747833   Iteration 60 of 100, tot loss = 4.652574576934179, l1: 0.00010249718907289207, l2: 0.00036276026876294056   Iteration 61 of 100, tot loss = 4.653371175781626, l1: 0.00010282894835967692, l2: 0.00036250816998797177   Iteration 62 of 100, tot loss = 4.625837504863739, l1: 0.00010244084196822388, l2: 0.0003601429092551341   Iteration 63 of 100, tot loss = 4.606041001895117, l1: 0.00010213761030056973, l2: 0.00035846649067631614   Iteration 64 of 100, tot loss = 4.581732200458646, l1: 0.00010173646785460733, l2: 0.0003564367527815193   Iteration 65 of 100, tot loss = 4.532940730681786, l1: 0.00010067057110763227, l2: 0.00035262350250447454   Iteration 66 of 100, tot loss = 4.502315747015404, l1: 0.00010039259105917264, l2: 0.00034983898413801484   Iteration 67 of 100, tot loss = 4.502389874031294, l1: 0.00010061631886611706, l2: 0.00034962266928137545   Iteration 68 of 100, tot loss = 4.4818878016051125, l1: 0.00010059597768892726, l2: 0.00034759280296224035   Iteration 69 of 100, tot loss = 4.460112860237342, l1: 9.992204412929308e-05, l2: 0.00034608924244716087   Iteration 70 of 100, tot loss = 4.4290522490228925, l1: 9.932415986051118e-05, l2: 0.0003435810656810645   Iteration 71 of 100, tot loss = 4.452433248640785, l1: 9.948868108962492e-05, l2: 0.000345754644872037   Iteration 72 of 100, tot loss = 4.498508001367251, l1: 0.00010033743648010487, l2: 0.0003495133645022482   Iteration 73 of 100, tot loss = 4.500034493942783, l1: 9.978330570148712e-05, l2: 0.00035022014474148593   Iteration 74 of 100, tot loss = 4.531627563205925, l1: 0.00010040734874515643, l2: 0.00035275540830224526   Iteration 75 of 100, tot loss = 4.526434795061747, l1: 0.00010003922895217935, l2: 0.00035260425133553026   Iteration 76 of 100, tot loss = 4.507288956328442, l1: 9.968440597156969e-05, l2: 0.0003510444905834064   Iteration 77 of 100, tot loss = 4.513744749032058, l1: 9.987848142219416e-05, l2: 0.00035149599461401995   Iteration 78 of 100, tot loss = 4.540170778066684, l1: 0.0001004056210102788, l2: 0.0003536114578683061   Iteration 79 of 100, tot loss = 4.533914504171927, l1: 0.00010046313787203377, l2: 0.0003529283135379311   Iteration 80 of 100, tot loss = 4.515695230662823, l1: 0.00010023889008152765, l2: 0.00035133063420289545   Iteration 81 of 100, tot loss = 4.532899625507402, l1: 0.00010035353760166024, l2: 0.0003529364263091271   Iteration 82 of 100, tot loss = 4.5433323252491835, l1: 0.00010073390397493069, l2: 0.00035359932983893244   Iteration 83 of 100, tot loss = 4.516419736735792, l1: 0.0001002192668282253, l2: 0.0003514227080222006   Iteration 84 of 100, tot loss = 4.518292419967198, l1: 0.00010029425465334152, l2: 0.00035153498841585434   Iteration 85 of 100, tot loss = 4.522476823189679, l1: 0.00010031798853614257, l2: 0.0003519296948284404   Iteration 86 of 100, tot loss = 4.5372875277386155, l1: 0.00010061822725875509, l2: 0.00035311052649143846   Iteration 87 of 100, tot loss = 4.53696622930724, l1: 0.00010042264434507641, l2: 0.0003532739795226572   Iteration 88 of 100, tot loss = 4.539845150980082, l1: 0.00010062000220304154, l2: 0.00035336451409503667   Iteration 89 of 100, tot loss = 4.518242400683714, l1: 0.00010030127292808951, l2: 0.00035152296815917756   Iteration 90 of 100, tot loss = 4.509849909941355, l1: 0.00010012453094532248, l2: 0.00035086046110437667   Iteration 91 of 100, tot loss = 4.5151314329315015, l1: 0.00010026143946151433, l2: 0.0003512517049694997   Iteration 92 of 100, tot loss = 4.526890388001567, l1: 0.00010071035629390163, l2: 0.00035197868363984924   Iteration 93 of 100, tot loss = 4.5487725055345924, l1: 0.00010093018902817451, l2: 0.0003539470622287212   Iteration 94 of 100, tot loss = 4.535907891202481, l1: 0.00010069221253674031, l2: 0.00035289857709010013   Iteration 95 of 100, tot loss = 4.523329214045876, l1: 0.00010047402569111192, l2: 0.00035185889612062294   Iteration 96 of 100, tot loss = 4.55765047793587, l1: 0.00010105195023394724, l2: 0.00035471309797685535   Iteration 97 of 100, tot loss = 4.560749536937045, l1: 0.00010098129286299959, l2: 0.0003550936613914787   Iteration 98 of 100, tot loss = 4.561080486190562, l1: 0.00010091848240075965, l2: 0.0003551895667386612   Iteration 99 of 100, tot loss = 4.569642650960672, l1: 0.00010088700083092636, l2: 0.0003560772651454876   Iteration 100 of 100, tot loss = 4.577209693193436, l1: 0.00010080222480610246, l2: 0.0003569187454559142
   End of epoch 1372; saving model... 

Epoch 1373 of 2000
   Iteration 1 of 100, tot loss = 6.651510715484619, l1: 0.00016035247244872153, l2: 0.000504798605106771   Iteration 2 of 100, tot loss = 5.892081260681152, l1: 0.00013790879893349484, l2: 0.00045129934733267874   Iteration 3 of 100, tot loss = 4.943741162618001, l1: 0.00011240920866839588, l2: 0.0003819649282377213   Iteration 4 of 100, tot loss = 5.122126340866089, l1: 0.00010965811816276982, l2: 0.0004025545349577442   Iteration 5 of 100, tot loss = 4.799092578887939, l1: 0.00010231530031887815, l2: 0.00037759396946057677   Iteration 6 of 100, tot loss = 4.716359456380208, l1: 0.00010059655687655322, l2: 0.000371039403641286   Iteration 7 of 100, tot loss = 4.900151797703335, l1: 0.0001018381236852812, l2: 0.0003881770743256701   Iteration 8 of 100, tot loss = 4.985160231590271, l1: 0.00010452290007378906, l2: 0.0003939931375498418   Iteration 9 of 100, tot loss = 4.96799209382799, l1: 0.00010587719265863093, l2: 0.0003909220298131307   Iteration 10 of 100, tot loss = 4.984656238555909, l1: 0.00010511899090488441, l2: 0.0003933466417947784   Iteration 11 of 100, tot loss = 4.947885903445157, l1: 0.00010473500746725635, l2: 0.0003900535891509869   Iteration 12 of 100, tot loss = 4.86089688539505, l1: 0.00010347056862277289, l2: 0.000382619126564047   Iteration 13 of 100, tot loss = 4.87114141537593, l1: 0.00010346548291496359, l2: 0.0003836486654248662   Iteration 14 of 100, tot loss = 5.058087161609104, l1: 0.0001066732345082398, l2: 0.00039913548763641823   Iteration 15 of 100, tot loss = 4.93471941947937, l1: 0.00010593184075939159, l2: 0.000387540107476525   Iteration 16 of 100, tot loss = 4.930061027407646, l1: 0.00010611757716105785, l2: 0.0003868885314659565   Iteration 17 of 100, tot loss = 4.906637374092551, l1: 0.0001049525586388293, l2: 0.00038571118442204725   Iteration 18 of 100, tot loss = 4.919924113485548, l1: 0.0001061967709069399, l2: 0.000385795646353573   Iteration 19 of 100, tot loss = 4.857439656006663, l1: 0.00010571797266541245, l2: 0.0003800259981194119   Iteration 20 of 100, tot loss = 4.823659694194793, l1: 0.00010493814334040507, l2: 0.0003774278309720103   Iteration 21 of 100, tot loss = 4.884445587793986, l1: 0.00010486699535422737, l2: 0.0003835775680185872   Iteration 22 of 100, tot loss = 4.855980168689381, l1: 0.00010387447566079737, l2: 0.0003817235443751666   Iteration 23 of 100, tot loss = 4.842904432960179, l1: 0.0001039503866037273, l2: 0.000380340059485246   Iteration 24 of 100, tot loss = 4.74957075715065, l1: 0.00010181753032156848, l2: 0.0003731395475673101   Iteration 25 of 100, tot loss = 4.657909145355225, l1: 9.966537880245596e-05, l2: 0.0003661255369661376   Iteration 26 of 100, tot loss = 4.679944790326632, l1: 0.00010091546517707265, l2: 0.00036707901465258776   Iteration 27 of 100, tot loss = 4.645563010816221, l1: 0.00010094455447634337, l2: 0.0003636117482394049   Iteration 28 of 100, tot loss = 4.6504654969487875, l1: 0.00010114890963970018, l2: 0.0003638976408443081   Iteration 29 of 100, tot loss = 4.644661796504054, l1: 0.00010076584520243944, l2: 0.0003637003352070741   Iteration 30 of 100, tot loss = 4.566680431365967, l1: 9.96260193157165e-05, l2: 0.0003570420245523565   Iteration 31 of 100, tot loss = 4.499419681487545, l1: 9.784292987821954e-05, l2: 0.00035209903885955894   Iteration 32 of 100, tot loss = 4.418116580694914, l1: 9.634525758883683e-05, l2: 0.00034546640108601423   Iteration 33 of 100, tot loss = 4.479342558167198, l1: 9.630653993085478e-05, l2: 0.00035162771801520705   Iteration 34 of 100, tot loss = 4.495153339470134, l1: 9.649373382939409e-05, l2: 0.00035302160198197644   Iteration 35 of 100, tot loss = 4.42269822869982, l1: 9.489824068233636e-05, l2: 0.0003473715840040573   Iteration 36 of 100, tot loss = 4.4308985902203455, l1: 9.498056594262987e-05, l2: 0.00034810929517132335   Iteration 37 of 100, tot loss = 4.448109655766873, l1: 9.468615383878853e-05, l2: 0.00035012481384910643   Iteration 38 of 100, tot loss = 4.451812107312052, l1: 9.467882735311593e-05, l2: 0.0003505023855005244   Iteration 39 of 100, tot loss = 4.4168926783097096, l1: 9.464683796231372e-05, l2: 0.00034704243126982014   Iteration 40 of 100, tot loss = 4.417122712731361, l1: 9.515242545603542e-05, l2: 0.00034655984745768365   Iteration 41 of 100, tot loss = 4.449308921651142, l1: 9.526845075547831e-05, l2: 0.00034966244377911365   Iteration 42 of 100, tot loss = 4.493570773374467, l1: 9.592123613055308e-05, l2: 0.0003534358426501664   Iteration 43 of 100, tot loss = 4.545211850210678, l1: 9.644632840252936e-05, l2: 0.00035807485847874697   Iteration 44 of 100, tot loss = 4.537976972081444, l1: 9.681035474138547e-05, l2: 0.000356987343944969   Iteration 45 of 100, tot loss = 4.548567891120911, l1: 9.695804344826482e-05, l2: 0.0003578987473803055   Iteration 46 of 100, tot loss = 4.595216878082441, l1: 9.804130224991873e-05, l2: 0.00036148038666199324   Iteration 47 of 100, tot loss = 4.555594162738069, l1: 9.725247822488916e-05, l2: 0.0003583069390399342   Iteration 48 of 100, tot loss = 4.57767433176438, l1: 9.789432033358025e-05, l2: 0.00035987311275675893   Iteration 49 of 100, tot loss = 4.546463633070187, l1: 9.751281649354199e-05, l2: 0.00035713354662554906   Iteration 50 of 100, tot loss = 4.525179836750031, l1: 9.66221864655381e-05, l2: 0.00035589579725638033   Iteration 51 of 100, tot loss = 4.537619078860564, l1: 9.713316666973554e-05, l2: 0.0003566287415713875   Iteration 52 of 100, tot loss = 4.502427814098505, l1: 9.613581313225531e-05, l2: 0.0003541069686784445   Iteration 53 of 100, tot loss = 4.480632064477453, l1: 9.587067235667557e-05, l2: 0.00035219253474661974   Iteration 54 of 100, tot loss = 4.4954892109941555, l1: 9.561110810965247e-05, l2: 0.00035393781419972784   Iteration 55 of 100, tot loss = 4.504652918468822, l1: 9.60870166388552e-05, l2: 0.000354378276493993   Iteration 56 of 100, tot loss = 4.495018348097801, l1: 9.608562264215184e-05, l2: 0.00035341621358903855   Iteration 57 of 100, tot loss = 4.4993953391125325, l1: 9.594634148494439e-05, l2: 0.0003539931936406024   Iteration 58 of 100, tot loss = 4.478507204302426, l1: 9.591710922591276e-05, l2: 0.0003519336126467759   Iteration 59 of 100, tot loss = 4.443440774739799, l1: 9.510009618618099e-05, l2: 0.00034924398272069394   Iteration 60 of 100, tot loss = 4.436181130011876, l1: 9.537637500519243e-05, l2: 0.0003482417399936821   Iteration 61 of 100, tot loss = 4.435701430821028, l1: 9.576947088105238e-05, l2: 0.0003478006742699988   Iteration 62 of 100, tot loss = 4.403751882814592, l1: 9.508993747247939e-05, l2: 0.00034528525312423644   Iteration 63 of 100, tot loss = 4.40438781655024, l1: 9.500288843622594e-05, l2: 0.0003454358950312737   Iteration 64 of 100, tot loss = 4.402808697894216, l1: 9.504731809784062e-05, l2: 0.00034523355384408205   Iteration 65 of 100, tot loss = 4.394718971619239, l1: 9.534041539434558e-05, l2: 0.0003441314842068375   Iteration 66 of 100, tot loss = 4.377076396436403, l1: 9.494172776281961e-05, l2: 0.0003427659140663419   Iteration 67 of 100, tot loss = 4.365384320714581, l1: 9.487806957296735e-05, l2: 0.00034166036461264507   Iteration 68 of 100, tot loss = 4.392468948574627, l1: 9.513709220995866e-05, l2: 0.00034410980515531264   Iteration 69 of 100, tot loss = 4.392369330793187, l1: 9.524002851558971e-05, l2: 0.0003439969072317489   Iteration 70 of 100, tot loss = 4.356189979825701, l1: 9.461626349158386e-05, l2: 0.0003410027370721634   Iteration 71 of 100, tot loss = 4.380060612315863, l1: 9.496513405498313e-05, l2: 0.0003430409296284574   Iteration 72 of 100, tot loss = 4.398045341173808, l1: 9.525230855918683e-05, l2: 0.00034455222824666026   Iteration 73 of 100, tot loss = 4.390165100358937, l1: 9.510433366115536e-05, l2: 0.0003439121790332337   Iteration 74 of 100, tot loss = 4.367129718935168, l1: 9.46802593790454e-05, l2: 0.0003420327152157001   Iteration 75 of 100, tot loss = 4.364532527923584, l1: 9.474395439610816e-05, l2: 0.00034170930079805356   Iteration 76 of 100, tot loss = 4.370186284968727, l1: 9.502121012797273e-05, l2: 0.00034199742061135014   Iteration 77 of 100, tot loss = 4.382341725485666, l1: 9.531743168511864e-05, l2: 0.00034291674299830835   Iteration 78 of 100, tot loss = 4.406393729723417, l1: 9.578764902238245e-05, l2: 0.00034485172625117673   Iteration 79 of 100, tot loss = 4.399927299233932, l1: 9.560424973930327e-05, l2: 0.0003443884823907497   Iteration 80 of 100, tot loss = 4.398842039704323, l1: 9.571101495566836e-05, l2: 0.000344173191479058   Iteration 81 of 100, tot loss = 4.391055004096326, l1: 9.588054335499355e-05, l2: 0.00034322495964631716   Iteration 82 of 100, tot loss = 4.379936796862904, l1: 9.604820248879572e-05, l2: 0.0003419454799036541   Iteration 83 of 100, tot loss = 4.363859702305621, l1: 9.592606498283357e-05, l2: 0.00034045990793516925   Iteration 84 of 100, tot loss = 4.352535707609994, l1: 9.578680757320919e-05, l2: 0.00033946676592763866   Iteration 85 of 100, tot loss = 4.323909267257242, l1: 9.51586451693027e-05, l2: 0.00033723228433005074   Iteration 86 of 100, tot loss = 4.308703271455543, l1: 9.516149509371919e-05, l2: 0.00033570883449080376   Iteration 87 of 100, tot loss = 4.3097012604790175, l1: 9.5170756610279e-05, l2: 0.0003357993715828092   Iteration 88 of 100, tot loss = 4.307511100714857, l1: 9.478634600864925e-05, l2: 0.00033596476615374826   Iteration 89 of 100, tot loss = 4.325278639793396, l1: 9.507573682394957e-05, l2: 0.0003374521290292189   Iteration 90 of 100, tot loss = 4.336192071437836, l1: 9.513132232920422e-05, l2: 0.00033848788686251886   Iteration 91 of 100, tot loss = 4.328845229777661, l1: 9.510552702992284e-05, l2: 0.00033777899807319045   Iteration 92 of 100, tot loss = 4.317164186550223, l1: 9.491784788659298e-05, l2: 0.0003367985729077507   Iteration 93 of 100, tot loss = 4.3395723617205055, l1: 9.534805979465776e-05, l2: 0.0003386091782651361   Iteration 94 of 100, tot loss = 4.347321788047222, l1: 9.536029024952478e-05, l2: 0.00033937189034483533   Iteration 95 of 100, tot loss = 4.354606396273563, l1: 9.561647139459015e-05, l2: 0.0003398441700387354   Iteration 96 of 100, tot loss = 4.351516707489888, l1: 9.561396173770238e-05, l2: 0.0003395377107153763   Iteration 97 of 100, tot loss = 4.365040945023606, l1: 9.583421533815707e-05, l2: 0.0003406698812524183   Iteration 98 of 100, tot loss = 4.380062491309886, l1: 9.637609374684481e-05, l2: 0.00034163015768673194   Iteration 99 of 100, tot loss = 4.388893314082213, l1: 9.606518124017366e-05, l2: 0.0003428241526684489   Iteration 100 of 100, tot loss = 4.379133766889572, l1: 9.587983131496003e-05, l2: 0.0003420335479313508
   End of epoch 1373; saving model... 

Epoch 1374 of 2000
   Iteration 1 of 100, tot loss = 3.9414143562316895, l1: 6.218827911652625e-05, l2: 0.00033195316791534424   Iteration 2 of 100, tot loss = 4.730107069015503, l1: 9.206284812535159e-05, l2: 0.0003809478657785803   Iteration 3 of 100, tot loss = 5.8153605461120605, l1: 0.00012005310418317094, l2: 0.0004614829473818342   Iteration 4 of 100, tot loss = 5.6352598667144775, l1: 0.00012100899220968131, l2: 0.0004425169972819276   Iteration 5 of 100, tot loss = 5.191591453552246, l1: 0.00011333366128383205, l2: 0.000405825482448563   Iteration 6 of 100, tot loss = 5.102272431055705, l1: 0.00011263199849054217, l2: 0.00039759524224791676   Iteration 7 of 100, tot loss = 5.215785639626639, l1: 0.000112493361354739, l2: 0.0004090852037604366   Iteration 8 of 100, tot loss = 5.331254839897156, l1: 0.00011207471470697783, l2: 0.0004210507686366327   Iteration 9 of 100, tot loss = 5.076883448494805, l1: 0.00010856183346024611, l2: 0.00039912651118356735   Iteration 10 of 100, tot loss = 4.918062973022461, l1: 0.0001050723614753224, l2: 0.00038673393573844804   Iteration 11 of 100, tot loss = 4.926375649192116, l1: 0.00010553842002991587, l2: 0.00038709914687470615   Iteration 12 of 100, tot loss = 4.851394931475322, l1: 0.00010445801854075398, l2: 0.000380681474780431   Iteration 13 of 100, tot loss = 5.069663597987248, l1: 0.00010769741549246156, l2: 0.0003992689453298226   Iteration 14 of 100, tot loss = 5.227375950132098, l1: 0.00011087115905996013, l2: 0.0004118664375190357   Iteration 15 of 100, tot loss = 5.168020089467366, l1: 0.00010953361585658664, l2: 0.0004072683970055853   Iteration 16 of 100, tot loss = 5.052039012312889, l1: 0.00010890871999436058, l2: 0.0003962951859648456   Iteration 17 of 100, tot loss = 4.966251962325153, l1: 0.0001089256175499245, l2: 0.0003876995837987017   Iteration 18 of 100, tot loss = 4.91228002972073, l1: 0.00010872941735821466, l2: 0.0003824985898164515   Iteration 19 of 100, tot loss = 5.2317879827399, l1: 0.00011290333579956113, l2: 0.0004102754671629028   Iteration 20 of 100, tot loss = 5.199388241767883, l1: 0.00011222976499993819, l2: 0.0004077090627106372   Iteration 21 of 100, tot loss = 5.234337897527785, l1: 0.00011224856869029325, l2: 0.0004111852246015111   Iteration 22 of 100, tot loss = 5.15028790994124, l1: 0.00011030087824275887, l2: 0.00040472791707460686   Iteration 23 of 100, tot loss = 5.139498648436173, l1: 0.00010976825749663556, l2: 0.00040418161370325834   Iteration 24 of 100, tot loss = 5.1732383370399475, l1: 0.000110176891212177, l2: 0.00040714694902514265   Iteration 25 of 100, tot loss = 5.165047817230224, l1: 0.00011087187071098015, l2: 0.0004056329181184992   Iteration 26 of 100, tot loss = 5.151927709579468, l1: 0.00011084032303187996, l2: 0.0004043524548671065   Iteration 27 of 100, tot loss = 5.239526730996591, l1: 0.00011196889412261683, l2: 0.00041198378689673347   Iteration 28 of 100, tot loss = 5.160588187830789, l1: 0.00011085537361006053, l2: 0.0004052034523088618   Iteration 29 of 100, tot loss = 5.15104656383909, l1: 0.00011009095283827327, l2: 0.00040501371178583336   Iteration 30 of 100, tot loss = 5.137662959098816, l1: 0.00011021011232514866, l2: 0.00040355619081916907   Iteration 31 of 100, tot loss = 5.10850771780937, l1: 0.00010962778586901785, l2: 0.0004012229929362694   Iteration 32 of 100, tot loss = 5.112605668604374, l1: 0.00010947667124128202, l2: 0.00040178390281653265   Iteration 33 of 100, tot loss = 5.040209466760809, l1: 0.0001078174464055337, l2: 0.0003962035072177197   Iteration 34 of 100, tot loss = 4.989934458452113, l1: 0.00010681742402512635, l2: 0.0003921760285533417   Iteration 35 of 100, tot loss = 5.053145068032401, l1: 0.00010772306642528357, l2: 0.0003975914466926562   Iteration 36 of 100, tot loss = 5.044790029525757, l1: 0.00010753392133564275, l2: 0.00039694508773714513   Iteration 37 of 100, tot loss = 4.980732982223098, l1: 0.00010627826476765394, l2: 0.0003917950395859677   Iteration 38 of 100, tot loss = 5.004852119245027, l1: 0.00010740330100433264, l2: 0.00039308191748875146   Iteration 39 of 100, tot loss = 5.000525609040872, l1: 0.00010755493004137698, l2: 0.00039249763656885194   Iteration 40 of 100, tot loss = 4.9233213543891905, l1: 0.00010576483709883178, l2: 0.0003865673035761574   Iteration 41 of 100, tot loss = 4.926244503114281, l1: 0.00010559903692011153, l2: 0.0003870254188702192   Iteration 42 of 100, tot loss = 4.952489852905273, l1: 0.00010604195025128622, l2: 0.0003892070412153511   Iteration 43 of 100, tot loss = 4.948663201442985, l1: 0.00010610758333716021, l2: 0.0003887587426593094   Iteration 44 of 100, tot loss = 4.983468066562306, l1: 0.00010718228474037129, l2: 0.00039116452700744213   Iteration 45 of 100, tot loss = 4.972409269544813, l1: 0.00010683204810548987, l2: 0.0003904088843329292   Iteration 46 of 100, tot loss = 5.018814760705699, l1: 0.00010736152351297358, l2: 0.0003945199578479135   Iteration 47 of 100, tot loss = 5.050160580493034, l1: 0.0001081424094043038, l2: 0.0003968736536409508   Iteration 48 of 100, tot loss = 5.030622969071071, l1: 0.00010772312271001283, l2: 0.00039533917909769417   Iteration 49 of 100, tot loss = 5.0811073926030375, l1: 0.00010815892981517375, l2: 0.0003999518150373419   Iteration 50 of 100, tot loss = 5.0673124217987064, l1: 0.00010761714918771758, l2: 0.00039911409810883925   Iteration 51 of 100, tot loss = 5.076705549277511, l1: 0.00010782464917557423, l2: 0.0003998459105074004   Iteration 52 of 100, tot loss = 5.071557870277991, l1: 0.00010789408742521818, l2: 0.00039926170435277384   Iteration 53 of 100, tot loss = 5.075697556981501, l1: 0.00010860460246957066, l2: 0.00039896515735099973   Iteration 54 of 100, tot loss = 5.058503309885661, l1: 0.00010803540606974382, l2: 0.00039781492900888054   Iteration 55 of 100, tot loss = 5.041147596185858, l1: 0.0001081022558289326, l2: 0.00039601250768596814   Iteration 56 of 100, tot loss = 5.0524884802954535, l1: 0.00010857121612519092, l2: 0.0003966776356329709   Iteration 57 of 100, tot loss = 5.044759775462904, l1: 0.0001088243578826159, l2: 0.00039565162360463945   Iteration 58 of 100, tot loss = 5.052031311495551, l1: 0.00010890438941965715, l2: 0.00039629874613481285   Iteration 59 of 100, tot loss = 5.014767614461608, l1: 0.00010784710573685608, l2: 0.0003936296601195709   Iteration 60 of 100, tot loss = 5.069062391916911, l1: 0.00010869923395754692, l2: 0.0003982070096147557   Iteration 61 of 100, tot loss = 5.055796029137784, l1: 0.00010867011235963904, l2: 0.0003969094951705793   Iteration 62 of 100, tot loss = 5.033687818434931, l1: 0.00010855292666453733, l2: 0.0003948158592984621   Iteration 63 of 100, tot loss = 5.00455267467196, l1: 0.0001077747320879199, l2: 0.00039268053914715966   Iteration 64 of 100, tot loss = 5.01859138160944, l1: 0.00010762550033405205, l2: 0.00039423364114554715   Iteration 65 of 100, tot loss = 5.000877600449782, l1: 0.00010691777072049892, l2: 0.00039316999251381133   Iteration 66 of 100, tot loss = 5.002093358473345, l1: 0.00010699423910409678, l2: 0.0003932151003303288   Iteration 67 of 100, tot loss = 4.995360566608942, l1: 0.0001073336394772163, l2: 0.00039220242039995516   Iteration 68 of 100, tot loss = 4.997214156038621, l1: 0.00010735149006744476, l2: 0.0003923699285437846   Iteration 69 of 100, tot loss = 5.002144640770512, l1: 0.00010752271998132093, l2: 0.0003926917472753026   Iteration 70 of 100, tot loss = 5.0323054994855605, l1: 0.0001082366541855403, l2: 0.0003949938986417172   Iteration 71 of 100, tot loss = 5.052518414779448, l1: 0.00010878830904454241, l2: 0.00039646353508660594   Iteration 72 of 100, tot loss = 5.042876727051205, l1: 0.00010862300450753537, l2: 0.0003956646707794991   Iteration 73 of 100, tot loss = 5.026344684705342, l1: 0.00010847931081701823, l2: 0.00039415516011888954   Iteration 74 of 100, tot loss = 5.03441789987925, l1: 0.00010878805595574384, l2: 0.000394653736981815   Iteration 75 of 100, tot loss = 4.997292483647664, l1: 0.00010810273844981566, l2: 0.0003916265128646046   Iteration 76 of 100, tot loss = 4.988842270876232, l1: 0.00010781510594058905, l2: 0.00039106912414641365   Iteration 77 of 100, tot loss = 4.976212362190346, l1: 0.00010773219495509787, l2: 0.00038988904440896466   Iteration 78 of 100, tot loss = 4.993094325065613, l1: 0.0001080592889756633, l2: 0.0003912501463976999   Iteration 79 of 100, tot loss = 4.992312090306342, l1: 0.00010828666453090602, l2: 0.00039094454709063224   Iteration 80 of 100, tot loss = 4.961541152000427, l1: 0.00010796294809551909, l2: 0.000388191169804486   Iteration 81 of 100, tot loss = 4.939514778278492, l1: 0.00010775321326362269, l2: 0.0003861982672170586   Iteration 82 of 100, tot loss = 4.953596638470161, l1: 0.000108253993747029, l2: 0.00038710567268270365   Iteration 83 of 100, tot loss = 4.9589412011295915, l1: 0.0001086800912162289, l2: 0.00038721403128085994   Iteration 84 of 100, tot loss = 4.950441383180165, l1: 0.00010867287753734716, l2: 0.0003863712630672602   Iteration 85 of 100, tot loss = 4.947919424842386, l1: 0.00010885186690275612, l2: 0.0003859400780022363   Iteration 86 of 100, tot loss = 4.929511835408765, l1: 0.0001086416261048641, l2: 0.00038430955989264646   Iteration 87 of 100, tot loss = 4.929750157498765, l1: 0.00010885084104783105, l2: 0.00038412417754819935   Iteration 88 of 100, tot loss = 4.956649021668867, l1: 0.00010911404669141947, l2: 0.000386550858159105   Iteration 89 of 100, tot loss = 4.935018491209223, l1: 0.00010869571523154887, l2: 0.00038480613671054833   Iteration 90 of 100, tot loss = 4.909260826640659, l1: 0.0001082742176852965, l2: 0.00038265186772655903   Iteration 91 of 100, tot loss = 4.910265259690338, l1: 0.00010787869949446439, l2: 0.000383147829599873   Iteration 92 of 100, tot loss = 4.897139142388883, l1: 0.00010761195082285785, l2: 0.0003821019663123677   Iteration 93 of 100, tot loss = 4.900042859456873, l1: 0.00010734740248818692, l2: 0.00038265688604580096   Iteration 94 of 100, tot loss = 4.89944529787023, l1: 0.00010731958242645982, l2: 0.0003826249500134702   Iteration 95 of 100, tot loss = 4.8855049308977625, l1: 0.00010702810656144529, l2: 0.00038152238902464314   Iteration 96 of 100, tot loss = 4.868515878915787, l1: 0.00010671741226057445, l2: 0.0003801341780066044   Iteration 97 of 100, tot loss = 4.872839396761865, l1: 0.00010699524166729763, l2: 0.00038028870047353324   Iteration 98 of 100, tot loss = 4.852249975107154, l1: 0.00010659517046201936, l2: 0.00037862982957894743   Iteration 99 of 100, tot loss = 4.861015876134236, l1: 0.00010668259471798354, l2: 0.0003794189958510958   Iteration 100 of 100, tot loss = 4.858327419757843, l1: 0.00010659878869773821, l2: 0.00037923395619145596
   End of epoch 1374; saving model... 

Epoch 1375 of 2000
   Iteration 1 of 100, tot loss = 1.6449568271636963, l1: 3.4971279092133045e-05, l2: 0.00012952440010849386   Iteration 2 of 100, tot loss = 3.795829653739929, l1: 6.515962741104886e-05, l2: 0.00031442332692677155   Iteration 3 of 100, tot loss = 3.8624122937520347, l1: 7.393264483350019e-05, l2: 0.00031230858197280514   Iteration 4 of 100, tot loss = 3.927431106567383, l1: 8.188660649466328e-05, l2: 0.00031085651062312536   Iteration 5 of 100, tot loss = 4.495064163208008, l1: 9.220028296113014e-05, l2: 0.0003573061403585598   Iteration 6 of 100, tot loss = 4.242056488990784, l1: 8.810124927549623e-05, l2: 0.0003361044024738173   Iteration 7 of 100, tot loss = 4.203558649335589, l1: 9.142409232611368e-05, l2: 0.00032893177454492876   Iteration 8 of 100, tot loss = 4.454668939113617, l1: 9.364669222122757e-05, l2: 0.00035182019928470254   Iteration 9 of 100, tot loss = 4.855979548560248, l1: 0.00010183414027172451, l2: 0.00038376381336193945   Iteration 10 of 100, tot loss = 5.0434142589569095, l1: 0.00010558743888395838, l2: 0.0003987539850641042   Iteration 11 of 100, tot loss = 4.793231357227672, l1: 0.00010142239923218519, l2: 0.0003779007344168018   Iteration 12 of 100, tot loss = 4.846698005994161, l1: 0.00010425000103471878, l2: 0.0003804197986028157   Iteration 13 of 100, tot loss = 4.807819879972017, l1: 0.00010465860367940667, l2: 0.0003761233820114285   Iteration 14 of 100, tot loss = 4.967595849718366, l1: 0.00010871925483765413, l2: 0.00038804032997826913   Iteration 15 of 100, tot loss = 4.9814058303833, l1: 0.00011070720914479656, l2: 0.0003874333730588357   Iteration 16 of 100, tot loss = 4.852975904941559, l1: 0.00010819414978868735, l2: 0.00037710343985963846   Iteration 17 of 100, tot loss = 4.772240217994241, l1: 0.00010551613355774487, l2: 0.00037170788666973   Iteration 18 of 100, tot loss = 4.73897663752238, l1: 0.00010635342571024214, l2: 0.00036754423652180575   Iteration 19 of 100, tot loss = 4.630966638263903, l1: 0.00010304347961209714, l2: 0.00036005318351702665   Iteration 20 of 100, tot loss = 4.6976711511611935, l1: 0.00010429411340737716, l2: 0.0003654730033304077   Iteration 21 of 100, tot loss = 4.736616293589274, l1: 0.00010457862594020775, l2: 0.0003690830067387738   Iteration 22 of 100, tot loss = 4.6769313703883775, l1: 0.00010422667004273866, l2: 0.0003634664694387042   Iteration 23 of 100, tot loss = 4.6277977279994795, l1: 0.00010343298534913312, l2: 0.00035934678959639985   Iteration 24 of 100, tot loss = 4.637031416098277, l1: 0.0001041188382563026, l2: 0.00035958430513953016   Iteration 25 of 100, tot loss = 4.688309020996094, l1: 0.00010493996902368962, l2: 0.0003638909355504438   Iteration 26 of 100, tot loss = 4.6979546913733845, l1: 0.00010553012627776928, l2: 0.0003642653447549002   Iteration 27 of 100, tot loss = 4.661042372385661, l1: 0.00010458153135712362, l2: 0.00036152270762680993   Iteration 28 of 100, tot loss = 4.747694747788565, l1: 0.00010482804594045904, l2: 0.00036994143166728985   Iteration 29 of 100, tot loss = 4.71546115546391, l1: 0.00010393801921023184, l2: 0.00036760809849385685   Iteration 30 of 100, tot loss = 4.614555764198303, l1: 0.00010182139170259083, l2: 0.00035963418680087973   Iteration 31 of 100, tot loss = 4.586204421135687, l1: 0.00010136211835009406, l2: 0.00035725832511220246   Iteration 32 of 100, tot loss = 4.564307935535908, l1: 0.00010191897933964356, l2: 0.00035451181520329555   Iteration 33 of 100, tot loss = 4.56137946880225, l1: 0.00010195427923022318, l2: 0.0003541836693219728   Iteration 34 of 100, tot loss = 4.617341988226947, l1: 0.00010272734770671163, l2: 0.0003590068535071195   Iteration 35 of 100, tot loss = 4.6310247080666676, l1: 0.00010287327116072577, l2: 0.00036022920228008715   Iteration 36 of 100, tot loss = 4.587501645088196, l1: 0.00010208394441077771, l2: 0.0003566662231201513   Iteration 37 of 100, tot loss = 4.607473863137735, l1: 0.0001016149675509754, l2: 0.00035913242184481506   Iteration 38 of 100, tot loss = 4.675574164641531, l1: 0.0001026441803237264, l2: 0.0003649132390580091   Iteration 39 of 100, tot loss = 4.661283065111209, l1: 0.00010200130493150283, l2: 0.0003641270039877735   Iteration 40 of 100, tot loss = 4.66728367805481, l1: 0.00010233385646643, l2: 0.00036439451287151314   Iteration 41 of 100, tot loss = 4.651948475256199, l1: 0.00010205679809143653, l2: 0.0003631380507324981   Iteration 42 of 100, tot loss = 4.715046735036941, l1: 0.00010290164876399406, l2: 0.00036860302636688134   Iteration 43 of 100, tot loss = 4.6912590403889505, l1: 0.00010263077908961731, l2: 0.00036649512692874426   Iteration 44 of 100, tot loss = 4.678256490013816, l1: 0.00010273967588793973, l2: 0.0003650859745737927   Iteration 45 of 100, tot loss = 4.696993022494846, l1: 0.00010304021650679513, l2: 0.0003666590873358978   Iteration 46 of 100, tot loss = 4.685153214827828, l1: 0.00010310051196859644, l2: 0.0003654148110755436   Iteration 47 of 100, tot loss = 4.699801049333938, l1: 0.00010363826863504848, l2: 0.0003663418378601683   Iteration 48 of 100, tot loss = 4.685473223527272, l1: 0.00010348541862488976, l2: 0.0003650619049343125   Iteration 49 of 100, tot loss = 4.67090308422945, l1: 0.00010286479018573954, l2: 0.0003642255193268766   Iteration 50 of 100, tot loss = 4.7453342008590695, l1: 0.00010401015060779173, l2: 0.0003705232695210725   Iteration 51 of 100, tot loss = 4.762413955202289, l1: 0.0001043520997507799, l2: 0.00037188929626626855   Iteration 52 of 100, tot loss = 4.764541795620551, l1: 0.00010474536478656004, l2: 0.0003717088156218569   Iteration 53 of 100, tot loss = 4.7437758760632205, l1: 0.0001046382508406477, l2: 0.00036973933763977774   Iteration 54 of 100, tot loss = 4.711325274573432, l1: 0.00010408179453709938, l2: 0.00036705073364794736   Iteration 55 of 100, tot loss = 4.6939928791739725, l1: 0.00010425308792036959, l2: 0.00036514620058534836   Iteration 56 of 100, tot loss = 4.68697515130043, l1: 0.00010467078748531224, l2: 0.0003640267285455983   Iteration 57 of 100, tot loss = 4.7312004147914415, l1: 0.0001050442616887219, l2: 0.00036807578049902396   Iteration 58 of 100, tot loss = 4.718708009555422, l1: 0.00010418466520385319, l2: 0.00036768613615001004   Iteration 59 of 100, tot loss = 4.750513218216977, l1: 0.0001045175388645799, l2: 0.00037053378345247486   Iteration 60 of 100, tot loss = 4.771000603834788, l1: 0.00010480250051235392, l2: 0.00037229756053420716   Iteration 61 of 100, tot loss = 4.776163292712853, l1: 0.00010448259943914905, l2: 0.0003731337308666104   Iteration 62 of 100, tot loss = 4.769639388207467, l1: 0.00010435676678401163, l2: 0.00037260717339625943   Iteration 63 of 100, tot loss = 4.767331634249006, l1: 0.00010456191439371896, l2: 0.0003721712502218517   Iteration 64 of 100, tot loss = 4.765406209975481, l1: 0.00010456368619315981, l2: 0.0003719769363215164   Iteration 65 of 100, tot loss = 4.766815812771137, l1: 0.00010470673514646478, l2: 0.0003719748479152958   Iteration 66 of 100, tot loss = 4.775417880578474, l1: 0.00010469509382374147, l2: 0.0003728466961712746   Iteration 67 of 100, tot loss = 4.780761893115827, l1: 0.00010405166035321708, l2: 0.00037402453082578896   Iteration 68 of 100, tot loss = 4.79032311369391, l1: 0.00010360896077840794, l2: 0.00037542335249942396   Iteration 69 of 100, tot loss = 4.780323446660802, l1: 0.00010352988122748144, l2: 0.00037450246547477025   Iteration 70 of 100, tot loss = 4.798021558352879, l1: 0.00010418259652007171, l2: 0.0003756195612368174   Iteration 71 of 100, tot loss = 4.770547000455185, l1: 0.00010340060216036926, l2: 0.0003736540994895491   Iteration 72 of 100, tot loss = 4.776663323243459, l1: 0.0001036330069028837, l2: 0.00037403332721118606   Iteration 73 of 100, tot loss = 4.780123913124816, l1: 0.0001034439595819655, l2: 0.0003745684337208992   Iteration 74 of 100, tot loss = 4.832791889036024, l1: 0.00010410575909977401, l2: 0.0003791734315863987   Iteration 75 of 100, tot loss = 4.836296882629394, l1: 0.00010434323359125604, l2: 0.0003792864561546594   Iteration 76 of 100, tot loss = 4.82450103446057, l1: 0.00010409040757957355, l2: 0.0003783596969840705   Iteration 77 of 100, tot loss = 4.820150471352911, l1: 0.00010387276387365244, l2: 0.00037814228449246614   Iteration 78 of 100, tot loss = 4.82107010560158, l1: 0.00010403213318801508, l2: 0.0003780748785431616   Iteration 79 of 100, tot loss = 4.824163295045683, l1: 0.00010435013310494538, l2: 0.0003780661975298831   Iteration 80 of 100, tot loss = 4.82215424478054, l1: 0.00010444153476782959, l2: 0.0003777738911594497   Iteration 81 of 100, tot loss = 4.8059942928361306, l1: 0.00010442725590424451, l2: 0.00037617217476284247   Iteration 82 of 100, tot loss = 4.83463022185535, l1: 0.00010489299236768402, l2: 0.0003785700317210995   Iteration 83 of 100, tot loss = 4.861349272440715, l1: 0.00010560777971762266, l2: 0.00038052714957171176   Iteration 84 of 100, tot loss = 4.842643456799643, l1: 0.00010507945340318006, l2: 0.0003791848943191802   Iteration 85 of 100, tot loss = 4.830579056459315, l1: 0.0001047984229551409, l2: 0.0003782594846972429   Iteration 86 of 100, tot loss = 4.819820226624954, l1: 0.00010453652421981746, l2: 0.00037744550033050133   Iteration 87 of 100, tot loss = 4.807321789620937, l1: 0.00010432529242457715, l2: 0.00037640688844538966   Iteration 88 of 100, tot loss = 4.7934104393829, l1: 0.00010376913029507374, l2: 0.000375571915686702   Iteration 89 of 100, tot loss = 4.806782660859354, l1: 0.00010394743526631736, l2: 0.0003767308325825908   Iteration 90 of 100, tot loss = 4.804144072532654, l1: 0.00010395437962466127, l2: 0.0003764600292520805   Iteration 91 of 100, tot loss = 4.803169839984768, l1: 0.00010424254151575452, l2: 0.00037607444376060433   Iteration 92 of 100, tot loss = 4.801184490970943, l1: 0.00010407456118074428, l2: 0.0003760438890110337   Iteration 93 of 100, tot loss = 4.803567858152492, l1: 0.00010405077538903683, l2: 0.0003763060113640442   Iteration 94 of 100, tot loss = 4.803501573014767, l1: 0.00010393843873084444, l2: 0.0003764117195424525   Iteration 95 of 100, tot loss = 4.805439539959556, l1: 0.00010415368451504037, l2: 0.0003763902702638389   Iteration 96 of 100, tot loss = 4.813201395173867, l1: 0.00010422587327714912, l2: 0.0003770942673024062   Iteration 97 of 100, tot loss = 4.808374318879904, l1: 0.00010420946202111267, l2: 0.0003766279707221103   Iteration 98 of 100, tot loss = 4.832733521656114, l1: 0.00010478192416485399, l2: 0.00037849142883752225   Iteration 99 of 100, tot loss = 4.855956768748736, l1: 0.00010516925866602722, l2: 0.0003804264189158049   Iteration 100 of 100, tot loss = 4.850055725574493, l1: 0.00010505522739549633, l2: 0.0003799503459595144
   End of epoch 1375; saving model... 

Epoch 1376 of 2000
   Iteration 1 of 100, tot loss = 3.9934780597686768, l1: 9.019170829560608e-05, l2: 0.0003091560793109238   Iteration 2 of 100, tot loss = 3.842794418334961, l1: 7.91382190072909e-05, l2: 0.0003051412058994174   Iteration 3 of 100, tot loss = 3.7096193631490073, l1: 8.54295309788237e-05, l2: 0.0002855323934151481   Iteration 4 of 100, tot loss = 3.7910242080688477, l1: 9.300628335040528e-05, l2: 0.0002860961321857758   Iteration 5 of 100, tot loss = 3.829623985290527, l1: 8.987210167106241e-05, l2: 0.00029309029341675343   Iteration 6 of 100, tot loss = 4.161153157552083, l1: 9.654734215776746e-05, l2: 0.0003195679746568203   Iteration 7 of 100, tot loss = 4.460536411830357, l1: 9.80021504801698e-05, l2: 0.0003480514860711992   Iteration 8 of 100, tot loss = 4.735224008560181, l1: 0.00010118120462720981, l2: 0.00037234119372442365   Iteration 9 of 100, tot loss = 4.697334978315565, l1: 0.00010171511934863197, l2: 0.0003680183759166135   Iteration 10 of 100, tot loss = 5.00990777015686, l1: 0.00010423765052109956, l2: 0.0003967531258240342   Iteration 11 of 100, tot loss = 4.988582741130482, l1: 0.00010461411280134185, l2: 0.0003942441623869606   Iteration 12 of 100, tot loss = 4.920443137486775, l1: 0.00010328993145473457, l2: 0.00038875438379667077   Iteration 13 of 100, tot loss = 4.891015382913443, l1: 0.00010110105022949238, l2: 0.0003880004895528635   Iteration 14 of 100, tot loss = 4.946921757289341, l1: 0.00010294141507724166, l2: 0.0003917507586135928   Iteration 15 of 100, tot loss = 4.96241356531779, l1: 0.00010444643606509393, l2: 0.0003917949196572105   Iteration 16 of 100, tot loss = 4.942061275243759, l1: 0.0001036148796629277, l2: 0.0003905912490154151   Iteration 17 of 100, tot loss = 4.779825505088358, l1: 0.00010189325300469885, l2: 0.0003760892988579786   Iteration 18 of 100, tot loss = 4.7903551128175526, l1: 0.00010353924436559383, l2: 0.00037549626964998123   Iteration 19 of 100, tot loss = 4.702515012339542, l1: 0.00010307090614285124, l2: 0.00036718059747822975   Iteration 20 of 100, tot loss = 4.714188706874848, l1: 0.00010315076433471404, l2: 0.0003682681082864292   Iteration 21 of 100, tot loss = 4.702182213465373, l1: 0.00010354001521288107, l2: 0.000366678207813363   Iteration 22 of 100, tot loss = 4.638663801279935, l1: 0.00010241143966347657, l2: 0.0003614549429833212   Iteration 23 of 100, tot loss = 4.6991086109824804, l1: 0.00010295751986711326, l2: 0.00036695334458035296   Iteration 24 of 100, tot loss = 4.716827243566513, l1: 0.00010316693108810189, l2: 0.000368515797163127   Iteration 25 of 100, tot loss = 4.741327886581421, l1: 0.00010461471130838617, l2: 0.00036951808142475783   Iteration 26 of 100, tot loss = 4.841455468764672, l1: 0.00010543316910535886, l2: 0.000378712381531771   Iteration 27 of 100, tot loss = 4.818308839091548, l1: 0.00010494706334106417, l2: 0.0003768838241403164   Iteration 28 of 100, tot loss = 4.782765090465546, l1: 0.00010427128654555418, l2: 0.00037400522601923773   Iteration 29 of 100, tot loss = 4.852394868587625, l1: 0.00010582201176820772, l2: 0.0003794174789900667   Iteration 30 of 100, tot loss = 4.884357937177023, l1: 0.00010718720635244002, l2: 0.000381248591778179   Iteration 31 of 100, tot loss = 4.948448757971486, l1: 0.0001080212743780125, l2: 0.00038682360525032686   Iteration 32 of 100, tot loss = 4.911204107105732, l1: 0.000107219324036123, l2: 0.0003839010905721807   Iteration 33 of 100, tot loss = 4.897176200693304, l1: 0.00010742426313685648, l2: 0.00038229335967282003   Iteration 34 of 100, tot loss = 4.829273518394022, l1: 0.00010585519938852966, l2: 0.0003770721553283853   Iteration 35 of 100, tot loss = 4.811964048658099, l1: 0.00010605804626330999, l2: 0.0003751383616223133   Iteration 36 of 100, tot loss = 4.794527570406596, l1: 0.00010527850119817433, l2: 0.0003741742592764139   Iteration 37 of 100, tot loss = 4.760737908853067, l1: 0.00010426221037837299, l2: 0.00037181158384899736   Iteration 38 of 100, tot loss = 4.711007657803987, l1: 0.00010380455550100458, l2: 0.00036729621359355455   Iteration 39 of 100, tot loss = 4.696033331064077, l1: 0.0001034883549810053, l2: 0.00036611498179785814   Iteration 40 of 100, tot loss = 4.652105581760407, l1: 0.00010292122715327423, l2: 0.0003622893345891498   Iteration 41 of 100, tot loss = 4.625654720678562, l1: 0.0001029433029854284, l2: 0.00035962217243206573   Iteration 42 of 100, tot loss = 4.600606963748024, l1: 0.0001030452674188252, l2: 0.0003570154318974043   Iteration 43 of 100, tot loss = 4.636493716129037, l1: 0.00010329832188882541, l2: 0.0003603510524763531   Iteration 44 of 100, tot loss = 4.628484455021945, l1: 0.00010275748090035367, l2: 0.00036009096683384   Iteration 45 of 100, tot loss = 4.647457461886936, l1: 0.00010345496896964808, l2: 0.0003612907790941083   Iteration 46 of 100, tot loss = 4.661457828853441, l1: 0.00010394167506804123, l2: 0.000362204109399539   Iteration 47 of 100, tot loss = 4.658416940810833, l1: 0.00010423545253170813, l2: 0.0003616062433835357   Iteration 48 of 100, tot loss = 4.6834960381189985, l1: 0.00010476400908980092, l2: 0.0003635855961571603   Iteration 49 of 100, tot loss = 4.6645088876996725, l1: 0.00010444302565052308, l2: 0.000362007864465823   Iteration 50 of 100, tot loss = 4.678470830917359, l1: 0.00010437611257657408, l2: 0.00036347097135148944   Iteration 51 of 100, tot loss = 4.645248328938203, l1: 0.00010403197552031819, l2: 0.0003604928579971231   Iteration 52 of 100, tot loss = 4.693052493608915, l1: 0.00010466558887856081, l2: 0.00036463966115171876   Iteration 53 of 100, tot loss = 4.712329810520388, l1: 0.00010499999147866202, l2: 0.0003662329902411175   Iteration 54 of 100, tot loss = 4.6930753434145895, l1: 0.00010478965284871972, l2: 0.00036451788171824   Iteration 55 of 100, tot loss = 4.677791092612527, l1: 0.00010444565255470066, l2: 0.0003633334568638185   Iteration 56 of 100, tot loss = 4.694379040173122, l1: 0.00010472757717902173, l2: 0.00036471032659132367   Iteration 57 of 100, tot loss = 4.675602825064408, l1: 0.0001044182123191524, l2: 0.000363142070056168   Iteration 58 of 100, tot loss = 4.636709558552709, l1: 0.00010346215809665315, l2: 0.00036020879784261743   Iteration 59 of 100, tot loss = 4.5898645590927645, l1: 0.00010241355686063751, l2: 0.0003565728991939607   Iteration 60 of 100, tot loss = 4.603673956791559, l1: 0.0001023936330966535, l2: 0.00035797376234161977   Iteration 61 of 100, tot loss = 4.634908322428093, l1: 0.0001027407208729543, l2: 0.00036075011088482304   Iteration 62 of 100, tot loss = 4.628908424608169, l1: 0.00010244631601146407, l2: 0.00036044452609794756   Iteration 63 of 100, tot loss = 4.631026250975473, l1: 0.0001026963344927589, l2: 0.00036040629031107067   Iteration 64 of 100, tot loss = 4.631268320605159, l1: 0.00010305161328005852, l2: 0.00036007521839565015   Iteration 65 of 100, tot loss = 4.6412829600847685, l1: 0.00010295306134279459, l2: 0.00036117523425043777   Iteration 66 of 100, tot loss = 4.648597859975063, l1: 0.00010317224074573129, l2: 0.00036168754520600265   Iteration 67 of 100, tot loss = 4.638333215642331, l1: 0.00010326078102738012, l2: 0.00036057254035415045   Iteration 68 of 100, tot loss = 4.6623227824183076, l1: 0.00010371861598661566, l2: 0.00036251366220633774   Iteration 69 of 100, tot loss = 4.672881059024645, l1: 0.00010404766431167735, l2: 0.0003632404411252102   Iteration 70 of 100, tot loss = 4.6998222368104114, l1: 0.00010438652103143146, l2: 0.00036559570248105697   Iteration 71 of 100, tot loss = 4.683614010542211, l1: 0.00010442723562936036, l2: 0.0003639341649291119   Iteration 72 of 100, tot loss = 4.679845748676194, l1: 0.00010466674004621584, l2: 0.00036331783444944047   Iteration 73 of 100, tot loss = 4.691508716099883, l1: 0.00010469044908149206, l2: 0.00036446042183454927   Iteration 74 of 100, tot loss = 4.693243353753476, l1: 0.0001045407911127817, l2: 0.00036478354328160957   Iteration 75 of 100, tot loss = 4.683825936317444, l1: 0.00010439227728056721, l2: 0.0003639903152361512   Iteration 76 of 100, tot loss = 4.691653432030427, l1: 0.00010410774317087519, l2: 0.0003650575990353613   Iteration 77 of 100, tot loss = 4.720481672844329, l1: 0.00010442270583801225, l2: 0.00036762546038847756   Iteration 78 of 100, tot loss = 4.697660472148504, l1: 0.00010436620453756172, l2: 0.0003653998415430602   Iteration 79 of 100, tot loss = 4.7031570522091055, l1: 0.00010445932961766066, l2: 0.0003658563745594171   Iteration 80 of 100, tot loss = 4.737205542623997, l1: 0.000104854404935395, l2: 0.00036886614816467045   Iteration 81 of 100, tot loss = 4.7667265129678045, l1: 0.00010529477987330649, l2: 0.00037137787041030134   Iteration 82 of 100, tot loss = 4.764269836065246, l1: 0.0001052437968521589, l2: 0.0003711831858509402   Iteration 83 of 100, tot loss = 4.761991519525827, l1: 0.00010523830146173924, l2: 0.00037096084971457766   Iteration 84 of 100, tot loss = 4.764413755564463, l1: 0.00010479232527619129, l2: 0.00037164904937353206   Iteration 85 of 100, tot loss = 4.836533671266892, l1: 0.00010568165346151491, l2: 0.0003779717133832438   Iteration 86 of 100, tot loss = 4.837246058985245, l1: 0.0001056960691764902, l2: 0.00037802853685841615   Iteration 87 of 100, tot loss = 4.888090272059386, l1: 0.00010657549952658632, l2: 0.0003822335279907967   Iteration 88 of 100, tot loss = 4.902133509516716, l1: 0.00010687267550482912, l2: 0.00038334067605980886   Iteration 89 of 100, tot loss = 4.893133571978366, l1: 0.00010681302191688303, l2: 0.0003825003355466813   Iteration 90 of 100, tot loss = 4.880191059907277, l1: 0.00010630437864165288, l2: 0.00038171472761961117   Iteration 91 of 100, tot loss = 4.85169946885371, l1: 0.00010595276930521672, l2: 0.0003792171778763737   Iteration 92 of 100, tot loss = 4.834445273098738, l1: 0.0001054453561031867, l2: 0.0003779991714944354   Iteration 93 of 100, tot loss = 4.813988010088603, l1: 0.00010515690980834626, l2: 0.0003762418914007984   Iteration 94 of 100, tot loss = 4.796022015683194, l1: 0.00010491469963218402, l2: 0.0003746875021185607   Iteration 95 of 100, tot loss = 4.800388191875658, l1: 0.00010524304165492618, l2: 0.00037479577739232855   Iteration 96 of 100, tot loss = 4.783350712309281, l1: 0.00010495715726695683, l2: 0.0003733779138504663   Iteration 97 of 100, tot loss = 4.774062882993639, l1: 0.00010511863350534582, l2: 0.0003722876545279431   Iteration 98 of 100, tot loss = 4.779543272086552, l1: 0.00010510389204552797, l2: 0.0003728504348795728   Iteration 99 of 100, tot loss = 4.763588184058064, l1: 0.0001049253748138341, l2: 0.0003714334431946315   Iteration 100 of 100, tot loss = 4.783022428750992, l1: 0.00010534444045333658, l2: 0.00037295780231943353
   End of epoch 1376; saving model... 

Epoch 1377 of 2000
   Iteration 1 of 100, tot loss = 3.162407159805298, l1: 5.061202682554722e-05, l2: 0.00026562868151813745   Iteration 2 of 100, tot loss = 3.425751566886902, l1: 5.1232753321528435e-05, l2: 0.0002913423959398642   Iteration 3 of 100, tot loss = 4.918850024541219, l1: 8.908059195770572e-05, l2: 0.00040280439619285363   Iteration 4 of 100, tot loss = 4.614252924919128, l1: 9.240096915164031e-05, l2: 0.00036902431020280346   Iteration 5 of 100, tot loss = 4.820688724517822, l1: 9.802670538192615e-05, l2: 0.0003840421559289098   Iteration 6 of 100, tot loss = 4.71090030670166, l1: 9.843708539847285e-05, l2: 0.00037265293940436095   Iteration 7 of 100, tot loss = 4.600883075169155, l1: 9.821442134645102e-05, l2: 0.00036187388052764746   Iteration 8 of 100, tot loss = 4.4507445096969604, l1: 9.656911606725771e-05, l2: 0.0003485053275653627   Iteration 9 of 100, tot loss = 4.5278131696912975, l1: 9.741659596329555e-05, l2: 0.00035536471275716193   Iteration 10 of 100, tot loss = 4.545517921447754, l1: 9.887671840260736e-05, l2: 0.0003556750656571239   Iteration 11 of 100, tot loss = 4.78884185444225, l1: 0.00010434748798037286, l2: 0.00037453669144517994   Iteration 12 of 100, tot loss = 4.7960045337677, l1: 0.00010359805916474822, l2: 0.000376002387686943   Iteration 13 of 100, tot loss = 5.09634883587177, l1: 0.00010875611042138189, l2: 0.000400878766623254   Iteration 14 of 100, tot loss = 4.968921269689288, l1: 0.00010488710306942397, l2: 0.00039200501799184294   Iteration 15 of 100, tot loss = 4.836821587880452, l1: 0.00010369665372612265, l2: 0.00037998549911814433   Iteration 16 of 100, tot loss = 4.850432991981506, l1: 0.00010318858244318108, l2: 0.0003818547138507711   Iteration 17 of 100, tot loss = 4.743085089851828, l1: 0.00010121037078480346, l2: 0.0003730981355628875   Iteration 18 of 100, tot loss = 4.778682377603319, l1: 0.00010212866113255991, l2: 0.00037573957443884056   Iteration 19 of 100, tot loss = 4.726789913679424, l1: 0.00010099425420967716, l2: 0.00037168473461765404   Iteration 20 of 100, tot loss = 4.713770020008087, l1: 0.0001003306182610686, l2: 0.00037104638104210606   Iteration 21 of 100, tot loss = 4.636333828880673, l1: 9.817132879453268e-05, l2: 0.00036546205090070584   Iteration 22 of 100, tot loss = 4.601677417755127, l1: 9.768903807773974e-05, l2: 0.0003624786999849179   Iteration 23 of 100, tot loss = 4.527844314989836, l1: 9.637615569416717e-05, l2: 0.00035640827200436235   Iteration 24 of 100, tot loss = 4.53779332836469, l1: 9.773391911949147e-05, l2: 0.0003560454103232284   Iteration 25 of 100, tot loss = 4.536828737258912, l1: 9.814752105739899e-05, l2: 0.0003555353492265567   Iteration 26 of 100, tot loss = 4.535404104452867, l1: 9.862715957681827e-05, l2: 0.00035491324748503615   Iteration 27 of 100, tot loss = 4.504844276993363, l1: 9.815192916667675e-05, l2: 0.00035233249514863863   Iteration 28 of 100, tot loss = 4.501807349068778, l1: 9.82276855471095e-05, l2: 0.0003519530463173786   Iteration 29 of 100, tot loss = 4.516010202210525, l1: 9.922483288141063e-05, l2: 0.0003523761833062524   Iteration 30 of 100, tot loss = 4.509986162185669, l1: 9.923298481832414e-05, l2: 0.0003517656281474046   Iteration 31 of 100, tot loss = 4.515828670993928, l1: 9.860401476016119e-05, l2: 0.0003529788495319325   Iteration 32 of 100, tot loss = 4.545120418071747, l1: 9.968897950329847e-05, l2: 0.00035482305838741013   Iteration 33 of 100, tot loss = 4.6592530048254766, l1: 0.00010214007301273699, l2: 0.0003637852236296925   Iteration 34 of 100, tot loss = 4.591002085629632, l1: 0.00010046202367127134, l2: 0.00035863818159080385   Iteration 35 of 100, tot loss = 4.5279952253614155, l1: 9.94287987331128e-05, l2: 0.00035337072081996924   Iteration 36 of 100, tot loss = 4.545142789681752, l1: 9.956029710641208e-05, l2: 0.00035495397929076315   Iteration 37 of 100, tot loss = 4.51557368845553, l1: 9.826048925468647e-05, l2: 0.0003532968765137264   Iteration 38 of 100, tot loss = 4.4991414609708285, l1: 9.808621995800517e-05, l2: 0.0003518279232881277   Iteration 39 of 100, tot loss = 4.526811960415962, l1: 9.927974464610602e-05, l2: 0.0003534014484522721   Iteration 40 of 100, tot loss = 4.548787611722946, l1: 0.00010022494607255794, l2: 0.0003546538122463971   Iteration 41 of 100, tot loss = 4.551311068418549, l1: 9.979229337371113e-05, l2: 0.00035533881050579976   Iteration 42 of 100, tot loss = 4.556814108576093, l1: 0.00010020423464344017, l2: 0.000355477173884754   Iteration 43 of 100, tot loss = 4.56572699546814, l1: 0.00010101586684508812, l2: 0.00035555683060137683   Iteration 44 of 100, tot loss = 4.564378256147558, l1: 0.00010063335503218696, l2: 0.0003558044680341316   Iteration 45 of 100, tot loss = 4.615573157204522, l1: 0.00010117745647827784, l2: 0.0003603798576578912   Iteration 46 of 100, tot loss = 4.558043052320895, l1: 0.00010034012508264783, l2: 0.00035546417854240406   Iteration 47 of 100, tot loss = 4.5465804845728774, l1: 0.00010006776930358102, l2: 0.00035459027709835707   Iteration 48 of 100, tot loss = 4.533729133506616, l1: 9.996371151525334e-05, l2: 0.0003534091999123727   Iteration 49 of 100, tot loss = 4.4708976356350645, l1: 9.85873703029938e-05, l2: 0.00034850239127099856   Iteration 50 of 100, tot loss = 4.5217157649993895, l1: 9.913503265124746e-05, l2: 0.00035303654280141925   Iteration 51 of 100, tot loss = 4.489728993060542, l1: 9.872866768787121e-05, l2: 0.0003502442304321163   Iteration 52 of 100, tot loss = 4.513815604723417, l1: 9.960155526082068e-05, l2: 0.0003517800042363188   Iteration 53 of 100, tot loss = 4.483590004579076, l1: 9.933299712292007e-05, l2: 0.0003490260027113968   Iteration 54 of 100, tot loss = 4.5265042737678245, l1: 0.00010006089050202044, l2: 0.00035258953600229387   Iteration 55 of 100, tot loss = 4.531118830767545, l1: 9.972077585354616e-05, l2: 0.00035339110618224365   Iteration 56 of 100, tot loss = 4.557160824537277, l1: 0.00010052617952689096, l2: 0.00035518990223083947   Iteration 57 of 100, tot loss = 4.549127432337978, l1: 0.00010024477919694363, l2: 0.0003546679637797193   Iteration 58 of 100, tot loss = 4.553983544481212, l1: 0.00010024916602936895, l2: 0.00035514918786718445   Iteration 59 of 100, tot loss = 4.624389797954236, l1: 0.00010149031041399181, l2: 0.00036094866811901615   Iteration 60 of 100, tot loss = 4.712812507152558, l1: 0.00010296268240684488, l2: 0.0003683185662036218   Iteration 61 of 100, tot loss = 4.688425146165441, l1: 0.0001028440960684837, l2: 0.00036599841643960716   Iteration 62 of 100, tot loss = 4.667602665962711, l1: 0.00010273596337590848, l2: 0.0003640243008821046   Iteration 63 of 100, tot loss = 4.653105463300433, l1: 0.00010277061295392554, l2: 0.00036253993127343526   Iteration 64 of 100, tot loss = 4.635330490767956, l1: 0.00010254499022721575, l2: 0.0003609880567410073   Iteration 65 of 100, tot loss = 4.62675187771137, l1: 0.0001023127906731903, l2: 0.0003603623950486788   Iteration 66 of 100, tot loss = 4.615733374248851, l1: 0.00010225325086415096, l2: 0.0003593200843059811   Iteration 67 of 100, tot loss = 4.603692442623537, l1: 0.00010185388247864738, l2: 0.0003585153594465384   Iteration 68 of 100, tot loss = 4.571234275312984, l1: 0.0001009925506557064, l2: 0.0003561308745060267   Iteration 69 of 100, tot loss = 4.533388350320899, l1: 0.00010010537005365367, l2: 0.0003532334626272685   Iteration 70 of 100, tot loss = 4.548806277343205, l1: 0.00010055181394688719, l2: 0.0003543288121623586   Iteration 71 of 100, tot loss = 4.576587191769774, l1: 0.00010099230020784143, l2: 0.0003566664173620307   Iteration 72 of 100, tot loss = 4.5661424017614785, l1: 0.00010095588019491213, l2: 0.00035565835817881936   Iteration 73 of 100, tot loss = 4.593723932357683, l1: 0.00010159755063247994, l2: 0.0003577748413017492   Iteration 74 of 100, tot loss = 4.586874505958042, l1: 0.00010135634565321616, l2: 0.0003573311038741628   Iteration 75 of 100, tot loss = 4.638803286552429, l1: 0.00010246021008545843, l2: 0.00036142011784249915   Iteration 76 of 100, tot loss = 4.658172094508221, l1: 0.00010282414275838834, l2: 0.00036299306626269275   Iteration 77 of 100, tot loss = 4.642848747117179, l1: 0.00010259357441100292, l2: 0.00036169130000960107   Iteration 78 of 100, tot loss = 4.647611173299643, l1: 0.00010274302934330757, l2: 0.00036201808810195263   Iteration 79 of 100, tot loss = 4.636974653111229, l1: 0.00010243744286008569, l2: 0.0003612600225488113   Iteration 80 of 100, tot loss = 4.618684111535549, l1: 0.00010218245547548577, l2: 0.00035968595575468497   Iteration 81 of 100, tot loss = 4.6108211161177834, l1: 0.00010208539521202364, l2: 0.0003589967165658262   Iteration 82 of 100, tot loss = 4.604137331974216, l1: 0.00010187034249389487, l2: 0.0003585433909526129   Iteration 83 of 100, tot loss = 4.619141560002982, l1: 0.00010178862513899971, l2: 0.00036012553089194514   Iteration 84 of 100, tot loss = 4.60632108648618, l1: 0.00010194242674645335, l2: 0.00035868968180217226   Iteration 85 of 100, tot loss = 4.610483849749846, l1: 0.00010201775692300597, l2: 0.0003590306278270231   Iteration 86 of 100, tot loss = 4.602319402750148, l1: 0.00010204995576418168, l2: 0.00035818198434859624   Iteration 87 of 100, tot loss = 4.580899640061389, l1: 0.00010172153307739163, l2: 0.00035636843058037114   Iteration 88 of 100, tot loss = 4.591602332212708, l1: 0.00010177378031915974, l2: 0.00035738645246138646   Iteration 89 of 100, tot loss = 4.576847902844461, l1: 0.00010160485352356813, l2: 0.00035607993622046235   Iteration 90 of 100, tot loss = 4.568183781041039, l1: 0.00010148030628948214, l2: 0.00035533807135329374   Iteration 91 of 100, tot loss = 4.565438945214827, l1: 0.0001010971322862283, l2: 0.0003554467616345641   Iteration 92 of 100, tot loss = 4.570971098930939, l1: 0.00010139900796670902, l2: 0.0003556981017659975   Iteration 93 of 100, tot loss = 4.566477820437441, l1: 0.00010142771462861058, l2: 0.0003552200673717595   Iteration 94 of 100, tot loss = 4.5798085940645095, l1: 0.00010161355845760208, l2: 0.000356367300868287   Iteration 95 of 100, tot loss = 4.565238477054395, l1: 0.00010147154079229375, l2: 0.00035505230687129754   Iteration 96 of 100, tot loss = 4.550345670431852, l1: 0.00010133665155838874, l2: 0.0003536979154432629   Iteration 97 of 100, tot loss = 4.540963707510958, l1: 0.00010108300212431056, l2: 0.00035301336849686817   Iteration 98 of 100, tot loss = 4.532674664137315, l1: 0.00010096436288185199, l2: 0.0003523031034778889   Iteration 99 of 100, tot loss = 4.519914028620479, l1: 0.00010076213430059424, l2: 0.0003512292687170857   Iteration 100 of 100, tot loss = 4.523128169775009, l1: 0.0001007203309200122, l2: 0.00035159248633135574
   End of epoch 1377; saving model... 

Epoch 1378 of 2000
   Iteration 1 of 100, tot loss = 4.755553245544434, l1: 0.00012478073767852038, l2: 0.000350774556864053   Iteration 2 of 100, tot loss = 4.560382604598999, l1: 0.00010383580593043007, l2: 0.00035220243444200605   Iteration 3 of 100, tot loss = 4.198248545328776, l1: 9.561709157424048e-05, l2: 0.00032420775581461686   Iteration 4 of 100, tot loss = 4.03612744808197, l1: 9.448484706808813e-05, l2: 0.00030912789225112647   Iteration 5 of 100, tot loss = 3.8976590156555178, l1: 9.416843095095828e-05, l2: 0.0002955974690848961   Iteration 6 of 100, tot loss = 4.054599682490031, l1: 9.765436576950985e-05, l2: 0.00030780560094475123   Iteration 7 of 100, tot loss = 4.1168646812438965, l1: 9.762497211340815e-05, l2: 0.0003140614945940407   Iteration 8 of 100, tot loss = 4.439648985862732, l1: 9.984514781535836e-05, l2: 0.000344119745932403   Iteration 9 of 100, tot loss = 4.306305779351129, l1: 9.75363347808727e-05, l2: 0.0003330942401791819   Iteration 10 of 100, tot loss = 4.262621164321899, l1: 9.600296325515956e-05, l2: 0.00033025914890458805   Iteration 11 of 100, tot loss = 4.288014628670433, l1: 9.899645705114712e-05, l2: 0.0003298050022832203   Iteration 12 of 100, tot loss = 4.2540779908498125, l1: 9.710143118960939e-05, l2: 0.0003283063645843261   Iteration 13 of 100, tot loss = 4.113443007835975, l1: 9.28300169806999e-05, l2: 0.0003185142805495371   Iteration 14 of 100, tot loss = 4.101273809160505, l1: 9.1398543840374e-05, l2: 0.000318728835346909   Iteration 15 of 100, tot loss = 4.234729226430257, l1: 9.324627074723443e-05, l2: 0.0003302266501123086   Iteration 16 of 100, tot loss = 4.259967058897018, l1: 9.438472807232756e-05, l2: 0.00033161197734443704   Iteration 17 of 100, tot loss = 4.295371728784898, l1: 9.584206803564859e-05, l2: 0.0003336951066397459   Iteration 18 of 100, tot loss = 4.1948425902260675, l1: 9.421757567906752e-05, l2: 0.00032526668575074937   Iteration 19 of 100, tot loss = 4.130697702106676, l1: 9.238355070058453e-05, l2: 0.00032068622194377606   Iteration 20 of 100, tot loss = 4.152193355560303, l1: 9.382097450725269e-05, l2: 0.00032139836403075603   Iteration 21 of 100, tot loss = 4.050499677658081, l1: 9.213136876323482e-05, l2: 0.00031291860172392   Iteration 22 of 100, tot loss = 4.1504913785240864, l1: 9.435674523543143e-05, l2: 0.00032069239585491067   Iteration 23 of 100, tot loss = 4.208464695059734, l1: 9.551990376127398e-05, l2: 0.00032532656990477574   Iteration 24 of 100, tot loss = 4.228126813968022, l1: 9.610197624472978e-05, l2: 0.000326710709487088   Iteration 25 of 100, tot loss = 4.254634065628052, l1: 9.71499879960902e-05, l2: 0.0003283134230878204   Iteration 26 of 100, tot loss = 4.23830456000108, l1: 9.734016202855855e-05, l2: 0.0003264902983433925   Iteration 27 of 100, tot loss = 4.233411965546785, l1: 9.738406758212174e-05, l2: 0.00032595713316738883   Iteration 28 of 100, tot loss = 4.27162720475878, l1: 9.90795038628026e-05, l2: 0.00032808322014586466   Iteration 29 of 100, tot loss = 4.343575691354686, l1: 0.00010044641159514992, l2: 0.0003339111602238925   Iteration 30 of 100, tot loss = 4.423882023493449, l1: 0.00010085163764112318, l2: 0.0003415365674300119   Iteration 31 of 100, tot loss = 4.403189736027872, l1: 0.00010089248239490834, l2: 0.0003394264936251866   Iteration 32 of 100, tot loss = 4.379503704607487, l1: 0.00010015746670433145, l2: 0.00033779290606616996   Iteration 33 of 100, tot loss = 4.335892048749057, l1: 9.954705081216878e-05, l2: 0.00033404215575887287   Iteration 34 of 100, tot loss = 4.398016123210683, l1: 9.968758154975469e-05, l2: 0.00034011403391452725   Iteration 35 of 100, tot loss = 4.3323727812085835, l1: 9.827857679088733e-05, l2: 0.00033495870427161986   Iteration 36 of 100, tot loss = 4.389056821664174, l1: 9.914856410533603e-05, l2: 0.0003397571211583757   Iteration 37 of 100, tot loss = 4.327439456372647, l1: 9.795008559563398e-05, l2: 0.00033479386341216897   Iteration 38 of 100, tot loss = 4.314841941783302, l1: 9.798213862890599e-05, l2: 0.00033350205883739124   Iteration 39 of 100, tot loss = 4.321751111593002, l1: 9.790146214072593e-05, l2: 0.0003342736517646326   Iteration 40 of 100, tot loss = 4.304478144645691, l1: 9.672731584942085e-05, l2: 0.00033372050129401034   Iteration 41 of 100, tot loss = 4.286242467601125, l1: 9.660848188171413e-05, l2: 0.0003320157677701833   Iteration 42 of 100, tot loss = 4.322793795948937, l1: 9.709767592929503e-05, l2: 0.000335181706759613   Iteration 43 of 100, tot loss = 4.316937030747879, l1: 9.680177255325171e-05, l2: 0.00033489193362562893   Iteration 44 of 100, tot loss = 4.3342655951326545, l1: 9.747692779042568e-05, l2: 0.0003359496341462628   Iteration 45 of 100, tot loss = 4.341363170411852, l1: 9.795652125224782e-05, l2: 0.0003361797978868708   Iteration 46 of 100, tot loss = 4.335153761117355, l1: 9.838402792925278e-05, l2: 0.0003351313501733112   Iteration 47 of 100, tot loss = 4.378402836779331, l1: 9.951097873705836e-05, l2: 0.0003383293072321512   Iteration 48 of 100, tot loss = 4.345220396916072, l1: 9.913119485342274e-05, l2: 0.0003353908468852751   Iteration 49 of 100, tot loss = 4.349927201562998, l1: 9.881297079786868e-05, l2: 0.00033617975149892877   Iteration 50 of 100, tot loss = 4.3414386367797855, l1: 9.821864769037348e-05, l2: 0.0003359252185327932   Iteration 51 of 100, tot loss = 4.350137617073807, l1: 9.821274429936345e-05, l2: 0.00033680101977569943   Iteration 52 of 100, tot loss = 4.418034040010893, l1: 9.940345996885578e-05, l2: 0.00034239994588111027   Iteration 53 of 100, tot loss = 4.4466471941965935, l1: 9.952757002638635e-05, l2: 0.0003451371509450013   Iteration 54 of 100, tot loss = 4.442057574236834, l1: 9.997390460975348e-05, l2: 0.0003442318540894323   Iteration 55 of 100, tot loss = 4.441351925243031, l1: 9.986584793130698e-05, l2: 0.00034426934593780474   Iteration 56 of 100, tot loss = 4.467451316969735, l1: 0.00010045245075421658, l2: 0.00034629268234961534   Iteration 57 of 100, tot loss = 4.460240740525095, l1: 0.00010025475217333766, l2: 0.00034576932299195935   Iteration 58 of 100, tot loss = 4.430988328210239, l1: 9.963346000686528e-05, l2: 0.00034346537396576704   Iteration 59 of 100, tot loss = 4.448089720839161, l1: 0.00010005232030318623, l2: 0.00034475665252411074   Iteration 60 of 100, tot loss = 4.477972571055094, l1: 0.000100758123092722, l2: 0.0003470391352796772   Iteration 61 of 100, tot loss = 4.458833346601392, l1: 0.00010033256104015593, l2: 0.00034555077498404647   Iteration 62 of 100, tot loss = 4.501579334658961, l1: 0.00010145349833398922, l2: 0.00034870443590578713   Iteration 63 of 100, tot loss = 4.46978693538242, l1: 0.0001008382156821123, l2: 0.0003461404784401465   Iteration 64 of 100, tot loss = 4.467102911323309, l1: 0.00010056774516442601, l2: 0.0003461425465047796   Iteration 65 of 100, tot loss = 4.512604885834914, l1: 0.00010106622173155372, l2: 0.000350194267453984   Iteration 66 of 100, tot loss = 4.491532741170941, l1: 0.00010038780882578894, l2: 0.0003487654658701186   Iteration 67 of 100, tot loss = 4.474687412603577, l1: 0.0001003711754017433, l2: 0.00034709756676135447   Iteration 68 of 100, tot loss = 4.486083647784064, l1: 0.00010095413055002128, l2: 0.0003476542346213725   Iteration 69 of 100, tot loss = 4.483328992041988, l1: 0.00010075750633141202, l2: 0.0003475753929587486   Iteration 70 of 100, tot loss = 4.494078595297677, l1: 0.00010086384016696164, l2: 0.0003485440193409366   Iteration 71 of 100, tot loss = 4.5267176292311975, l1: 0.00010159397916909321, l2: 0.00035107778352488515   Iteration 72 of 100, tot loss = 4.537642161051433, l1: 0.0001016720368372464, l2: 0.00035209217862251937   Iteration 73 of 100, tot loss = 4.515787402244463, l1: 0.0001010424182399444, l2: 0.00035053632128026897   Iteration 74 of 100, tot loss = 4.493390173525424, l1: 0.00010044083329203941, l2: 0.0003488981833593366   Iteration 75 of 100, tot loss = 4.51380402247111, l1: 0.00010071955563034862, l2: 0.0003506608458701521   Iteration 76 of 100, tot loss = 4.534854669319956, l1: 0.0001006329897025257, l2: 0.0003528524765371051   Iteration 77 of 100, tot loss = 4.522465433393206, l1: 0.00010043037754813112, l2: 0.00035181616519142377   Iteration 78 of 100, tot loss = 4.536547660827637, l1: 0.00010061380070953582, l2: 0.0003530409646769747   Iteration 79 of 100, tot loss = 4.519360497028013, l1: 0.00010007064688407015, l2: 0.00035186540195423686   Iteration 80 of 100, tot loss = 4.499274241924286, l1: 9.964968962776766e-05, l2: 0.00035027773374167736   Iteration 81 of 100, tot loss = 4.496531698438856, l1: 9.947471317215708e-05, l2: 0.0003501784558017037   Iteration 82 of 100, tot loss = 4.4717390275583035, l1: 9.883443795488125e-05, l2: 0.00034833946403700904   Iteration 83 of 100, tot loss = 4.449808545859463, l1: 9.846017139834767e-05, l2: 0.00034652068244618735   Iteration 84 of 100, tot loss = 4.456636264210656, l1: 9.808513009158473e-05, l2: 0.00034757849546925473   Iteration 85 of 100, tot loss = 4.465861673916088, l1: 9.819841840961838e-05, l2: 0.00034838774796191823   Iteration 86 of 100, tot loss = 4.524691498556803, l1: 9.916871000990964e-05, l2: 0.00035330043803781356   Iteration 87 of 100, tot loss = 4.510672889906784, l1: 9.905750320986149e-05, l2: 0.0003520097841452486   Iteration 88 of 100, tot loss = 4.493061908266761, l1: 9.881251042481216e-05, l2: 0.00035049367877284317   Iteration 89 of 100, tot loss = 4.511105154337508, l1: 9.889802818639162e-05, l2: 0.0003522124858913169   Iteration 90 of 100, tot loss = 4.494328440560235, l1: 9.846793549917897e-05, l2: 0.0003509649072333963   Iteration 91 of 100, tot loss = 4.516461765373146, l1: 9.882471487879149e-05, l2: 0.0003528214602828394   Iteration 92 of 100, tot loss = 4.526167962862098, l1: 9.908489526265919e-05, l2: 0.00035353189963169154   Iteration 93 of 100, tot loss = 4.535888841075282, l1: 9.941508789623147e-05, l2: 0.0003541737947592972   Iteration 94 of 100, tot loss = 4.546416247144658, l1: 9.955476239253024e-05, l2: 0.0003550868612055251   Iteration 95 of 100, tot loss = 4.527667718184622, l1: 9.930387202889267e-05, l2: 0.00035346289866873505   Iteration 96 of 100, tot loss = 4.512267204622428, l1: 9.878510589563423e-05, l2: 0.00035244161351026076   Iteration 97 of 100, tot loss = 4.499705324467924, l1: 9.848404457893453e-05, l2: 0.0003514864868136392   Iteration 98 of 100, tot loss = 4.494445815378306, l1: 9.84264694793359e-05, l2: 0.0003510181109328298   Iteration 99 of 100, tot loss = 4.485572012988004, l1: 9.831161349376833e-05, l2: 0.0003502455866611045   Iteration 100 of 100, tot loss = 4.476566591262817, l1: 9.834173819399439e-05, l2: 0.00034931491987663324
   End of epoch 1378; saving model... 

Epoch 1379 of 2000
   Iteration 1 of 100, tot loss = 3.5132851600646973, l1: 8.229509694501758e-05, l2: 0.0002690334222279489   Iteration 2 of 100, tot loss = 5.249754190444946, l1: 0.00010672978532966226, l2: 0.0004182456177659333   Iteration 3 of 100, tot loss = 4.230377276738484, l1: 9.575257475565498e-05, l2: 0.0003272851463407278   Iteration 4 of 100, tot loss = 4.61234313249588, l1: 9.423045776202343e-05, l2: 0.00036700384225696325   Iteration 5 of 100, tot loss = 5.02896637916565, l1: 9.792493510758504e-05, l2: 0.0004049716982990503   Iteration 6 of 100, tot loss = 4.714437484741211, l1: 8.967355825006962e-05, l2: 0.00038177018965749693   Iteration 7 of 100, tot loss = 4.595435755593436, l1: 8.577257540309802e-05, l2: 0.00037377099839172194   Iteration 8 of 100, tot loss = 4.692703604698181, l1: 9.175228387903189e-05, l2: 0.00037751808122266084   Iteration 9 of 100, tot loss = 5.146657307942708, l1: 0.00010027882833835772, l2: 0.0004143869090411398   Iteration 10 of 100, tot loss = 5.093602037429809, l1: 0.00010067878683912568, l2: 0.0004086814209586009   Iteration 11 of 100, tot loss = 5.0210637179288, l1: 0.00010100218919846652, l2: 0.00040110418657687575   Iteration 12 of 100, tot loss = 4.953592737515767, l1: 0.00010288307566952426, l2: 0.0003924762034633507   Iteration 13 of 100, tot loss = 5.128296998830942, l1: 0.00010447809304773378, l2: 0.00040835161388923344   Iteration 14 of 100, tot loss = 5.192262581416538, l1: 0.00010576412595193168, l2: 0.00041346213921704996   Iteration 15 of 100, tot loss = 5.163930130004883, l1: 0.00010729318552572901, l2: 0.00040909983411741756   Iteration 16 of 100, tot loss = 5.2264672219753265, l1: 0.00010798017001434346, l2: 0.0004146665578446118   Iteration 17 of 100, tot loss = 5.112093883402207, l1: 0.00010592908802209422, l2: 0.0004052803064148654   Iteration 18 of 100, tot loss = 5.178274777200487, l1: 0.00010716696164713035, l2: 0.0004106605224983974   Iteration 19 of 100, tot loss = 5.087264224102623, l1: 0.00010613123480365367, l2: 0.0004025951937747825   Iteration 20 of 100, tot loss = 5.1965938925743105, l1: 0.00010907667492574547, l2: 0.000410582720360253   Iteration 21 of 100, tot loss = 5.1760424091702415, l1: 0.00010907657408443767, l2: 0.00040852767276755044   Iteration 22 of 100, tot loss = 5.2012146061117, l1: 0.00011011021309638058, l2: 0.0004100112538170916   Iteration 23 of 100, tot loss = 5.211471049681954, l1: 0.00011129716142971554, l2: 0.0004098499510133558   Iteration 24 of 100, tot loss = 5.1352515916029615, l1: 0.00011053369932293815, l2: 0.0004029914671264123   Iteration 25 of 100, tot loss = 5.062953462600708, l1: 0.00010899453016463667, l2: 0.00039730082266032697   Iteration 26 of 100, tot loss = 5.0306679010391235, l1: 0.00010815970745170489, l2: 0.00039490708933534246   Iteration 27 of 100, tot loss = 4.921668538340816, l1: 0.00010552581591548881, l2: 0.00038664104444992346   Iteration 28 of 100, tot loss = 4.928440698555538, l1: 0.00010567226880604201, l2: 0.00038717180879237797   Iteration 29 of 100, tot loss = 4.895261197254576, l1: 0.00010518921621481021, l2: 0.0003843369109933425   Iteration 30 of 100, tot loss = 4.846849568684896, l1: 0.00010496017275727355, l2: 0.00037972479137048744   Iteration 31 of 100, tot loss = 4.817490962243849, l1: 0.0001049861875212481, l2: 0.0003767629156843008   Iteration 32 of 100, tot loss = 4.755579739809036, l1: 0.00010393359275440162, l2: 0.00037162438820814714   Iteration 33 of 100, tot loss = 4.690111405921705, l1: 0.00010360796119185221, l2: 0.00036540318670273393   Iteration 34 of 100, tot loss = 4.69936661159291, l1: 0.00010410358513849239, l2: 0.0003658330828971777   Iteration 35 of 100, tot loss = 4.7112973621913365, l1: 0.00010403444030089304, l2: 0.0003670953027072496   Iteration 36 of 100, tot loss = 4.769146853023106, l1: 0.00010561829003563616, l2: 0.0003712964013781554   Iteration 37 of 100, tot loss = 4.773226609101167, l1: 0.00010513259405490464, l2: 0.00037219007365795707   Iteration 38 of 100, tot loss = 4.75573404211747, l1: 0.00010511695482388237, l2: 0.00037045645532136977   Iteration 39 of 100, tot loss = 4.733836351296841, l1: 0.00010449856162244358, l2: 0.0003688850791569656   Iteration 40 of 100, tot loss = 4.77837615609169, l1: 0.00010551046634645899, l2: 0.00037232715512800494   Iteration 41 of 100, tot loss = 4.776925790600661, l1: 0.00010501474841986214, l2: 0.0003726778359566929   Iteration 42 of 100, tot loss = 4.833678671291897, l1: 0.00010571627630963034, l2: 0.00037765159677725197   Iteration 43 of 100, tot loss = 4.763216379076936, l1: 0.00010417935744382788, l2: 0.00037214228630250016   Iteration 44 of 100, tot loss = 4.691641756079414, l1: 0.00010301100188702218, l2: 0.00036615317946019456   Iteration 45 of 100, tot loss = 4.661762595176697, l1: 0.000102555532551681, l2: 0.0003636207326457629   Iteration 46 of 100, tot loss = 4.7274794137996174, l1: 0.00010400124491475846, l2: 0.00036874670220878096   Iteration 47 of 100, tot loss = 4.774729198597847, l1: 0.00010511490077949426, l2: 0.00037235802549130697   Iteration 48 of 100, tot loss = 4.733987413346767, l1: 0.00010462820061244808, l2: 0.00036877054708384094   Iteration 49 of 100, tot loss = 4.714369421102563, l1: 0.00010441515947530541, l2: 0.0003670217884806631   Iteration 50 of 100, tot loss = 4.747015483379364, l1: 0.00010478094278369098, l2: 0.00036992061111959626   Iteration 51 of 100, tot loss = 4.738685783217935, l1: 0.00010496000123013943, l2: 0.00036890858281563564   Iteration 52 of 100, tot loss = 4.69579198039495, l1: 0.00010444985239771016, l2: 0.0003651293511491251   Iteration 53 of 100, tot loss = 4.743046151017243, l1: 0.00010534644883042553, l2: 0.00036895817257178386   Iteration 54 of 100, tot loss = 4.768378162825549, l1: 0.0001054603181492658, l2: 0.0003713775048608012   Iteration 55 of 100, tot loss = 4.7731184200807055, l1: 0.0001051305555103516, l2: 0.00037218129291431977   Iteration 56 of 100, tot loss = 4.790011152625084, l1: 0.00010490928512548894, l2: 0.0003740918368099041   Iteration 57 of 100, tot loss = 4.830786757301866, l1: 0.00010532789772760385, l2: 0.0003777507848561774   Iteration 58 of 100, tot loss = 4.888397860115972, l1: 0.00010612565454031373, l2: 0.00038271413898749825   Iteration 59 of 100, tot loss = 4.890483421794439, l1: 0.00010654039289289296, l2: 0.00038250795675548173   Iteration 60 of 100, tot loss = 4.8693758626778925, l1: 0.00010619440132965489, l2: 0.0003807431924845635   Iteration 61 of 100, tot loss = 4.857753235785688, l1: 0.00010634805805805582, l2: 0.0003794272730583096   Iteration 62 of 100, tot loss = 4.892283083931092, l1: 0.00010664405319051096, l2: 0.0003825842632654859   Iteration 63 of 100, tot loss = 4.85003804214417, l1: 0.00010608118510202667, l2: 0.00037892262711437303   Iteration 64 of 100, tot loss = 4.84000021032989, l1: 0.0001061718438677417, l2: 0.00037782818560572196   Iteration 65 of 100, tot loss = 4.827558493614196, l1: 0.00010627477671592855, l2: 0.0003764810814083411   Iteration 66 of 100, tot loss = 4.8192660501509, l1: 0.00010619325555374168, l2: 0.000375733357941499   Iteration 67 of 100, tot loss = 4.832414091523014, l1: 0.00010667731568439683, l2: 0.00037656410164182394   Iteration 68 of 100, tot loss = 4.835574877612731, l1: 0.00010711876337368535, l2: 0.00037643873217936796   Iteration 69 of 100, tot loss = 4.816011706988017, l1: 0.00010694807797731103, l2: 0.0003746531002579486   Iteration 70 of 100, tot loss = 4.805979294436319, l1: 0.00010693222076432513, l2: 0.00037366571559687144   Iteration 71 of 100, tot loss = 4.833181705273373, l1: 0.00010732356967194609, l2: 0.0003759946081169199   Iteration 72 of 100, tot loss = 4.782274067401886, l1: 0.0001062256167945937, l2: 0.0003720017970812882   Iteration 73 of 100, tot loss = 4.80048610739512, l1: 0.00010660488459669143, l2: 0.00037344373343592153   Iteration 74 of 100, tot loss = 4.799115258294183, l1: 0.0001066984286562483, l2: 0.0003732131044671405   Iteration 75 of 100, tot loss = 4.812642059326172, l1: 0.00010695408190561768, l2: 0.00037431013081610826   Iteration 76 of 100, tot loss = 4.836989227094148, l1: 0.00010710677332482239, l2: 0.0003765921559725508   Iteration 77 of 100, tot loss = 4.862420521773301, l1: 0.00010757268512849817, l2: 0.00037866937347714997   Iteration 78 of 100, tot loss = 4.848891212390019, l1: 0.00010729492402251941, l2: 0.00037759420350388   Iteration 79 of 100, tot loss = 4.882269011268133, l1: 0.0001074710774365842, l2: 0.0003807558299556575   Iteration 80 of 100, tot loss = 4.8651114970445635, l1: 0.00010745192007561854, l2: 0.0003790592357290734   Iteration 81 of 100, tot loss = 4.878434348989416, l1: 0.0001073650438442717, l2: 0.0003804783966502945   Iteration 82 of 100, tot loss = 4.889462113380432, l1: 0.00010737944160987022, l2: 0.00038156677550343774   Iteration 83 of 100, tot loss = 4.934702427990465, l1: 0.0001080696799680843, l2: 0.0003854005681862774   Iteration 84 of 100, tot loss = 4.951609171572185, l1: 0.0001082858665367461, l2: 0.0003868750560026716   Iteration 85 of 100, tot loss = 4.961412556031171, l1: 0.00010835453076116905, l2: 0.0003877867301686338   Iteration 86 of 100, tot loss = 4.937168276587198, l1: 0.00010816464245456087, l2: 0.00038555219048988835   Iteration 87 of 100, tot loss = 4.916867609681754, l1: 0.00010787250110966517, l2: 0.00038381426499142237   Iteration 88 of 100, tot loss = 4.904735134406523, l1: 0.0001076271222783279, l2: 0.00038284639603344573   Iteration 89 of 100, tot loss = 4.876914686031556, l1: 0.00010706691351474598, l2: 0.0003806245599100314   Iteration 90 of 100, tot loss = 4.8842900885476, l1: 0.00010731641349492646, l2: 0.00038111260000732726   Iteration 91 of 100, tot loss = 4.883510801818344, l1: 0.0001075140021534785, l2: 0.00038083708228781347   Iteration 92 of 100, tot loss = 4.845837313195934, l1: 0.00010671877384004841, l2: 0.0003778649617110521   Iteration 93 of 100, tot loss = 4.856898794892014, l1: 0.00010684059828148043, l2: 0.0003788492857335856   Iteration 94 of 100, tot loss = 4.883338141948618, l1: 0.00010709992409132649, l2: 0.00038123389435411884   Iteration 95 of 100, tot loss = 4.854252637060065, l1: 0.00010667160834265432, l2: 0.0003787536596830346   Iteration 96 of 100, tot loss = 4.830374081929524, l1: 0.00010643991030671411, l2: 0.0003765975019026276   Iteration 97 of 100, tot loss = 4.823648634645128, l1: 0.00010631968996808, l2: 0.00037604517731863583   Iteration 98 of 100, tot loss = 4.804999426919586, l1: 0.0001060503120465817, l2: 0.00037444963438284335   Iteration 99 of 100, tot loss = 4.779648157081219, l1: 0.00010564504466883866, l2: 0.00037231977477509794   Iteration 100 of 100, tot loss = 4.797774469852447, l1: 0.00010564928943495034, l2: 0.0003741281610564329
   End of epoch 1379; saving model... 

Epoch 1380 of 2000
   Iteration 1 of 100, tot loss = 5.30128812789917, l1: 0.00010674364602891728, l2: 0.00042338520870544016   Iteration 2 of 100, tot loss = 4.9749860763549805, l1: 0.0001077415763575118, l2: 0.00038975705683697015   Iteration 3 of 100, tot loss = 4.226243257522583, l1: 9.61422532175978e-05, l2: 0.000326482094048212   Iteration 4 of 100, tot loss = 4.446657478809357, l1: 0.00010333082173019648, l2: 0.00034133494773413986   Iteration 5 of 100, tot loss = 4.88318943977356, l1: 0.00010614472557790577, l2: 0.0003821742371656001   Iteration 6 of 100, tot loss = 4.755814592043559, l1: 9.779851946708125e-05, l2: 0.00037778296003428596   Iteration 7 of 100, tot loss = 4.622168779373169, l1: 9.233459786628373e-05, l2: 0.0003698822984006256   Iteration 8 of 100, tot loss = 4.841249793767929, l1: 9.617485693524941e-05, l2: 0.0003879501346091274   Iteration 9 of 100, tot loss = 4.845708979500665, l1: 9.739908798817649e-05, l2: 0.00038717182016827993   Iteration 10 of 100, tot loss = 5.044119143486023, l1: 0.00010273949628754053, l2: 0.000401672424050048   Iteration 11 of 100, tot loss = 4.9943578243255615, l1: 0.00010120134174940176, l2: 0.00039823444570753384   Iteration 12 of 100, tot loss = 5.043700118859609, l1: 0.00010230329614084137, l2: 0.0004020667208048205   Iteration 13 of 100, tot loss = 4.977661627989549, l1: 0.00010377087584437014, l2: 0.00039399528983407293   Iteration 14 of 100, tot loss = 4.952238508633205, l1: 0.00010399662408287571, l2: 0.00039122723059595695   Iteration 15 of 100, tot loss = 5.211779228846232, l1: 0.0001076090016188876, l2: 0.00041356892713035145   Iteration 16 of 100, tot loss = 5.092287763953209, l1: 0.00010603464420455566, l2: 0.000403194137106766   Iteration 17 of 100, tot loss = 5.0026608354905076, l1: 0.0001055855859971523, l2: 0.00039468050133163   Iteration 18 of 100, tot loss = 5.028713835610284, l1: 0.00010500779191918102, l2: 0.00039786359573352255   Iteration 19 of 100, tot loss = 4.904321846209075, l1: 0.00010354483387537154, l2: 0.0003868873540532628   Iteration 20 of 100, tot loss = 4.860211658477783, l1: 0.00010307091160939308, l2: 0.0003829502558801323   Iteration 21 of 100, tot loss = 4.849338281722296, l1: 0.00010224253509520731, l2: 0.00038269129332288036   Iteration 22 of 100, tot loss = 4.903279001062566, l1: 0.00010265182390867267, l2: 0.00038767607739745557   Iteration 23 of 100, tot loss = 5.002781038698942, l1: 0.00010492708315723576, l2: 0.00039535102247179526   Iteration 24 of 100, tot loss = 5.017817457516988, l1: 0.00010486600376680144, l2: 0.0003969157429916474   Iteration 25 of 100, tot loss = 5.057986316680908, l1: 0.00010620242435834371, l2: 0.0003995962091721594   Iteration 26 of 100, tot loss = 4.996652126312256, l1: 0.00010539294355392206, l2: 0.00039427227098852967   Iteration 27 of 100, tot loss = 4.983344342973497, l1: 0.00010581170270732535, l2: 0.00039252273402073316   Iteration 28 of 100, tot loss = 5.0283006599971225, l1: 0.00010593292024298404, l2: 0.0003968971494552014   Iteration 29 of 100, tot loss = 5.09752998680904, l1: 0.00010693958719112461, l2: 0.000402813414031447   Iteration 30 of 100, tot loss = 5.011082061131796, l1: 0.00010537029023301632, l2: 0.000395737918249021   Iteration 31 of 100, tot loss = 4.967306006339289, l1: 0.00010573945767264963, l2: 0.00039099114515157716   Iteration 32 of 100, tot loss = 4.944655992090702, l1: 0.00010552115861628408, l2: 0.0003889444424203248   Iteration 33 of 100, tot loss = 4.992744568622474, l1: 0.00010608636390922281, l2: 0.00039318809373953354   Iteration 34 of 100, tot loss = 4.963404599358054, l1: 0.000105949553043876, l2: 0.0003903909074484973   Iteration 35 of 100, tot loss = 4.975281238555908, l1: 0.00010650084077497013, l2: 0.00039102728312302915   Iteration 36 of 100, tot loss = 4.946055862638685, l1: 0.00010617515110627412, l2: 0.00038843043552737474   Iteration 37 of 100, tot loss = 4.935335378389101, l1: 0.00010644292312979421, l2: 0.00038709061462558   Iteration 38 of 100, tot loss = 4.985033901114213, l1: 0.00010697785039251597, l2: 0.0003915255388404292   Iteration 39 of 100, tot loss = 4.907814569962331, l1: 0.0001052752575980356, l2: 0.0003855061983743396   Iteration 40 of 100, tot loss = 4.845630836486817, l1: 0.00010434414762130472, l2: 0.00038021893487893976   Iteration 41 of 100, tot loss = 4.8251532810490305, l1: 0.00010428604085662788, l2: 0.0003782292862036606   Iteration 42 of 100, tot loss = 4.768825054168701, l1: 0.00010354598860084523, l2: 0.0003733365156222135   Iteration 43 of 100, tot loss = 4.754183525262877, l1: 0.00010281848179062742, l2: 0.00037259986924635636   Iteration 44 of 100, tot loss = 4.757369680838152, l1: 0.00010263965569637631, l2: 0.0003730973105782389   Iteration 45 of 100, tot loss = 4.730970250235663, l1: 0.00010215954793642999, l2: 0.00037093747523613273   Iteration 46 of 100, tot loss = 4.747780255649401, l1: 0.00010199225843003343, l2: 0.00037278576485772174   Iteration 47 of 100, tot loss = 4.798203929941705, l1: 0.00010251119284533915, l2: 0.00037730919776801414   Iteration 48 of 100, tot loss = 4.785745138923327, l1: 0.00010218694084566475, l2: 0.00037638757142606966   Iteration 49 of 100, tot loss = 4.773872390085337, l1: 0.00010198579518461829, l2: 0.00037540144249036605   Iteration 50 of 100, tot loss = 4.7823340654373165, l1: 0.00010218797367997468, l2: 0.000376045432058163   Iteration 51 of 100, tot loss = 4.783157269159953, l1: 0.00010252059491547555, l2: 0.00037579513132097385   Iteration 52 of 100, tot loss = 4.746612379184136, l1: 0.00010187509654385324, l2: 0.0003727861406402483   Iteration 53 of 100, tot loss = 4.780309555665502, l1: 0.00010278263982640192, l2: 0.00037524831467000594   Iteration 54 of 100, tot loss = 4.788162642055088, l1: 0.00010289121214386628, l2: 0.0003759250504555422   Iteration 55 of 100, tot loss = 4.787598579580133, l1: 0.00010340074474118988, l2: 0.0003753591114549305   Iteration 56 of 100, tot loss = 4.77244787982532, l1: 0.00010329779745786385, l2: 0.0003739469889946382   Iteration 57 of 100, tot loss = 4.765597121757374, l1: 0.00010324367745820302, l2: 0.0003733160329079909   Iteration 58 of 100, tot loss = 4.752573436704175, l1: 0.00010317943389085925, l2: 0.00037207790818025264   Iteration 59 of 100, tot loss = 4.751670655557665, l1: 0.00010331988658250894, l2: 0.0003718471776603295   Iteration 60 of 100, tot loss = 4.75470013221105, l1: 0.00010301232493172089, l2: 0.0003724576864139332   Iteration 61 of 100, tot loss = 4.724462782750364, l1: 0.00010260241296997325, l2: 0.00036984386321416764   Iteration 62 of 100, tot loss = 4.7653831897243375, l1: 0.00010359924401141583, l2: 0.00037293907230524645   Iteration 63 of 100, tot loss = 4.744326337935433, l1: 0.00010324976502156388, l2: 0.00037118286635386686   Iteration 64 of 100, tot loss = 4.743787411600351, l1: 0.00010325241328246193, l2: 0.00037112632526259404   Iteration 65 of 100, tot loss = 4.75861964225769, l1: 0.00010303807816503999, l2: 0.000372823883439056   Iteration 66 of 100, tot loss = 4.749312982414708, l1: 0.00010315469549051627, l2: 0.00037177660057998514   Iteration 67 of 100, tot loss = 4.744333526981411, l1: 0.00010344559746384342, l2: 0.000370987753536719   Iteration 68 of 100, tot loss = 4.788118134526646, l1: 0.00010383814750478931, l2: 0.00037497366429306567   Iteration 69 of 100, tot loss = 4.757376359856647, l1: 0.00010350959464499348, l2: 0.0003722280397187864   Iteration 70 of 100, tot loss = 4.7224324532917565, l1: 0.00010261778697895352, l2: 0.00036962545668107593   Iteration 71 of 100, tot loss = 4.726313278708659, l1: 0.00010272354658399577, l2: 0.0003699077794190601   Iteration 72 of 100, tot loss = 4.706413375006782, l1: 0.00010212897157341811, l2: 0.0003685123641237927   Iteration 73 of 100, tot loss = 4.71499407128112, l1: 0.00010208418765838884, l2: 0.0003694152178032298   Iteration 74 of 100, tot loss = 4.701164387367867, l1: 0.00010198161082273047, l2: 0.00036813482615429704   Iteration 75 of 100, tot loss = 4.724201437632243, l1: 0.00010236577208464344, l2: 0.00037005436955951155   Iteration 76 of 100, tot loss = 4.715561126407824, l1: 0.00010240172320520382, l2: 0.000369154387475359   Iteration 77 of 100, tot loss = 4.704135795692345, l1: 0.00010248353996226419, l2: 0.0003679300376437846   Iteration 78 of 100, tot loss = 4.7028120297652025, l1: 0.00010222100461713779, l2: 0.000368060196752851   Iteration 79 of 100, tot loss = 4.703879241701923, l1: 0.00010202162800713359, l2: 0.000368366294608251   Iteration 80 of 100, tot loss = 4.719356608390808, l1: 0.00010224850339000114, l2: 0.00036968715576222164   Iteration 81 of 100, tot loss = 4.715657793445351, l1: 0.00010202636882622531, l2: 0.0003695394085679939   Iteration 82 of 100, tot loss = 4.7047613684724015, l1: 0.0001016619558291362, l2: 0.00036881417930466917   Iteration 83 of 100, tot loss = 4.703254156802074, l1: 0.00010153140386367352, l2: 0.0003687940104527915   Iteration 84 of 100, tot loss = 4.729903337501344, l1: 0.00010186000638766148, l2: 0.0003711303253935295   Iteration 85 of 100, tot loss = 4.69967713917003, l1: 0.00010126803437528638, l2: 0.0003686996774323394   Iteration 86 of 100, tot loss = 4.708929652391478, l1: 0.00010170304233534532, l2: 0.0003691899211506013   Iteration 87 of 100, tot loss = 4.686446140552389, l1: 0.00010152871534997737, l2: 0.00036711589685486007   Iteration 88 of 100, tot loss = 4.709276621991938, l1: 0.00010181807734129093, l2: 0.0003691095826070523   Iteration 89 of 100, tot loss = 4.713846356681223, l1: 0.00010171175794250835, l2: 0.0003696728755753969   Iteration 90 of 100, tot loss = 4.71551087167528, l1: 0.00010196220371451798, l2: 0.0003695888811812943   Iteration 91 of 100, tot loss = 4.745868441822765, l1: 0.00010237181337728104, l2: 0.00037221502877930485   Iteration 92 of 100, tot loss = 4.742020321928936, l1: 0.00010228187234726805, l2: 0.0003719201579414126   Iteration 93 of 100, tot loss = 4.743596020565238, l1: 0.00010224880589771846, l2: 0.00037211079450239057   Iteration 94 of 100, tot loss = 4.714846912850725, l1: 0.00010168831296199487, l2: 0.0003697963767719673   Iteration 95 of 100, tot loss = 4.7332393872110465, l1: 0.00010197925742096758, l2: 0.0003713446794886534   Iteration 96 of 100, tot loss = 4.731748037040234, l1: 0.00010210668055303056, l2: 0.00037106812154282426   Iteration 97 of 100, tot loss = 4.716632147425229, l1: 0.00010188145236113378, l2: 0.00036978176086853   Iteration 98 of 100, tot loss = 4.6999704205259984, l1: 0.00010182859348008475, l2: 0.0003681684470300715   Iteration 99 of 100, tot loss = 4.6919798754682445, l1: 0.00010163656520719565, l2: 0.00036756142064331643   Iteration 100 of 100, tot loss = 4.707700610160828, l1: 0.00010174901308346307, l2: 0.0003690210463537369
   End of epoch 1380; saving model... 

Epoch 1381 of 2000
   Iteration 1 of 100, tot loss = 2.637624740600586, l1: 6.800877599744126e-05, l2: 0.00019575371698010713   Iteration 2 of 100, tot loss = 4.423957109451294, l1: 8.875127241481096e-05, l2: 0.0003536444346536882   Iteration 3 of 100, tot loss = 4.438063621520996, l1: 9.326907214320575e-05, l2: 0.0003505372878862545   Iteration 4 of 100, tot loss = 4.26802796125412, l1: 9.689562284620479e-05, l2: 0.00032990717227221467   Iteration 5 of 100, tot loss = 4.685793256759643, l1: 0.00010490250424481929, l2: 0.0003636768233263865   Iteration 6 of 100, tot loss = 4.806891004244487, l1: 0.00011120449683706586, l2: 0.0003694846066840303   Iteration 7 of 100, tot loss = 4.6432908943721225, l1: 0.00010867116881334888, l2: 0.0003556579239167539   Iteration 8 of 100, tot loss = 4.59705051779747, l1: 0.0001060685572156217, l2: 0.00035363649840292055   Iteration 9 of 100, tot loss = 4.811031262079875, l1: 0.00011026238401730855, l2: 0.00037084075059586513   Iteration 10 of 100, tot loss = 4.578876280784607, l1: 0.00010334316248190589, l2: 0.00035454447206575426   Iteration 11 of 100, tot loss = 4.523942708969116, l1: 0.00010428414547773586, l2: 0.0003481101329353723   Iteration 12 of 100, tot loss = 4.496100842952728, l1: 0.00010098472315197189, l2: 0.00034862536752674106   Iteration 13 of 100, tot loss = 4.465714876468365, l1: 0.00010035566251295117, l2: 0.00034621583136658254   Iteration 14 of 100, tot loss = 4.686752200126648, l1: 0.00010418844613013789, l2: 0.0003644867808491524   Iteration 15 of 100, tot loss = 4.671086931228638, l1: 0.00010553481018481156, l2: 0.000361573890162011   Iteration 16 of 100, tot loss = 4.594207555055618, l1: 0.00010512708240639768, l2: 0.00035429367926553823   Iteration 17 of 100, tot loss = 4.7172470373265885, l1: 0.00010626749109873986, l2: 0.00036545721677077166   Iteration 18 of 100, tot loss = 4.612355550130208, l1: 0.0001042054330658478, l2: 0.00035703012528958626   Iteration 19 of 100, tot loss = 4.66730280926353, l1: 0.00010585704646808536, l2: 0.00036087323602068384   Iteration 20 of 100, tot loss = 4.758260488510132, l1: 0.00010836608344106935, l2: 0.00036745996549143457   Iteration 21 of 100, tot loss = 4.982819103059315, l1: 0.0001117074633449582, l2: 0.0003865744490342747   Iteration 22 of 100, tot loss = 5.01469011740251, l1: 0.00011250252017899501, l2: 0.0003889664929540066   Iteration 23 of 100, tot loss = 5.057597616444463, l1: 0.00011363286972956975, l2: 0.00039212689317418665   Iteration 24 of 100, tot loss = 4.914745608965556, l1: 0.00011034084839896725, l2: 0.00038113371374493   Iteration 25 of 100, tot loss = 4.885838756561279, l1: 0.00010852990162675269, l2: 0.00038005397480446847   Iteration 26 of 100, tot loss = 4.9511957718775825, l1: 0.00010882659360224631, l2: 0.00038629298405094934   Iteration 27 of 100, tot loss = 4.999014730806704, l1: 0.00010914453997867424, l2: 0.0003907569340231863   Iteration 28 of 100, tot loss = 4.923047874655042, l1: 0.00010824996297742473, l2: 0.00038405482493024986   Iteration 29 of 100, tot loss = 4.821159272358336, l1: 0.00010639549258610651, l2: 0.0003757204349626434   Iteration 30 of 100, tot loss = 4.7775014241536455, l1: 0.00010509870323100282, l2: 0.0003726514391019009   Iteration 31 of 100, tot loss = 4.722723099493211, l1: 0.00010425603402602005, l2: 0.00036801627532939517   Iteration 32 of 100, tot loss = 4.755442976951599, l1: 0.00010496562310891022, l2: 0.00037057867484691087   Iteration 33 of 100, tot loss = 4.74250501574892, l1: 0.0001046270872669111, l2: 0.00036962341541904164   Iteration 34 of 100, tot loss = 4.730270988800946, l1: 0.00010372357096725061, l2: 0.00036930352935622283   Iteration 35 of 100, tot loss = 4.687254469735282, l1: 0.00010327423234619865, l2: 0.0003654512156832165   Iteration 36 of 100, tot loss = 4.7235614326265125, l1: 0.00010398671222194227, l2: 0.00036836943319132034   Iteration 37 of 100, tot loss = 4.729215969910493, l1: 0.00010403990893030061, l2: 0.0003688816894380678   Iteration 38 of 100, tot loss = 4.7329314256969255, l1: 0.00010403324488020429, l2: 0.00036925989881733825   Iteration 39 of 100, tot loss = 4.675339283087315, l1: 0.00010268921444852215, l2: 0.00036484471526749147   Iteration 40 of 100, tot loss = 4.644800317287445, l1: 0.00010158788691114751, l2: 0.00036289214658609127   Iteration 41 of 100, tot loss = 4.614272042018611, l1: 0.00010114965725068299, l2: 0.0003602775491677543   Iteration 42 of 100, tot loss = 4.6124302716482255, l1: 0.00010112855540813013, l2: 0.00036011447394100415   Iteration 43 of 100, tot loss = 4.650451787682467, l1: 0.00010178277790505807, l2: 0.0003632624030551808   Iteration 44 of 100, tot loss = 4.691934796896848, l1: 0.00010245413506543792, l2: 0.0003667393469764978   Iteration 45 of 100, tot loss = 4.674537478552924, l1: 0.0001024329049363991, l2: 0.00036502084533114815   Iteration 46 of 100, tot loss = 4.713316761929056, l1: 0.00010329232756466017, l2: 0.0003680393511667321   Iteration 47 of 100, tot loss = 4.707359658910873, l1: 0.0001031844047711855, l2: 0.00036755156365193505   Iteration 48 of 100, tot loss = 4.697673698266347, l1: 0.00010311865776202467, l2: 0.00036664871398291626   Iteration 49 of 100, tot loss = 4.651815317115005, l1: 0.00010250153414113922, l2: 0.00036267999961154954   Iteration 50 of 100, tot loss = 4.669572219848633, l1: 0.00010254358268866781, l2: 0.00036441364092752336   Iteration 51 of 100, tot loss = 4.658684739879534, l1: 0.00010263400448214573, l2: 0.00036323447106862626   Iteration 52 of 100, tot loss = 4.661359860346868, l1: 0.00010282495105247317, l2: 0.0003633110368704925   Iteration 53 of 100, tot loss = 4.643327020249277, l1: 0.00010190874295317972, l2: 0.0003624239608091917   Iteration 54 of 100, tot loss = 4.630985825150101, l1: 0.00010188961917905274, l2: 0.00036120896510072743   Iteration 55 of 100, tot loss = 4.608702239123258, l1: 0.00010126881798813966, l2: 0.0003596014078621837   Iteration 56 of 100, tot loss = 4.580008885690144, l1: 0.00010090104219021409, l2: 0.00035709984825059237   Iteration 57 of 100, tot loss = 4.570608954680593, l1: 0.00010071026471243321, l2: 0.000356350632245538   Iteration 58 of 100, tot loss = 4.583950400352478, l1: 0.00010117807700037796, l2: 0.00035721696421487964   Iteration 59 of 100, tot loss = 4.576933549622358, l1: 0.00010087074400162546, l2: 0.00035682261203242053   Iteration 60 of 100, tot loss = 4.6072614073753355, l1: 0.00010148234529575954, l2: 0.00035924379675028226   Iteration 61 of 100, tot loss = 4.589375124603022, l1: 0.00010109629034022724, l2: 0.00035784122358519037   Iteration 62 of 100, tot loss = 4.553983146144498, l1: 0.00010030960906841492, l2: 0.0003550887070765208   Iteration 63 of 100, tot loss = 4.53693725949242, l1: 9.99861206057378e-05, l2: 0.0003537076069430138   Iteration 64 of 100, tot loss = 4.541059672832489, l1: 9.992524894641974e-05, l2: 0.00035418071979620436   Iteration 65 of 100, tot loss = 4.53433938393226, l1: 9.981055000725274e-05, l2: 0.0003536233900096984   Iteration 66 of 100, tot loss = 4.542161074551669, l1: 0.0001000913146003079, l2: 0.000354124794698278   Iteration 67 of 100, tot loss = 4.549368858337402, l1: 0.00010043541792651583, l2: 0.00035450146937527374   Iteration 68 of 100, tot loss = 4.526519403738134, l1: 0.000100125081885113, l2: 0.00035252685997607736   Iteration 69 of 100, tot loss = 4.559795172318168, l1: 0.00010072333954166675, l2: 0.00035525617894946015   Iteration 70 of 100, tot loss = 4.555276571001325, l1: 0.00010107584260237802, l2: 0.00035445181586380516   Iteration 71 of 100, tot loss = 4.564759207443452, l1: 0.0001014114858034972, l2: 0.00035506443605876303   Iteration 72 of 100, tot loss = 4.628923250569238, l1: 0.00010236268119317376, l2: 0.00036052964424015954   Iteration 73 of 100, tot loss = 4.63113456229641, l1: 0.00010270374162881699, l2: 0.00036040971522882886   Iteration 74 of 100, tot loss = 4.61993037043391, l1: 0.00010241733941035597, l2: 0.00035957569796025654   Iteration 75 of 100, tot loss = 4.6165072504679365, l1: 0.00010235925166246791, l2: 0.0003592914738692343   Iteration 76 of 100, tot loss = 4.591543423502069, l1: 0.0001020829626404999, l2: 0.00035707138043730274   Iteration 77 of 100, tot loss = 4.587316060995127, l1: 0.00010176553797617846, l2: 0.00035696606871417976   Iteration 78 of 100, tot loss = 4.584050692044771, l1: 0.00010132762112427886, l2: 0.0003570774488019733   Iteration 79 of 100, tot loss = 4.572763956045803, l1: 0.00010100640793098137, l2: 0.000356269988549661   Iteration 80 of 100, tot loss = 4.580821973085404, l1: 0.00010133280093214126, l2: 0.00035674939754244407   Iteration 81 of 100, tot loss = 4.579860516536383, l1: 0.0001012173368642969, l2: 0.00035676871603182344   Iteration 82 of 100, tot loss = 4.592049685920157, l1: 0.00010166198031894477, l2: 0.00035754298981529003   Iteration 83 of 100, tot loss = 4.571524849857193, l1: 0.00010132203818332539, l2: 0.00035583044834300336   Iteration 84 of 100, tot loss = 4.604135683604649, l1: 0.00010206461367834847, l2: 0.0003583489562429133   Iteration 85 of 100, tot loss = 4.590123695485732, l1: 0.00010192921857678276, l2: 0.00035708315245916737   Iteration 86 of 100, tot loss = 4.6241040645643725, l1: 0.00010263630484100865, l2: 0.0003597741035560449   Iteration 87 of 100, tot loss = 4.6470004525677915, l1: 0.00010271920473314822, l2: 0.0003619808422658464   Iteration 88 of 100, tot loss = 4.638497750867497, l1: 0.00010272741904125063, l2: 0.0003611223579686008   Iteration 89 of 100, tot loss = 4.634900974423698, l1: 0.00010283883014207266, l2: 0.0003606512693928952   Iteration 90 of 100, tot loss = 4.661947343084547, l1: 0.00010333007537863321, l2: 0.0003628646612115618   Iteration 91 of 100, tot loss = 4.639436816121195, l1: 0.00010297969784805101, l2: 0.0003609639858880725   Iteration 92 of 100, tot loss = 4.649192545724952, l1: 0.00010318379463569727, l2: 0.00036173546228066857   Iteration 93 of 100, tot loss = 4.6638445136367634, l1: 0.00010321135027146327, l2: 0.0003631731033042294   Iteration 94 of 100, tot loss = 4.686839600826832, l1: 0.0001037344854341156, l2: 0.0003649494763986366   Iteration 95 of 100, tot loss = 4.685024246416594, l1: 0.00010394710414785597, l2: 0.0003645553221423669   Iteration 96 of 100, tot loss = 4.703223620851834, l1: 0.00010411244875285774, l2: 0.00036620991507637274   Iteration 97 of 100, tot loss = 4.6858992355386, l1: 0.00010385369927293534, l2: 0.000364736226008072   Iteration 98 of 100, tot loss = 4.694833086461437, l1: 0.00010377832044543204, l2: 0.0003657049896808493   Iteration 99 of 100, tot loss = 4.72433859651739, l1: 0.00010415713371581991, l2: 0.00036827672767979707   Iteration 100 of 100, tot loss = 4.734077308177948, l1: 0.00010424831016280223, l2: 0.0003691594224073924
   End of epoch 1381; saving model... 

Epoch 1382 of 2000
   Iteration 1 of 100, tot loss = 3.748903512954712, l1: 0.00010191007459070534, l2: 0.00027298027998767793   Iteration 2 of 100, tot loss = 3.9970980882644653, l1: 0.00010602925249258988, l2: 0.00029368056857492775   Iteration 3 of 100, tot loss = 4.600576639175415, l1: 0.00011275291277949388, l2: 0.0003473047593918939   Iteration 4 of 100, tot loss = 4.69525021314621, l1: 0.0001082682119886158, l2: 0.00036125681799603626   Iteration 5 of 100, tot loss = 5.0755836963653564, l1: 0.00011203378526261076, l2: 0.0003955245891120285   Iteration 6 of 100, tot loss = 5.52777882417043, l1: 0.00011840553027771723, l2: 0.0004343723558122292   Iteration 7 of 100, tot loss = 5.449833154678345, l1: 0.00011851368409614744, l2: 0.0004264696326572448   Iteration 8 of 100, tot loss = 5.275306850671768, l1: 0.0001184703560284106, l2: 0.0004090603324584663   Iteration 9 of 100, tot loss = 5.16674542427063, l1: 0.00011382662341929972, l2: 0.00040284792258818116   Iteration 10 of 100, tot loss = 5.198792815208435, l1: 0.00011555422242963687, l2: 0.0004043250606628135   Iteration 11 of 100, tot loss = 5.088180195201527, l1: 0.00010976109677523544, l2: 0.0003990569226020439   Iteration 12 of 100, tot loss = 4.98880672454834, l1: 0.00010762257018844441, l2: 0.0003912581014446914   Iteration 13 of 100, tot loss = 4.997766458071196, l1: 0.00010971630567487759, l2: 0.0003900603400185131   Iteration 14 of 100, tot loss = 4.945038318634033, l1: 0.00011025340167439676, l2: 0.00038425042917619327   Iteration 15 of 100, tot loss = 4.862145868937175, l1: 0.0001097499844036065, l2: 0.0003764646011404693   Iteration 16 of 100, tot loss = 4.845254600048065, l1: 0.00010968452579618315, l2: 0.0003748409308172995   Iteration 17 of 100, tot loss = 4.9434997895184685, l1: 0.00011229116053608082, l2: 0.0003820588137707947   Iteration 18 of 100, tot loss = 5.0257332060072155, l1: 0.00011287467229218844, l2: 0.0003896986454492435   Iteration 19 of 100, tot loss = 4.972320330770392, l1: 0.00011192392105984159, l2: 0.00038530810975077515   Iteration 20 of 100, tot loss = 4.885407173633576, l1: 0.00011172596205142327, l2: 0.00037681475296267306   Iteration 21 of 100, tot loss = 4.7540139470781595, l1: 0.00010845088711773445, l2: 0.0003669505053299612   Iteration 22 of 100, tot loss = 4.712268460880626, l1: 0.00010665942847315984, l2: 0.0003645674157517285   Iteration 23 of 100, tot loss = 4.654934986777928, l1: 0.00010575945096839543, l2: 0.0003597340464298411   Iteration 24 of 100, tot loss = 4.601593047380447, l1: 0.00010443806271117258, l2: 0.0003557212409456649   Iteration 25 of 100, tot loss = 4.603276891708374, l1: 0.00010501230135560036, l2: 0.00035531538596842436   Iteration 26 of 100, tot loss = 4.5559070477118855, l1: 0.00010474738379483685, l2: 0.0003508433191410194   Iteration 27 of 100, tot loss = 4.569619055147524, l1: 0.00010460659244994599, l2: 0.00035235531093483723   Iteration 28 of 100, tot loss = 4.51484044109072, l1: 0.00010322327138315554, l2: 0.00034826077067659104   Iteration 29 of 100, tot loss = 4.657302601584073, l1: 0.00010488903002436112, l2: 0.0003608412275909735   Iteration 30 of 100, tot loss = 4.706690192222595, l1: 0.00010570129209857745, l2: 0.00036496772372629493   Iteration 31 of 100, tot loss = 4.8308748506730605, l1: 0.00010820223378100162, l2: 0.0003748852468561381   Iteration 32 of 100, tot loss = 4.768455654382706, l1: 0.00010713285109886783, l2: 0.0003697127094710595   Iteration 33 of 100, tot loss = 4.705389282920144, l1: 0.00010584946280825093, l2: 0.00036468946156233096   Iteration 34 of 100, tot loss = 4.690970434862025, l1: 0.00010576972871443585, l2: 0.0003633273102904615   Iteration 35 of 100, tot loss = 4.756764248439244, l1: 0.00010756084728719932, l2: 0.00036811557448735197   Iteration 36 of 100, tot loss = 4.719114310211605, l1: 0.00010676645029484967, l2: 0.0003651449773719327   Iteration 37 of 100, tot loss = 4.691023356205708, l1: 0.00010611822201738235, l2: 0.00036298411007895057   Iteration 38 of 100, tot loss = 4.696037825785186, l1: 0.0001062386666154979, l2: 0.00036336511287740186   Iteration 39 of 100, tot loss = 4.7328126430511475, l1: 0.0001065404410324752, l2: 0.0003667408197473448   Iteration 40 of 100, tot loss = 4.6810500681400296, l1: 0.00010567546323727583, l2: 0.00036242953974578994   Iteration 41 of 100, tot loss = 4.67905068397522, l1: 0.00010538911862190949, l2: 0.00036251594535381785   Iteration 42 of 100, tot loss = 4.676053665933155, l1: 0.0001057124590304392, l2: 0.0003618929029991185   Iteration 43 of 100, tot loss = 4.7386986433073535, l1: 0.0001065222968463786, l2: 0.00036734756357090676   Iteration 44 of 100, tot loss = 4.776112193411047, l1: 0.00010694261320829604, l2: 0.00037066860287185676   Iteration 45 of 100, tot loss = 4.758610280354818, l1: 0.0001064790289900783, l2: 0.000369381995470677   Iteration 46 of 100, tot loss = 4.803676750348962, l1: 0.00010716492618920039, l2: 0.00037320274461070886   Iteration 47 of 100, tot loss = 4.814690021758384, l1: 0.00010656748054316267, l2: 0.0003749015174454395   Iteration 48 of 100, tot loss = 4.8425836364428205, l1: 0.00010681556750569143, l2: 0.0003774427923417534   Iteration 49 of 100, tot loss = 4.846772982149708, l1: 0.00010712029429019562, l2: 0.0003775570001714502   Iteration 50 of 100, tot loss = 4.830470714569092, l1: 0.00010732632290455513, l2: 0.0003757207447779365   Iteration 51 of 100, tot loss = 4.839258343565698, l1: 0.0001079774110443324, l2: 0.0003759484191986677   Iteration 52 of 100, tot loss = 4.833366779180674, l1: 0.00010810575216825012, l2: 0.000375230922285342   Iteration 53 of 100, tot loss = 4.8043493279870955, l1: 0.00010785681551392629, l2: 0.00037257811373082113   Iteration 54 of 100, tot loss = 4.791876850304781, l1: 0.00010722365132770156, l2: 0.0003719640296400973   Iteration 55 of 100, tot loss = 4.818841097571633, l1: 0.00010718127649108117, l2: 0.0003747028288092803   Iteration 56 of 100, tot loss = 4.8257733796324045, l1: 0.00010726505739252648, l2: 0.00037531227592678206   Iteration 57 of 100, tot loss = 4.829936726051464, l1: 0.00010763925105525312, l2: 0.0003753544164314997   Iteration 58 of 100, tot loss = 4.809258078706676, l1: 0.0001075447028691495, l2: 0.00037338109967973213   Iteration 59 of 100, tot loss = 4.782702583377644, l1: 0.00010689444062043518, l2: 0.0003713758127621801   Iteration 60 of 100, tot loss = 4.796117997169494, l1: 0.00010672890615145055, l2: 0.0003728828887687996   Iteration 61 of 100, tot loss = 4.765979086766477, l1: 0.0001064579097504468, l2: 0.00037013999421195296   Iteration 62 of 100, tot loss = 4.7815807173329015, l1: 0.00010694378344792753, l2: 0.00037121428308197325   Iteration 63 of 100, tot loss = 4.780552515907893, l1: 0.00010684265321793241, l2: 0.00037121259383783334   Iteration 64 of 100, tot loss = 4.800270967185497, l1: 0.00010686761834222125, l2: 0.0003731594738383137   Iteration 65 of 100, tot loss = 4.773633058254536, l1: 0.00010649580452501631, l2: 0.00037086749664292886   Iteration 66 of 100, tot loss = 4.758902889309508, l1: 0.00010607506159100343, l2: 0.00036981522289076537   Iteration 67 of 100, tot loss = 4.748983539752106, l1: 0.00010565348028608445, l2: 0.0003692448693466609   Iteration 68 of 100, tot loss = 4.739257700303021, l1: 0.00010587511978348416, l2: 0.0003680506458348485   Iteration 69 of 100, tot loss = 4.785214057867078, l1: 0.00010623688781313409, l2: 0.0003722845142255065   Iteration 70 of 100, tot loss = 4.804741621017456, l1: 0.00010641774154334728, l2: 0.0003740564167466281   Iteration 71 of 100, tot loss = 4.826864309713874, l1: 0.00010695030177372809, l2: 0.000375736125623545   Iteration 72 of 100, tot loss = 4.8236997326215105, l1: 0.00010698941969167208, l2: 0.00037538055039476603   Iteration 73 of 100, tot loss = 4.815285467121699, l1: 0.0001071876154938228, l2: 0.000374340927882171   Iteration 74 of 100, tot loss = 4.806711048693271, l1: 0.00010714661314252865, l2: 0.0003735244886106434   Iteration 75 of 100, tot loss = 4.810717175801595, l1: 0.00010687893780414015, l2: 0.00037419277631367244   Iteration 76 of 100, tot loss = 4.823434697954278, l1: 0.00010730275052513829, l2: 0.0003750407160719317   Iteration 77 of 100, tot loss = 4.781995485355328, l1: 0.00010645794312737265, l2: 0.0003717416021844384   Iteration 78 of 100, tot loss = 4.79695798494877, l1: 0.0001067289452830431, l2: 0.0003729668498682515   Iteration 79 of 100, tot loss = 4.797283948222293, l1: 0.00010656871998872714, l2: 0.00037315967181458765   Iteration 80 of 100, tot loss = 4.805861833691597, l1: 0.00010642071069923986, l2: 0.0003741654697478225   Iteration 81 of 100, tot loss = 4.813849399119247, l1: 0.00010624977760555878, l2: 0.0003751351595619107   Iteration 82 of 100, tot loss = 4.826746937705249, l1: 0.00010667149652407516, l2: 0.00037600319446050333   Iteration 83 of 100, tot loss = 4.842817217470651, l1: 0.00010690651461446984, l2: 0.0003773752039442882   Iteration 84 of 100, tot loss = 4.844981440476009, l1: 0.00010668207951745163, l2: 0.0003778160613971219   Iteration 85 of 100, tot loss = 4.859716373331406, l1: 0.00010684336435481194, l2: 0.0003791282699025674   Iteration 86 of 100, tot loss = 4.8461079597473145, l1: 0.00010677531357688103, l2: 0.0003778354793910186   Iteration 87 of 100, tot loss = 4.835729505823947, l1: 0.0001065134162055837, l2: 0.00037705953138941984   Iteration 88 of 100, tot loss = 4.83497108112682, l1: 0.0001063410574137048, l2: 0.0003771560476789918   Iteration 89 of 100, tot loss = 4.820950438467304, l1: 0.00010598121523063139, l2: 0.00037611382540898776   Iteration 90 of 100, tot loss = 4.816339831882053, l1: 0.0001060037654193972, l2: 0.0003756302146131121   Iteration 91 of 100, tot loss = 4.814415171906188, l1: 0.00010592899299488955, l2: 0.0003755125209943719   Iteration 92 of 100, tot loss = 4.790417951086293, l1: 0.00010529059126502429, l2: 0.0003737512006805248   Iteration 93 of 100, tot loss = 4.783669266649472, l1: 0.0001049196492800469, l2: 0.00037344727392611346   Iteration 94 of 100, tot loss = 4.771396233680401, l1: 0.00010474699337600156, l2: 0.00037239262646254886   Iteration 95 of 100, tot loss = 4.749313442330611, l1: 0.00010452397829411846, l2: 0.0003704073625206212   Iteration 96 of 100, tot loss = 4.744334273040295, l1: 0.00010439044854138047, l2: 0.00037004297557056515   Iteration 97 of 100, tot loss = 4.747992011689648, l1: 0.0001045602645164538, l2: 0.0003702389332135784   Iteration 98 of 100, tot loss = 4.750787844463271, l1: 0.00010450430510195011, l2: 0.0003705744757655561   Iteration 99 of 100, tot loss = 4.7363864147301875, l1: 0.00010394693820883115, l2: 0.00036969169984028366   Iteration 100 of 100, tot loss = 4.719124119281769, l1: 0.00010370662421337329, l2: 0.0003682057842524955
   End of epoch 1382; saving model... 

Epoch 1383 of 2000
   Iteration 1 of 100, tot loss = 3.423344373703003, l1: 0.00010039169865194708, l2: 0.00024194274737965316   Iteration 2 of 100, tot loss = 3.5428028106689453, l1: 9.42440346989315e-05, l2: 0.00026003625680459663   Iteration 3 of 100, tot loss = 3.41814915339152, l1: 9.209842877074455e-05, l2: 0.00024971649205933016   Iteration 4 of 100, tot loss = 4.705024600028992, l1: 0.0001109686945710564, l2: 0.00035953377664554864   Iteration 5 of 100, tot loss = 4.271211385726929, l1: 9.85605627647601e-05, l2: 0.00032856058387551455   Iteration 6 of 100, tot loss = 4.861440936724345, l1: 0.00010924945308943279, l2: 0.0003768946456451279   Iteration 7 of 100, tot loss = 4.98857569694519, l1: 0.00010365118838048406, l2: 0.00039520638737095784   Iteration 8 of 100, tot loss = 4.852888971567154, l1: 9.947736089088721e-05, l2: 0.00038581153967243154   Iteration 9 of 100, tot loss = 4.88135912683275, l1: 0.00010267658944940194, l2: 0.0003854593297647726   Iteration 10 of 100, tot loss = 4.983495736122132, l1: 0.00010620583852869459, l2: 0.0003921437441022135   Iteration 11 of 100, tot loss = 5.161555528640747, l1: 0.00011009452084015885, l2: 0.00040606104117945176   Iteration 12 of 100, tot loss = 5.069470465183258, l1: 0.00010944015593850054, l2: 0.00039750690120854415   Iteration 13 of 100, tot loss = 4.961829057106605, l1: 0.0001080736222390372, l2: 0.0003881092919842698   Iteration 14 of 100, tot loss = 5.008607506752014, l1: 0.00010801723744537282, l2: 0.0003928435211751743   Iteration 15 of 100, tot loss = 5.049897845586141, l1: 0.0001093300879195643, l2: 0.00039565970558517923   Iteration 16 of 100, tot loss = 5.106304869055748, l1: 0.00010956420101138065, l2: 0.00040106629330693977   Iteration 17 of 100, tot loss = 4.979018870521994, l1: 0.00010787821605952238, l2: 0.0003900236778837793   Iteration 18 of 100, tot loss = 4.846624599562751, l1: 0.00010615880698120843, l2: 0.0003785036589963258   Iteration 19 of 100, tot loss = 4.9493296899293595, l1: 0.000108329595317819, l2: 0.00038660337955470343   Iteration 20 of 100, tot loss = 4.842008829116821, l1: 0.00010690606286516413, l2: 0.00037729482501163146   Iteration 21 of 100, tot loss = 4.829633417583647, l1: 0.00010679703111582923, l2: 0.00037616631639788726   Iteration 22 of 100, tot loss = 4.787409023805098, l1: 0.0001061714769589757, l2: 0.0003725694305930202   Iteration 23 of 100, tot loss = 4.656935209813326, l1: 0.00010316813495592214, l2: 0.0003625253910127946   Iteration 24 of 100, tot loss = 4.666578526298205, l1: 0.00010345550905791849, l2: 0.000363202347822759   Iteration 25 of 100, tot loss = 4.538694610595703, l1: 0.00010054819198558107, l2: 0.00035332127299625427   Iteration 26 of 100, tot loss = 4.547330819643461, l1: 0.00010055432581933789, l2: 0.00035417876083206816   Iteration 27 of 100, tot loss = 4.5221030270611795, l1: 0.00010097067219989926, l2: 0.0003512396355357918   Iteration 28 of 100, tot loss = 4.493783712387085, l1: 0.00010130332339031156, l2: 0.00034807505237820024   Iteration 29 of 100, tot loss = 4.394776262086014, l1: 9.905597120262522e-05, l2: 0.00034042165949860393   Iteration 30 of 100, tot loss = 4.375072646141052, l1: 9.807169177899293e-05, l2: 0.0003394355769463194   Iteration 31 of 100, tot loss = 4.354401780712989, l1: 9.796581845524e-05, l2: 0.000337474363116217   Iteration 32 of 100, tot loss = 4.348565302789211, l1: 9.78787205667686e-05, l2: 0.0003369778128217149   Iteration 33 of 100, tot loss = 4.362248572436246, l1: 9.898079687528397e-05, l2: 0.0003372440630548888   Iteration 34 of 100, tot loss = 4.463186130804174, l1: 0.00010053933200491455, l2: 0.00034577928368782843   Iteration 35 of 100, tot loss = 4.54924693107605, l1: 0.00010207620247716217, l2: 0.0003528484929120168   Iteration 36 of 100, tot loss = 4.536963336997562, l1: 0.00010159260748170911, l2: 0.0003521037283967922   Iteration 37 of 100, tot loss = 4.514048434592582, l1: 0.00010096518913947826, l2: 0.00035043965633392235   Iteration 38 of 100, tot loss = 4.498691182387502, l1: 0.00010026260104878977, l2: 0.0003496065194332531   Iteration 39 of 100, tot loss = 4.507095495859782, l1: 0.00010006645094015253, l2: 0.0003506431004587704   Iteration 40 of 100, tot loss = 4.563382995128632, l1: 0.00010122919657078455, l2: 0.00035510910565790256   Iteration 41 of 100, tot loss = 4.57999408535841, l1: 0.00010175867034221727, l2: 0.00035624073998627773   Iteration 42 of 100, tot loss = 4.555674047697158, l1: 0.00010209430244382627, l2: 0.00035347310386853116   Iteration 43 of 100, tot loss = 4.59071023519649, l1: 0.00010304922211934205, l2: 0.000356021803210836   Iteration 44 of 100, tot loss = 4.591422747481953, l1: 0.00010276602907569296, l2: 0.0003563762475096155   Iteration 45 of 100, tot loss = 4.587353860007392, l1: 0.00010328468755081607, l2: 0.00035545070002424635   Iteration 46 of 100, tot loss = 4.639788155970366, l1: 0.00010372366515461498, l2: 0.00036025515209589884   Iteration 47 of 100, tot loss = 4.603530630152276, l1: 0.00010313495612477864, l2: 0.0003572181083925782   Iteration 48 of 100, tot loss = 4.561097025871277, l1: 0.0001020403976781381, l2: 0.00035406930661944597   Iteration 49 of 100, tot loss = 4.515871972453837, l1: 0.00010140391554605045, l2: 0.0003501832831475161   Iteration 50 of 100, tot loss = 4.545976572036743, l1: 0.00010174763468967285, l2: 0.00035285002290038394   Iteration 51 of 100, tot loss = 4.51538185044831, l1: 0.0001012082785903248, l2: 0.00035032990711567665   Iteration 52 of 100, tot loss = 4.502367198467255, l1: 0.00010095309649981433, l2: 0.0003492836244381928   Iteration 53 of 100, tot loss = 4.470809878043409, l1: 0.00010045384665575529, l2: 0.0003466271421375667   Iteration 54 of 100, tot loss = 4.506461651236923, l1: 0.00010044834326200308, l2: 0.00035019782323312637   Iteration 55 of 100, tot loss = 4.515697431564331, l1: 0.00010071698072186502, l2: 0.0003508527638835155   Iteration 56 of 100, tot loss = 4.503830573388508, l1: 0.00010048816557173268, l2: 0.00034989489280893134   Iteration 57 of 100, tot loss = 4.531217219536765, l1: 0.00010057724725093546, l2: 0.0003525444754223715   Iteration 58 of 100, tot loss = 4.552852552512596, l1: 0.00010094456310666986, l2: 0.00035434069361834757   Iteration 59 of 100, tot loss = 4.550385406461813, l1: 0.00010119020694450443, l2: 0.0003538483354732615   Iteration 60 of 100, tot loss = 4.537832852204641, l1: 0.00010126722900167806, l2: 0.0003525160580466036   Iteration 61 of 100, tot loss = 4.509472616383286, l1: 0.00010104454176158827, l2: 0.00034990272132611117   Iteration 62 of 100, tot loss = 4.5169832514178365, l1: 0.00010075759582245917, l2: 0.0003509407310517535   Iteration 63 of 100, tot loss = 4.534699368098425, l1: 0.00010098779678209832, l2: 0.0003524821416552489   Iteration 64 of 100, tot loss = 4.546210948377848, l1: 0.00010132158598707974, l2: 0.00035329951037965657   Iteration 65 of 100, tot loss = 4.56892440869258, l1: 0.00010148071154477432, l2: 0.00035541173084101713   Iteration 66 of 100, tot loss = 4.5352070584441675, l1: 0.00010090896725931529, l2: 0.0003526117399098549   Iteration 67 of 100, tot loss = 4.49429109381206, l1: 0.000100081642623122, l2: 0.000349347468130221   Iteration 68 of 100, tot loss = 4.507445337141261, l1: 9.969460819882122e-05, l2: 0.0003510499268218957   Iteration 69 of 100, tot loss = 4.484837202058322, l1: 9.891726823179457e-05, l2: 0.0003495664532765395   Iteration 70 of 100, tot loss = 4.492372718879155, l1: 9.926435034555783e-05, l2: 0.0003499729224131443   Iteration 71 of 100, tot loss = 4.462661697830953, l1: 9.880181562653939e-05, l2: 0.000347464355028523   Iteration 72 of 100, tot loss = 4.4588379677799015, l1: 9.875463325038759e-05, l2: 0.00034712916446248226   Iteration 73 of 100, tot loss = 4.442527651786804, l1: 9.857067497038883e-05, l2: 0.0003456820910062947   Iteration 74 of 100, tot loss = 4.444442418781486, l1: 9.898105113156694e-05, l2: 0.0003454631919708304   Iteration 75 of 100, tot loss = 4.430914155642191, l1: 9.882833090766022e-05, l2: 0.0003442630858626217   Iteration 76 of 100, tot loss = 4.4421339050719615, l1: 9.883188690046633e-05, l2: 0.00034538150526044006   Iteration 77 of 100, tot loss = 4.409864272390093, l1: 9.812083764889205e-05, l2: 0.00034286559112642155   Iteration 78 of 100, tot loss = 4.417519632058266, l1: 9.809708544941177e-05, l2: 0.00034365487954346463   Iteration 79 of 100, tot loss = 4.42131804363637, l1: 9.828443281445357e-05, l2: 0.00034384737347541497   Iteration 80 of 100, tot loss = 4.447197349369526, l1: 9.859451088232163e-05, l2: 0.00034612522558745694   Iteration 81 of 100, tot loss = 4.482629583205706, l1: 9.920989427489896e-05, l2: 0.00034905306583149704   Iteration 82 of 100, tot loss = 4.494842431894162, l1: 9.95890804472033e-05, l2: 0.00034989516457355376   Iteration 83 of 100, tot loss = 4.506165546107005, l1: 9.935603687274607e-05, l2: 0.00035126051947569856   Iteration 84 of 100, tot loss = 4.51226006377311, l1: 9.936966329232313e-05, l2: 0.000351856344682996   Iteration 85 of 100, tot loss = 4.539078596059014, l1: 9.981226796619868e-05, l2: 0.0003540955927318839   Iteration 86 of 100, tot loss = 4.558366943237393, l1: 9.995435171139285e-05, l2: 0.00035588234391599504   Iteration 87 of 100, tot loss = 4.559494710516655, l1: 0.0001001755184518714, l2: 0.00035577395348140606   Iteration 88 of 100, tot loss = 4.562086789445444, l1: 0.0001001774453884123, l2: 0.0003560312346632021   Iteration 89 of 100, tot loss = 4.539597590317887, l1: 9.979165447424584e-05, l2: 0.0003541681056795119   Iteration 90 of 100, tot loss = 4.52794113026725, l1: 9.964536515730692e-05, l2: 0.0003531487487230657   Iteration 91 of 100, tot loss = 4.526930446153159, l1: 9.993256761655364e-05, l2: 0.00035276047784354086   Iteration 92 of 100, tot loss = 4.501977305049482, l1: 9.929745196448336e-05, l2: 0.00035090027924287943   Iteration 93 of 100, tot loss = 4.510029763303777, l1: 9.941868906495191e-05, l2: 0.0003515842878597698   Iteration 94 of 100, tot loss = 4.504770219326019, l1: 9.931528163972271e-05, l2: 0.0003511617407561874   Iteration 95 of 100, tot loss = 4.48982439668555, l1: 9.90231083338394e-05, l2: 0.0003499593316136222   Iteration 96 of 100, tot loss = 4.47823491320014, l1: 9.875718395354245e-05, l2: 0.0003490663078385599   Iteration 97 of 100, tot loss = 4.4822688704913425, l1: 9.881038365455959e-05, l2: 0.00034941650372593826   Iteration 98 of 100, tot loss = 4.467529519480102, l1: 9.848858514617729e-05, l2: 0.0003482643672269864   Iteration 99 of 100, tot loss = 4.478680440873811, l1: 9.872266063478427e-05, l2: 0.0003491453841126364   Iteration 100 of 100, tot loss = 4.488465956449509, l1: 9.890872723190115e-05, l2: 0.0003499378691776656
   End of epoch 1383; saving model... 

Epoch 1384 of 2000
   Iteration 1 of 100, tot loss = 6.178197383880615, l1: 0.00012815308582503349, l2: 0.0004896666505374014   Iteration 2 of 100, tot loss = 5.886063575744629, l1: 0.00012840494309784845, l2: 0.0004602013941621408   Iteration 3 of 100, tot loss = 5.42211373647054, l1: 0.0001277561435320725, l2: 0.00041445522219873965   Iteration 4 of 100, tot loss = 4.773195326328278, l1: 0.00011283289313723799, l2: 0.0003644866337708663   Iteration 5 of 100, tot loss = 4.910664892196655, l1: 0.00011860377417178824, l2: 0.00037246271094772967   Iteration 6 of 100, tot loss = 5.1710472504297895, l1: 0.00012103456901968457, l2: 0.0003960701506002806   Iteration 7 of 100, tot loss = 5.0369618620191305, l1: 0.00011758106328280909, l2: 0.0003861151199089363   Iteration 8 of 100, tot loss = 5.193949729204178, l1: 0.00011741042817448033, l2: 0.00040198454189521726   Iteration 9 of 100, tot loss = 5.437972942988078, l1: 0.00012039683012214179, l2: 0.000423400460729479   Iteration 10 of 100, tot loss = 5.362656092643737, l1: 0.00011949964755331166, l2: 0.0004167659571976401   Iteration 11 of 100, tot loss = 5.1974922743710605, l1: 0.0001186687983466651, l2: 0.0004010804259451106   Iteration 12 of 100, tot loss = 5.5666543046633405, l1: 0.0001233849992180088, l2: 0.00043328043223785545   Iteration 13 of 100, tot loss = 5.366886047216562, l1: 0.00011941043451840345, l2: 0.00041727817062145244   Iteration 14 of 100, tot loss = 5.440009202275958, l1: 0.00012131206260944185, l2: 0.00042268885798486214   Iteration 15 of 100, tot loss = 5.277948125203451, l1: 0.00011890045619414499, l2: 0.0004088943562237546   Iteration 16 of 100, tot loss = 5.361656218767166, l1: 0.0001212997512993752, l2: 0.00041486586724204244   Iteration 17 of 100, tot loss = 5.22544069851146, l1: 0.00011997389639659291, l2: 0.00040257016970666456   Iteration 18 of 100, tot loss = 5.111567536989848, l1: 0.0001189267653292821, l2: 0.0003922299858661265   Iteration 19 of 100, tot loss = 5.335472044191863, l1: 0.00012182081570101313, l2: 0.00041172638501800403   Iteration 20 of 100, tot loss = 5.2608266234397885, l1: 0.00011918449927179608, l2: 0.0004068981601449195   Iteration 21 of 100, tot loss = 5.143224772952852, l1: 0.00011671343444114817, l2: 0.00039760903954239827   Iteration 22 of 100, tot loss = 5.0643135634335605, l1: 0.00011575260313375938, l2: 0.0003906787500124086   Iteration 23 of 100, tot loss = 5.012192943821782, l1: 0.00011493658878506444, l2: 0.00038628270292816603   Iteration 24 of 100, tot loss = 5.05228653550148, l1: 0.00011594304881631008, l2: 0.0003892856002494227   Iteration 25 of 100, tot loss = 4.943693351745606, l1: 0.00011393483640858903, l2: 0.00038043449399992823   Iteration 26 of 100, tot loss = 4.885735750198364, l1: 0.0001115485100364402, l2: 0.0003770250600959676   Iteration 27 of 100, tot loss = 4.9056321779886884, l1: 0.00011229061338882375, l2: 0.0003782726007220508   Iteration 28 of 100, tot loss = 4.878808856010437, l1: 0.00011055687134233137, l2: 0.00037732401168406274   Iteration 29 of 100, tot loss = 4.9655789835699675, l1: 0.00011233000297015051, l2: 0.0003842278935641436   Iteration 30 of 100, tot loss = 5.024804035822551, l1: 0.00011305127882224041, l2: 0.00038942912263640516   Iteration 31 of 100, tot loss = 5.094388731064335, l1: 0.00011481661591284548, l2: 0.0003946222554546811   Iteration 32 of 100, tot loss = 5.090746656060219, l1: 0.0001144619440083261, l2: 0.00039461272081098286   Iteration 33 of 100, tot loss = 5.110800670854973, l1: 0.00011451012500346582, l2: 0.0003965699420818551   Iteration 34 of 100, tot loss = 5.141194161246805, l1: 0.00011354892397802734, l2: 0.00040057049173971307   Iteration 35 of 100, tot loss = 5.166287408556257, l1: 0.00011420187189027534, l2: 0.00040242686851083167   Iteration 36 of 100, tot loss = 5.1894599596659345, l1: 0.00011389026268362714, l2: 0.00040505573189067136   Iteration 37 of 100, tot loss = 5.1965653574144515, l1: 0.00011366868568222492, l2: 0.00040598784972963904   Iteration 38 of 100, tot loss = 5.220251070825677, l1: 0.00011392921719801195, l2: 0.00040809588972479105   Iteration 39 of 100, tot loss = 5.182916470063039, l1: 0.00011358314944887892, l2: 0.000404708497136688   Iteration 40 of 100, tot loss = 5.140598028898239, l1: 0.00011332055792081518, l2: 0.00040073924465104936   Iteration 41 of 100, tot loss = 5.169256867432013, l1: 0.00011335905325789255, l2: 0.00040356663237439423   Iteration 42 of 100, tot loss = 5.133604549226307, l1: 0.00011205079231114636, l2: 0.0004013096615727547   Iteration 43 of 100, tot loss = 5.1601980120636695, l1: 0.00011198932186870965, l2: 0.00040403047785011315   Iteration 44 of 100, tot loss = 5.105134324593977, l1: 0.00011104672748016046, l2: 0.00039946670354269867   Iteration 45 of 100, tot loss = 5.067585817972819, l1: 0.00011036754949600436, l2: 0.00039639103116415855   Iteration 46 of 100, tot loss = 5.055803133093792, l1: 0.00011081162641779014, l2: 0.0003947686855099164   Iteration 47 of 100, tot loss = 5.029075800104344, l1: 0.00011005648496954582, l2: 0.0003928510937220516   Iteration 48 of 100, tot loss = 5.065527732173602, l1: 0.0001110528879356328, l2: 0.00039549988499250804   Iteration 49 of 100, tot loss = 5.0579239446289685, l1: 0.00011076572003990545, l2: 0.0003950266741876661   Iteration 50 of 100, tot loss = 5.012167921066284, l1: 0.00010961418993247208, l2: 0.00039160260173957793   Iteration 51 of 100, tot loss = 4.993245732550528, l1: 0.00010919203893589678, l2: 0.0003901325341989743   Iteration 52 of 100, tot loss = 5.027607606007503, l1: 0.00010940721564950833, l2: 0.0003933535447756115   Iteration 53 of 100, tot loss = 5.0218118541645556, l1: 0.00010973576697650506, l2: 0.00039244541856635996   Iteration 54 of 100, tot loss = 5.034162706798977, l1: 0.00010999524258497533, l2: 0.00039342102805945886   Iteration 55 of 100, tot loss = 5.0132978309284555, l1: 0.00011000990839833817, l2: 0.00039131987458941613   Iteration 56 of 100, tot loss = 5.001408410923822, l1: 0.00010997914634052515, l2: 0.00039016169466776773   Iteration 57 of 100, tot loss = 5.008493712073879, l1: 0.0001101260789718046, l2: 0.00039072329149859256   Iteration 58 of 100, tot loss = 5.033946707330901, l1: 0.00011049856471674565, l2: 0.00039289610516601083   Iteration 59 of 100, tot loss = 4.978075540671914, l1: 0.00010942085596764365, l2: 0.00038838669730713435   Iteration 60 of 100, tot loss = 4.984251670042673, l1: 0.00010972551899612881, l2: 0.00038869964676753926   Iteration 61 of 100, tot loss = 4.993219457688879, l1: 0.00011007137096593859, l2: 0.00038925057361055104   Iteration 62 of 100, tot loss = 4.990613918150625, l1: 0.00010939785105649443, l2: 0.00038966353965787996   Iteration 63 of 100, tot loss = 4.987063858244154, l1: 0.0001094742101341826, l2: 0.00038923217456728694   Iteration 64 of 100, tot loss = 4.964135300368071, l1: 0.00010900068775754335, l2: 0.0003874128410643607   Iteration 65 of 100, tot loss = 4.922108268737793, l1: 0.00010821870197944988, l2: 0.0003839921237578472   Iteration 66 of 100, tot loss = 4.912848494269631, l1: 0.00010807662544142327, l2: 0.00038320822314937794   Iteration 67 of 100, tot loss = 4.873638672615165, l1: 0.00010736418058079278, l2: 0.0003799996857875402   Iteration 68 of 100, tot loss = 4.8749958767610435, l1: 0.00010671804937052965, l2: 0.0003807815367455238   Iteration 69 of 100, tot loss = 4.863868665004122, l1: 0.00010638020451195821, l2: 0.00038000666034455156   Iteration 70 of 100, tot loss = 4.895908028738839, l1: 0.00010715284614499459, l2: 0.00038243795466509514   Iteration 71 of 100, tot loss = 4.865388786289054, l1: 0.00010669941491895014, l2: 0.00037983946176737703   Iteration 72 of 100, tot loss = 4.852479431364271, l1: 0.00010666027138641867, l2: 0.0003785876698303683   Iteration 73 of 100, tot loss = 4.841469379320537, l1: 0.00010641106404962133, l2: 0.00037773587196357293   Iteration 74 of 100, tot loss = 4.831452401908669, l1: 0.00010603468355249199, l2: 0.00037711055424674794   Iteration 75 of 100, tot loss = 4.84756908416748, l1: 0.00010624125934555195, l2: 0.00037851564721980445   Iteration 76 of 100, tot loss = 4.84358737343236, l1: 0.00010615742494816291, l2: 0.0003782013106836913   Iteration 77 of 100, tot loss = 4.869784528558904, l1: 0.00010671546411898604, l2: 0.00038026298669185005   Iteration 78 of 100, tot loss = 4.92665922947419, l1: 0.00010762508440492615, l2: 0.00038504083689379814   Iteration 79 of 100, tot loss = 4.896995749654649, l1: 0.00010727145748700699, l2: 0.0003824281158836417   Iteration 80 of 100, tot loss = 4.917052084207535, l1: 0.00010787932183120574, l2: 0.00038382588481908895   Iteration 81 of 100, tot loss = 4.903030860571214, l1: 0.00010752962304122551, l2: 0.00038277346121268   Iteration 82 of 100, tot loss = 4.9071658704339, l1: 0.00010790772715309442, l2: 0.00038280885822065855   Iteration 83 of 100, tot loss = 4.911257766815553, l1: 0.00010795232438214449, l2: 0.0003831734505961705   Iteration 84 of 100, tot loss = 4.904993903069269, l1: 0.00010782596633535355, l2: 0.00038267342201184614   Iteration 85 of 100, tot loss = 4.900847704270307, l1: 0.00010771034195546664, l2: 0.0003823744264938047   Iteration 86 of 100, tot loss = 4.9059568283169765, l1: 0.00010782855480461284, l2: 0.00038276712608464105   Iteration 87 of 100, tot loss = 4.896455863426471, l1: 0.00010726307873378626, l2: 0.00038238250548100293   Iteration 88 of 100, tot loss = 4.901102667505091, l1: 0.00010743271592549387, l2: 0.0003826775482593803   Iteration 89 of 100, tot loss = 4.882878397287947, l1: 0.00010710495070873477, l2: 0.00038118288624021905   Iteration 90 of 100, tot loss = 4.898874968952603, l1: 0.00010749197309552174, l2: 0.0003823955205411443   Iteration 91 of 100, tot loss = 4.882535185132708, l1: 0.00010740586259379307, l2: 0.00038084765265511536   Iteration 92 of 100, tot loss = 4.867343112178471, l1: 0.0001068988010007635, l2: 0.0003798355069391065   Iteration 93 of 100, tot loss = 4.8483201893427035, l1: 0.00010673923554682543, l2: 0.00037809278016724695   Iteration 94 of 100, tot loss = 4.8352127430286815, l1: 0.00010668513688018565, l2: 0.0003768361340452758   Iteration 95 of 100, tot loss = 4.819832530774568, l1: 0.00010644790485170424, l2: 0.00037553534509099433   Iteration 96 of 100, tot loss = 4.807146154344082, l1: 0.00010629440271259227, l2: 0.00037442020963377826   Iteration 97 of 100, tot loss = 4.846206785477314, l1: 0.00010686696690960893, l2: 0.00037775370883443346   Iteration 98 of 100, tot loss = 4.847682386028524, l1: 0.00010679273587551766, l2: 0.0003779754997409253   Iteration 99 of 100, tot loss = 4.846234116891418, l1: 0.00010673718755086414, l2: 0.0003778862211596917   Iteration 100 of 100, tot loss = 4.843259394168854, l1: 0.00010686703790270257, l2: 0.00037745889872894625
   End of epoch 1384; saving model... 

Epoch 1385 of 2000
   Iteration 1 of 100, tot loss = 3.0634942054748535, l1: 6.002590816933662e-05, l2: 0.00024632352869957685   Iteration 2 of 100, tot loss = 5.5038604736328125, l1: 9.592800051905215e-05, l2: 0.0004544580588117242   Iteration 3 of 100, tot loss = 5.284231821695964, l1: 0.00010395233888023843, l2: 0.00042447085919169086   Iteration 4 of 100, tot loss = 5.162374138832092, l1: 0.00010531667430768721, l2: 0.0004109207511646673   Iteration 5 of 100, tot loss = 4.856426858901978, l1: 9.940676827682182e-05, l2: 0.0003862359269987792   Iteration 6 of 100, tot loss = 4.653447349866231, l1: 9.746641201976065e-05, l2: 0.0003678783299013351   Iteration 7 of 100, tot loss = 4.857452154159546, l1: 0.00010332673056317227, l2: 0.0003824184948046293   Iteration 8 of 100, tot loss = 4.838936299085617, l1: 0.00010500122607481899, l2: 0.0003788924150285311   Iteration 9 of 100, tot loss = 4.800446059968737, l1: 0.00010652214485970844, l2: 0.0003735224696962784   Iteration 10 of 100, tot loss = 4.671290159225464, l1: 0.00010470708584762179, l2: 0.0003624219389166683   Iteration 11 of 100, tot loss = 4.728698080236262, l1: 0.00010731175676813687, l2: 0.0003655580592087724   Iteration 12 of 100, tot loss = 4.575404644012451, l1: 0.00010543163504432111, l2: 0.0003521088365232572   Iteration 13 of 100, tot loss = 4.520651633922871, l1: 0.00010587947830432453, l2: 0.00034618569099201035   Iteration 14 of 100, tot loss = 4.713650975908552, l1: 0.00010952913050589683, l2: 0.00036183597278847756   Iteration 15 of 100, tot loss = 4.809403705596924, l1: 0.00011240328458370641, l2: 0.0003685370941335956   Iteration 16 of 100, tot loss = 4.794248521327972, l1: 0.00011086383301517344, l2: 0.00036856102815363556   Iteration 17 of 100, tot loss = 4.953449894400204, l1: 0.00011403267468695584, l2: 0.00038131232277544984   Iteration 18 of 100, tot loss = 5.02174350950453, l1: 0.00011482473402995513, l2: 0.00038734962516981695   Iteration 19 of 100, tot loss = 4.995295022663317, l1: 0.00011403596650588473, l2: 0.00038549354404063995   Iteration 20 of 100, tot loss = 5.020016098022461, l1: 0.00011498447274789214, l2: 0.0003870171451126225   Iteration 21 of 100, tot loss = 5.1341946465628485, l1: 0.00011542591770800452, l2: 0.0003979935524209092   Iteration 22 of 100, tot loss = 5.066506580872969, l1: 0.0001152208262117876, l2: 0.0003914298365337097   Iteration 23 of 100, tot loss = 5.152267186538033, l1: 0.00011711438223162828, l2: 0.00039811234075940496   Iteration 24 of 100, tot loss = 5.084661573171616, l1: 0.00011626431387412595, l2: 0.00039220184650427353   Iteration 25 of 100, tot loss = 5.202764101028443, l1: 0.00011695681983837858, l2: 0.0004033195925876498   Iteration 26 of 100, tot loss = 5.171637489245488, l1: 0.00011544601371637188, l2: 0.0004017177382663179   Iteration 27 of 100, tot loss = 5.2514600488874645, l1: 0.00011559311894449854, l2: 0.0004095528899967946   Iteration 28 of 100, tot loss = 5.197581197534289, l1: 0.00011429966018892759, l2: 0.00040545846318959126   Iteration 29 of 100, tot loss = 5.181910490167552, l1: 0.00011351848586165379, l2: 0.00040467256672102315   Iteration 30 of 100, tot loss = 5.202496727307637, l1: 0.00011421098048837545, l2: 0.00040603869517023367   Iteration 31 of 100, tot loss = 5.271717048460437, l1: 0.00011543974992931791, l2: 0.000411731957067405   Iteration 32 of 100, tot loss = 5.268628261983395, l1: 0.00011444498704804573, l2: 0.00041241784128942527   Iteration 33 of 100, tot loss = 5.233192205429077, l1: 0.00011368004638687566, l2: 0.0004096391758966175   Iteration 34 of 100, tot loss = 5.190491627244389, l1: 0.00011341395656927489, l2: 0.00040563520793939997   Iteration 35 of 100, tot loss = 5.193630238941737, l1: 0.00011360529668828739, l2: 0.0004057577287312597   Iteration 36 of 100, tot loss = 5.2552942633628845, l1: 0.00011415022082575079, l2: 0.0004113792068286178   Iteration 37 of 100, tot loss = 5.247391758738337, l1: 0.00011406187459706907, l2: 0.0004106773019491418   Iteration 38 of 100, tot loss = 5.1650703016080355, l1: 0.00011264720945435233, l2: 0.0004038598214576364   Iteration 39 of 100, tot loss = 5.133230093197945, l1: 0.00011245934822513029, l2: 0.000400863662108373   Iteration 40 of 100, tot loss = 5.099409812688828, l1: 0.00011211611545149935, l2: 0.00039782486674084795   Iteration 41 of 100, tot loss = 5.065191681792096, l1: 0.00011200680349181156, l2: 0.00039451236576807297   Iteration 42 of 100, tot loss = 5.065796142532712, l1: 0.00011188228364473962, l2: 0.00039469733192596496   Iteration 43 of 100, tot loss = 5.045055295145789, l1: 0.00011177924492120266, l2: 0.00039272628579732724   Iteration 44 of 100, tot loss = 5.0278937003829265, l1: 0.00011175348134367431, l2: 0.00039103589006117545   Iteration 45 of 100, tot loss = 5.065058830049303, l1: 0.00011183719697227288, l2: 0.00039466868782053803   Iteration 46 of 100, tot loss = 5.037954294163248, l1: 0.00011137423770497148, l2: 0.000392421193954879   Iteration 47 of 100, tot loss = 5.007096980480438, l1: 0.00011077220505227315, l2: 0.000389937495012728   Iteration 48 of 100, tot loss = 4.974251985549927, l1: 0.00011037685832585946, l2: 0.000387048341811654   Iteration 49 of 100, tot loss = 4.947131575370322, l1: 0.00011019509497136638, l2: 0.00038451806443733907   Iteration 50 of 100, tot loss = 4.91333044052124, l1: 0.00010968992777634413, l2: 0.00038164311787113546   Iteration 51 of 100, tot loss = 4.979853143879011, l1: 0.00011092414919688713, l2: 0.0003870611670244412   Iteration 52 of 100, tot loss = 4.954074767919687, l1: 0.00011027876798135157, l2: 0.0003851287104995348   Iteration 53 of 100, tot loss = 4.966234054205553, l1: 0.00011009595728641868, l2: 0.00038652744952518987   Iteration 54 of 100, tot loss = 4.935939426775332, l1: 0.0001092048216917276, l2: 0.00038438912263329795   Iteration 55 of 100, tot loss = 4.925990772247315, l1: 0.00010937886294083331, l2: 0.00038322021560320123   Iteration 56 of 100, tot loss = 4.894256468330111, l1: 0.00010891230093485709, l2: 0.0003805133468891394   Iteration 57 of 100, tot loss = 4.866585877903721, l1: 0.00010790711581776442, l2: 0.0003787514729641803   Iteration 58 of 100, tot loss = 4.8700823413914645, l1: 0.00010761152024597635, l2: 0.00037939671466151123   Iteration 59 of 100, tot loss = 4.852683855315386, l1: 0.00010739173740148544, l2: 0.00037787664879973727   Iteration 60 of 100, tot loss = 4.870424592494965, l1: 0.00010746148400357925, l2: 0.00037958097649000894   Iteration 61 of 100, tot loss = 4.847049099500062, l1: 0.00010709677520876017, l2: 0.0003776081361078856   Iteration 62 of 100, tot loss = 4.813298909894882, l1: 0.00010612367054085141, l2: 0.00037520622150153825   Iteration 63 of 100, tot loss = 4.837115908425952, l1: 0.00010683352934032132, l2: 0.00037687806212084577   Iteration 64 of 100, tot loss = 4.822986971586943, l1: 0.00010648252242617673, l2: 0.00037581617539217405   Iteration 65 of 100, tot loss = 4.821358838448158, l1: 0.0001066832275402983, l2: 0.00037545265714960315   Iteration 66 of 100, tot loss = 4.841325785174514, l1: 0.0001068959376094757, l2: 0.000377236641229088   Iteration 67 of 100, tot loss = 4.8341864578759495, l1: 0.00010665089630221239, l2: 0.0003767677497226439   Iteration 68 of 100, tot loss = 4.837868231184342, l1: 0.00010669600118597052, l2: 0.0003770908217976445   Iteration 69 of 100, tot loss = 4.82874686130579, l1: 0.00010645257698606524, l2: 0.00037642210899966943   Iteration 70 of 100, tot loss = 4.808022614887783, l1: 0.00010609232272794803, l2: 0.00037470993847819043   Iteration 71 of 100, tot loss = 4.821282648704421, l1: 0.00010641785954415443, l2: 0.00037571040501090054   Iteration 72 of 100, tot loss = 4.820444703102112, l1: 0.00010630142590647058, l2: 0.00037574304406007286   Iteration 73 of 100, tot loss = 4.81538204950829, l1: 0.00010624030759049363, l2: 0.00037529789711099933   Iteration 74 of 100, tot loss = 4.793312059866415, l1: 0.00010627272043879006, l2: 0.0003730584855499753   Iteration 75 of 100, tot loss = 4.76037532488505, l1: 0.00010574038038612344, l2: 0.00037029715216097735   Iteration 76 of 100, tot loss = 4.75664527478971, l1: 0.00010562408193723776, l2: 0.00037004044555176635   Iteration 77 of 100, tot loss = 4.736326316734413, l1: 0.00010536953763599363, l2: 0.0003682630939421232   Iteration 78 of 100, tot loss = 4.724798746598073, l1: 0.0001052239967509358, l2: 0.00036725587769745826   Iteration 79 of 100, tot loss = 4.703968552094471, l1: 0.0001045097077955769, l2: 0.00036588714743929007   Iteration 80 of 100, tot loss = 4.693332067131996, l1: 0.00010440423670843302, l2: 0.0003649289701570524   Iteration 81 of 100, tot loss = 4.671635642463778, l1: 0.00010398933312742121, l2: 0.0003631742312544152   Iteration 82 of 100, tot loss = 4.682840001292345, l1: 0.00010387507839967417, l2: 0.0003644089220182561   Iteration 83 of 100, tot loss = 4.671950116214982, l1: 0.00010368572378843876, l2: 0.000363509288178308   Iteration 84 of 100, tot loss = 4.689535714331127, l1: 0.00010380332098845559, l2: 0.00036515025082432356   Iteration 85 of 100, tot loss = 4.682319882336785, l1: 0.00010369409518556067, l2: 0.0003645378931918565   Iteration 86 of 100, tot loss = 4.682318182878716, l1: 0.00010387627205704502, l2: 0.0003643555466497187   Iteration 87 of 100, tot loss = 4.660064763036267, l1: 0.00010350043772507042, l2: 0.00036250603895088463   Iteration 88 of 100, tot loss = 4.676241105253046, l1: 0.00010360180877713984, l2: 0.0003640223019994499   Iteration 89 of 100, tot loss = 4.6977142591154974, l1: 0.00010402324988625766, l2: 0.00036574817645844865   Iteration 90 of 100, tot loss = 4.705133644739787, l1: 0.00010429876561425367, l2: 0.00036621459940862323   Iteration 91 of 100, tot loss = 4.705985090234777, l1: 0.00010431550813686605, l2: 0.0003662830011427894   Iteration 92 of 100, tot loss = 4.688701461190763, l1: 0.00010407009821115618, l2: 0.00036480004817586513   Iteration 93 of 100, tot loss = 4.70189340396594, l1: 0.00010452202504209082, l2: 0.00036566731565687   Iteration 94 of 100, tot loss = 4.697153033094203, l1: 0.00010450394252752341, l2: 0.00036521136102469717   Iteration 95 of 100, tot loss = 4.683224991748207, l1: 0.00010405031532986628, l2: 0.00036427218384280996   Iteration 96 of 100, tot loss = 4.672733108202617, l1: 0.00010364539665867294, l2: 0.0003636279141877215   Iteration 97 of 100, tot loss = 4.685248925513828, l1: 0.00010357275140890612, l2: 0.00036495214087538155   Iteration 98 of 100, tot loss = 4.704896177564349, l1: 0.00010400273878280578, l2: 0.0003664868784894958   Iteration 99 of 100, tot loss = 4.699513656924469, l1: 0.00010398420797880842, l2: 0.0003659671572015875   Iteration 100 of 100, tot loss = 4.7219383335113525, l1: 0.0001041136037747492, l2: 0.0003680802292365115
   End of epoch 1385; saving model... 

Epoch 1386 of 2000
   Iteration 1 of 100, tot loss = 6.504863262176514, l1: 0.00013175848289392889, l2: 0.0005187278147786856   Iteration 2 of 100, tot loss = 4.974669694900513, l1: 0.00011554866068763658, l2: 0.00038191830390132964   Iteration 3 of 100, tot loss = 4.533714850743611, l1: 0.00010926476049159343, l2: 0.00034410672378726304   Iteration 4 of 100, tot loss = 4.634335935115814, l1: 0.00011015705604222603, l2: 0.00035327654040884227   Iteration 5 of 100, tot loss = 4.428529071807861, l1: 0.00011033427144866437, l2: 0.0003325186378788203   Iteration 6 of 100, tot loss = 4.991840998331706, l1: 0.00011666886712191626, l2: 0.00038251522831463564   Iteration 7 of 100, tot loss = 4.608176061085293, l1: 0.00010961277647376326, l2: 0.00035120482581468034   Iteration 8 of 100, tot loss = 4.769143134355545, l1: 0.00011076885220973054, l2: 0.00036614545933844056   Iteration 9 of 100, tot loss = 4.678936693403456, l1: 0.0001087542138217638, l2: 0.0003591394520804493   Iteration 10 of 100, tot loss = 5.014357614517212, l1: 0.00011519530671648681, l2: 0.00038624045118922367   Iteration 11 of 100, tot loss = 4.858283541419289, l1: 0.00011158661205131052, l2: 0.0003742417386754162   Iteration 12 of 100, tot loss = 5.036739408969879, l1: 0.00011538402698837065, l2: 0.00038828991455375217   Iteration 13 of 100, tot loss = 5.011115055817824, l1: 0.00011511893591467434, l2: 0.00038599257039515156   Iteration 14 of 100, tot loss = 4.98817333153316, l1: 0.0001132074222758612, l2: 0.0003856099117131505   Iteration 15 of 100, tot loss = 4.926498969395955, l1: 0.00011081285289643953, l2: 0.0003818370430963114   Iteration 16 of 100, tot loss = 4.945345357060432, l1: 0.00011229926531086676, l2: 0.0003822352691713604   Iteration 17 of 100, tot loss = 5.146815426209393, l1: 0.00011614619185874129, l2: 0.00039853534904336007   Iteration 18 of 100, tot loss = 5.126975523100959, l1: 0.00011565779722231027, l2: 0.00039703975279634405   Iteration 19 of 100, tot loss = 5.108537335144846, l1: 0.00011510640883658964, l2: 0.00039574732210511635   Iteration 20 of 100, tot loss = 5.141951954364776, l1: 0.00011547721042006742, l2: 0.00039871798144304195   Iteration 21 of 100, tot loss = 5.043880644298735, l1: 0.0001122812715212127, l2: 0.00039210678923631175   Iteration 22 of 100, tot loss = 5.08388317715038, l1: 0.00011253000362061853, l2: 0.00039585831043289295   Iteration 23 of 100, tot loss = 5.0910704654196035, l1: 0.00011203230061927688, l2: 0.00039707474175172496   Iteration 24 of 100, tot loss = 5.058102488517761, l1: 0.00011263651428331893, l2: 0.00039317372951093904   Iteration 25 of 100, tot loss = 5.054530944824219, l1: 0.00011241325919399969, l2: 0.0003930398292141035   Iteration 26 of 100, tot loss = 5.048434862723718, l1: 0.00011194525373307093, l2: 0.0003928982259822078   Iteration 27 of 100, tot loss = 5.085029672693323, l1: 0.00011317244409130783, l2: 0.00039533051690172953   Iteration 28 of 100, tot loss = 5.0657307761056085, l1: 0.0001124211122649805, l2: 0.0003941519592314892   Iteration 29 of 100, tot loss = 5.075333167766702, l1: 0.00011215862882793239, l2: 0.0003953746822060503   Iteration 30 of 100, tot loss = 5.075502888361613, l1: 0.00011288827045063954, l2: 0.000394662012210271   Iteration 31 of 100, tot loss = 5.030467679423671, l1: 0.00011281512249092151, l2: 0.0003902316389177295   Iteration 32 of 100, tot loss = 4.965246580541134, l1: 0.00011153499724514404, l2: 0.00038498965432154364   Iteration 33 of 100, tot loss = 4.915172352935329, l1: 0.00011059520120122188, l2: 0.00038092202819164163   Iteration 34 of 100, tot loss = 4.9111588351866775, l1: 0.00011049746182620497, l2: 0.00038061841598177766   Iteration 35 of 100, tot loss = 4.899689585821969, l1: 0.00011048031602903003, l2: 0.0003794886367229213   Iteration 36 of 100, tot loss = 4.890726983547211, l1: 0.00011012215337460575, l2: 0.00037895053982437175   Iteration 37 of 100, tot loss = 4.802723897470011, l1: 0.00010803933420558335, l2: 0.00037223305057646156   Iteration 38 of 100, tot loss = 4.750190684669896, l1: 0.00010730784714197446, l2: 0.0003677112162174461   Iteration 39 of 100, tot loss = 4.709768032416319, l1: 0.00010689745231036049, l2: 0.00036407934609227453   Iteration 40 of 100, tot loss = 4.715110033750534, l1: 0.00010686372825148282, l2: 0.0003646472694526892   Iteration 41 of 100, tot loss = 4.696845950149909, l1: 0.00010714564960813377, l2: 0.00036253893986435196   Iteration 42 of 100, tot loss = 4.745336895897275, l1: 0.00010720848686538549, l2: 0.00036732519705158967   Iteration 43 of 100, tot loss = 4.706333997637727, l1: 0.00010660460484782667, l2: 0.0003640287894847651   Iteration 44 of 100, tot loss = 4.686928673224016, l1: 0.00010633838808809577, l2: 0.00036235447392259215   Iteration 45 of 100, tot loss = 4.7084745195176865, l1: 0.00010715392547556096, l2: 0.0003636935211640472   Iteration 46 of 100, tot loss = 4.69395867637966, l1: 0.00010705004641077603, l2: 0.0003623458162663788   Iteration 47 of 100, tot loss = 4.6425024042738245, l1: 0.00010602983503362184, l2: 0.0003582204006577624   Iteration 48 of 100, tot loss = 4.589232901732127, l1: 0.0001051900005677453, l2: 0.00035373328470692894   Iteration 49 of 100, tot loss = 4.5909236596555125, l1: 0.00010419781518772206, l2: 0.00035489454529276685   Iteration 50 of 100, tot loss = 4.585755834579468, l1: 0.00010427783490740694, l2: 0.00035429774346994235   Iteration 51 of 100, tot loss = 4.567591784047146, l1: 0.0001042183744264584, l2: 0.0003525407989988761   Iteration 52 of 100, tot loss = 4.555221259593964, l1: 0.00010429136860833611, l2: 0.0003512307524103492   Iteration 53 of 100, tot loss = 4.524878821283017, l1: 0.00010380109336626945, l2: 0.0003486867839764839   Iteration 54 of 100, tot loss = 4.547557676279986, l1: 0.00010432655121635266, l2: 0.00035042921162029315   Iteration 55 of 100, tot loss = 4.580868534608321, l1: 0.00010466978565091268, l2: 0.0003534170637563379   Iteration 56 of 100, tot loss = 4.556814355509622, l1: 0.00010420954822620843, l2: 0.00035147188334251823   Iteration 57 of 100, tot loss = 4.575719942126358, l1: 0.00010426827934121288, l2: 0.00035330371113224447   Iteration 58 of 100, tot loss = 4.557721795706914, l1: 0.00010378069180597808, l2: 0.00035199148410826857   Iteration 59 of 100, tot loss = 4.555525545346534, l1: 0.00010379620098165568, l2: 0.0003517563496736052   Iteration 60 of 100, tot loss = 4.530502076943716, l1: 0.00010328406169719528, l2: 0.00034976614212306836   Iteration 61 of 100, tot loss = 4.5244383929205725, l1: 0.00010311697298645607, l2: 0.0003493268622016748   Iteration 62 of 100, tot loss = 4.512715385806176, l1: 0.0001029879588919181, l2: 0.00034828357550600967   Iteration 63 of 100, tot loss = 4.479659856311859, l1: 0.00010242316645980325, l2: 0.00034554281511292275   Iteration 64 of 100, tot loss = 4.475596394389868, l1: 0.00010222270236681652, l2: 0.0003453369333783485   Iteration 65 of 100, tot loss = 4.463601772601788, l1: 0.00010194837108988745, l2: 0.0003444118024512696   Iteration 66 of 100, tot loss = 4.4372480385231245, l1: 0.00010154279648717916, l2: 0.00034218200391588846   Iteration 67 of 100, tot loss = 4.4558579423534335, l1: 0.00010164782030024885, l2: 0.0003439379700046005   Iteration 68 of 100, tot loss = 4.47453574923908, l1: 0.00010189525025506394, l2: 0.0003455583207730396   Iteration 69 of 100, tot loss = 4.448578848355059, l1: 0.00010158604329816349, l2: 0.0003432718378093526   Iteration 70 of 100, tot loss = 4.51032453264509, l1: 0.00010233914758178539, l2: 0.00034869330161849837   Iteration 71 of 100, tot loss = 4.4899927931772154, l1: 0.00010190482842030896, l2: 0.00034709444698307034   Iteration 72 of 100, tot loss = 4.451717461148898, l1: 0.00010103079598088193, l2: 0.00034414094625390135   Iteration 73 of 100, tot loss = 4.428455517716603, l1: 0.00010062048311563758, l2: 0.00034222506486638514   Iteration 74 of 100, tot loss = 4.416469411270039, l1: 0.00010075954565223042, l2: 0.00034088739158579374   Iteration 75 of 100, tot loss = 4.404176863034566, l1: 0.00010021864116424695, l2: 0.0003401990415295586   Iteration 76 of 100, tot loss = 4.485675195330067, l1: 0.00010129529054163293, l2: 0.0003472722250750705   Iteration 77 of 100, tot loss = 4.479676046928802, l1: 0.00010120945430040215, l2: 0.00034675814643012385   Iteration 78 of 100, tot loss = 4.482696384955675, l1: 0.0001010378160012456, l2: 0.0003472318186630638   Iteration 79 of 100, tot loss = 4.471863159650488, l1: 0.00010062006627846085, l2: 0.00034656624594862467   Iteration 80 of 100, tot loss = 4.460475687682629, l1: 0.00010006772249653295, l2: 0.0003459798424955807   Iteration 81 of 100, tot loss = 4.447803999170845, l1: 0.00010016663149998086, l2: 0.00034461376461326893   Iteration 82 of 100, tot loss = 4.426302341426291, l1: 9.992313163998117e-05, l2: 0.00034270709875272587   Iteration 83 of 100, tot loss = 4.4139088018831, l1: 9.963680520697206e-05, l2: 0.0003417540712952502   Iteration 84 of 100, tot loss = 4.419540227878661, l1: 9.963461061921581e-05, l2: 0.0003423194088792938   Iteration 85 of 100, tot loss = 4.414875303997713, l1: 9.944005017613938e-05, l2: 0.0003420474772985258   Iteration 86 of 100, tot loss = 4.386408294356147, l1: 9.872980614133222e-05, l2: 0.00033991102032433806   Iteration 87 of 100, tot loss = 4.3660276936388565, l1: 9.851042579674837e-05, l2: 0.00033809234078400823   Iteration 88 of 100, tot loss = 4.379209419543093, l1: 9.860638221487699e-05, l2: 0.000339314557012668   Iteration 89 of 100, tot loss = 4.389740455016661, l1: 9.869811233993767e-05, l2: 0.0003402759309588628   Iteration 90 of 100, tot loss = 4.3944272504912485, l1: 9.875167145057478e-05, l2: 0.0003406910515170441   Iteration 91 of 100, tot loss = 4.385151746508839, l1: 9.866204921278979e-05, l2: 0.00033985312332803446   Iteration 92 of 100, tot loss = 4.368539679309596, l1: 9.86187209017099e-05, l2: 0.0003382352449040374   Iteration 93 of 100, tot loss = 4.379111437387364, l1: 9.887350398624036e-05, l2: 0.00033903763765939863   Iteration 94 of 100, tot loss = 4.378056332151941, l1: 9.902040549972531e-05, l2: 0.00033878522523817547   Iteration 95 of 100, tot loss = 4.356003501540736, l1: 9.847522619805347e-05, l2: 0.00033712512154842873   Iteration 96 of 100, tot loss = 4.347720139970382, l1: 9.822823797852227e-05, l2: 0.00033654377345252823   Iteration 97 of 100, tot loss = 4.360236884392414, l1: 9.849392892984847e-05, l2: 0.00033752975721973157   Iteration 98 of 100, tot loss = 4.384163174093986, l1: 9.897153670733263e-05, l2: 0.00033944477852818804   Iteration 99 of 100, tot loss = 4.401671080878287, l1: 9.891935997534393e-05, l2: 0.0003412477461905736   Iteration 100 of 100, tot loss = 4.408805221319199, l1: 9.916081355186179e-05, l2: 0.0003417197063390631
   End of epoch 1386; saving model... 

Epoch 1387 of 2000
   Iteration 1 of 100, tot loss = 5.645142078399658, l1: 9.324515849584714e-05, l2: 0.0004712690133601427   Iteration 2 of 100, tot loss = 5.432919979095459, l1: 0.00010668326649465598, l2: 0.0004366087232483551   Iteration 3 of 100, tot loss = 6.238937218983968, l1: 0.00011895616626134142, l2: 0.0005049375564946482   Iteration 4 of 100, tot loss = 6.086343169212341, l1: 0.00011924369027838111, l2: 0.0004893906225333922   Iteration 5 of 100, tot loss = 5.72373218536377, l1: 0.00011366007383912801, l2: 0.0004587131435982883   Iteration 6 of 100, tot loss = 6.098259290059407, l1: 0.00011790489224949852, l2: 0.00049192103324458   Iteration 7 of 100, tot loss = 5.7925567626953125, l1: 0.00010899039209886854, l2: 0.0004702652804553509   Iteration 8 of 100, tot loss = 5.494266241788864, l1: 0.00010572065457381541, l2: 0.00044370596515364014   Iteration 9 of 100, tot loss = 5.345088985231188, l1: 0.00010382975112103547, l2: 0.000430679142785569   Iteration 10 of 100, tot loss = 5.2411936044692995, l1: 0.00010492745277588256, l2: 0.00041919190261978655   Iteration 11 of 100, tot loss = 5.309290690855547, l1: 0.00010488480686696924, l2: 0.0004260442563628947   Iteration 12 of 100, tot loss = 5.211396038532257, l1: 0.00010395243528667682, l2: 0.00041718716238392517   Iteration 13 of 100, tot loss = 5.171200477159941, l1: 0.00010408688159749055, l2: 0.00041303315755123127   Iteration 14 of 100, tot loss = 5.094423583575657, l1: 0.00010487744397583551, l2: 0.00040456490699268343   Iteration 15 of 100, tot loss = 4.9631010373433435, l1: 0.00010313441210504, l2: 0.00039317568492454785   Iteration 16 of 100, tot loss = 4.864069640636444, l1: 0.00010205234320892487, l2: 0.00038435461465269327   Iteration 17 of 100, tot loss = 4.863653014687931, l1: 0.00010218162060482427, l2: 0.0003841836758724907   Iteration 18 of 100, tot loss = 4.736533866988288, l1: 9.883067185809422e-05, l2: 0.0003748227100004442   Iteration 19 of 100, tot loss = 4.911724705445139, l1: 0.00010227312189894484, l2: 0.00038889934397670194   Iteration 20 of 100, tot loss = 4.978940165042877, l1: 0.00010469947010278702, l2: 0.0003931945400836412   Iteration 21 of 100, tot loss = 5.015950872784569, l1: 0.00010545489411535007, l2: 0.00039614018792885223   Iteration 22 of 100, tot loss = 4.920013471083208, l1: 0.0001037784254136072, l2: 0.00038822291654386475   Iteration 23 of 100, tot loss = 4.861880167670872, l1: 0.00010211830395628172, l2: 0.0003840697077659728   Iteration 24 of 100, tot loss = 4.869443267583847, l1: 0.00010233860424098869, l2: 0.00038460571825756534   Iteration 25 of 100, tot loss = 4.849684724807739, l1: 0.00010270547732943669, l2: 0.00038226299162488433   Iteration 26 of 100, tot loss = 4.829936972031226, l1: 0.00010193906116630667, l2: 0.00038105463262092177   Iteration 27 of 100, tot loss = 4.860550659674185, l1: 0.00010295854196926855, l2: 0.0003830965212761873   Iteration 28 of 100, tot loss = 4.852479960237231, l1: 0.0001033323509805736, l2: 0.00038191564245997664   Iteration 29 of 100, tot loss = 4.859186410903931, l1: 0.00010392881035684319, l2: 0.0003819898275682574   Iteration 30 of 100, tot loss = 4.886892008781433, l1: 0.00010358006499397257, l2: 0.00038510913194234796   Iteration 31 of 100, tot loss = 4.925884977463753, l1: 0.00010477091681452529, l2: 0.00038781757569766694   Iteration 32 of 100, tot loss = 4.93901214748621, l1: 0.00010544301903792075, l2: 0.00038845819062771625   Iteration 33 of 100, tot loss = 4.970788977362893, l1: 0.00010477973189677648, l2: 0.0003922991608939783   Iteration 34 of 100, tot loss = 4.892385356566486, l1: 0.00010287712083321393, l2: 0.0003863614102181814   Iteration 35 of 100, tot loss = 4.9280123574393135, l1: 0.00010361822080864971, l2: 0.0003891830099746585   Iteration 36 of 100, tot loss = 4.846377664142185, l1: 0.00010282161343234798, l2: 0.0003818161482437669   Iteration 37 of 100, tot loss = 4.818781388772501, l1: 0.0001026317709270311, l2: 0.0003792463634257532   Iteration 38 of 100, tot loss = 4.795174284985191, l1: 0.00010169640633026383, l2: 0.0003778210180155982   Iteration 39 of 100, tot loss = 4.747016973984548, l1: 0.00010117854570778899, l2: 0.0003735231472931516   Iteration 40 of 100, tot loss = 4.769553393125534, l1: 0.00010133999094250613, l2: 0.00037561534336418847   Iteration 41 of 100, tot loss = 4.719288070027421, l1: 0.00010059723043501967, l2: 0.0003713315719079862   Iteration 42 of 100, tot loss = 4.673677932648432, l1: 0.00010003189163398929, l2: 0.00036733589721344676   Iteration 43 of 100, tot loss = 4.692661595899005, l1: 0.00010002470256484569, l2: 0.00036924145202054965   Iteration 44 of 100, tot loss = 4.683179877021096, l1: 0.0001001689167672091, l2: 0.0003681490664100486   Iteration 45 of 100, tot loss = 4.765140597025553, l1: 0.0001010477507305849, l2: 0.0003754663051545827   Iteration 46 of 100, tot loss = 4.736865836641063, l1: 0.00010074690789738467, l2: 0.0003729396719237506   Iteration 47 of 100, tot loss = 4.749856426360759, l1: 0.00010079295904514321, l2: 0.00037419268065082663   Iteration 48 of 100, tot loss = 4.779880647857984, l1: 0.00010168956881292009, l2: 0.00037629849399915355   Iteration 49 of 100, tot loss = 4.7800726452652285, l1: 0.00010145529590865446, l2: 0.00037655196650362364   Iteration 50 of 100, tot loss = 4.776964211463929, l1: 0.00010145621345145628, l2: 0.0003762402050779201   Iteration 51 of 100, tot loss = 4.829395401711557, l1: 0.00010229244727787434, l2: 0.00038064709053073516   Iteration 52 of 100, tot loss = 4.848113211301657, l1: 0.00010305609696213371, l2: 0.00038175522198658797   Iteration 53 of 100, tot loss = 4.832572644611575, l1: 0.00010273546498831151, l2: 0.0003805217975069646   Iteration 54 of 100, tot loss = 4.842785089104264, l1: 0.0001031734710623924, l2: 0.0003811050358948436   Iteration 55 of 100, tot loss = 4.857630586624145, l1: 0.0001038782401751219, l2: 0.00038188481729858637   Iteration 56 of 100, tot loss = 4.826162053006036, l1: 0.00010357651743626255, l2: 0.0003790396866471773   Iteration 57 of 100, tot loss = 4.811451849184539, l1: 0.00010318748523754868, l2: 0.0003779576989217547   Iteration 58 of 100, tot loss = 4.801130455115746, l1: 0.00010336421009610761, l2: 0.00037674883436732766   Iteration 59 of 100, tot loss = 4.764410568495928, l1: 0.000103043450631588, l2: 0.00037339760530227003   Iteration 60 of 100, tot loss = 4.722541149457296, l1: 0.00010205320735015752, l2: 0.00037020090652125265   Iteration 61 of 100, tot loss = 4.766713400356105, l1: 0.00010299838334608242, l2: 0.00037367295518662535   Iteration 62 of 100, tot loss = 4.730263856149489, l1: 0.00010252582010692887, l2: 0.00037050056397997503   Iteration 63 of 100, tot loss = 4.736781566862076, l1: 0.00010262624305647073, l2: 0.00037105191228283007   Iteration 64 of 100, tot loss = 4.704296518117189, l1: 0.00010227726579614682, l2: 0.00036815238490817137   Iteration 65 of 100, tot loss = 4.6641870755415695, l1: 0.00010168258864163922, l2: 0.00036473611790615207   Iteration 66 of 100, tot loss = 4.661654938351024, l1: 0.00010160716170282808, l2: 0.0003645583316084054   Iteration 67 of 100, tot loss = 4.722954155793831, l1: 0.00010263200661640114, l2: 0.00036966340805737495   Iteration 68 of 100, tot loss = 4.729133307933807, l1: 0.00010276854920769384, l2: 0.00037014478071154477   Iteration 69 of 100, tot loss = 4.713454436564791, l1: 0.00010246212669712342, l2: 0.00036888331616891253   Iteration 70 of 100, tot loss = 4.708795925549098, l1: 0.00010250707013515888, l2: 0.0003683725218122293   Iteration 71 of 100, tot loss = 4.6939200817699165, l1: 0.00010237071075325859, l2: 0.0003670212967668883   Iteration 72 of 100, tot loss = 4.686643905109829, l1: 0.00010243355619523855, l2: 0.00036623083401031583   Iteration 73 of 100, tot loss = 4.696256304440433, l1: 0.00010255356606300155, l2: 0.0003670720640276495   Iteration 74 of 100, tot loss = 4.682917607797159, l1: 0.00010242510275880655, l2: 0.0003658666576705621   Iteration 75 of 100, tot loss = 4.669317763646443, l1: 0.00010167505689120542, l2: 0.00036525671913598974   Iteration 76 of 100, tot loss = 4.660612605120006, l1: 0.00010155489814535126, l2: 0.00036450636214699204   Iteration 77 of 100, tot loss = 4.65387563891225, l1: 0.00010153109566066656, l2: 0.000363856468231768   Iteration 78 of 100, tot loss = 4.641178424541767, l1: 0.00010173573051967348, l2: 0.0003623821121903184   Iteration 79 of 100, tot loss = 4.626733203477498, l1: 0.00010157313586147856, l2: 0.0003611001847724466   Iteration 80 of 100, tot loss = 4.6350543886423115, l1: 0.00010192333502345718, l2: 0.0003615821038692957   Iteration 81 of 100, tot loss = 4.625846777433231, l1: 0.0001018749999836731, l2: 0.0003607096779303924   Iteration 82 of 100, tot loss = 4.609944581985474, l1: 0.00010169298885313498, l2: 0.00035930146951591824   Iteration 83 of 100, tot loss = 4.6058043112237765, l1: 0.00010186665518214949, l2: 0.0003587137760471059   Iteration 84 of 100, tot loss = 4.591268885703314, l1: 0.00010166435727331278, l2: 0.00035746253117741576   Iteration 85 of 100, tot loss = 4.597883723763859, l1: 0.00010186168664778748, l2: 0.00035792668521239914   Iteration 86 of 100, tot loss = 4.587662857632305, l1: 0.0001020538943838653, l2: 0.00035671239074449554   Iteration 87 of 100, tot loss = 4.607792854309082, l1: 0.00010232532670555487, l2: 0.0003584539584665634   Iteration 88 of 100, tot loss = 4.631551439111883, l1: 0.00010267046292772812, l2: 0.00036048468014500526   Iteration 89 of 100, tot loss = 4.635131825222058, l1: 0.00010248122230274184, l2: 0.00036103195926760523   Iteration 90 of 100, tot loss = 4.671212821536594, l1: 0.00010288250398136572, l2: 0.0003642387772237675   Iteration 91 of 100, tot loss = 4.651698623384748, l1: 0.0001025032009807886, l2: 0.00036266666023408646   Iteration 92 of 100, tot loss = 4.669253989406254, l1: 0.00010300088576079123, l2: 0.0003639245117918345   Iteration 93 of 100, tot loss = 4.659838553397886, l1: 0.0001029674684652628, l2: 0.00036301638559258035   Iteration 94 of 100, tot loss = 4.65710756626535, l1: 0.00010295425346022114, l2: 0.0003627565019996837   Iteration 95 of 100, tot loss = 4.651506549433658, l1: 0.00010293402133117381, l2: 0.0003622166325313676   Iteration 96 of 100, tot loss = 4.646263947089513, l1: 0.0001028733715126388, l2: 0.0003617530222375838   Iteration 97 of 100, tot loss = 4.651701396273584, l1: 0.00010284185727587756, l2: 0.00036232828159010055   Iteration 98 of 100, tot loss = 4.65239895119959, l1: 0.00010269459061400623, l2: 0.0003625453038174393   Iteration 99 of 100, tot loss = 4.6464250497143675, l1: 0.00010256220536915624, l2: 0.00036208029875221354   Iteration 100 of 100, tot loss = 4.661131191253662, l1: 0.00010283730727678631, l2: 0.00036327581095974893
   End of epoch 1387; saving model... 

Epoch 1388 of 2000
   Iteration 1 of 100, tot loss = 4.183686256408691, l1: 7.597899093525484e-05, l2: 0.0003423896268941462   Iteration 2 of 100, tot loss = 5.804131031036377, l1: 0.00010620171451591887, l2: 0.00047421138151548803   Iteration 3 of 100, tot loss = 5.204358100891113, l1: 0.00010174591928565253, l2: 0.0004186898877378553   Iteration 4 of 100, tot loss = 5.810622692108154, l1: 0.00011626696868916042, l2: 0.00046479529555654153   Iteration 5 of 100, tot loss = 5.1846541404724125, l1: 0.00010682564316084608, l2: 0.000411639767116867   Iteration 6 of 100, tot loss = 5.2942116260528564, l1: 0.00010852245031856, l2: 0.0004208987108237731   Iteration 7 of 100, tot loss = 5.188773904527936, l1: 0.00010299705276598356, l2: 0.0004158803390704894   Iteration 8 of 100, tot loss = 5.070978999137878, l1: 0.00010184822076553246, l2: 0.0004052496788062854   Iteration 9 of 100, tot loss = 5.104363759358724, l1: 0.00010484424920933734, l2: 0.00040559212761258497   Iteration 10 of 100, tot loss = 5.197147226333618, l1: 0.00010627144074533134, l2: 0.00041344328055856747   Iteration 11 of 100, tot loss = 5.21004598790949, l1: 0.00010754747596696357, l2: 0.000413457123530944   Iteration 12 of 100, tot loss = 5.208716869354248, l1: 0.00010782814812652457, l2: 0.0004130435396897762   Iteration 13 of 100, tot loss = 5.21746092576247, l1: 0.00010895198255849, l2: 0.0004127941132397749   Iteration 14 of 100, tot loss = 5.162796156747, l1: 0.00010953257827038345, l2: 0.00040674704048017574   Iteration 15 of 100, tot loss = 5.052006324132283, l1: 0.00010564845215412788, l2: 0.00039955218380782753   Iteration 16 of 100, tot loss = 5.110321953892708, l1: 0.00010744912128757278, l2: 0.0004035830797874951   Iteration 17 of 100, tot loss = 4.918827919399037, l1: 0.00010363807536230204, l2: 0.0003882447216550217   Iteration 18 of 100, tot loss = 4.803344375557369, l1: 0.00010121696767681796, l2: 0.0003791174741410133   Iteration 19 of 100, tot loss = 4.711926341056824, l1: 0.00010032148680980562, l2: 0.0003708711515891513   Iteration 20 of 100, tot loss = 4.714955765008926, l1: 0.00010103647400683258, l2: 0.00037045910576125605   Iteration 21 of 100, tot loss = 4.732291681425912, l1: 0.00010077666956931353, l2: 0.0003724525020169538   Iteration 22 of 100, tot loss = 4.701740118590268, l1: 0.00010147005278585394, l2: 0.0003687039623566141   Iteration 23 of 100, tot loss = 4.74447298568228, l1: 0.00010184941777918974, l2: 0.0003725978857129002   Iteration 24 of 100, tot loss = 4.831017340222995, l1: 0.00010299916114793935, l2: 0.00038010257776477374   Iteration 25 of 100, tot loss = 4.757077574729919, l1: 0.00010296898748492823, l2: 0.000372738775331527   Iteration 26 of 100, tot loss = 4.7728107663301325, l1: 0.00010358866431768268, l2: 0.0003736924174098441   Iteration 27 of 100, tot loss = 4.762542049090068, l1: 0.00010274785502891366, l2: 0.0003735063537196429   Iteration 28 of 100, tot loss = 4.691129356622696, l1: 0.00010214428688673902, l2: 0.00036696865208796225   Iteration 29 of 100, tot loss = 4.6251555599015335, l1: 0.0001016673275695086, l2: 0.00036084823157831   Iteration 30 of 100, tot loss = 4.623831713199616, l1: 0.00010260119694673145, l2: 0.0003597819770220667   Iteration 31 of 100, tot loss = 4.546753733388839, l1: 0.00010182644473388791, l2: 0.00035284893144102344   Iteration 32 of 100, tot loss = 4.5996248461306095, l1: 0.0001028897640935611, l2: 0.0003570727249098127   Iteration 33 of 100, tot loss = 4.618523688027353, l1: 0.00010324911934069611, l2: 0.00035860325354433644   Iteration 34 of 100, tot loss = 4.6198592641774345, l1: 0.00010380788273109561, l2: 0.00035817804746329784   Iteration 35 of 100, tot loss = 4.610552382469177, l1: 0.000103941181441769, l2: 0.0003571140596510044   Iteration 36 of 100, tot loss = 4.618173787991206, l1: 0.00010434012619953137, l2: 0.0003574772555212904   Iteration 37 of 100, tot loss = 4.717398285865784, l1: 0.00010630011309183681, l2: 0.000365439716669907   Iteration 38 of 100, tot loss = 4.698414862155914, l1: 0.00010629809491403744, l2: 0.000363543392220316   Iteration 39 of 100, tot loss = 4.669269112440256, l1: 0.00010615256570207958, l2: 0.0003607743464481945   Iteration 40 of 100, tot loss = 4.808321610093117, l1: 0.00010792960856633726, l2: 0.0003729025542270392   Iteration 41 of 100, tot loss = 4.810095662023963, l1: 0.00010771245983849484, l2: 0.00037329710832592557   Iteration 42 of 100, tot loss = 4.73818549371901, l1: 0.00010594855848466977, l2: 0.0003678699926933318   Iteration 43 of 100, tot loss = 4.71269399343535, l1: 0.00010591206346573524, l2: 0.0003653573373814525   Iteration 44 of 100, tot loss = 4.701884413307363, l1: 0.00010581416658665562, l2: 0.000364374276449036   Iteration 45 of 100, tot loss = 4.674897848235236, l1: 0.0001058177183343408, l2: 0.00036167206837692197   Iteration 46 of 100, tot loss = 4.677847911482272, l1: 0.00010552102748593113, l2: 0.00036226376529236364   Iteration 47 of 100, tot loss = 4.6778635851880335, l1: 0.00010566216673048094, l2: 0.00036212419310802633   Iteration 48 of 100, tot loss = 4.683342752357324, l1: 0.00010578445971987094, l2: 0.00036254981690338656   Iteration 49 of 100, tot loss = 4.699483255950772, l1: 0.00010605822706343226, l2: 0.00036389009946272995   Iteration 50 of 100, tot loss = 4.6943652319908145, l1: 0.00010623992420732975, l2: 0.00036319660022854805   Iteration 51 of 100, tot loss = 4.687816374442157, l1: 0.00010634321883759078, l2: 0.0003624384196739936   Iteration 52 of 100, tot loss = 4.6623016985563135, l1: 0.00010580793661154949, l2: 0.0003604222343151028   Iteration 53 of 100, tot loss = 4.671275393018183, l1: 0.000106263984014491, l2: 0.00036086355674273845   Iteration 54 of 100, tot loss = 4.679906873791306, l1: 0.0001063243072605002, l2: 0.00036166638138065873   Iteration 55 of 100, tot loss = 4.681248571655967, l1: 0.00010684930841142142, l2: 0.00036127555014734916   Iteration 56 of 100, tot loss = 4.683217246617589, l1: 0.00010671633961984688, l2: 0.00036160538625803644   Iteration 57 of 100, tot loss = 4.646512711257265, l1: 0.00010613158762377377, l2: 0.00035851968456419153   Iteration 58 of 100, tot loss = 4.6806964648181, l1: 0.00010660740510251469, l2: 0.0003614622421145182   Iteration 59 of 100, tot loss = 4.713919253672584, l1: 0.00010714110051640862, l2: 0.00036425082510242523   Iteration 60 of 100, tot loss = 4.716260466972987, l1: 0.00010720578078083539, l2: 0.00036442026612348855   Iteration 61 of 100, tot loss = 4.696705296391347, l1: 0.00010688579331159775, l2: 0.00036278473629356654   Iteration 62 of 100, tot loss = 4.709960639476776, l1: 0.00010710299876894081, l2: 0.0003638930657255133   Iteration 63 of 100, tot loss = 4.69896602062952, l1: 0.00010689811380062666, l2: 0.00036299848856980956   Iteration 64 of 100, tot loss = 4.670294279232621, l1: 0.00010639260813150031, l2: 0.00036063682023268484   Iteration 65 of 100, tot loss = 4.669820647973281, l1: 0.00010615029683461986, l2: 0.00036083176852956127   Iteration 66 of 100, tot loss = 4.661045155741951, l1: 0.00010643171481206082, l2: 0.00035967280101348564   Iteration 67 of 100, tot loss = 4.686563116400989, l1: 0.00010700717904177536, l2: 0.0003616491329609486   Iteration 68 of 100, tot loss = 4.681201915530598, l1: 0.00010720244994017241, l2: 0.0003609177418933113   Iteration 69 of 100, tot loss = 4.68257855159649, l1: 0.0001071412275516617, l2: 0.00036111662811363465   Iteration 70 of 100, tot loss = 4.673733985424041, l1: 0.00010690703129512258, l2: 0.00036046636793928753   Iteration 71 of 100, tot loss = 4.673942231796157, l1: 0.00010723813691481449, l2: 0.00036015608663585456   Iteration 72 of 100, tot loss = 4.660397279593679, l1: 0.00010691741509718768, l2: 0.00035912231336017914   Iteration 73 of 100, tot loss = 4.670582714146131, l1: 0.00010724398462988487, l2: 0.0003598142873371428   Iteration 74 of 100, tot loss = 4.7027847976297945, l1: 0.00010787301682678328, l2: 0.00036240546319891726   Iteration 75 of 100, tot loss = 4.669618719418843, l1: 0.00010700909328685763, l2: 0.0003599527790599192   Iteration 76 of 100, tot loss = 4.678384534622493, l1: 0.00010700933628533293, l2: 0.0003608291178687451   Iteration 77 of 100, tot loss = 4.709203916710693, l1: 0.00010731025220404451, l2: 0.0003636101400252891   Iteration 78 of 100, tot loss = 4.702769711995736, l1: 0.00010682219022456616, l2: 0.0003634547818084964   Iteration 79 of 100, tot loss = 4.706771444670761, l1: 0.00010687285188557521, l2: 0.000363804293682952   Iteration 80 of 100, tot loss = 4.740318141877651, l1: 0.00010746647408268472, l2: 0.00036656534120993456   Iteration 81 of 100, tot loss = 4.743649952205611, l1: 0.00010738499209473059, l2: 0.0003669800038843266   Iteration 82 of 100, tot loss = 4.726466322817454, l1: 0.0001070410334494493, l2: 0.0003656055996543728   Iteration 83 of 100, tot loss = 4.717065663222807, l1: 0.00010684042130575897, l2: 0.00036486614574762397   Iteration 84 of 100, tot loss = 4.713220813444683, l1: 0.0001069903653063063, l2: 0.0003643317168531385   Iteration 85 of 100, tot loss = 4.736105213445776, l1: 0.00010736933639718164, l2: 0.0003662411854533917   Iteration 86 of 100, tot loss = 4.7193761162979655, l1: 0.00010668021752999901, l2: 0.0003652573946584558   Iteration 87 of 100, tot loss = 4.73129289588709, l1: 0.0001068822575375791, l2: 0.00036624703277303867   Iteration 88 of 100, tot loss = 4.71067823740569, l1: 0.00010658441632668573, l2: 0.0003644834082065658   Iteration 89 of 100, tot loss = 4.725836149762186, l1: 0.00010692371183671476, l2: 0.00036565990388665476   Iteration 90 of 100, tot loss = 4.722808012697432, l1: 0.00010701900278541467, l2: 0.00036526179917725836   Iteration 91 of 100, tot loss = 4.694661047432449, l1: 0.00010627319368343176, l2: 0.0003631929118585374   Iteration 92 of 100, tot loss = 4.68192230488943, l1: 0.00010601774370709039, l2: 0.0003621744877107077   Iteration 93 of 100, tot loss = 4.6704828828893685, l1: 0.00010580778024466057, l2: 0.00036124050897616214   Iteration 94 of 100, tot loss = 4.67748650210969, l1: 0.0001060291546154792, l2: 0.0003617194959859146   Iteration 95 of 100, tot loss = 4.693138048523351, l1: 0.00010633685608474097, l2: 0.00036297694926983433   Iteration 96 of 100, tot loss = 4.714047746111949, l1: 0.00010673336680611101, l2: 0.0003646714079271381   Iteration 97 of 100, tot loss = 4.696058685017615, l1: 0.0001064464831307708, l2: 0.000363159385274558   Iteration 98 of 100, tot loss = 4.705429304619225, l1: 0.00010642108786762075, l2: 0.00036412184197949813   Iteration 99 of 100, tot loss = 4.700390438840847, l1: 0.00010631019419330794, l2: 0.0003637288490691307   Iteration 100 of 100, tot loss = 4.702020305395126, l1: 0.00010635799979354488, l2: 0.0003638440303620882
   End of epoch 1388; saving model... 

Epoch 1389 of 2000
   Iteration 1 of 100, tot loss = 6.038471698760986, l1: 0.00013771122030448169, l2: 0.00046613591257482767   Iteration 2 of 100, tot loss = 5.617578029632568, l1: 0.0001255148854397703, l2: 0.0004362429026514292   Iteration 3 of 100, tot loss = 4.680198748906453, l1: 0.00010573622906425346, l2: 0.0003622836326636995   Iteration 4 of 100, tot loss = 4.046987593173981, l1: 9.443125418329146e-05, l2: 0.00031026749638840556   Iteration 5 of 100, tot loss = 4.105016565322876, l1: 9.432181104784831e-05, l2: 0.00031617984059266745   Iteration 6 of 100, tot loss = 4.145983974138896, l1: 9.12335514537214e-05, l2: 0.00032336484582629055   Iteration 7 of 100, tot loss = 4.7614001887185236, l1: 9.777732338989154e-05, l2: 0.0003783626972498106   Iteration 8 of 100, tot loss = 4.617589294910431, l1: 9.802217027754523e-05, l2: 0.00036373676266521215   Iteration 9 of 100, tot loss = 4.564693503909641, l1: 9.925677133853444e-05, l2: 0.00035721258286179765   Iteration 10 of 100, tot loss = 4.726069784164428, l1: 0.00010084445384563878, l2: 0.0003717625280842185   Iteration 11 of 100, tot loss = 4.833865642547607, l1: 0.00010020883796228604, l2: 0.00038317772983150047   Iteration 12 of 100, tot loss = 4.890463511149089, l1: 0.00010028560366966606, l2: 0.0003887607526849024   Iteration 13 of 100, tot loss = 4.658116248937754, l1: 9.725881290460865e-05, l2: 0.0003685528163959344   Iteration 14 of 100, tot loss = 4.572634032794407, l1: 9.512798039525348e-05, l2: 0.00036213542521831447   Iteration 15 of 100, tot loss = 4.633400392532349, l1: 9.63713430489103e-05, l2: 0.00036696869938168675   Iteration 16 of 100, tot loss = 4.531755656003952, l1: 9.399927012054832e-05, l2: 0.0003591762988435221   Iteration 17 of 100, tot loss = 4.565774721257827, l1: 9.48291490203701e-05, l2: 0.0003617483247687821   Iteration 18 of 100, tot loss = 4.635290596220228, l1: 9.574658987629745e-05, l2: 0.0003677824723935272   Iteration 19 of 100, tot loss = 4.600853204727173, l1: 9.664565885415006e-05, l2: 0.0003634396654026779   Iteration 20 of 100, tot loss = 4.576351392269134, l1: 9.510220661468338e-05, l2: 0.0003625329358328599   Iteration 21 of 100, tot loss = 4.6652306488582065, l1: 9.827049749825771e-05, l2: 0.0003682525705135915   Iteration 22 of 100, tot loss = 4.56619376485998, l1: 9.67969826888293e-05, l2: 0.0003598223966863853   Iteration 23 of 100, tot loss = 4.570915626442951, l1: 9.797948966835342e-05, l2: 0.00035911207416337794   Iteration 24 of 100, tot loss = 4.63457919160525, l1: 9.956663052435033e-05, l2: 0.0003638912882403626   Iteration 25 of 100, tot loss = 4.5451608085632325, l1: 9.805587702430784e-05, l2: 0.0003564602037658915   Iteration 26 of 100, tot loss = 4.5637837923490086, l1: 9.844470500516203e-05, l2: 0.00035793367323304457   Iteration 27 of 100, tot loss = 4.570325356942636, l1: 9.920054839717018e-05, l2: 0.00035783198621141274   Iteration 28 of 100, tot loss = 4.576651130403791, l1: 9.878425095978725e-05, l2: 0.00035888086183279356   Iteration 29 of 100, tot loss = 4.5722180234974825, l1: 9.830396497975392e-05, l2: 0.0003589178372278874   Iteration 30 of 100, tot loss = 4.571821053822835, l1: 9.890375440591015e-05, l2: 0.00035827835138964775   Iteration 31 of 100, tot loss = 4.599017604704826, l1: 0.00010015936911053535, l2: 0.00035974239196342924   Iteration 32 of 100, tot loss = 4.667729079723358, l1: 0.0001018311224925128, l2: 0.0003649417853921477   Iteration 33 of 100, tot loss = 4.672582192854448, l1: 0.00010195876022793748, l2: 0.00036529945976905185   Iteration 34 of 100, tot loss = 4.67267565166249, l1: 0.00010157500368864824, l2: 0.0003656925627557725   Iteration 35 of 100, tot loss = 4.634583357402256, l1: 0.00010106055124197156, l2: 0.00036239778564777224   Iteration 36 of 100, tot loss = 4.640745381514232, l1: 0.00010140141502132692, l2: 0.0003626731239718437   Iteration 37 of 100, tot loss = 4.594994396776767, l1: 9.995690758129254e-05, l2: 0.00035954253296556607   Iteration 38 of 100, tot loss = 4.631120876262062, l1: 0.00010074204721221529, l2: 0.0003623700402768966   Iteration 39 of 100, tot loss = 4.578884020829812, l1: 9.963478833714571e-05, l2: 0.0003582536136934486   Iteration 40 of 100, tot loss = 4.569909054040909, l1: 9.962691847249516e-05, l2: 0.0003573639864043798   Iteration 41 of 100, tot loss = 4.534457834755502, l1: 9.939360754243552e-05, l2: 0.00035405217559167704   Iteration 42 of 100, tot loss = 4.534494002660115, l1: 9.937627354312488e-05, l2: 0.0003540731268003583   Iteration 43 of 100, tot loss = 4.550181666085886, l1: 0.00010010933692683466, l2: 0.00035490882975525807   Iteration 44 of 100, tot loss = 4.614496296102351, l1: 0.00010166336828247454, l2: 0.0003597862609736198   Iteration 45 of 100, tot loss = 4.560546196831597, l1: 0.00010039072043986784, l2: 0.0003556638991641295   Iteration 46 of 100, tot loss = 4.596605217975119, l1: 0.00010046294514525115, l2: 0.0003591975758393781   Iteration 47 of 100, tot loss = 4.561687586155344, l1: 9.996798465355359e-05, l2: 0.0003562007734720457   Iteration 48 of 100, tot loss = 4.583674008647601, l1: 9.974182800457736e-05, l2: 0.00035862557221359265   Iteration 49 of 100, tot loss = 4.600789911892949, l1: 0.00010025970357451208, l2: 0.0003598192867072185   Iteration 50 of 100, tot loss = 4.603842368125916, l1: 0.00010068575575132855, l2: 0.0003596984798787162   Iteration 51 of 100, tot loss = 4.650019023932662, l1: 0.0001016810954966144, l2: 0.0003633208055843544   Iteration 52 of 100, tot loss = 4.617329666247735, l1: 0.00010132689077671294, l2: 0.0003604060744132417   Iteration 53 of 100, tot loss = 4.563590702020897, l1: 0.00010010757683150312, l2: 0.00035625149194977055   Iteration 54 of 100, tot loss = 4.5885973727261575, l1: 0.00010053428256166323, l2: 0.000358325452974963   Iteration 55 of 100, tot loss = 4.632434701919555, l1: 0.000100984007837145, l2: 0.0003622594606977972   Iteration 56 of 100, tot loss = 4.605770839112146, l1: 0.0001005644352777121, l2: 0.00036001264691419365   Iteration 57 of 100, tot loss = 4.57693043926306, l1: 0.00010007715779162633, l2: 0.00035761588425661453   Iteration 58 of 100, tot loss = 4.5511346438835405, l1: 9.954938205141686e-05, l2: 0.000355564080407554   Iteration 59 of 100, tot loss = 4.532968327150506, l1: 9.903495706335673e-05, l2: 0.0003542618735267197   Iteration 60 of 100, tot loss = 4.534201502799988, l1: 9.916195237262097e-05, l2: 0.00035425819563291344   Iteration 61 of 100, tot loss = 4.536602731610908, l1: 9.969220591897955e-05, l2: 0.00035396806483508134   Iteration 62 of 100, tot loss = 4.542626403993176, l1: 0.00010007879720961359, l2: 0.00035418384105733206   Iteration 63 of 100, tot loss = 4.525575834607321, l1: 9.973630493198196e-05, l2: 0.0003528212763494738   Iteration 64 of 100, tot loss = 4.539734430611134, l1: 0.00010017162190933959, l2: 0.0003538018190738512   Iteration 65 of 100, tot loss = 4.538550296196571, l1: 0.00010049997044216769, l2: 0.00035335505694652407   Iteration 66 of 100, tot loss = 4.518383600495079, l1: 0.00010036042046770241, l2: 0.00035147793713611355   Iteration 67 of 100, tot loss = 4.534086736280527, l1: 0.00010052544525051281, l2: 0.00035288322603142363   Iteration 68 of 100, tot loss = 4.502824299475726, l1: 9.986747591028688e-05, l2: 0.0003504149518609184   Iteration 69 of 100, tot loss = 4.460051090820976, l1: 9.92155545500809e-05, l2: 0.0003467895525239368   Iteration 70 of 100, tot loss = 4.437855502537318, l1: 9.888726802143667e-05, l2: 0.0003448982805171649   Iteration 71 of 100, tot loss = 4.446909098558023, l1: 9.914814536771277e-05, l2: 0.0003455427632475285   Iteration 72 of 100, tot loss = 4.44675377342436, l1: 9.901151578459475e-05, l2: 0.00034566386026805657   Iteration 73 of 100, tot loss = 4.433501436285777, l1: 9.881088124230912e-05, l2: 0.00034453926122769685   Iteration 74 of 100, tot loss = 4.4351819109272315, l1: 9.91226322209538e-05, l2: 0.000344395557968292   Iteration 75 of 100, tot loss = 4.452583640416464, l1: 9.944844602917631e-05, l2: 0.00034580991710148133   Iteration 76 of 100, tot loss = 4.4582876563072205, l1: 9.958764435156666e-05, l2: 0.00034624112049641553   Iteration 77 of 100, tot loss = 4.482615907470901, l1: 9.98595312378345e-05, l2: 0.00034840205862328964   Iteration 78 of 100, tot loss = 4.495805364388686, l1: 0.0001003070776791085, l2: 0.00034927345805977564   Iteration 79 of 100, tot loss = 4.494159128092512, l1: 0.00010041325049890394, l2: 0.0003490026614185175   Iteration 80 of 100, tot loss = 4.506618496775627, l1: 0.00010086550128107774, l2: 0.0003497963472000265   Iteration 81 of 100, tot loss = 4.524958107206556, l1: 0.00010127162520937552, l2: 0.0003512241843660983   Iteration 82 of 100, tot loss = 4.542270105059554, l1: 0.00010140873662127955, l2: 0.00035281827282543837   Iteration 83 of 100, tot loss = 4.580273246190634, l1: 0.0001016727717753208, l2: 0.0003563545517874489   Iteration 84 of 100, tot loss = 4.564683147839138, l1: 0.00010158035183849279, l2: 0.0003548879618999005   Iteration 85 of 100, tot loss = 4.5503786030937645, l1: 0.00010139965846011525, l2: 0.0003536382007844034   Iteration 86 of 100, tot loss = 4.571097257525422, l1: 0.0001017948797684661, l2: 0.0003553148446879404   Iteration 87 of 100, tot loss = 4.560499311863691, l1: 0.000101673638333711, l2: 0.0003543762916173176   Iteration 88 of 100, tot loss = 4.5499512092633685, l1: 0.0001013768879519458, l2: 0.00035361823186328615   Iteration 89 of 100, tot loss = 4.53257820043671, l1: 0.00010133878201353521, l2: 0.00035191903695571395   Iteration 90 of 100, tot loss = 4.538106102413601, l1: 0.00010149475856451318, l2: 0.00035231585053326043   Iteration 91 of 100, tot loss = 4.540371601398174, l1: 0.00010145776724419812, l2: 0.00035257939197146077   Iteration 92 of 100, tot loss = 4.556656008181364, l1: 0.00010152911296324379, l2: 0.000354136487500079   Iteration 93 of 100, tot loss = 4.554667303639073, l1: 0.00010146035688243286, l2: 0.0003540063730186923   Iteration 94 of 100, tot loss = 4.575408149272837, l1: 0.00010171888301401657, l2: 0.0003558219317749311   Iteration 95 of 100, tot loss = 4.572034760525352, l1: 0.00010167448007881544, l2: 0.0003555289960594995   Iteration 96 of 100, tot loss = 4.574997584025065, l1: 0.00010154235011820371, l2: 0.00035595740829800587   Iteration 97 of 100, tot loss = 4.5626763904217595, l1: 0.00010125454381401123, l2: 0.00035501309538790574   Iteration 98 of 100, tot loss = 4.570902123743174, l1: 0.00010148017790182779, l2: 0.0003556100345289154   Iteration 99 of 100, tot loss = 4.578386687269115, l1: 0.0001018192691697899, l2: 0.0003560193999690436   Iteration 100 of 100, tot loss = 4.574571132659912, l1: 0.00010148320208827499, l2: 0.00035597391142800917
   End of epoch 1389; saving model... 

Epoch 1390 of 2000
   Iteration 1 of 100, tot loss = 6.446696758270264, l1: 0.00014615290274377912, l2: 0.0004985167761333287   Iteration 2 of 100, tot loss = 5.639551639556885, l1: 0.00012116918514948338, l2: 0.00044278598215896636   Iteration 3 of 100, tot loss = 4.923300266265869, l1: 0.00011132890116035317, l2: 0.00038100112578831613   Iteration 4 of 100, tot loss = 4.378754138946533, l1: 9.573433180776192e-05, l2: 0.0003421410801820457   Iteration 5 of 100, tot loss = 4.16722092628479, l1: 9.335727590951138e-05, l2: 0.00032336481381207706   Iteration 6 of 100, tot loss = 4.1339638233184814, l1: 8.835328541560254e-05, l2: 0.00032504309395638603   Iteration 7 of 100, tot loss = 4.0422302314213345, l1: 8.804514540575578e-05, l2: 0.0003161778773314187   Iteration 8 of 100, tot loss = 4.2170723378658295, l1: 9.270243799619493e-05, l2: 0.0003290047934569884   Iteration 9 of 100, tot loss = 4.200404670503405, l1: 9.542049560372511e-05, l2: 0.0003246199695341703   Iteration 10 of 100, tot loss = 4.0452563762664795, l1: 9.025457875395659e-05, l2: 0.00031427105714101347   Iteration 11 of 100, tot loss = 3.938549757003784, l1: 8.904205639158714e-05, l2: 0.00030481291733766824   Iteration 12 of 100, tot loss = 4.20289009809494, l1: 9.48340709025312e-05, l2: 0.00032545493498522166   Iteration 13 of 100, tot loss = 4.21778024159945, l1: 9.623106850015644e-05, l2: 0.0003255469536480422   Iteration 14 of 100, tot loss = 4.02054455450603, l1: 9.173590634808144e-05, l2: 0.0003103185476252942   Iteration 15 of 100, tot loss = 4.221957039833069, l1: 9.430532008991577e-05, l2: 0.0003278903808677569   Iteration 16 of 100, tot loss = 4.242352955043316, l1: 9.486634348832013e-05, l2: 0.0003293689487691154   Iteration 17 of 100, tot loss = 4.169759547009187, l1: 9.235989166293567e-05, l2: 0.00032461606058538617   Iteration 18 of 100, tot loss = 4.165761225753361, l1: 9.21395571317084e-05, l2: 0.00032443656447058957   Iteration 19 of 100, tot loss = 4.115029504424648, l1: 9.152667476691452e-05, l2: 0.0003199762738260784   Iteration 20 of 100, tot loss = 4.096986120939254, l1: 9.149699453701033e-05, l2: 0.0003182016153004952   Iteration 21 of 100, tot loss = 4.109168069703238, l1: 9.141731234627688e-05, l2: 0.0003194994919578589   Iteration 22 of 100, tot loss = 4.08088910037821, l1: 9.065602237321649e-05, l2: 0.0003174328845819797   Iteration 23 of 100, tot loss = 4.143841271815092, l1: 9.314297036009678e-05, l2: 0.0003212411538697779   Iteration 24 of 100, tot loss = 4.0970243364572525, l1: 9.251812768222105e-05, l2: 0.00031718430303347606   Iteration 25 of 100, tot loss = 4.137104163169861, l1: 9.362272845464759e-05, l2: 0.00032008768641389907   Iteration 26 of 100, tot loss = 4.154557241843297, l1: 9.390247284430258e-05, l2: 0.0003215532499150588   Iteration 27 of 100, tot loss = 4.19429095144625, l1: 9.339151424851648e-05, l2: 0.00032603757938853015   Iteration 28 of 100, tot loss = 4.195547695670809, l1: 9.284425362108908e-05, l2: 0.00032671051407565495   Iteration 29 of 100, tot loss = 4.200927730264334, l1: 9.330983485942225e-05, l2: 0.00032678293666384855   Iteration 30 of 100, tot loss = 4.140744936466217, l1: 9.175140391259144e-05, l2: 0.000322323088767007   Iteration 31 of 100, tot loss = 4.098084192122182, l1: 9.131760124282371e-05, l2: 0.00031849081748195235   Iteration 32 of 100, tot loss = 4.117405507713556, l1: 9.099485987462685e-05, l2: 0.0003207456902600825   Iteration 33 of 100, tot loss = 4.107377496632663, l1: 9.055963781782228e-05, l2: 0.0003201781108862523   Iteration 34 of 100, tot loss = 4.212125830790576, l1: 9.26623292530969e-05, l2: 0.00032855025389889143   Iteration 35 of 100, tot loss = 4.24067633152008, l1: 9.28748575721069e-05, l2: 0.00033119277546315323   Iteration 36 of 100, tot loss = 4.197056230571535, l1: 9.183914739373399e-05, l2: 0.00032786647595154744   Iteration 37 of 100, tot loss = 4.212119208799826, l1: 9.131993283517659e-05, l2: 0.00032989198823358764   Iteration 38 of 100, tot loss = 4.2590796853366655, l1: 9.198051281457178e-05, l2: 0.0003339274561240975   Iteration 39 of 100, tot loss = 4.2768547748908015, l1: 9.230393334291875e-05, l2: 0.00033538154541299894   Iteration 40 of 100, tot loss = 4.328251078724861, l1: 9.369794970552903e-05, l2: 0.0003391271595319267   Iteration 41 of 100, tot loss = 4.345450497255093, l1: 9.358251288465066e-05, l2: 0.00034096253760408884   Iteration 42 of 100, tot loss = 4.370887316408611, l1: 9.429454699524545e-05, l2: 0.0003427941853823584   Iteration 43 of 100, tot loss = 4.311196981474411, l1: 9.297743836637089e-05, l2: 0.00033814226057878586   Iteration 44 of 100, tot loss = 4.323072303425182, l1: 9.346283323710932e-05, l2: 0.00033884439802601594   Iteration 45 of 100, tot loss = 4.335293049282498, l1: 9.433590611378249e-05, l2: 0.00033919339960751435   Iteration 46 of 100, tot loss = 4.366444421851116, l1: 9.472290619906626e-05, l2: 0.0003419215367023793   Iteration 47 of 100, tot loss = 4.330637287586294, l1: 9.438071294348351e-05, l2: 0.000338683016300994   Iteration 48 of 100, tot loss = 4.328693563739459, l1: 9.484131055614853e-05, l2: 0.0003380280462200365   Iteration 49 of 100, tot loss = 4.3405740358391585, l1: 9.543158368466478e-05, l2: 0.0003386258203488756   Iteration 50 of 100, tot loss = 4.3218778705596925, l1: 9.496979393588845e-05, l2: 0.00033721799380145964   Iteration 51 of 100, tot loss = 4.339892985774021, l1: 9.550953373214797e-05, l2: 0.0003384797652388978   Iteration 52 of 100, tot loss = 4.303693583378425, l1: 9.493937131292814e-05, l2: 0.00033542998733957153   Iteration 53 of 100, tot loss = 4.31180895049617, l1: 9.530613478657544e-05, l2: 0.0003358747599808991   Iteration 54 of 100, tot loss = 4.321950863908838, l1: 9.571235103875451e-05, l2: 0.0003364827352925204   Iteration 55 of 100, tot loss = 4.331013345718384, l1: 9.636194782532667e-05, l2: 0.00033673938675995237   Iteration 56 of 100, tot loss = 4.334023215941021, l1: 9.60896740512648e-05, l2: 0.00033731264790861005   Iteration 57 of 100, tot loss = 4.344538868519297, l1: 9.619913620745617e-05, l2: 0.00033825475047546716   Iteration 58 of 100, tot loss = 4.386906932140219, l1: 9.67046455806322e-05, l2: 0.0003419860468370487   Iteration 59 of 100, tot loss = 4.3463803044820235, l1: 9.618744951370961e-05, l2: 0.00033845058005989797   Iteration 60 of 100, tot loss = 4.358127107222875, l1: 9.644721618921419e-05, l2: 0.0003393654941949838   Iteration 61 of 100, tot loss = 4.364989802485606, l1: 9.670499078862629e-05, l2: 0.0003397939891313951   Iteration 62 of 100, tot loss = 4.345620691776276, l1: 9.630815046905135e-05, l2: 0.0003382539183686247   Iteration 63 of 100, tot loss = 4.374968943141756, l1: 9.660016474386101e-05, l2: 0.000340896729049125   Iteration 64 of 100, tot loss = 4.38566623441875, l1: 9.669082891150538e-05, l2: 0.0003418757939925854   Iteration 65 of 100, tot loss = 4.3815372301982, l1: 9.663953191752975e-05, l2: 0.00034151419061415186   Iteration 66 of 100, tot loss = 4.3591887427098825, l1: 9.657381104427094e-05, l2: 0.00033934506262250414   Iteration 67 of 100, tot loss = 4.383897943283195, l1: 9.700483535008101e-05, l2: 0.0003413849585401172   Iteration 68 of 100, tot loss = 4.388126434648738, l1: 9.736368846300619e-05, l2: 0.00034144895492799524   Iteration 69 of 100, tot loss = 4.376727064450582, l1: 9.725716607636718e-05, l2: 0.00034041554043643123   Iteration 70 of 100, tot loss = 4.347299250534602, l1: 9.65908053331077e-05, l2: 0.00033813911972434393   Iteration 71 of 100, tot loss = 4.355053493674372, l1: 9.691555558828453e-05, l2: 0.0003385897934876225   Iteration 72 of 100, tot loss = 4.318015837007099, l1: 9.614266345023579e-05, l2: 0.00033565891984229285   Iteration 73 of 100, tot loss = 4.323346082478354, l1: 9.6222676254238e-05, l2: 0.00033611193146199396   Iteration 74 of 100, tot loss = 4.3302106309581445, l1: 9.63752483732753e-05, l2: 0.0003366458140835921   Iteration 75 of 100, tot loss = 4.328993584314982, l1: 9.657700191989231e-05, l2: 0.0003363223560154438   Iteration 76 of 100, tot loss = 4.316107875422428, l1: 9.619438562941995e-05, l2: 0.00033541640158019667   Iteration 77 of 100, tot loss = 4.320975346998735, l1: 9.628856744936232e-05, l2: 0.0003358089669710388   Iteration 78 of 100, tot loss = 4.3281159951136665, l1: 9.631239798834894e-05, l2: 0.00033649920153085334   Iteration 79 of 100, tot loss = 4.337363436252256, l1: 9.665242308799378e-05, l2: 0.0003370839201320623   Iteration 80 of 100, tot loss = 4.326432532072067, l1: 9.64895426932344e-05, l2: 0.00033615370994084516   Iteration 81 of 100, tot loss = 4.356710628226951, l1: 9.688903895791701e-05, l2: 0.00033878202314408104   Iteration 82 of 100, tot loss = 4.368879207750646, l1: 9.704081364871831e-05, l2: 0.00033984710668247736   Iteration 83 of 100, tot loss = 4.357506662966257, l1: 9.678252733344806e-05, l2: 0.0003389681385078254   Iteration 84 of 100, tot loss = 4.359208949974605, l1: 9.669778648371686e-05, l2: 0.0003392231079661066   Iteration 85 of 100, tot loss = 4.40691367037156, l1: 9.752563992686405e-05, l2: 0.00034316572744180177   Iteration 86 of 100, tot loss = 4.410476587539495, l1: 9.784754589883621e-05, l2: 0.000343200113410455   Iteration 87 of 100, tot loss = 4.410408357094074, l1: 9.756176959122424e-05, l2: 0.0003434790665653117   Iteration 88 of 100, tot loss = 4.458264890042218, l1: 9.839749455667598e-05, l2: 0.0003474289951554965   Iteration 89 of 100, tot loss = 4.469613404756181, l1: 9.857247555935808e-05, l2: 0.00034838886519506824   Iteration 90 of 100, tot loss = 4.45293869972229, l1: 9.822721027982576e-05, l2: 0.0003470666599847997   Iteration 91 of 100, tot loss = 4.459647933205406, l1: 9.841555196204694e-05, l2: 0.00034754924190865197   Iteration 92 of 100, tot loss = 4.483843803405762, l1: 9.862158466326629e-05, l2: 0.0003497627960472449   Iteration 93 of 100, tot loss = 4.476252266155776, l1: 9.849480373167774e-05, l2: 0.0003491304232738912   Iteration 94 of 100, tot loss = 4.458052848247772, l1: 9.803175325554974e-05, l2: 0.00034777353195092103   Iteration 95 of 100, tot loss = 4.442056595651727, l1: 9.792745082737192e-05, l2: 0.0003462782091350834   Iteration 96 of 100, tot loss = 4.436731211841106, l1: 9.789788244537097e-05, l2: 0.00034577523926297243   Iteration 97 of 100, tot loss = 4.445908042573437, l1: 9.830696869835968e-05, l2: 0.00034628383627463826   Iteration 98 of 100, tot loss = 4.449871386800494, l1: 9.827187423812872e-05, l2: 0.0003467152652162013   Iteration 99 of 100, tot loss = 4.422342842275446, l1: 9.763326137673551e-05, l2: 0.0003446010235377183   Iteration 100 of 100, tot loss = 4.421592004299164, l1: 9.773345034773229e-05, l2: 0.0003444257509545423
   End of epoch 1390; saving model... 

Epoch 1391 of 2000
   Iteration 1 of 100, tot loss = 4.855208873748779, l1: 0.00010489344276720658, l2: 0.0003806274617090821   Iteration 2 of 100, tot loss = 4.233027696609497, l1: 9.722622417029925e-05, l2: 0.000326076551573351   Iteration 3 of 100, tot loss = 4.086206674575806, l1: 0.00010313237241158883, l2: 0.0003054882981814444   Iteration 4 of 100, tot loss = 4.014135539531708, l1: 9.208712890540482e-05, l2: 0.0003093264240305871   Iteration 5 of 100, tot loss = 4.266305685043335, l1: 9.89764761470724e-05, l2: 0.00032765408977866175   Iteration 6 of 100, tot loss = 4.005732138951619, l1: 9.330261491413694e-05, l2: 0.00030727059493074194   Iteration 7 of 100, tot loss = 4.180894034249442, l1: 9.525848820755658e-05, l2: 0.0003228309069527313   Iteration 8 of 100, tot loss = 4.164902031421661, l1: 9.677881962488755e-05, l2: 0.00031971137468644883   Iteration 9 of 100, tot loss = 4.131332185533312, l1: 9.643304131006719e-05, l2: 0.00031670016889822565   Iteration 10 of 100, tot loss = 4.221680498123169, l1: 9.71453275269596e-05, l2: 0.0003250227149692364   Iteration 11 of 100, tot loss = 4.362903811714866, l1: 9.979813760516912e-05, l2: 0.0003364922370317138   Iteration 12 of 100, tot loss = 4.4968316952387495, l1: 0.00010266709038357173, l2: 0.0003470160748596148   Iteration 13 of 100, tot loss = 4.539309575007512, l1: 0.00010301054890777987, l2: 0.0003509204039941184   Iteration 14 of 100, tot loss = 4.607202359608242, l1: 0.00010254290854001218, l2: 0.00035817732090695893   Iteration 15 of 100, tot loss = 4.448094097773234, l1: 0.00010006960922813354, l2: 0.00034473979321774094   Iteration 16 of 100, tot loss = 4.439275607466698, l1: 9.996526273425843e-05, l2: 0.0003439622933001374   Iteration 17 of 100, tot loss = 4.430616476956536, l1: 9.81534794846084e-05, l2: 0.00034490816538249524   Iteration 18 of 100, tot loss = 4.367649330033196, l1: 9.7793052191264e-05, l2: 0.0003389718791974398   Iteration 19 of 100, tot loss = 4.537934215445268, l1: 0.0001006900939070252, l2: 0.00035310332513242766   Iteration 20 of 100, tot loss = 4.521301352977753, l1: 9.957610727724386e-05, l2: 0.0003525540254486259   Iteration 21 of 100, tot loss = 4.584316128776187, l1: 0.00010140498367642674, l2: 0.0003570266257849566   Iteration 22 of 100, tot loss = 4.564118591221896, l1: 0.00010043063676593275, l2: 0.00035598121824229815   Iteration 23 of 100, tot loss = 4.526338732760886, l1: 0.00010020541752885987, l2: 0.00035242845099318123   Iteration 24 of 100, tot loss = 4.542967230081558, l1: 0.00010016476502035705, l2: 0.0003541319535239988   Iteration 25 of 100, tot loss = 4.558562002182007, l1: 0.00010078128150780685, l2: 0.00035507491615135225   Iteration 26 of 100, tot loss = 4.601763569391691, l1: 0.0001012551711028209, l2: 0.00035892118299111293   Iteration 27 of 100, tot loss = 4.5957182866555675, l1: 0.00010089090277755599, l2: 0.00035868092338744274   Iteration 28 of 100, tot loss = 4.608561609472547, l1: 0.00010170753388852713, l2: 0.0003591486243489531   Iteration 29 of 100, tot loss = 4.541924731484775, l1: 0.0001000415431917645, l2: 0.0003541509273606513   Iteration 30 of 100, tot loss = 4.496459865570069, l1: 9.932127213687635e-05, l2: 0.00035032471253847083   Iteration 31 of 100, tot loss = 4.5200671226747575, l1: 0.00010048433121541635, l2: 0.0003515223797876388   Iteration 32 of 100, tot loss = 4.556013509631157, l1: 0.00010102567262038065, l2: 0.00035457567719276994   Iteration 33 of 100, tot loss = 4.555928649324359, l1: 0.00010098841051909734, l2: 0.00035460445338465047   Iteration 34 of 100, tot loss = 4.567148825701545, l1: 0.0001017964112928466, l2: 0.0003549184715238345   Iteration 35 of 100, tot loss = 4.566073908124651, l1: 0.00010155808122362942, l2: 0.0003550493100192398   Iteration 36 of 100, tot loss = 4.56286444928911, l1: 0.00010179421305009682, l2: 0.00035449223246865184   Iteration 37 of 100, tot loss = 4.524806112856479, l1: 0.00010045388429520048, l2: 0.00035202672714493364   Iteration 38 of 100, tot loss = 4.561676000293932, l1: 0.00010117012444473068, l2: 0.00035499747517812804   Iteration 39 of 100, tot loss = 4.546262606596335, l1: 9.993594386077557e-05, l2: 0.000354690316807813   Iteration 40 of 100, tot loss = 4.5510763883590695, l1: 9.971047711587744e-05, l2: 0.0003553971619112417   Iteration 41 of 100, tot loss = 4.512463883655827, l1: 9.889776957584772e-05, l2: 0.00035234861907821783   Iteration 42 of 100, tot loss = 4.495007753372192, l1: 9.937910484344077e-05, l2: 0.0003501216706354171   Iteration 43 of 100, tot loss = 4.508797301802525, l1: 9.945518628905288e-05, l2: 0.0003514245442158088   Iteration 44 of 100, tot loss = 4.498050613836809, l1: 9.993711170285347e-05, l2: 0.0003498679500560022   Iteration 45 of 100, tot loss = 4.431967128647698, l1: 9.840520465837067e-05, l2: 0.00034479150862251926   Iteration 46 of 100, tot loss = 4.447129205517147, l1: 9.866611427518681e-05, l2: 0.0003460468061625168   Iteration 47 of 100, tot loss = 4.4707899169718965, l1: 9.976810798299142e-05, l2: 0.00034731088352929604   Iteration 48 of 100, tot loss = 4.502602892617385, l1: 0.0001002583176917445, l2: 0.0003500019715829694   Iteration 49 of 100, tot loss = 4.4955291529091035, l1: 9.994170005131532e-05, l2: 0.00034961121573174675   Iteration 50 of 100, tot loss = 4.484233710765839, l1: 9.970785460609477e-05, l2: 0.00034871551688411275   Iteration 51 of 100, tot loss = 4.512895198429332, l1: 0.00010036093754381142, l2: 0.00035092858303744604   Iteration 52 of 100, tot loss = 4.521935946666277, l1: 0.00010058431570751754, l2: 0.00035160928004748153   Iteration 53 of 100, tot loss = 4.519540955435555, l1: 0.00010025013013872458, l2: 0.00035170396641094484   Iteration 54 of 100, tot loss = 4.4897487936196505, l1: 9.977371485513652e-05, l2: 0.0003492011653886746   Iteration 55 of 100, tot loss = 4.508633091232993, l1: 0.00010060688331246968, l2: 0.0003502564260005866   Iteration 56 of 100, tot loss = 4.509880693895476, l1: 0.00010096757101304579, l2: 0.0003500204983148641   Iteration 57 of 100, tot loss = 4.548600102725782, l1: 0.00010180841395355713, l2: 0.00035305159652029703   Iteration 58 of 100, tot loss = 4.555166480870082, l1: 0.00010175073193436376, l2: 0.0003537659166764131   Iteration 59 of 100, tot loss = 4.549849653648118, l1: 0.00010212802657930543, l2: 0.00035285693876962256   Iteration 60 of 100, tot loss = 4.521065392096838, l1: 0.0001012917256957735, l2: 0.0003508148131610748   Iteration 61 of 100, tot loss = 4.5403750275002155, l1: 0.00010201878437522387, l2: 0.00035201871782430586   Iteration 62 of 100, tot loss = 4.550196276557061, l1: 0.00010251310812882073, l2: 0.00035250651865592977   Iteration 63 of 100, tot loss = 4.601228401774452, l1: 0.00010352095803624713, l2: 0.0003566018813650035   Iteration 64 of 100, tot loss = 4.565580880269408, l1: 0.00010283890520668137, l2: 0.0003537191820441876   Iteration 65 of 100, tot loss = 4.552057293745188, l1: 0.00010242401639026446, l2: 0.0003527817124939667   Iteration 66 of 100, tot loss = 4.562505416797869, l1: 0.0001022470357080844, l2: 0.00035400350613847627   Iteration 67 of 100, tot loss = 4.557032896511591, l1: 0.00010251157187097428, l2: 0.000353191717837958   Iteration 68 of 100, tot loss = 4.519493185422, l1: 0.00010182092030257027, l2: 0.0003501283982212258   Iteration 69 of 100, tot loss = 4.514924973681353, l1: 0.00010148490384934753, l2: 0.00035000759321585485   Iteration 70 of 100, tot loss = 4.5250324232237675, l1: 0.00010174017219729388, l2: 0.0003507630698939985   Iteration 71 of 100, tot loss = 4.514794294263275, l1: 0.00010198556598149647, l2: 0.00034949386319798145   Iteration 72 of 100, tot loss = 4.497060724430614, l1: 0.00010177888184949147, l2: 0.0003479271901798913   Iteration 73 of 100, tot loss = 4.504569989361175, l1: 0.00010165714431348359, l2: 0.0003487998540935821   Iteration 74 of 100, tot loss = 4.503193798902872, l1: 0.00010197356710410236, l2: 0.00034834581242034184   Iteration 75 of 100, tot loss = 4.509021283785502, l1: 0.00010225250089812713, l2: 0.0003486496273156566   Iteration 76 of 100, tot loss = 4.488121297798659, l1: 0.0001019352441864229, l2: 0.000346876885630895   Iteration 77 of 100, tot loss = 4.47516472463484, l1: 0.00010176474711738297, l2: 0.00034575172548421187   Iteration 78 of 100, tot loss = 4.488064584059593, l1: 0.00010211443893636804, l2: 0.00034669201919220935   Iteration 79 of 100, tot loss = 4.492358384253103, l1: 0.00010220224716180428, l2: 0.00034703359092936464   Iteration 80 of 100, tot loss = 4.507275737822056, l1: 0.00010249881029267272, l2: 0.00034822876286852986   Iteration 81 of 100, tot loss = 4.476755873656567, l1: 0.00010174575782227215, l2: 0.0003459298289574993   Iteration 82 of 100, tot loss = 4.459405159077993, l1: 0.00010124155326083405, l2: 0.00034469896215851784   Iteration 83 of 100, tot loss = 4.45788130559117, l1: 0.00010118544386967691, l2: 0.0003446026862821968   Iteration 84 of 100, tot loss = 4.4962785456861765, l1: 0.00010197820592154694, l2: 0.0003476496480115678   Iteration 85 of 100, tot loss = 4.510990281666026, l1: 0.00010226105338969158, l2: 0.00034883797420394224   Iteration 86 of 100, tot loss = 4.496553070323412, l1: 0.00010218609383818423, l2: 0.00034746921275739643   Iteration 87 of 100, tot loss = 4.556468244256644, l1: 0.00010273511637431762, l2: 0.00035291170691683266   Iteration 88 of 100, tot loss = 4.572487271644852, l1: 0.00010279671199565531, l2: 0.0003544520139423019   Iteration 89 of 100, tot loss = 4.559554122806935, l1: 0.00010259812523635743, l2: 0.0003533572856345988   Iteration 90 of 100, tot loss = 4.525775663057963, l1: 0.0001018315493032181, l2: 0.0003507460157076518   Iteration 91 of 100, tot loss = 4.545978779321189, l1: 0.00010210523328418936, l2: 0.0003524926429675831   Iteration 92 of 100, tot loss = 4.536343095095261, l1: 0.00010192768414803457, l2: 0.00035170662380076703   Iteration 93 of 100, tot loss = 4.515687596413397, l1: 0.00010147104389503628, l2: 0.0003500977141146738   Iteration 94 of 100, tot loss = 4.522995596236371, l1: 0.00010172976037714025, l2: 0.0003505697977937203   Iteration 95 of 100, tot loss = 4.511001852939003, l1: 0.00010132401125462677, l2: 0.0003497761726305869   Iteration 96 of 100, tot loss = 4.4927138686180115, l1: 0.00010096517564761598, l2: 0.00034830620991973166   Iteration 97 of 100, tot loss = 4.483526291306486, l1: 0.00010081993995177963, l2: 0.000347532687940366   Iteration 98 of 100, tot loss = 4.498417452890045, l1: 0.0001011647691855763, l2: 0.0003486769750526612   Iteration 99 of 100, tot loss = 4.493754129217129, l1: 0.00010082586081210534, l2: 0.00034854955123996153   Iteration 100 of 100, tot loss = 4.509692323207855, l1: 0.0001009628527026507, l2: 0.00035000637915800326
   End of epoch 1391; saving model... 

Epoch 1392 of 2000
   Iteration 1 of 100, tot loss = 7.692744255065918, l1: 0.00012411951320245862, l2: 0.0006451549124903977   Iteration 2 of 100, tot loss = 5.164045572280884, l1: 9.539530947222374e-05, l2: 0.0004210092520224862   Iteration 3 of 100, tot loss = 5.516752243041992, l1: 0.00010498466872377321, l2: 0.00044669055205304176   Iteration 4 of 100, tot loss = 5.121635138988495, l1: 9.477856474404689e-05, l2: 0.0004173849411017727   Iteration 5 of 100, tot loss = 5.238855791091919, l1: 0.00010468514956301078, l2: 0.0004192004184005782   Iteration 6 of 100, tot loss = 4.921760241190593, l1: 0.00010123104705902126, l2: 0.0003909449660568498   Iteration 7 of 100, tot loss = 4.577464001519339, l1: 9.310184084045301e-05, l2: 0.00036464454855636826   Iteration 8 of 100, tot loss = 4.55260106921196, l1: 9.771585519047221e-05, l2: 0.00035754424607148394   Iteration 9 of 100, tot loss = 4.558566437827216, l1: 9.930385001805714e-05, l2: 0.00035655278609030775   Iteration 10 of 100, tot loss = 4.807700276374817, l1: 0.00010479447882971726, l2: 0.00037597554037347434   Iteration 11 of 100, tot loss = 4.918354142795909, l1: 0.00010660830924858932, l2: 0.00038522709721953356   Iteration 12 of 100, tot loss = 4.712880969047546, l1: 0.00010330951893896174, l2: 0.00036797857198204537   Iteration 13 of 100, tot loss = 4.730875602135291, l1: 0.00010348471998720645, l2: 0.00036960283558493335   Iteration 14 of 100, tot loss = 4.905559437615531, l1: 0.00010715284744427274, l2: 0.0003834030929803183   Iteration 15 of 100, tot loss = 4.92354097366333, l1: 0.00010734209354268387, l2: 0.00038501200130364546   Iteration 16 of 100, tot loss = 4.9366108775138855, l1: 0.00010744992323452607, l2: 0.00038621116345893824   Iteration 17 of 100, tot loss = 4.842751110301299, l1: 0.00010761364041279783, l2: 0.00037666146989281783   Iteration 18 of 100, tot loss = 4.815866947174072, l1: 0.00010714418183649993, l2: 0.00037444251211127266   Iteration 19 of 100, tot loss = 4.8138351691396615, l1: 0.00010698227522784452, l2: 0.00037440124019906903   Iteration 20 of 100, tot loss = 4.894525766372681, l1: 0.00010854218453459907, l2: 0.00038091039241408   Iteration 21 of 100, tot loss = 4.789571444193522, l1: 0.00010677136915979819, l2: 0.0003721857751120946   Iteration 22 of 100, tot loss = 4.697583837942644, l1: 0.00010459352316419509, l2: 0.0003651648611676964   Iteration 23 of 100, tot loss = 4.671697295230368, l1: 0.00010356111559541087, l2: 0.00036360861470355934   Iteration 24 of 100, tot loss = 4.730701853831609, l1: 0.00010453418781253276, l2: 0.0003685359976467832   Iteration 25 of 100, tot loss = 4.691414709091187, l1: 0.00010460802528541535, l2: 0.0003645334462635219   Iteration 26 of 100, tot loss = 4.59587522653433, l1: 0.00010328674397896975, l2: 0.00035630077931832953   Iteration 27 of 100, tot loss = 4.575122744948776, l1: 0.00010368016201364635, l2: 0.0003538321131297077   Iteration 28 of 100, tot loss = 4.63501581123897, l1: 0.00010437514304482778, l2: 0.0003591264378753424   Iteration 29 of 100, tot loss = 4.653210426199025, l1: 0.0001050521547360153, l2: 0.00036026888721670696   Iteration 30 of 100, tot loss = 4.659558884302775, l1: 0.00010493051692416581, l2: 0.00036102537172458444   Iteration 31 of 100, tot loss = 4.696711894004576, l1: 0.00010521550317324938, l2: 0.000364455685954571   Iteration 32 of 100, tot loss = 4.698475584387779, l1: 0.00010476202555764758, l2: 0.00036508553284875234   Iteration 33 of 100, tot loss = 4.7209177884188565, l1: 0.00010484560996040024, l2: 0.00036724616912772143   Iteration 34 of 100, tot loss = 4.70519897517036, l1: 0.00010443318366442862, l2: 0.00036608671507565305   Iteration 35 of 100, tot loss = 4.694290052141462, l1: 0.00010451955188597952, l2: 0.0003649094545315685   Iteration 36 of 100, tot loss = 4.770051466094123, l1: 0.00010519277606767396, l2: 0.00037181237106172677   Iteration 37 of 100, tot loss = 4.848195024438806, l1: 0.00010669141586215512, l2: 0.0003781280880699538   Iteration 38 of 100, tot loss = 4.771850494961989, l1: 0.00010559515718150099, l2: 0.0003715898936589886   Iteration 39 of 100, tot loss = 4.768553901941348, l1: 0.00010557071315512682, l2: 0.0003712846784112163   Iteration 40 of 100, tot loss = 4.765466436743736, l1: 0.00010530525705689797, l2: 0.0003712413876201026   Iteration 41 of 100, tot loss = 4.720594606748441, l1: 0.00010438207835072606, l2: 0.0003676773833321044   Iteration 42 of 100, tot loss = 4.6850128542809255, l1: 0.00010359823344125678, l2: 0.0003649030532826492   Iteration 43 of 100, tot loss = 4.671726207400477, l1: 0.00010355754158470433, l2: 0.00036361508056532245   Iteration 44 of 100, tot loss = 4.717438516291705, l1: 0.00010462960629163057, l2: 0.00036711424664562486   Iteration 45 of 100, tot loss = 4.76847919093238, l1: 0.00010548309914560781, l2: 0.0003713648213306442   Iteration 46 of 100, tot loss = 4.7457085977429925, l1: 0.0001055501759211715, l2: 0.0003690206848778357   Iteration 47 of 100, tot loss = 4.713575761368934, l1: 0.00010495368057510201, l2: 0.0003664038968247738   Iteration 48 of 100, tot loss = 4.694750276704629, l1: 0.00010396730711666653, l2: 0.00036550772195672227   Iteration 49 of 100, tot loss = 4.679829682622637, l1: 0.00010394781320985426, l2: 0.00036403515622048275   Iteration 50 of 100, tot loss = 4.640867502689361, l1: 0.00010308882207027637, l2: 0.00036099792923778297   Iteration 51 of 100, tot loss = 4.648225506146749, l1: 0.00010345941786106457, l2: 0.0003613631335753655   Iteration 52 of 100, tot loss = 4.658423948746461, l1: 0.00010312840836727097, l2: 0.0003627139874599659   Iteration 53 of 100, tot loss = 4.662182495279132, l1: 0.00010312465129349873, l2: 0.0003630935993124643   Iteration 54 of 100, tot loss = 4.686557613037251, l1: 0.00010341789042016629, l2: 0.00036523787248707204   Iteration 55 of 100, tot loss = 4.685584954781966, l1: 0.00010366924821441486, l2: 0.0003648892489515922   Iteration 56 of 100, tot loss = 4.685051366686821, l1: 0.00010331291923648678, l2: 0.0003651922192407905   Iteration 57 of 100, tot loss = 4.676980029072678, l1: 0.00010291698862075511, l2: 0.0003647810157609025   Iteration 58 of 100, tot loss = 4.675934333225777, l1: 0.00010276750893761597, l2: 0.0003648259258499883   Iteration 59 of 100, tot loss = 4.686410501851874, l1: 0.0001031367678710228, l2: 0.00036550428367109367   Iteration 60 of 100, tot loss = 4.6719546576341, l1: 0.00010315477738913615, l2: 0.00036404068960109725   Iteration 61 of 100, tot loss = 4.696793034428456, l1: 0.00010381925378288677, l2: 0.0003658600502693262   Iteration 62 of 100, tot loss = 4.704401602668147, l1: 0.0001039715628599703, l2: 0.00036646859793578303   Iteration 63 of 100, tot loss = 4.674533410677834, l1: 0.0001029916593180378, l2: 0.0003644616822601252   Iteration 64 of 100, tot loss = 4.644923334941268, l1: 0.00010246785598155839, l2: 0.0003620244779085624   Iteration 65 of 100, tot loss = 4.635932795818036, l1: 0.0001021072388259479, l2: 0.00036148604121990504   Iteration 66 of 100, tot loss = 4.606456299622853, l1: 0.00010118024880339914, l2: 0.00035946538176204103   Iteration 67 of 100, tot loss = 4.616718382977727, l1: 0.00010118726624650255, l2: 0.0003604845725062456   Iteration 68 of 100, tot loss = 4.592894464731216, l1: 0.00010059640793342089, l2: 0.00035869303910123347   Iteration 69 of 100, tot loss = 4.577419168707253, l1: 0.00010061743840038696, l2: 0.0003571244788885899   Iteration 70 of 100, tot loss = 4.552985734598977, l1: 0.000100282044045993, l2: 0.00035501653016711186   Iteration 71 of 100, tot loss = 4.571930977660165, l1: 0.0001007187284543966, l2: 0.00035647437010045675   Iteration 72 of 100, tot loss = 4.571105942130089, l1: 0.00010088037197419908, l2: 0.00035623022328460746   Iteration 73 of 100, tot loss = 4.560170563932967, l1: 0.00010074213528303966, l2: 0.00035527492223518   Iteration 74 of 100, tot loss = 4.571783012634999, l1: 0.00010079470961795164, l2: 0.0003563835927854745   Iteration 75 of 100, tot loss = 4.579837930997213, l1: 0.00010099328210344538, l2: 0.0003569905124216651   Iteration 76 of 100, tot loss = 4.590421574680429, l1: 0.00010115307907797519, l2: 0.0003578890795516169   Iteration 77 of 100, tot loss = 4.613281006936903, l1: 0.00010145218468758024, l2: 0.0003598759167407361   Iteration 78 of 100, tot loss = 4.634450177351634, l1: 0.00010219016323203388, l2: 0.0003612548549799846   Iteration 79 of 100, tot loss = 4.618355658989918, l1: 0.00010195962634877762, l2: 0.00035987593969654384   Iteration 80 of 100, tot loss = 4.659377031028271, l1: 0.00010236308908133651, l2: 0.00036357461412990234   Iteration 81 of 100, tot loss = 4.6643677920471, l1: 0.00010258501837889515, l2: 0.00036385176070235717   Iteration 82 of 100, tot loss = 4.672412504510182, l1: 0.00010278092561681505, l2: 0.00036446032452925177   Iteration 83 of 100, tot loss = 4.649557252964342, l1: 0.00010245852382763861, l2: 0.00036249720107158086   Iteration 84 of 100, tot loss = 4.617712161370686, l1: 0.00010175635335534545, l2: 0.00036001486244328717   Iteration 85 of 100, tot loss = 4.625611864819247, l1: 0.00010184370279175175, l2: 0.0003607174835291088   Iteration 86 of 100, tot loss = 4.591699177442595, l1: 0.00010110464249602195, l2: 0.0003580652750273192   Iteration 87 of 100, tot loss = 4.594380454085339, l1: 0.00010146864726015462, l2: 0.0003579693979211687   Iteration 88 of 100, tot loss = 4.594331333583051, l1: 0.00010148818017279899, l2: 0.0003579449527354551   Iteration 89 of 100, tot loss = 4.5894579659687, l1: 0.0001012014662990807, l2: 0.0003577443295919724   Iteration 90 of 100, tot loss = 4.593060470951928, l1: 0.00010133433477474481, l2: 0.0003579717118150762   Iteration 91 of 100, tot loss = 4.571238401171925, l1: 0.00010105164775658133, l2: 0.00035607219183725396   Iteration 92 of 100, tot loss = 4.5881909015386, l1: 0.00010130885691425014, l2: 0.00035751023318405953   Iteration 93 of 100, tot loss = 4.606679976627391, l1: 0.00010178188731711138, l2: 0.00035888611017325793   Iteration 94 of 100, tot loss = 4.613042150406128, l1: 0.00010165814068812401, l2: 0.00035964607377536595   Iteration 95 of 100, tot loss = 4.600012266008478, l1: 0.00010154175788078358, l2: 0.0003584594683305017   Iteration 96 of 100, tot loss = 4.599640149623156, l1: 0.00010133645158324119, l2: 0.00035862756309749483   Iteration 97 of 100, tot loss = 4.585150724833774, l1: 0.00010084252928734095, l2: 0.00035767254303209484   Iteration 98 of 100, tot loss = 4.575321731518726, l1: 0.00010087854025186971, l2: 0.0003566536327766977   Iteration 99 of 100, tot loss = 4.609140149270646, l1: 0.00010130671632001083, l2: 0.0003596072983598769   Iteration 100 of 100, tot loss = 4.598011871576309, l1: 0.00010112714371643961, l2: 0.000358674043091014
   End of epoch 1392; saving model... 

Epoch 1393 of 2000
   Iteration 1 of 100, tot loss = 4.417840480804443, l1: 9.636905451770872e-05, l2: 0.00034541499917395413   Iteration 2 of 100, tot loss = 5.239081382751465, l1: 0.00011630434892140329, l2: 0.00040760378760751337   Iteration 3 of 100, tot loss = 4.775706052780151, l1: 0.00011058146386252095, l2: 0.00036698913512130577   Iteration 4 of 100, tot loss = 4.421229004859924, l1: 0.00010496760660316795, l2: 0.00033715529571054503   Iteration 5 of 100, tot loss = 4.053445482254029, l1: 0.00010229359613731504, l2: 0.0003030509513337165   Iteration 6 of 100, tot loss = 4.457230846087138, l1: 0.0001073561652447097, l2: 0.00033836691970160854   Iteration 7 of 100, tot loss = 4.240290539605277, l1: 0.00010581573691784538, l2: 0.0003182133160797613   Iteration 8 of 100, tot loss = 4.453421264886856, l1: 0.0001098465245377156, l2: 0.0003354956024850253   Iteration 9 of 100, tot loss = 4.4059478706783715, l1: 0.00010904468541007696, l2: 0.00033155010249983106   Iteration 10 of 100, tot loss = 4.451858305931092, l1: 0.00011030447712983005, l2: 0.00033488135377410797   Iteration 11 of 100, tot loss = 4.532809582623568, l1: 0.00011111327264436774, l2: 0.00034216768372888595   Iteration 12 of 100, tot loss = 4.3628450234731035, l1: 0.00010762623666475217, l2: 0.0003286582638490169   Iteration 13 of 100, tot loss = 4.4290762681227465, l1: 0.00010804592322808906, l2: 0.0003348617028677836   Iteration 14 of 100, tot loss = 4.354289276259286, l1: 0.00010656126479651513, l2: 0.0003288676619247001   Iteration 15 of 100, tot loss = 4.334251260757446, l1: 0.0001057278704441463, l2: 0.0003276972536696121   Iteration 16 of 100, tot loss = 4.415540799498558, l1: 0.00010700262828322593, l2: 0.0003345514496686519   Iteration 17 of 100, tot loss = 4.429064568351297, l1: 0.0001062440941870377, l2: 0.0003366623602821218   Iteration 18 of 100, tot loss = 4.371643582979838, l1: 0.00010483612681532072, l2: 0.0003323282297868799   Iteration 19 of 100, tot loss = 4.497896207006354, l1: 0.00010686150620274834, l2: 0.00034292811285826916   Iteration 20 of 100, tot loss = 4.557173478603363, l1: 0.00010798338917084038, l2: 0.00034773395527736283   Iteration 21 of 100, tot loss = 4.587734347298031, l1: 0.00010857146498026504, l2: 0.0003502019674106989   Iteration 22 of 100, tot loss = 4.560564864765514, l1: 0.00010846670920727775, l2: 0.00034758977430597457   Iteration 23 of 100, tot loss = 4.606666606405507, l1: 0.00010961038693178283, l2: 0.0003510562711871108   Iteration 24 of 100, tot loss = 4.509839902321498, l1: 0.00010777701027109288, l2: 0.00034320697765603353   Iteration 25 of 100, tot loss = 4.5882175922393795, l1: 0.00010959714185446501, l2: 0.00034922461374662815   Iteration 26 of 100, tot loss = 4.564370256203872, l1: 0.00010911438417119476, l2: 0.0003473226377256931   Iteration 27 of 100, tot loss = 4.520905856732969, l1: 0.00010751642629555945, l2: 0.00034457415609655007   Iteration 28 of 100, tot loss = 4.485434693949563, l1: 0.00010693787719771666, l2: 0.0003416055894506696   Iteration 29 of 100, tot loss = 4.520149880442126, l1: 0.00010625524712317013, l2: 0.0003457597390636159   Iteration 30 of 100, tot loss = 4.50691724618276, l1: 0.00010582637939175281, l2: 0.0003448653432618206   Iteration 31 of 100, tot loss = 4.5921968721574355, l1: 0.00010740312849990122, l2: 0.0003518165575505625   Iteration 32 of 100, tot loss = 4.605382539331913, l1: 0.00010824876358128677, l2: 0.00035228949036536505   Iteration 33 of 100, tot loss = 4.61191523436344, l1: 0.0001090554175210524, l2: 0.0003521361054894938   Iteration 34 of 100, tot loss = 4.628417050137239, l1: 0.00010913767897256869, l2: 0.0003537040260305409   Iteration 35 of 100, tot loss = 4.666896104812622, l1: 0.000109853755358407, l2: 0.0003568358561356685   Iteration 36 of 100, tot loss = 4.6655915511979, l1: 0.00010927328407382852, l2: 0.0003572858719659659   Iteration 37 of 100, tot loss = 4.7234548684713005, l1: 0.0001099487387880406, l2: 0.000362396749158113   Iteration 38 of 100, tot loss = 4.654689839011745, l1: 0.00010869820164914832, l2: 0.0003567707830562109   Iteration 39 of 100, tot loss = 4.6690395428584175, l1: 0.00010940062799282037, l2: 0.00035750332664555084   Iteration 40 of 100, tot loss = 4.623341971635819, l1: 0.00010865003769140457, l2: 0.00035368415956327226   Iteration 41 of 100, tot loss = 4.697487441504874, l1: 0.00010942911609252006, l2: 0.0003603196290474976   Iteration 42 of 100, tot loss = 4.666462114879063, l1: 0.00010851602079734828, l2: 0.00035813019141122434   Iteration 43 of 100, tot loss = 4.708784802015438, l1: 0.0001088770601876773, l2: 0.00036200142071614864   Iteration 44 of 100, tot loss = 4.665585647929799, l1: 0.00010758620936196002, l2: 0.0003589723561493553   Iteration 45 of 100, tot loss = 4.616774649090237, l1: 0.00010635095572474206, l2: 0.00035532651009917676   Iteration 46 of 100, tot loss = 4.62243089986884, l1: 0.00010633512133411031, l2: 0.0003559079698828535   Iteration 47 of 100, tot loss = 4.598347329078837, l1: 0.00010642753454220006, l2: 0.0003534071992691408   Iteration 48 of 100, tot loss = 4.5884810090065, l1: 0.00010607569349000794, l2: 0.00035277240840514423   Iteration 49 of 100, tot loss = 4.564284674975337, l1: 0.0001057948000528327, l2: 0.00035063366848756846   Iteration 50 of 100, tot loss = 4.529024648666382, l1: 0.00010519260315049905, l2: 0.000347709862398915   Iteration 51 of 100, tot loss = 4.52457956239289, l1: 0.00010491573066603593, l2: 0.0003475422261045406   Iteration 52 of 100, tot loss = 4.51057925591102, l1: 0.00010484558914378376, l2: 0.00034621233666817157   Iteration 53 of 100, tot loss = 4.531291826716009, l1: 0.00010540447328821756, l2: 0.00034772470881195986   Iteration 54 of 100, tot loss = 4.509179949760437, l1: 0.00010473437194175656, l2: 0.00034618362217399946   Iteration 55 of 100, tot loss = 4.483578057722612, l1: 0.00010455953194071877, l2: 0.000343798273190094   Iteration 56 of 100, tot loss = 4.4846987554005215, l1: 0.00010480452322423974, l2: 0.0003436653519851721   Iteration 57 of 100, tot loss = 4.47800120136194, l1: 0.00010448956961402458, l2: 0.00034331055046590207   Iteration 58 of 100, tot loss = 4.458495181182335, l1: 0.00010376576022375455, l2: 0.00034208375792990533   Iteration 59 of 100, tot loss = 4.486820859424139, l1: 0.00010354408257595516, l2: 0.00034513800346758217   Iteration 60 of 100, tot loss = 4.449712761243185, l1: 0.00010262124366515006, l2: 0.00034235003258800136   Iteration 61 of 100, tot loss = 4.4732426580835565, l1: 0.00010284469066928218, l2: 0.0003444795753448041   Iteration 62 of 100, tot loss = 4.4819851459995395, l1: 0.00010279687299888821, l2: 0.00034540164192402436   Iteration 63 of 100, tot loss = 4.537512892768497, l1: 0.00010352681778068285, l2: 0.00035022447183210816   Iteration 64 of 100, tot loss = 4.518329367041588, l1: 0.00010310547224889888, l2: 0.00034872746482506045   Iteration 65 of 100, tot loss = 4.512795228224534, l1: 0.00010290724413397794, l2: 0.000348372279236523   Iteration 66 of 100, tot loss = 4.508141373143052, l1: 0.00010273133055747938, l2: 0.0003480828075253433   Iteration 67 of 100, tot loss = 4.497330327532184, l1: 0.00010200709389277679, l2: 0.00034772593931142075   Iteration 68 of 100, tot loss = 4.472706661504858, l1: 0.00010121574215351276, l2: 0.00034605492456809287   Iteration 69 of 100, tot loss = 4.473368202430614, l1: 0.00010089838347546217, l2: 0.0003464384368482464   Iteration 70 of 100, tot loss = 4.496362972259521, l1: 0.00010129978306524987, l2: 0.0003483365146426617   Iteration 71 of 100, tot loss = 4.516194417443074, l1: 0.00010172519660685999, l2: 0.00034989424490100953   Iteration 72 of 100, tot loss = 4.493255767557356, l1: 0.0001015711600808168, l2: 0.0003477544166040995   Iteration 73 of 100, tot loss = 4.52640875724897, l1: 0.00010198977642711763, l2: 0.00035065109901327946   Iteration 74 of 100, tot loss = 4.541050672531128, l1: 0.00010246196856118135, l2: 0.00035164309874007076   Iteration 75 of 100, tot loss = 4.571215826670329, l1: 0.00010311994041937093, l2: 0.0003540016417779649   Iteration 76 of 100, tot loss = 4.590768180395427, l1: 0.00010360194056901444, l2: 0.0003554748769889365   Iteration 77 of 100, tot loss = 4.613520708951083, l1: 0.00010423935997426848, l2: 0.00035711271063176695   Iteration 78 of 100, tot loss = 4.601640374232561, l1: 0.00010395329035161875, l2: 0.00035621074699193763   Iteration 79 of 100, tot loss = 4.59102528608298, l1: 0.00010349146225651773, l2: 0.0003556110662234287   Iteration 80 of 100, tot loss = 4.576556232571602, l1: 0.0001032615568874462, l2: 0.00035439406619843796   Iteration 81 of 100, tot loss = 4.6013181386170565, l1: 0.00010378312211365864, l2: 0.00035634869188616445   Iteration 82 of 100, tot loss = 4.599159592535438, l1: 0.00010398008116015529, l2: 0.00035593587824955537   Iteration 83 of 100, tot loss = 4.5973472566489715, l1: 0.00010388487170124988, l2: 0.0003558498544615682   Iteration 84 of 100, tot loss = 4.594410331476302, l1: 0.00010365457655141308, l2: 0.0003557864573459873   Iteration 85 of 100, tot loss = 4.600879772971658, l1: 0.00010373794485691606, l2: 0.00035635003296192737   Iteration 86 of 100, tot loss = 4.610757237256959, l1: 0.00010379154145782597, l2: 0.0003572841827029384   Iteration 87 of 100, tot loss = 4.6199071407318115, l1: 0.00010407882881077842, l2: 0.0003579118860759719   Iteration 88 of 100, tot loss = 4.619804628870704, l1: 0.00010404576956220394, l2: 0.0003579346942966698   Iteration 89 of 100, tot loss = 4.620338453335709, l1: 0.00010411931145242991, l2: 0.0003579145350056606   Iteration 90 of 100, tot loss = 4.6060757795969645, l1: 0.00010396716461400502, l2: 0.0003566404143283661   Iteration 91 of 100, tot loss = 4.5942881133530165, l1: 0.00010353521457335983, l2: 0.0003558935977807678   Iteration 92 of 100, tot loss = 4.5742341254068455, l1: 0.00010291348561345923, l2: 0.0003545099280017894   Iteration 93 of 100, tot loss = 4.551137547339162, l1: 0.0001025360496339178, l2: 0.0003525777062731645   Iteration 94 of 100, tot loss = 4.527566696735138, l1: 0.00010189894194983471, l2: 0.00035085772879996357   Iteration 95 of 100, tot loss = 4.508878208461561, l1: 0.00010159901556969424, l2: 0.00034928880631923676   Iteration 96 of 100, tot loss = 4.498171741763751, l1: 0.0001015302329581876, l2: 0.0003482869421228922   Iteration 97 of 100, tot loss = 4.483014168198576, l1: 0.00010128032714494543, l2: 0.00034702109060725486   Iteration 98 of 100, tot loss = 4.459879802197826, l1: 0.0001006400796504007, l2: 0.00034534790148074755   Iteration 99 of 100, tot loss = 4.47137862022477, l1: 0.00010074430824118676, l2: 0.0003463935546195981   Iteration 100 of 100, tot loss = 4.458067193031311, l1: 0.00010067686169350054, l2: 0.00034512985846959056
   End of epoch 1393; saving model... 

Epoch 1394 of 2000
   Iteration 1 of 100, tot loss = 3.435331106185913, l1: 0.00011162290320498869, l2: 0.0002319102204637602   Iteration 2 of 100, tot loss = 4.718440413475037, l1: 0.00012066644922015257, l2: 0.0003511775939841755   Iteration 3 of 100, tot loss = 4.558456500371297, l1: 0.00011095545050920919, l2: 0.0003448902037537967   Iteration 4 of 100, tot loss = 5.22133868932724, l1: 0.00012187829088361468, l2: 0.0004002555760962423   Iteration 5 of 100, tot loss = 5.315635538101196, l1: 0.00012529242521850393, l2: 0.00040627112321089953   Iteration 6 of 100, tot loss = 5.1887145439783735, l1: 0.0001195608674606774, l2: 0.00039931058563524857   Iteration 7 of 100, tot loss = 4.9926890305110385, l1: 0.00011342873975601313, l2: 0.00038584016354954135   Iteration 8 of 100, tot loss = 4.969677418470383, l1: 0.00011475156043161405, l2: 0.0003822161834250437   Iteration 9 of 100, tot loss = 4.924324856864081, l1: 0.00011539921656043993, l2: 0.00037703326961491257   Iteration 10 of 100, tot loss = 4.999209094047546, l1: 0.00011532902208273299, l2: 0.00038459188799606637   Iteration 11 of 100, tot loss = 5.0274221463636914, l1: 0.00011492953556378119, l2: 0.0003878126775486056   Iteration 12 of 100, tot loss = 4.752309928337733, l1: 0.00011011769978116111, l2: 0.0003651132919912925   Iteration 13 of 100, tot loss = 4.709492655900808, l1: 0.00011078822381722812, l2: 0.00036016104198866885   Iteration 14 of 100, tot loss = 4.683780814920153, l1: 0.00011071288619340132, l2: 0.0003576651948346158   Iteration 15 of 100, tot loss = 4.690822259585063, l1: 0.00011033045933193838, l2: 0.00035875176787764453   Iteration 16 of 100, tot loss = 4.779245175421238, l1: 0.00011137784940729034, l2: 0.0003665466688289598   Iteration 17 of 100, tot loss = 4.714273628066568, l1: 0.00011034534434335964, l2: 0.0003610820195343181   Iteration 18 of 100, tot loss = 4.657683948675792, l1: 0.00010920222686965847, l2: 0.0003565661694058993   Iteration 19 of 100, tot loss = 4.646114443477831, l1: 0.00010979459291679392, l2: 0.000354816852593917   Iteration 20 of 100, tot loss = 4.626330107450485, l1: 0.00010856695007532835, l2: 0.0003540660607541213   Iteration 21 of 100, tot loss = 4.6561667294729325, l1: 0.00010783797603965337, l2: 0.00035777869613541825   Iteration 22 of 100, tot loss = 4.592567622661591, l1: 0.000106279853339815, l2: 0.00035297690754470585   Iteration 23 of 100, tot loss = 4.548549802406974, l1: 0.00010561817148497894, l2: 0.00034923680694824407   Iteration 24 of 100, tot loss = 4.562233592073123, l1: 0.00010513552570046159, l2: 0.0003510878320109138   Iteration 25 of 100, tot loss = 4.553480935096741, l1: 0.00010537957219639792, l2: 0.00034996851958567275   Iteration 26 of 100, tot loss = 4.471576823638036, l1: 0.00010401138010470626, l2: 0.00034314630046048056   Iteration 27 of 100, tot loss = 4.5510094474863125, l1: 0.00010587471047916484, l2: 0.0003492262317363552   Iteration 28 of 100, tot loss = 4.575438043900898, l1: 0.00010696906662945236, l2: 0.00035057473594081657   Iteration 29 of 100, tot loss = 4.558605444842372, l1: 0.00010659721713103675, l2: 0.0003492633251629866   Iteration 30 of 100, tot loss = 4.500378866990407, l1: 0.00010570908925728872, l2: 0.0003443287954723928   Iteration 31 of 100, tot loss = 4.577928839191314, l1: 0.00010728281303008478, l2: 0.00035051006852881983   Iteration 32 of 100, tot loss = 4.569257635623217, l1: 0.00010651208185663563, l2: 0.00035041367823396286   Iteration 33 of 100, tot loss = 4.547818339232243, l1: 0.000106213220781435, l2: 0.0003485686097657449   Iteration 34 of 100, tot loss = 4.532907384283402, l1: 0.00010543445836554063, l2: 0.00034785627705038194   Iteration 35 of 100, tot loss = 4.501327981267656, l1: 0.00010555483417452446, l2: 0.0003445779607448328   Iteration 36 of 100, tot loss = 4.512542555729548, l1: 0.0001055191826000939, l2: 0.0003457350695599517   Iteration 37 of 100, tot loss = 4.443104550645158, l1: 0.00010432806939698403, l2: 0.0003399823824135662   Iteration 38 of 100, tot loss = 4.402661342369883, l1: 0.00010339995232674195, l2: 0.0003368661780673153   Iteration 39 of 100, tot loss = 4.454951573640872, l1: 0.0001039048016910107, l2: 0.00034159035231613624   Iteration 40 of 100, tot loss = 4.4539245665073395, l1: 0.00010363358578615589, l2: 0.00034175886776210973   Iteration 41 of 100, tot loss = 4.452885540520272, l1: 0.00010406889646774066, l2: 0.0003412196552167965   Iteration 42 of 100, tot loss = 4.464593325342451, l1: 0.0001043668758246072, l2: 0.0003420924554770214   Iteration 43 of 100, tot loss = 4.489459242931632, l1: 0.0001046595841830293, l2: 0.000344286339261385   Iteration 44 of 100, tot loss = 4.467395652424205, l1: 0.00010454008357059634, l2: 0.000342199481151924   Iteration 45 of 100, tot loss = 4.456119394302368, l1: 0.0001045341026231957, l2: 0.00034107783657317566   Iteration 46 of 100, tot loss = 4.41304800303086, l1: 0.00010314818594096046, l2: 0.00033815661442096587   Iteration 47 of 100, tot loss = 4.391586653729703, l1: 0.00010284850585331368, l2: 0.0003363101597952141   Iteration 48 of 100, tot loss = 4.37523357073466, l1: 0.00010267369687729418, l2: 0.0003348496604000199   Iteration 49 of 100, tot loss = 4.417562825339181, l1: 0.00010308819935521662, l2: 0.0003386680837138556   Iteration 50 of 100, tot loss = 4.444834594726562, l1: 0.00010369371731940191, l2: 0.00034078974262229166   Iteration 51 of 100, tot loss = 4.456515658135507, l1: 0.00010379844160542787, l2: 0.00034185312445818755   Iteration 52 of 100, tot loss = 4.415905750714815, l1: 0.00010259281876082801, l2: 0.0003389977563738984   Iteration 53 of 100, tot loss = 4.421728664973997, l1: 0.00010290597945743873, l2: 0.00033926688706313417   Iteration 54 of 100, tot loss = 4.40321233537462, l1: 0.00010227135222791091, l2: 0.00033804988103198683   Iteration 55 of 100, tot loss = 4.408307517658581, l1: 0.00010224369874446315, l2: 0.0003385870522999374   Iteration 56 of 100, tot loss = 4.434114260332925, l1: 0.00010319725392296927, l2: 0.00034021417182624906   Iteration 57 of 100, tot loss = 4.436073035524602, l1: 0.00010341714783990348, l2: 0.0003401901555837851   Iteration 58 of 100, tot loss = 4.430592339614342, l1: 0.00010349842310022032, l2: 0.00033956081046951646   Iteration 59 of 100, tot loss = 4.4149137634342, l1: 0.00010313329865743596, l2: 0.0003383580771111094   Iteration 60 of 100, tot loss = 4.452493250370026, l1: 0.000103891865182959, l2: 0.0003413574591832003   Iteration 61 of 100, tot loss = 4.449175901100284, l1: 0.00010402672121360455, l2: 0.00034089086801061775   Iteration 62 of 100, tot loss = 4.43126864587107, l1: 0.00010348863871777505, l2: 0.00033963822504794677   Iteration 63 of 100, tot loss = 4.383756179658193, l1: 0.00010235325611401571, l2: 0.00033602236097009616   Iteration 64 of 100, tot loss = 4.406826552003622, l1: 0.00010279647307243067, l2: 0.00033788618100061285   Iteration 65 of 100, tot loss = 4.3788324429438665, l1: 0.00010222109373829041, l2: 0.0003356621496585341   Iteration 66 of 100, tot loss = 4.440564971981627, l1: 0.00010290027790743449, l2: 0.00034115621923719476   Iteration 67 of 100, tot loss = 4.4442074974971035, l1: 0.0001031565317582624, l2: 0.0003412642181501724   Iteration 68 of 100, tot loss = 4.425229268915513, l1: 0.00010247006310451338, l2: 0.0003400528639758704   Iteration 69 of 100, tot loss = 4.437875802966132, l1: 0.00010238762131294884, l2: 0.00034139995904811457   Iteration 70 of 100, tot loss = 4.442530025754656, l1: 0.00010218846728093922, l2: 0.00034206453575669524   Iteration 71 of 100, tot loss = 4.452224529964823, l1: 0.00010229867959270438, l2: 0.0003429237743109976   Iteration 72 of 100, tot loss = 4.451897157563104, l1: 0.00010235496948654245, l2: 0.0003428347472436144   Iteration 73 of 100, tot loss = 4.454364985635836, l1: 0.00010268089153376497, l2: 0.0003427556078611516   Iteration 74 of 100, tot loss = 4.449069905925441, l1: 0.00010235079133889412, l2: 0.00034255620020967153   Iteration 75 of 100, tot loss = 4.467658748626709, l1: 0.0001026230794377625, l2: 0.0003441427961418716   Iteration 76 of 100, tot loss = 4.475395052056563, l1: 0.00010247965665736334, l2: 0.0003450598488646057   Iteration 77 of 100, tot loss = 4.494777431735745, l1: 0.00010239732865238103, l2: 0.00034708041512912334   Iteration 78 of 100, tot loss = 4.5089097512074, l1: 0.00010267885451097615, l2: 0.0003482121212119893   Iteration 79 of 100, tot loss = 4.501892533483384, l1: 0.000102830529002962, l2: 0.0003473587249239164   Iteration 80 of 100, tot loss = 4.539057061076164, l1: 0.00010342247451262665, l2: 0.0003504832317958062   Iteration 81 of 100, tot loss = 4.52511496602753, l1: 0.00010321765930038291, l2: 0.00034929383759901367   Iteration 82 of 100, tot loss = 4.565021584673626, l1: 0.00010358726117041575, l2: 0.0003529148975425547   Iteration 83 of 100, tot loss = 4.564147840063256, l1: 0.00010359079116977842, l2: 0.00035282399311468163   Iteration 84 of 100, tot loss = 4.570616364479065, l1: 0.00010381078564456576, l2: 0.00035325085134120727   Iteration 85 of 100, tot loss = 4.549491545733283, l1: 0.0001034584450227318, l2: 0.0003514907102833283   Iteration 86 of 100, tot loss = 4.5236339458199435, l1: 0.00010293923261024458, l2: 0.0003494241625662124   Iteration 87 of 100, tot loss = 4.591230184182353, l1: 0.00010386201418065145, l2: 0.0003552610044074566   Iteration 88 of 100, tot loss = 4.573607244274833, l1: 0.00010358308426558241, l2: 0.00035377764032256726   Iteration 89 of 100, tot loss = 4.585388569349654, l1: 0.0001040650194421217, l2: 0.0003544738375761002   Iteration 90 of 100, tot loss = 4.599385547637939, l1: 0.00010417405157770392, l2: 0.0003557645034258409   Iteration 91 of 100, tot loss = 4.62319679574652, l1: 0.00010472925058930631, l2: 0.0003575904293729064   Iteration 92 of 100, tot loss = 4.648124368294425, l1: 0.00010502413450645373, l2: 0.00035978830269216456   Iteration 93 of 100, tot loss = 4.642910162607829, l1: 0.00010500113613263864, l2: 0.0003592898808350857   Iteration 94 of 100, tot loss = 4.6270708809507655, l1: 0.00010469363293482952, l2: 0.00035801345578139707   Iteration 95 of 100, tot loss = 4.645363237983302, l1: 0.0001048115113869579, l2: 0.0003597248128866532   Iteration 96 of 100, tot loss = 4.658046084145705, l1: 0.00010524147830892616, l2: 0.0003605631310013753   Iteration 97 of 100, tot loss = 4.640763825976972, l1: 0.00010473785991540429, l2: 0.00035933852362261663   Iteration 98 of 100, tot loss = 4.620508162342772, l1: 0.00010435894000693224, l2: 0.00035769187710758734   Iteration 99 of 100, tot loss = 4.614566140704685, l1: 0.00010415766103901062, l2: 0.0003572989535498   Iteration 100 of 100, tot loss = 4.58966490983963, l1: 0.00010377732236520387, l2: 0.00035518916927685496
   End of epoch 1394; saving model... 

Epoch 1395 of 2000
   Iteration 1 of 100, tot loss = 3.9388630390167236, l1: 0.00011464348062872887, l2: 0.00027924281312152743   Iteration 2 of 100, tot loss = 5.7023314237594604, l1: 0.00013369150110520422, l2: 0.0004365416243672371   Iteration 3 of 100, tot loss = 5.5586362679799395, l1: 0.00012464350826727846, l2: 0.00043122010538354516   Iteration 4 of 100, tot loss = 5.184300720691681, l1: 0.00011319601981085725, l2: 0.0004052340373164043   Iteration 5 of 100, tot loss = 4.745410966873169, l1: 0.00010623062989907339, l2: 0.00036831045290455224   Iteration 6 of 100, tot loss = 4.263927201430003, l1: 9.609298710226237e-05, l2: 0.0003302997210994363   Iteration 7 of 100, tot loss = 4.463350687708173, l1: 9.809773421563608e-05, l2: 0.00034823732650173563   Iteration 8 of 100, tot loss = 4.549018904566765, l1: 0.00010053102641904843, l2: 0.0003543708553479519   Iteration 9 of 100, tot loss = 4.71058902475569, l1: 0.00010308209271493575, l2: 0.00036797679947792657   Iteration 10 of 100, tot loss = 4.674201190471649, l1: 0.00010533906788623426, l2: 0.00036208104284014554   Iteration 11 of 100, tot loss = 4.523141373287547, l1: 0.00010398364016013643, l2: 0.00034833048994187266   Iteration 12 of 100, tot loss = 4.422521660725276, l1: 0.00010203501036206337, l2: 0.0003402171484291709   Iteration 13 of 100, tot loss = 4.3915804624557495, l1: 0.00010167386202598349, l2: 0.00033748417515128566   Iteration 14 of 100, tot loss = 4.235301571232932, l1: 9.936521356784007e-05, l2: 0.0003241649350716865   Iteration 15 of 100, tot loss = 4.372892196973165, l1: 0.00010133882436396865, l2: 0.0003359503869432956   Iteration 16 of 100, tot loss = 4.362227566540241, l1: 0.00010091296348946344, l2: 0.0003353097854414955   Iteration 17 of 100, tot loss = 4.303203800145318, l1: 9.997063748682301e-05, l2: 0.00033034973517608116   Iteration 18 of 100, tot loss = 4.377138131194645, l1: 0.00010028079385746323, l2: 0.00033743301497047976   Iteration 19 of 100, tot loss = 4.2070376433824235, l1: 9.662626229998607e-05, l2: 0.0003240774983025499   Iteration 20 of 100, tot loss = 4.252692764997482, l1: 9.718896144477185e-05, l2: 0.00032808031101012604   Iteration 21 of 100, tot loss = 4.250122427940369, l1: 9.662665494640048e-05, l2: 0.0003283855843446439   Iteration 22 of 100, tot loss = 4.197302346879786, l1: 9.680146864742379e-05, l2: 0.00032292876262958583   Iteration 23 of 100, tot loss = 4.32398371592812, l1: 9.743120798426072e-05, l2: 0.0003349671599647516   Iteration 24 of 100, tot loss = 4.46633214255174, l1: 0.00010018987965546937, l2: 0.0003464433314851097   Iteration 25 of 100, tot loss = 4.4890029954910275, l1: 0.00010052032273961232, l2: 0.0003483799734385684   Iteration 26 of 100, tot loss = 4.52751160126466, l1: 0.00010196318544331007, l2: 0.00035078797093144833   Iteration 27 of 100, tot loss = 4.657315329269126, l1: 0.00010473884968733828, l2: 0.00036099267836780873   Iteration 28 of 100, tot loss = 4.644571972744806, l1: 0.0001051311602558209, l2: 0.0003593260319446147   Iteration 29 of 100, tot loss = 4.70875607276785, l1: 0.0001064231022108539, l2: 0.0003644525006228801   Iteration 30 of 100, tot loss = 4.658750514189403, l1: 0.0001060627999928935, l2: 0.00035981224694599706   Iteration 31 of 100, tot loss = 4.724762205154665, l1: 0.00010719498259864087, l2: 0.0003652812334738912   Iteration 32 of 100, tot loss = 4.744432028383017, l1: 0.00010764332137114252, l2: 0.00036679987806564895   Iteration 33 of 100, tot loss = 4.7752811222365406, l1: 0.00010852354700380766, l2: 0.0003690045612694865   Iteration 34 of 100, tot loss = 4.682759958155015, l1: 0.0001066036207596292, l2: 0.0003616723712612464   Iteration 35 of 100, tot loss = 4.657707738876343, l1: 0.0001051916517358872, l2: 0.00036057911825732194   Iteration 36 of 100, tot loss = 4.790081004301707, l1: 0.00010668208263571917, l2: 0.0003723260126409716   Iteration 37 of 100, tot loss = 4.8670035504006055, l1: 0.00010767416028397133, l2: 0.00037902618921204547   Iteration 38 of 100, tot loss = 5.014607134618257, l1: 0.00010956420172940278, l2: 0.0003918965075603123   Iteration 39 of 100, tot loss = 4.948328262720352, l1: 0.00010793956691766373, l2: 0.00038689325540475786   Iteration 40 of 100, tot loss = 4.960197675228119, l1: 0.00010827466912814998, l2: 0.0003877450939398841   Iteration 41 of 100, tot loss = 4.909391589281036, l1: 0.00010739008921962346, l2: 0.000383549065878558   Iteration 42 of 100, tot loss = 4.915824969609578, l1: 0.00010788522931328597, l2: 0.0003836972642775869   Iteration 43 of 100, tot loss = 4.856307961219965, l1: 0.00010724667045842791, l2: 0.000378384122382687   Iteration 44 of 100, tot loss = 4.864464456384832, l1: 0.00010739082782410233, l2: 0.0003790556149231799   Iteration 45 of 100, tot loss = 4.862771352132161, l1: 0.00010721006102458988, l2: 0.0003790670716423645   Iteration 46 of 100, tot loss = 4.812393701594809, l1: 0.00010667834565380042, l2: 0.00037456102180168153   Iteration 47 of 100, tot loss = 4.787010908126831, l1: 0.00010619320613493092, l2: 0.00037250788173355916   Iteration 48 of 100, tot loss = 4.801344568530719, l1: 0.00010668652195514976, l2: 0.000373447932209577   Iteration 49 of 100, tot loss = 4.745706996139215, l1: 0.00010532278900761728, l2: 0.00036924790810171173   Iteration 50 of 100, tot loss = 4.735243034362793, l1: 0.00010501728262170218, l2: 0.0003685070185747463   Iteration 51 of 100, tot loss = 4.6858378204644895, l1: 0.00010438501839434692, l2: 0.0003641987615778549   Iteration 52 of 100, tot loss = 4.642448865450346, l1: 0.00010318835274329803, l2: 0.0003610565320111793   Iteration 53 of 100, tot loss = 4.636049810445534, l1: 0.00010314219651023313, l2: 0.0003604627831318451   Iteration 54 of 100, tot loss = 4.6377876158113835, l1: 0.00010314005703548901, l2: 0.0003606387028951388   Iteration 55 of 100, tot loss = 4.637866115570068, l1: 0.00010313017952615734, l2: 0.0003606564305796796   Iteration 56 of 100, tot loss = 4.694841248648507, l1: 0.00010367036845439413, l2: 0.00036581375479727285   Iteration 57 of 100, tot loss = 4.705763883758009, l1: 0.00010363260802829213, l2: 0.0003669437789715614   Iteration 58 of 100, tot loss = 4.682897292334458, l1: 0.00010334827549803746, l2: 0.0003649414523759584   Iteration 59 of 100, tot loss = 4.671573974318424, l1: 0.00010300710623673463, l2: 0.0003641502899897159   Iteration 60 of 100, tot loss = 4.7074148774147035, l1: 0.00010327537347620819, l2: 0.00036746611331182064   Iteration 61 of 100, tot loss = 4.693947979661285, l1: 0.00010326456483243583, l2: 0.00036613023245886546   Iteration 62 of 100, tot loss = 4.704907448061051, l1: 0.00010302121494708943, l2: 0.00036746952877541434   Iteration 63 of 100, tot loss = 4.72539628498138, l1: 0.00010389369812660984, l2: 0.00036864592876222486   Iteration 64 of 100, tot loss = 4.723540164530277, l1: 0.00010411245091290766, l2: 0.0003682415639332248   Iteration 65 of 100, tot loss = 4.697362602674044, l1: 0.00010385915845220621, l2: 0.0003658771003336789   Iteration 66 of 100, tot loss = 4.685318856528311, l1: 0.00010356354885681702, l2: 0.0003649683352374599   Iteration 67 of 100, tot loss = 4.6630513952739205, l1: 0.00010326828824099042, l2: 0.00036303684975058816   Iteration 68 of 100, tot loss = 4.649507455966052, l1: 0.00010345228945559083, l2: 0.00036149845458139144   Iteration 69 of 100, tot loss = 4.62199394253717, l1: 0.00010254885152486774, l2: 0.0003596505411078924   Iteration 70 of 100, tot loss = 4.635713788441249, l1: 0.00010284610881561613, l2: 0.0003607252680272463   Iteration 71 of 100, tot loss = 4.637876430027921, l1: 0.00010285978479119031, l2: 0.00036092785576073136   Iteration 72 of 100, tot loss = 4.660769568549262, l1: 0.00010295218973826397, l2: 0.00036312476458988385   Iteration 73 of 100, tot loss = 4.637651247520969, l1: 0.00010267002379469101, l2: 0.0003610950985752773   Iteration 74 of 100, tot loss = 4.664169491948308, l1: 0.00010314163636074816, l2: 0.00036327531012608923   Iteration 75 of 100, tot loss = 4.692499459584554, l1: 0.00010358802018648323, l2: 0.00036566192332732804   Iteration 76 of 100, tot loss = 4.704545635926096, l1: 0.0001037861954357236, l2: 0.000366668365945531   Iteration 77 of 100, tot loss = 4.701271515387993, l1: 0.00010323877543303208, l2: 0.00036688837384904867   Iteration 78 of 100, tot loss = 4.708403183863713, l1: 0.00010332623577806562, l2: 0.00036751408063140936   Iteration 79 of 100, tot loss = 4.679858080948455, l1: 0.00010279893739365596, l2: 0.000365186868477878   Iteration 80 of 100, tot loss = 4.689759939908981, l1: 0.00010293215796082222, l2: 0.0003660438335828076   Iteration 81 of 100, tot loss = 4.694149470623628, l1: 0.00010287425103331456, l2: 0.00036654069369017827   Iteration 82 of 100, tot loss = 4.725078489722275, l1: 0.0001033955926154371, l2: 0.00036911225408136776   Iteration 83 of 100, tot loss = 4.732358294797231, l1: 0.00010349719355784704, l2: 0.00036973863391623644   Iteration 84 of 100, tot loss = 4.732661621911185, l1: 0.00010346373325037781, l2: 0.0003698024268357715   Iteration 85 of 100, tot loss = 4.724721398073084, l1: 0.00010331450265606263, l2: 0.0003691576353637228   Iteration 86 of 100, tot loss = 4.73151954939199, l1: 0.00010299526932845597, l2: 0.0003701566839841097   Iteration 87 of 100, tot loss = 4.706271396286186, l1: 0.00010243286652993625, l2: 0.0003681942716343917   Iteration 88 of 100, tot loss = 4.699685123833743, l1: 0.00010235104326222262, l2: 0.0003676174678317197   Iteration 89 of 100, tot loss = 4.696560098883811, l1: 0.00010247488208232824, l2: 0.00036718112652339143   Iteration 90 of 100, tot loss = 4.698266977734036, l1: 0.00010274354693441031, l2: 0.00036708314962905   Iteration 91 of 100, tot loss = 4.723122030824095, l1: 0.00010321322682372468, l2: 0.0003690989752802844   Iteration 92 of 100, tot loss = 4.7116583456163825, l1: 0.00010267553317016396, l2: 0.0003684903005468305   Iteration 93 of 100, tot loss = 4.7143298195254415, l1: 0.00010262077690855731, l2: 0.0003688122041464623   Iteration 94 of 100, tot loss = 4.704359052029062, l1: 0.00010260077079750617, l2: 0.00036783513349562667   Iteration 95 of 100, tot loss = 4.698510599136353, l1: 0.00010269945364837584, l2: 0.0003671516052379861   Iteration 96 of 100, tot loss = 4.709052639702956, l1: 0.00010275912639675273, l2: 0.0003681461364521965   Iteration 97 of 100, tot loss = 4.716547177009976, l1: 0.00010284081823912631, l2: 0.0003688138986598057   Iteration 98 of 100, tot loss = 4.742985903000345, l1: 0.000103414812415116, l2: 0.00037088377702071293   Iteration 99 of 100, tot loss = 4.7431382434536715, l1: 0.00010353237428147384, l2: 0.0003707814489690013   Iteration 100 of 100, tot loss = 4.739159061908722, l1: 0.0001034686122147832, l2: 0.00037044729258923326
   End of epoch 1395; saving model... 

Epoch 1396 of 2000
   Iteration 1 of 100, tot loss = 1.51134192943573, l1: 3.816924072452821e-05, l2: 0.00011296496086288244   Iteration 2 of 100, tot loss = 3.403542697429657, l1: 7.827668196114246e-05, l2: 0.00026207759947283193   Iteration 3 of 100, tot loss = 3.3914495706558228, l1: 7.876743741993171e-05, l2: 0.0002603775307458515   Iteration 4 of 100, tot loss = 3.4695991575717926, l1: 8.294123745145043e-05, l2: 0.0002640186903590802   Iteration 5 of 100, tot loss = 3.9002788782119753, l1: 9.1010319738416e-05, l2: 0.00029901758243795485   Iteration 6 of 100, tot loss = 4.680261949698131, l1: 0.00010648920033418108, l2: 0.00036153700542248163   Iteration 7 of 100, tot loss = 4.692362121173313, l1: 0.00011035030339761371, l2: 0.00035888591290651154   Iteration 8 of 100, tot loss = 4.723574712872505, l1: 0.00010944671930701588, l2: 0.00036291075048211496   Iteration 9 of 100, tot loss = 4.59848071469201, l1: 0.00010865712384757999, l2: 0.000351190946983277   Iteration 10 of 100, tot loss = 4.357565462589264, l1: 0.00010245008015772328, l2: 0.00033330646401736886   Iteration 11 of 100, tot loss = 4.438976255330172, l1: 0.00010489312857812779, l2: 0.00033900449687445706   Iteration 12 of 100, tot loss = 4.64970017472903, l1: 0.00010706210499241327, l2: 0.00035790791055963683   Iteration 13 of 100, tot loss = 4.83852075613462, l1: 0.00011031644065112162, l2: 0.0003735356328364175   Iteration 14 of 100, tot loss = 4.748312839439937, l1: 0.00010879976616706699, l2: 0.0003660315165429243   Iteration 15 of 100, tot loss = 4.751184296607971, l1: 0.00010943311887482803, l2: 0.0003656853106804192   Iteration 16 of 100, tot loss = 4.80083891004324, l1: 0.00011194358921784442, l2: 0.0003681403013615636   Iteration 17 of 100, tot loss = 4.810140490531921, l1: 0.00011227011504268055, l2: 0.0003687439311761409   Iteration 18 of 100, tot loss = 4.78497451543808, l1: 0.00011213511404801263, l2: 0.00036636233562603593   Iteration 19 of 100, tot loss = 4.763243744247838, l1: 0.00011211975499080788, l2: 0.00036420461899404853   Iteration 20 of 100, tot loss = 4.7255135715007786, l1: 0.00010996168020938057, l2: 0.00036258967593312265   Iteration 21 of 100, tot loss = 4.8283446573075794, l1: 0.00011061859861781289, l2: 0.00037221586431509683   Iteration 22 of 100, tot loss = 4.758036423813213, l1: 0.00010808574353921523, l2: 0.0003677178954769095   Iteration 23 of 100, tot loss = 4.717722172322481, l1: 0.00010624543321445462, l2: 0.0003655267811036142   Iteration 24 of 100, tot loss = 4.732058549920718, l1: 0.00010519382537192239, l2: 0.00036801202562249574   Iteration 25 of 100, tot loss = 4.73437605381012, l1: 0.00010544806296820752, l2: 0.0003679895389359444   Iteration 26 of 100, tot loss = 4.7134006802852335, l1: 0.00010419111458759289, l2: 0.00036714895059748623   Iteration 27 of 100, tot loss = 4.679914355278015, l1: 0.00010422762271547172, l2: 0.0003637638098249833   Iteration 28 of 100, tot loss = 4.593265988997051, l1: 0.00010231363441042569, l2: 0.0003570129613633201   Iteration 29 of 100, tot loss = 4.567017937528676, l1: 0.00010230796647976667, l2: 0.00035439382441711194   Iteration 30 of 100, tot loss = 4.6614307284355165, l1: 0.0001038226608216064, l2: 0.00036232040874892844   Iteration 31 of 100, tot loss = 4.7137743772998935, l1: 0.00010382139460547947, l2: 0.00036755604096763434   Iteration 32 of 100, tot loss = 4.791176620870829, l1: 0.00010573702968486032, l2: 0.0003733806292984809   Iteration 33 of 100, tot loss = 4.772872090339661, l1: 0.00010611472030039735, l2: 0.0003711724857566878   Iteration 34 of 100, tot loss = 4.764484829762402, l1: 0.00010595476498351404, l2: 0.0003704937155228382   Iteration 35 of 100, tot loss = 4.703028266770499, l1: 0.00010422818533178153, l2: 0.00036607463844120505   Iteration 36 of 100, tot loss = 4.61561804678705, l1: 0.00010221849647172753, l2: 0.00035934330541446496   Iteration 37 of 100, tot loss = 4.619949099179861, l1: 0.00010244987287159027, l2: 0.0003595450343218053   Iteration 38 of 100, tot loss = 4.582048143211164, l1: 0.00010158498470876733, l2: 0.0003566198272318089   Iteration 39 of 100, tot loss = 4.546608817883027, l1: 0.00010027720269042616, l2: 0.00035438367623823864   Iteration 40 of 100, tot loss = 4.525772753357887, l1: 0.00010008685021603014, l2: 0.00035249042230134364   Iteration 41 of 100, tot loss = 4.522584313299598, l1: 9.97596720614039e-05, l2: 0.0003524987561565756   Iteration 42 of 100, tot loss = 4.492077620256515, l1: 9.936471092043489e-05, l2: 0.00034984304788633294   Iteration 43 of 100, tot loss = 4.496084654053976, l1: 9.941275459633014e-05, l2: 0.000350195708072766   Iteration 44 of 100, tot loss = 4.496070609851317, l1: 9.90894227883969e-05, l2: 0.0003505176355247386   Iteration 45 of 100, tot loss = 4.448404892285665, l1: 9.827701659459207e-05, l2: 0.0003465634702782457   Iteration 46 of 100, tot loss = 4.40706590725028, l1: 9.704376474394888e-05, l2: 0.00034366282362379536   Iteration 47 of 100, tot loss = 4.380785761995519, l1: 9.669514645096608e-05, l2: 0.00034138342697530034   Iteration 48 of 100, tot loss = 4.358801034589608, l1: 9.699568522592017e-05, l2: 0.0003388844158204544   Iteration 49 of 100, tot loss = 4.362578662074342, l1: 9.740750183296694e-05, l2: 0.0003388503620434286   Iteration 50 of 100, tot loss = 4.368384730815888, l1: 9.694622385723051e-05, l2: 0.0003398922467022203   Iteration 51 of 100, tot loss = 4.35087590825324, l1: 9.608447441018606e-05, l2: 0.0003390031140274825   Iteration 52 of 100, tot loss = 4.3167921877824345, l1: 9.512741437972111e-05, l2: 0.0003365518020403285   Iteration 53 of 100, tot loss = 4.328573175196378, l1: 9.565923275384296e-05, l2: 0.0003371980823225008   Iteration 54 of 100, tot loss = 4.314183449303663, l1: 9.591970720740156e-05, l2: 0.00033549863534669083   Iteration 55 of 100, tot loss = 4.334245952692899, l1: 9.600621673267928e-05, l2: 0.0003374183756849644   Iteration 56 of 100, tot loss = 4.306654506496021, l1: 9.563502570537301e-05, l2: 0.000335030422190487   Iteration 57 of 100, tot loss = 4.2902915456838775, l1: 9.534722534896758e-05, l2: 0.00033368192650908766   Iteration 58 of 100, tot loss = 4.281794295228761, l1: 9.545803694546239e-05, l2: 0.00033272138986423417   Iteration 59 of 100, tot loss = 4.3028361211388795, l1: 9.55776499028878e-05, l2: 0.0003347059593918749   Iteration 60 of 100, tot loss = 4.25918826063474, l1: 9.467520776524907e-05, l2: 0.0003312436155586814   Iteration 61 of 100, tot loss = 4.258192670149882, l1: 9.487492857037158e-05, l2: 0.00033094433563776683   Iteration 62 of 100, tot loss = 4.245281767460607, l1: 9.470142116873419e-05, l2: 0.0003298267528730174   Iteration 63 of 100, tot loss = 4.223584570581951, l1: 9.426915419190043e-05, l2: 0.0003280893001901282   Iteration 64 of 100, tot loss = 4.239204058423638, l1: 9.446283905845121e-05, l2: 0.0003294575637937669   Iteration 65 of 100, tot loss = 4.249252071747413, l1: 9.499512777368251e-05, l2: 0.0003299300767069396   Iteration 66 of 100, tot loss = 4.231161592584668, l1: 9.485191698000894e-05, l2: 0.0003282642399867517   Iteration 67 of 100, tot loss = 4.256475966368148, l1: 9.53195121294798e-05, l2: 0.0003303280814790598   Iteration 68 of 100, tot loss = 4.257463726927252, l1: 9.474740187108503e-05, l2: 0.000330998967804001   Iteration 69 of 100, tot loss = 4.226102395334106, l1: 9.415333461483765e-05, l2: 0.00032845690198566604   Iteration 70 of 100, tot loss = 4.210473099776677, l1: 9.37675986019063e-05, l2: 0.00032727970849789147   Iteration 71 of 100, tot loss = 4.202927097468309, l1: 9.365403139512842e-05, l2: 0.0003266386756620986   Iteration 72 of 100, tot loss = 4.217421390944057, l1: 9.398224185335696e-05, l2: 0.0003277598940864361   Iteration 73 of 100, tot loss = 4.225623736642811, l1: 9.395620758343832e-05, l2: 0.0003286061628262372   Iteration 74 of 100, tot loss = 4.239077843524314, l1: 9.414228392219033e-05, l2: 0.00032976549748029257   Iteration 75 of 100, tot loss = 4.249226301511129, l1: 9.462064311568004e-05, l2: 0.0003303019842132926   Iteration 76 of 100, tot loss = 4.276998650086553, l1: 9.510521517528871e-05, l2: 0.00033259464697422167   Iteration 77 of 100, tot loss = 4.279288853917803, l1: 9.53354359004955e-05, l2: 0.000332593446515815   Iteration 78 of 100, tot loss = 4.28951369646268, l1: 9.535960378296948e-05, l2: 0.00033359176356894657   Iteration 79 of 100, tot loss = 4.287361300444301, l1: 9.491652742292013e-05, l2: 0.0003338196002465637   Iteration 80 of 100, tot loss = 4.315937159955501, l1: 9.531455175419979e-05, l2: 0.0003362791620020289   Iteration 81 of 100, tot loss = 4.298131058245529, l1: 9.50887371778034e-05, l2: 0.0003347243663831901   Iteration 82 of 100, tot loss = 4.303477617298684, l1: 9.529451141666291e-05, l2: 0.0003350532479533108   Iteration 83 of 100, tot loss = 4.307648800941835, l1: 9.528221496758163e-05, l2: 0.0003354826627203813   Iteration 84 of 100, tot loss = 4.284105801866168, l1: 9.476784864437788e-05, l2: 0.00033364272912010727   Iteration 85 of 100, tot loss = 4.277008198289311, l1: 9.459444999640041e-05, l2: 0.00033310636747902367   Iteration 86 of 100, tot loss = 4.310919164225113, l1: 9.490760135651176e-05, l2: 0.00033618431222939127   Iteration 87 of 100, tot loss = 4.314968099539307, l1: 9.493731641700363e-05, l2: 0.00033655949037297275   Iteration 88 of 100, tot loss = 4.325621172785759, l1: 9.490943392103708e-05, l2: 0.00033765268050377597   Iteration 89 of 100, tot loss = 4.296616173862072, l1: 9.434560727160634e-05, l2: 0.00033531600728453127   Iteration 90 of 100, tot loss = 4.317150428560045, l1: 9.431102039848661e-05, l2: 0.00033740402011769925   Iteration 91 of 100, tot loss = 4.315942570403382, l1: 9.447619527486245e-05, l2: 0.0003371180593583651   Iteration 92 of 100, tot loss = 4.349315788434899, l1: 9.503962523329234e-05, l2: 0.0003398919510326105   Iteration 93 of 100, tot loss = 4.367930499456262, l1: 9.50804655252385e-05, l2: 0.0003417125819826759   Iteration 94 of 100, tot loss = 4.373792638169959, l1: 9.510260334788287e-05, l2: 0.00034227665820892186   Iteration 95 of 100, tot loss = 4.397267893741005, l1: 9.550972072726214e-05, l2: 0.00034421706621192   Iteration 96 of 100, tot loss = 4.387120718757312, l1: 9.549623124864108e-05, l2: 0.0003432158379534182   Iteration 97 of 100, tot loss = 4.405314194787409, l1: 9.565775547615857e-05, l2: 0.0003448736616266296   Iteration 98 of 100, tot loss = 4.404623980424842, l1: 9.585614197374302e-05, l2: 0.0003446062536178423   Iteration 99 of 100, tot loss = 4.426885190636221, l1: 9.637270925326668e-05, l2: 0.0003463158077052371   Iteration 100 of 100, tot loss = 4.439031310081482, l1: 9.644282559747807e-05, l2: 0.0003474603031645529
   End of epoch 1396; saving model... 

Epoch 1397 of 2000
   Iteration 1 of 100, tot loss = 3.45687198638916, l1: 8.024142880458385e-05, l2: 0.0002654457639437169   Iteration 2 of 100, tot loss = 4.518226385116577, l1: 0.00010637977538863197, l2: 0.00034544286609161645   Iteration 3 of 100, tot loss = 4.737085342407227, l1: 0.00010535533753378938, l2: 0.0003683532025509824   Iteration 4 of 100, tot loss = 4.713400602340698, l1: 0.00010387068505224306, l2: 0.00036746937985299155   Iteration 5 of 100, tot loss = 4.318513250350952, l1: 9.875702671706677e-05, l2: 0.0003330943000037223   Iteration 6 of 100, tot loss = 4.255796710650126, l1: 9.813411452341825e-05, l2: 0.0003274455569529285   Iteration 7 of 100, tot loss = 4.3564271586281915, l1: 9.833546313789807e-05, l2: 0.00033730725408531725   Iteration 8 of 100, tot loss = 4.153332859277725, l1: 9.50472258409718e-05, l2: 0.0003202860607416369   Iteration 9 of 100, tot loss = 4.2994965182410345, l1: 9.827222351709174e-05, l2: 0.00033167743175807927   Iteration 10 of 100, tot loss = 4.435767769813538, l1: 0.00010170911118621007, l2: 0.00034186766715720295   Iteration 11 of 100, tot loss = 4.505207040093162, l1: 0.00010279389971401542, l2: 0.00034772680638442665   Iteration 12 of 100, tot loss = 4.5915908217430115, l1: 0.00010515782438839476, l2: 0.0003540012573163646   Iteration 13 of 100, tot loss = 4.639559837488028, l1: 0.00010515343959783562, l2: 0.0003588025462634575   Iteration 14 of 100, tot loss = 4.537626828466143, l1: 0.00010326199325001133, l2: 0.00035050069085887766   Iteration 15 of 100, tot loss = 4.451767015457153, l1: 0.00010110609631131713, l2: 0.0003440706069037939   Iteration 16 of 100, tot loss = 4.484501853585243, l1: 0.00010225170262856409, l2: 0.0003461984842942911   Iteration 17 of 100, tot loss = 4.560996265972362, l1: 0.00010422029214062015, l2: 0.00035187933570020557   Iteration 18 of 100, tot loss = 4.605770972039965, l1: 0.00010527427204781109, l2: 0.00035530282588701486   Iteration 19 of 100, tot loss = 4.704654831635325, l1: 0.00010749489975799071, l2: 0.0003629705851811818   Iteration 20 of 100, tot loss = 4.607893216609955, l1: 0.00010562218776613008, l2: 0.00035516713469405656   Iteration 21 of 100, tot loss = 4.532764752705892, l1: 0.000102853834895151, l2: 0.00035042264096860195   Iteration 22 of 100, tot loss = 4.500204909931529, l1: 0.00010249189389552075, l2: 0.0003475285980543545   Iteration 23 of 100, tot loss = 4.521812646285348, l1: 0.00010233306301929785, l2: 0.0003498482028194501   Iteration 24 of 100, tot loss = 4.518389085928599, l1: 0.00010214111140764241, l2: 0.0003496977982043366   Iteration 25 of 100, tot loss = 4.6321305084228515, l1: 0.00010358279716456309, l2: 0.0003596302546793595   Iteration 26 of 100, tot loss = 4.606879601111779, l1: 0.00010283760863016002, l2: 0.0003578503523245812   Iteration 27 of 100, tot loss = 4.69032026220251, l1: 0.00010429867715747268, l2: 0.0003647333495017072   Iteration 28 of 100, tot loss = 4.7463348763329645, l1: 0.00010448934413683933, l2: 0.00037014414400411103   Iteration 29 of 100, tot loss = 4.72284251245959, l1: 0.00010465910650211675, l2: 0.00036762514530986164   Iteration 30 of 100, tot loss = 4.836285654703776, l1: 0.00010635837388690561, l2: 0.0003772701898318095   Iteration 31 of 100, tot loss = 4.845357756460866, l1: 0.00010643500295813166, l2: 0.0003781007708899016   Iteration 32 of 100, tot loss = 4.814987272024155, l1: 0.0001062623641701066, l2: 0.000375236360468989   Iteration 33 of 100, tot loss = 4.793486002719764, l1: 0.00010632854691445546, l2: 0.0003730200507120709   Iteration 34 of 100, tot loss = 4.7772877356585335, l1: 0.00010650568904512671, l2: 0.00037122308233793937   Iteration 35 of 100, tot loss = 4.823027678898403, l1: 0.00010699087558480512, l2: 0.0003753118911325665   Iteration 36 of 100, tot loss = 4.866844389173719, l1: 0.00010751336897859194, l2: 0.00037917107005745894   Iteration 37 of 100, tot loss = 4.852556963224669, l1: 0.00010755607729299447, l2: 0.00037769961911819974   Iteration 38 of 100, tot loss = 4.929327387558787, l1: 0.00010849776107964939, l2: 0.0003844349791363225   Iteration 39 of 100, tot loss = 4.88384364812802, l1: 0.00010748775933349792, l2: 0.0003808966068437514   Iteration 40 of 100, tot loss = 4.819577771425247, l1: 0.00010615323053571046, l2: 0.00037580454772978555   Iteration 41 of 100, tot loss = 4.812440680294502, l1: 0.0001062419534009234, l2: 0.0003750021161267352   Iteration 42 of 100, tot loss = 4.858246252650306, l1: 0.00010652501640877398, l2: 0.00037929961087814135   Iteration 43 of 100, tot loss = 4.818829785945804, l1: 0.00010614464901114251, l2: 0.00037573833156615325   Iteration 44 of 100, tot loss = 4.789991942319003, l1: 0.00010564753632603573, l2: 0.00037335166019726205   Iteration 45 of 100, tot loss = 4.730922253926595, l1: 0.00010444268466219203, l2: 0.0003686495429267072   Iteration 46 of 100, tot loss = 4.703536997670713, l1: 0.00010447915140147138, l2: 0.00036587455036604535   Iteration 47 of 100, tot loss = 4.669194074387246, l1: 0.0001037199045698644, l2: 0.00036319950471651996   Iteration 48 of 100, tot loss = 4.720405076940854, l1: 0.00010417060540627669, l2: 0.0003678699031297583   Iteration 49 of 100, tot loss = 4.716445888791766, l1: 0.00010424615152724258, l2: 0.00036739843792985287   Iteration 50 of 100, tot loss = 4.6960016536712645, l1: 0.00010411719187686686, l2: 0.0003654829744482413   Iteration 51 of 100, tot loss = 4.6439615464678, l1: 0.00010309818325742312, l2: 0.0003612979723816263   Iteration 52 of 100, tot loss = 4.619101299689366, l1: 0.00010319227686313052, l2: 0.0003587178541168284   Iteration 53 of 100, tot loss = 4.638617475077791, l1: 0.00010311464575352147, l2: 0.0003607471029818901   Iteration 54 of 100, tot loss = 4.692141245912622, l1: 0.00010352026139217842, l2: 0.00036569386483515027   Iteration 55 of 100, tot loss = 4.670488114790483, l1: 0.00010327812633477151, l2: 0.0003637706866191531   Iteration 56 of 100, tot loss = 4.655886901276452, l1: 0.00010304365059710108, l2: 0.00036254504084354267   Iteration 57 of 100, tot loss = 4.642210040176124, l1: 0.00010273477783541016, l2: 0.0003614862272801825   Iteration 58 of 100, tot loss = 4.634638342364081, l1: 0.00010234693287076943, l2: 0.000361116902585591   Iteration 59 of 100, tot loss = 4.633182962061995, l1: 0.00010228046646215401, l2: 0.0003610378312799385   Iteration 60 of 100, tot loss = 4.620808068911234, l1: 0.00010253498900662332, l2: 0.0003595458193255278   Iteration 61 of 100, tot loss = 4.597672302214826, l1: 0.00010253929010149642, l2: 0.0003572279412287944   Iteration 62 of 100, tot loss = 4.624042222576756, l1: 0.00010289334257381157, l2: 0.0003595108810846033   Iteration 63 of 100, tot loss = 4.623829951362004, l1: 0.0001029123402928697, l2: 0.00035947065598087473   Iteration 64 of 100, tot loss = 4.651609975844622, l1: 0.0001035380448684009, l2: 0.00036162295350550266   Iteration 65 of 100, tot loss = 4.652850939677312, l1: 0.00010371194859124864, l2: 0.00036157314607407896   Iteration 66 of 100, tot loss = 4.680061315045212, l1: 0.00010434188935147232, l2: 0.00036366424363663873   Iteration 67 of 100, tot loss = 4.67836050133207, l1: 0.00010450415177715581, l2: 0.0003633318999313068   Iteration 68 of 100, tot loss = 4.646228891961715, l1: 0.00010411629891142879, l2: 0.0003605065916524515   Iteration 69 of 100, tot loss = 4.671240478322126, l1: 0.00010469600217049077, l2: 0.00036242804671401507   Iteration 70 of 100, tot loss = 4.6508032390049525, l1: 0.00010441205886309035, l2: 0.00036066826620039395   Iteration 71 of 100, tot loss = 4.639329507317342, l1: 0.00010425109776150121, l2: 0.00035968185406552077   Iteration 72 of 100, tot loss = 4.639473391903771, l1: 0.00010428358998776982, l2: 0.0003596637505122797   Iteration 73 of 100, tot loss = 4.652609557321627, l1: 0.00010429703571103284, l2: 0.00036096392133215776   Iteration 74 of 100, tot loss = 4.6435503186406315, l1: 0.00010424984823141524, l2: 0.0003601051849309934   Iteration 75 of 100, tot loss = 4.644786364237468, l1: 0.00010421863727970048, l2: 0.0003602600006464248   Iteration 76 of 100, tot loss = 4.650992311929402, l1: 0.00010428928018020662, l2: 0.00036080995223049906   Iteration 77 of 100, tot loss = 4.62222290967966, l1: 0.00010373025158172256, l2: 0.00035849204056370135   Iteration 78 of 100, tot loss = 4.596796277241829, l1: 0.0001034689315323097, l2: 0.0003562106973661755   Iteration 79 of 100, tot loss = 4.592294587364679, l1: 0.00010359826631463852, l2: 0.00035563119391680847   Iteration 80 of 100, tot loss = 4.578092324733734, l1: 0.00010337278317820164, l2: 0.00035443645083432784   Iteration 81 of 100, tot loss = 4.5659202410851, l1: 0.00010312411285229717, l2: 0.0003534679128256638   Iteration 82 of 100, tot loss = 4.57392030227475, l1: 0.00010346250467303368, l2: 0.00035392952690372335   Iteration 83 of 100, tot loss = 4.554483985326376, l1: 0.00010311286209196032, l2: 0.00035233553783008717   Iteration 84 of 100, tot loss = 4.570496249766577, l1: 0.0001034399006346662, l2: 0.00035360972573731227   Iteration 85 of 100, tot loss = 4.578833644530352, l1: 0.00010338199662167908, l2: 0.0003545013689504498   Iteration 86 of 100, tot loss = 4.574540279632391, l1: 0.00010332939276596271, l2: 0.0003541246363500588   Iteration 87 of 100, tot loss = 4.567912586804094, l1: 0.00010323122298631325, l2: 0.0003535600368069999   Iteration 88 of 100, tot loss = 4.5760084309361195, l1: 0.00010338189184205311, l2: 0.00035421895236967515   Iteration 89 of 100, tot loss = 4.58803866686446, l1: 0.00010372058349776637, l2: 0.00035508328429397114   Iteration 90 of 100, tot loss = 4.591496724552578, l1: 0.00010385615985594794, l2: 0.0003552935133080205   Iteration 91 of 100, tot loss = 4.590773391199636, l1: 0.00010417447629151866, l2: 0.0003549028634142315   Iteration 92 of 100, tot loss = 4.58992038861565, l1: 0.00010404823463076077, l2: 0.0003549438047427785   Iteration 93 of 100, tot loss = 4.557729539050851, l1: 0.00010331306823714566, l2: 0.00035245988614386527   Iteration 94 of 100, tot loss = 4.564116444993527, l1: 0.0001035387713853468, l2: 0.00035287287337655956   Iteration 95 of 100, tot loss = 4.585089681023046, l1: 0.00010370280046787996, l2: 0.00035480616804163317   Iteration 96 of 100, tot loss = 4.569245971739292, l1: 0.00010361358439088993, l2: 0.00035331101298652356   Iteration 97 of 100, tot loss = 4.568049379230774, l1: 0.0001036511279118113, l2: 0.00035315381030590493   Iteration 98 of 100, tot loss = 4.546435288020542, l1: 0.00010311134766587722, l2: 0.0003515321813934787   Iteration 99 of 100, tot loss = 4.582939851163614, l1: 0.00010384576300493055, l2: 0.0003544482229938354   Iteration 100 of 100, tot loss = 4.589465212821961, l1: 0.00010377322672866285, l2: 0.0003551732953928877
   End of epoch 1397; saving model... 

Epoch 1398 of 2000
   Iteration 1 of 100, tot loss = 4.148362159729004, l1: 0.00011563092994038016, l2: 0.00029920527595095336   Iteration 2 of 100, tot loss = 6.159478664398193, l1: 0.00014099843974690884, l2: 0.0004749494110001251   Iteration 3 of 100, tot loss = 4.865929921468099, l1: 0.0001161832406069152, l2: 0.00037040974711999297   Iteration 4 of 100, tot loss = 4.50658792257309, l1: 0.00011315586743876338, l2: 0.0003375029191374779   Iteration 5 of 100, tot loss = 4.653768301010132, l1: 0.00011440871458034962, l2: 0.0003509681089781225   Iteration 6 of 100, tot loss = 4.786500652631124, l1: 0.00011836177388128515, l2: 0.00036028828859950107   Iteration 7 of 100, tot loss = 4.832059962408883, l1: 0.00011777878847039704, l2: 0.0003654272046073207   Iteration 8 of 100, tot loss = 4.5714512169361115, l1: 0.00011262367570452625, l2: 0.00034452144245733507   Iteration 9 of 100, tot loss = 4.311340437995063, l1: 0.00010697163331011932, l2: 0.0003241624055792474   Iteration 10 of 100, tot loss = 4.355952167510987, l1: 0.00010573628314887173, l2: 0.0003298589290352538   Iteration 11 of 100, tot loss = 4.281526977365667, l1: 0.00010216919591502202, l2: 0.0003259834986899725   Iteration 12 of 100, tot loss = 4.369180579980214, l1: 0.0001036304153482585, l2: 0.0003332876367494464   Iteration 13 of 100, tot loss = 4.466816810461191, l1: 0.00010333506753130887, l2: 0.0003433466056146874   Iteration 14 of 100, tot loss = 4.714161072458539, l1: 0.00010806156725655975, l2: 0.00036335453374444375   Iteration 15 of 100, tot loss = 4.827474164962768, l1: 0.00011064322704138856, l2: 0.0003721041837707162   Iteration 16 of 100, tot loss = 4.776108965277672, l1: 0.00010869646939681843, l2: 0.00036891442323394585   Iteration 17 of 100, tot loss = 4.8423694582546455, l1: 0.0001110550801968202, l2: 0.00037318186129114647   Iteration 18 of 100, tot loss = 4.872440695762634, l1: 0.00011167487213646786, l2: 0.0003755691933393892   Iteration 19 of 100, tot loss = 4.823587831697966, l1: 0.00011129510944317047, l2: 0.00037106367036406144   Iteration 20 of 100, tot loss = 4.878008854389191, l1: 0.00011147843797516543, l2: 0.0003763224449357949   Iteration 21 of 100, tot loss = 4.9505539281027655, l1: 0.00011168005897031565, l2: 0.0003833753331231752   Iteration 22 of 100, tot loss = 4.9701090400869194, l1: 0.00011199639596849342, l2: 0.00038501450564415956   Iteration 23 of 100, tot loss = 4.88182082383529, l1: 0.00011062106711071227, l2: 0.00037756101380142826   Iteration 24 of 100, tot loss = 4.809258540471395, l1: 0.00010991934110885875, l2: 0.00037100651146223146   Iteration 25 of 100, tot loss = 4.73860918045044, l1: 0.00010928821662673726, l2: 0.0003645727003458887   Iteration 26 of 100, tot loss = 4.691900766812838, l1: 0.00010821588428091042, l2: 0.00036097419125816   Iteration 27 of 100, tot loss = 4.683493861445674, l1: 0.00010763053217454365, l2: 0.00036071885282311724   Iteration 28 of 100, tot loss = 4.791998028755188, l1: 0.00010977830087150713, l2: 0.0003694215016106942   Iteration 29 of 100, tot loss = 4.77620739772402, l1: 0.00010896206643745496, l2: 0.00036865867301018844   Iteration 30 of 100, tot loss = 4.710291703542073, l1: 0.00010759082627676738, l2: 0.000363438343629241   Iteration 31 of 100, tot loss = 4.664084180708854, l1: 0.00010659814213267378, l2: 0.00035981027570913636   Iteration 32 of 100, tot loss = 4.6564417108893394, l1: 0.00010643379937391728, l2: 0.0003592103703340399   Iteration 33 of 100, tot loss = 4.63851049452117, l1: 0.00010589063634997177, l2: 0.0003579604116649452   Iteration 34 of 100, tot loss = 4.608620215864742, l1: 0.00010526515410781619, l2: 0.0003555968655478757   Iteration 35 of 100, tot loss = 4.574331324441093, l1: 0.00010480799205002508, l2: 0.00035262513889132863   Iteration 36 of 100, tot loss = 4.554490725199382, l1: 0.0001043111030109382, l2: 0.0003511379683105689   Iteration 37 of 100, tot loss = 4.604117457931106, l1: 0.00010451910949651958, l2: 0.0003558926353533123   Iteration 38 of 100, tot loss = 4.614429762488918, l1: 0.00010456818161305579, l2: 0.0003568747932532508   Iteration 39 of 100, tot loss = 4.635862228197929, l1: 0.00010568013199073717, l2: 0.0003579060893314771   Iteration 40 of 100, tot loss = 4.6475819706916806, l1: 0.00010614710899972123, l2: 0.0003586110862670466   Iteration 41 of 100, tot loss = 4.658346827437238, l1: 0.00010638176261237254, l2: 0.0003594529173314208   Iteration 42 of 100, tot loss = 4.609488844871521, l1: 0.00010570454538711125, l2: 0.0003552443363116167   Iteration 43 of 100, tot loss = 4.646868811097256, l1: 0.00010606286242512207, l2: 0.00035862401628671865   Iteration 44 of 100, tot loss = 4.673721557313746, l1: 0.00010652781284906351, l2: 0.00036084433999018404   Iteration 45 of 100, tot loss = 4.649454122119479, l1: 0.000106122444008684, l2: 0.0003588229657099065   Iteration 46 of 100, tot loss = 4.598499458769093, l1: 0.00010500871133484432, l2: 0.0003548412319303126   Iteration 47 of 100, tot loss = 4.551888912282092, l1: 0.00010429145495823406, l2: 0.0003508974335458566   Iteration 48 of 100, tot loss = 4.5573536058266955, l1: 0.00010432299541207612, l2: 0.000351412362154709   Iteration 49 of 100, tot loss = 4.560648275881397, l1: 0.0001042018430984617, l2: 0.0003518629822124518   Iteration 50 of 100, tot loss = 4.57948595046997, l1: 0.0001049272769887466, l2: 0.00035302131524076683   Iteration 51 of 100, tot loss = 4.566688780691109, l1: 0.00010484231404238836, l2: 0.00035182656132497404   Iteration 52 of 100, tot loss = 4.587911990972666, l1: 0.00010483403457328677, l2: 0.00035395716174836986   Iteration 53 of 100, tot loss = 4.601047362921373, l1: 0.00010483211399504107, l2: 0.00035527261948204195   Iteration 54 of 100, tot loss = 4.588913590819748, l1: 0.00010482742786903314, l2: 0.0003540639286425344   Iteration 55 of 100, tot loss = 4.610099107568915, l1: 0.00010528172437757761, l2: 0.0003557281841164116   Iteration 56 of 100, tot loss = 4.5705998710223605, l1: 0.00010461113106430275, l2: 0.0003524488539759269   Iteration 57 of 100, tot loss = 4.588680133484957, l1: 0.00010459789783661173, l2: 0.00035427011377913385   Iteration 58 of 100, tot loss = 4.565362210931449, l1: 0.00010372499171604307, l2: 0.00035281122786570983   Iteration 59 of 100, tot loss = 4.593767566195989, l1: 0.00010431061969414922, l2: 0.0003550661351902694   Iteration 60 of 100, tot loss = 4.596519831816355, l1: 0.00010426218238232346, l2: 0.0003553897991272   Iteration 61 of 100, tot loss = 4.614672938331228, l1: 0.00010461287718976192, l2: 0.00035685441575058905   Iteration 62 of 100, tot loss = 4.60328721615576, l1: 0.00010473130215916993, l2: 0.0003555974187967818   Iteration 63 of 100, tot loss = 4.598963968337528, l1: 0.00010411002250972189, l2: 0.0003557863732105091   Iteration 64 of 100, tot loss = 4.601605039089918, l1: 0.00010423789308333653, l2: 0.00035592261019701255   Iteration 65 of 100, tot loss = 4.602586947954618, l1: 0.00010407763542249225, l2: 0.0003561810585849274   Iteration 66 of 100, tot loss = 4.595361792679989, l1: 0.00010386587327406177, l2: 0.0003556703053287823   Iteration 67 of 100, tot loss = 4.637742914370636, l1: 0.00010472285609480355, l2: 0.0003590514351711122   Iteration 68 of 100, tot loss = 4.663071390460519, l1: 0.0001053712186309413, l2: 0.0003609359204454128   Iteration 69 of 100, tot loss = 4.676442239595496, l1: 0.0001053410397498342, l2: 0.00036230318474353874   Iteration 70 of 100, tot loss = 4.694961019924709, l1: 0.00010590732729594622, l2: 0.0003635887746765677   Iteration 71 of 100, tot loss = 4.695312060100932, l1: 0.00010568471340557665, l2: 0.00036384649271420925   Iteration 72 of 100, tot loss = 4.669244633780585, l1: 0.00010526468213356566, l2: 0.0003616597813460329   Iteration 73 of 100, tot loss = 4.668288426856472, l1: 0.0001050037796743061, l2: 0.00036182506300974637   Iteration 74 of 100, tot loss = 4.648451634355493, l1: 0.00010459074415062, l2: 0.00036025441914790536   Iteration 75 of 100, tot loss = 4.6429702726999915, l1: 0.00010467209804725523, l2: 0.00035962492906643697   Iteration 76 of 100, tot loss = 4.671096215122624, l1: 0.00010463314069248124, l2: 0.0003624764800693619   Iteration 77 of 100, tot loss = 4.705052614212036, l1: 0.00010516520602063142, l2: 0.0003653400546989053   Iteration 78 of 100, tot loss = 4.690104212516394, l1: 0.00010527020985580001, l2: 0.0003637402107228692   Iteration 79 of 100, tot loss = 4.698377932174297, l1: 0.00010533748009268643, l2: 0.0003645003127093321   Iteration 80 of 100, tot loss = 4.695846745371819, l1: 0.00010522382590352208, l2: 0.00036436084847082386   Iteration 81 of 100, tot loss = 4.677207302164148, l1: 0.00010508405866408951, l2: 0.0003626366714691099   Iteration 82 of 100, tot loss = 4.707527297299083, l1: 0.00010517623643575395, l2: 0.00036557649298451795   Iteration 83 of 100, tot loss = 4.706932179899101, l1: 0.00010544212115297364, l2: 0.0003652510968533468   Iteration 84 of 100, tot loss = 4.695658964770181, l1: 0.00010480790821193728, l2: 0.00036475798818476807   Iteration 85 of 100, tot loss = 4.693855552112355, l1: 0.00010487252015189048, l2: 0.00036451303492546737   Iteration 86 of 100, tot loss = 4.7019840756128, l1: 0.00010516963409954346, l2: 0.00036502877324088586   Iteration 87 of 100, tot loss = 4.686387821175586, l1: 0.00010460159156254718, l2: 0.00036403719030036964   Iteration 88 of 100, tot loss = 4.683889890258962, l1: 0.00010465766329368556, l2: 0.00036373132570588496   Iteration 89 of 100, tot loss = 4.703023222055328, l1: 0.00010481904005092894, l2: 0.0003654832823863881   Iteration 90 of 100, tot loss = 4.6962950944900514, l1: 0.0001048568323312793, l2: 0.0003647726771305315   Iteration 91 of 100, tot loss = 4.699194889802199, l1: 0.0001049855188114804, l2: 0.00036493396999923367   Iteration 92 of 100, tot loss = 4.686403277127639, l1: 0.00010439509613839302, l2: 0.00036424523141645096   Iteration 93 of 100, tot loss = 4.664236983945293, l1: 0.00010385657687263403, l2: 0.0003625671211842908   Iteration 94 of 100, tot loss = 4.641934014381246, l1: 0.00010320758802334511, l2: 0.00036098581303451013   Iteration 95 of 100, tot loss = 4.630575601678146, l1: 0.00010285244968958738, l2: 0.0003602051101073525   Iteration 96 of 100, tot loss = 4.621383319298427, l1: 0.00010273817751264384, l2: 0.00035940015383554663   Iteration 97 of 100, tot loss = 4.613361874806512, l1: 0.00010288507140093944, l2: 0.0003584511153527161   Iteration 98 of 100, tot loss = 4.613568135670254, l1: 0.00010284869120146233, l2: 0.00035850812157744316   Iteration 99 of 100, tot loss = 4.602287757276285, l1: 0.0001025945241807024, l2: 0.0003576342507695892   Iteration 100 of 100, tot loss = 4.603092796802521, l1: 0.0001026387001547846, l2: 0.0003576705789600965
   End of epoch 1398; saving model... 

Epoch 1399 of 2000
   Iteration 1 of 100, tot loss = 4.403555870056152, l1: 9.047637286130339e-05, l2: 0.00034987920662388206   Iteration 2 of 100, tot loss = 4.514659643173218, l1: 9.059397780220024e-05, l2: 0.0003608719853218645   Iteration 3 of 100, tot loss = 4.802670796712239, l1: 9.856497248013814e-05, l2: 0.0003817021012461434   Iteration 4 of 100, tot loss = 4.765363931655884, l1: 9.519354534859303e-05, l2: 0.00038134284841362387   Iteration 5 of 100, tot loss = 4.766619396209717, l1: 0.00010067704279208556, l2: 0.00037598489434458314   Iteration 6 of 100, tot loss = 4.709290901819865, l1: 0.00010073634136157732, l2: 0.00037019274896010756   Iteration 7 of 100, tot loss = 4.428478513445173, l1: 9.75398558823924e-05, l2: 0.00034530799686243493   Iteration 8 of 100, tot loss = 4.617351949214935, l1: 0.00010128678331966512, l2: 0.0003604484154493548   Iteration 9 of 100, tot loss = 4.469223101933797, l1: 9.810043088186326e-05, l2: 0.00034882188386594254   Iteration 10 of 100, tot loss = 4.388159728050232, l1: 9.479640648351051e-05, l2: 0.0003440195694565773   Iteration 11 of 100, tot loss = 4.63061820377003, l1: 0.00010065522920657796, l2: 0.00036240659061480653   Iteration 12 of 100, tot loss = 4.734199424584706, l1: 0.00010214123055144834, l2: 0.0003712787123125357   Iteration 13 of 100, tot loss = 4.828134298324585, l1: 0.00010425611309563884, l2: 0.0003785573174424756   Iteration 14 of 100, tot loss = 4.6071341548647196, l1: 9.924453739326314e-05, l2: 0.00036146887889896916   Iteration 15 of 100, tot loss = 4.613245725631714, l1: 0.000100805704035641, l2: 0.00036051886757680526   Iteration 16 of 100, tot loss = 4.467804968357086, l1: 9.701551493890292e-05, l2: 0.000349764981365297   Iteration 17 of 100, tot loss = 4.445742607116699, l1: 9.448297543499125e-05, l2: 0.00035009128452443024   Iteration 18 of 100, tot loss = 4.350860622194078, l1: 9.345504420505474e-05, l2: 0.00034163101695917756   Iteration 19 of 100, tot loss = 4.381480493043599, l1: 9.506302797633812e-05, l2: 0.00034308501926716417   Iteration 20 of 100, tot loss = 4.5915433168411255, l1: 9.862335809884826e-05, l2: 0.0003605309735576157   Iteration 21 of 100, tot loss = 4.557374534152803, l1: 9.710200694306488e-05, l2: 0.0003586354467274976   Iteration 22 of 100, tot loss = 4.4742373336445205, l1: 9.569397678371223e-05, l2: 0.00035172975714720616   Iteration 23 of 100, tot loss = 4.445381776146267, l1: 9.529308827831815e-05, l2: 0.00034924509046781486   Iteration 24 of 100, tot loss = 4.4749889473120374, l1: 9.501547250086635e-05, l2: 0.0003524834216174592   Iteration 25 of 100, tot loss = 4.4465082740783695, l1: 9.502241868176497e-05, l2: 0.0003496284078573808   Iteration 26 of 100, tot loss = 4.457194218268762, l1: 9.494258105965295e-05, l2: 0.0003507768409550548   Iteration 27 of 100, tot loss = 4.371612707773845, l1: 9.294859787419059e-05, l2: 0.00034421267250932204   Iteration 28 of 100, tot loss = 4.425378952707563, l1: 9.418723714458923e-05, l2: 0.00034835065681753416   Iteration 29 of 100, tot loss = 4.425575848283438, l1: 9.529014704608487e-05, l2: 0.0003472674369073377   Iteration 30 of 100, tot loss = 4.44464643796285, l1: 9.615176786610391e-05, l2: 0.00034831287436342485   Iteration 31 of 100, tot loss = 4.43811087454519, l1: 9.688411427627978e-05, l2: 0.00034692697133117864   Iteration 32 of 100, tot loss = 4.428790554404259, l1: 9.660126681865222e-05, l2: 0.0003462777858658228   Iteration 33 of 100, tot loss = 4.441710038618608, l1: 9.706382193006434e-05, l2: 0.000347107179660463   Iteration 34 of 100, tot loss = 4.542433808831608, l1: 9.821318139181719e-05, l2: 0.0003560301967595211   Iteration 35 of 100, tot loss = 4.5408112253461566, l1: 9.858458101267129e-05, l2: 0.00035549653860341226   Iteration 36 of 100, tot loss = 4.491209579838647, l1: 9.791664009147401e-05, l2: 0.00035120431493851356   Iteration 37 of 100, tot loss = 4.5006396770477295, l1: 9.829952427566782e-05, l2: 0.0003517644398852025   Iteration 38 of 100, tot loss = 4.5719149677377, l1: 9.978725354717186e-05, l2: 0.000357404239797401   Iteration 39 of 100, tot loss = 4.606330633163452, l1: 9.996510599217474e-05, l2: 0.0003606679536646996   Iteration 40 of 100, tot loss = 4.5969191014766695, l1: 9.991793149310979e-05, l2: 0.0003597739752876805   Iteration 41 of 100, tot loss = 4.625954436092842, l1: 0.00010057133237374151, l2: 0.0003620241083653371   Iteration 42 of 100, tot loss = 4.664559733299982, l1: 0.00010140883413051987, l2: 0.000365047137338912   Iteration 43 of 100, tot loss = 4.641836016677146, l1: 0.00010085836004501014, l2: 0.0003633252395788034   Iteration 44 of 100, tot loss = 4.671691206368533, l1: 0.00010144641989675371, l2: 0.0003657226992469408   Iteration 45 of 100, tot loss = 4.658057800928751, l1: 0.00010172264568003205, l2: 0.00036408313349561974   Iteration 46 of 100, tot loss = 4.638754279717155, l1: 0.00010152475055213512, l2: 0.00036235067660606506   Iteration 47 of 100, tot loss = 4.608431795810131, l1: 0.00010105880620416274, l2: 0.0003597843723911594   Iteration 48 of 100, tot loss = 4.585530325770378, l1: 0.0001008915542873486, l2: 0.00035766147721005837   Iteration 49 of 100, tot loss = 4.584823477024934, l1: 0.00010063674911792504, l2: 0.0003578455982687028   Iteration 50 of 100, tot loss = 4.590835919380188, l1: 0.00010035018283815588, l2: 0.000358733409375418   Iteration 51 of 100, tot loss = 4.6015862249860575, l1: 0.00010047446659311889, l2: 0.0003596841555668552   Iteration 52 of 100, tot loss = 4.58213059260295, l1: 0.00010027551902917017, l2: 0.00035793753984482744   Iteration 53 of 100, tot loss = 4.546442004869569, l1: 9.961562256717545e-05, l2: 0.0003550285775486802   Iteration 54 of 100, tot loss = 4.527154979882417, l1: 9.922353281042349e-05, l2: 0.0003534919648060437   Iteration 55 of 100, tot loss = 4.537292554161765, l1: 9.93417034227274e-05, l2: 0.000354387551355599   Iteration 56 of 100, tot loss = 4.506570518016815, l1: 9.884696497205628e-05, l2: 0.00035181008596347444   Iteration 57 of 100, tot loss = 4.501557550932231, l1: 9.865569837417927e-05, l2: 0.0003515000563837882   Iteration 58 of 100, tot loss = 4.502803966916841, l1: 9.877837780142476e-05, l2: 0.0003515020188983883   Iteration 59 of 100, tot loss = 4.520787958371437, l1: 9.932477522682856e-05, l2: 0.0003527540205958126   Iteration 60 of 100, tot loss = 4.5184289852778114, l1: 9.892085563478759e-05, l2: 0.00035292204289968747   Iteration 61 of 100, tot loss = 4.5339095240733664, l1: 9.914053350898864e-05, l2: 0.00035425041898604114   Iteration 62 of 100, tot loss = 4.5314719907699095, l1: 9.936779614720089e-05, l2: 0.0003537794031980147   Iteration 63 of 100, tot loss = 4.535410956730918, l1: 9.931933677834755e-05, l2: 0.0003542217593695525   Iteration 64 of 100, tot loss = 4.521199267357588, l1: 9.920068686142258e-05, l2: 0.00035291924018565624   Iteration 65 of 100, tot loss = 4.525175450398372, l1: 9.952445179806091e-05, l2: 0.00035299309372651175   Iteration 66 of 100, tot loss = 4.547836061679956, l1: 9.938115109627799e-05, l2: 0.0003554024559420985   Iteration 67 of 100, tot loss = 4.557625767010362, l1: 9.934903008892868e-05, l2: 0.0003564135471633427   Iteration 68 of 100, tot loss = 4.5484861415975235, l1: 9.91323305938482e-05, l2: 0.00035571628434220604   Iteration 69 of 100, tot loss = 4.568499420000159, l1: 9.928845200034709e-05, l2: 0.0003575614910400675   Iteration 70 of 100, tot loss = 4.543774601391384, l1: 9.88487992539636e-05, l2: 0.0003555286619562789   Iteration 71 of 100, tot loss = 4.527883328182597, l1: 9.880286321071842e-05, l2: 0.00035398547077702334   Iteration 72 of 100, tot loss = 4.531601422362858, l1: 9.893506593370048e-05, l2: 0.0003542250772928431   Iteration 73 of 100, tot loss = 4.523944884130399, l1: 9.895480551465083e-05, l2: 0.0003534396837245392   Iteration 74 of 100, tot loss = 4.551979184150696, l1: 9.936454429210957e-05, l2: 0.0003558333750383148   Iteration 75 of 100, tot loss = 4.56026302019755, l1: 9.953346486630228e-05, l2: 0.00035649283808500816   Iteration 76 of 100, tot loss = 4.542457135100114, l1: 9.919206839090054e-05, l2: 0.0003550536457623821   Iteration 77 of 100, tot loss = 4.513456979355254, l1: 9.872390520675773e-05, l2: 0.0003526217934249163   Iteration 78 of 100, tot loss = 4.502634146274665, l1: 9.877362460359179e-05, l2: 0.00035148979040035885   Iteration 79 of 100, tot loss = 4.500421928454049, l1: 9.89691672910777e-05, l2: 0.0003510730260396612   Iteration 80 of 100, tot loss = 4.498787742853165, l1: 9.913418166433985e-05, l2: 0.00035074459319730524   Iteration 81 of 100, tot loss = 4.490825305750341, l1: 9.927880424067642e-05, l2: 0.00034980372691751217   Iteration 82 of 100, tot loss = 4.4806403212431, l1: 9.871704139918509e-05, l2: 0.0003493469910875031   Iteration 83 of 100, tot loss = 4.461493997688753, l1: 9.852310575801509e-05, l2: 0.0003476262943614386   Iteration 84 of 100, tot loss = 4.4605918895630605, l1: 9.861393486755246e-05, l2: 0.00034744525414897085   Iteration 85 of 100, tot loss = 4.449388097314274, l1: 9.847713736235164e-05, l2: 0.0003464616723837988   Iteration 86 of 100, tot loss = 4.468886727510497, l1: 9.861146383289366e-05, l2: 0.0003482772091195139   Iteration 87 of 100, tot loss = 4.439525863219952, l1: 9.793519824723199e-05, l2: 0.0003460173884168503   Iteration 88 of 100, tot loss = 4.434153951027176, l1: 9.806279645503805e-05, l2: 0.00034535259907319084   Iteration 89 of 100, tot loss = 4.416723973295662, l1: 9.77762745483386e-05, l2: 0.0003438961232379288   Iteration 90 of 100, tot loss = 4.434283700254229, l1: 9.822226125430056e-05, l2: 0.00034520610934123396   Iteration 91 of 100, tot loss = 4.423763590854603, l1: 9.818706386575506e-05, l2: 0.00034418929583178117   Iteration 92 of 100, tot loss = 4.420464838328569, l1: 9.838610723185728e-05, l2: 0.0003436603770039611   Iteration 93 of 100, tot loss = 4.403189332254471, l1: 9.818540662132775e-05, l2: 0.00034213352711215335   Iteration 94 of 100, tot loss = 4.389321059622663, l1: 9.800872094080872e-05, l2: 0.0003409233853590001   Iteration 95 of 100, tot loss = 4.386304570499219, l1: 9.805697493567621e-05, l2: 0.00034057348277297263   Iteration 96 of 100, tot loss = 4.379361270616452, l1: 9.809104324176587e-05, l2: 0.0003398450843026997   Iteration 97 of 100, tot loss = 4.369584404316145, l1: 9.7955597290642e-05, l2: 0.0003390028436977858   Iteration 98 of 100, tot loss = 4.363222493200886, l1: 9.796964131593846e-05, l2: 0.0003383526082889519   Iteration 99 of 100, tot loss = 4.373742071065036, l1: 9.827439039281211e-05, l2: 0.00033909981676984126   Iteration 100 of 100, tot loss = 4.414581896066665, l1: 9.910680277243956e-05, l2: 0.0003423513866437133
   End of epoch 1399; saving model... 

Epoch 1400 of 2000
   Iteration 1 of 100, tot loss = 2.8749754428863525, l1: 8.230601088143885e-05, l2: 0.00020519152167253196   Iteration 2 of 100, tot loss = 4.276883959770203, l1: 0.00010158942677662708, l2: 0.0003260989469708875   Iteration 3 of 100, tot loss = 3.9820515314737954, l1: 9.806804397764306e-05, l2: 0.0003001370932906866   Iteration 4 of 100, tot loss = 3.6737602949142456, l1: 9.281721031584311e-05, l2: 0.00027455880263005383   Iteration 5 of 100, tot loss = 3.687067413330078, l1: 9.229188435710967e-05, l2: 0.0002764148433925584   Iteration 6 of 100, tot loss = 3.868332862854004, l1: 8.835892852706213e-05, l2: 0.00029847435022626695   Iteration 7 of 100, tot loss = 3.6923257623400008, l1: 8.268885721918195e-05, l2: 0.00028654371063956726   Iteration 8 of 100, tot loss = 3.887039214372635, l1: 8.651541065773927e-05, l2: 0.00030218850406527054   Iteration 9 of 100, tot loss = 4.011206229527791, l1: 9.050846953565876e-05, l2: 0.0003106121455655537   Iteration 10 of 100, tot loss = 4.053610682487488, l1: 9.088864753721282e-05, l2: 0.00031447241344721987   Iteration 11 of 100, tot loss = 4.155371644280174, l1: 9.191873224071142e-05, l2: 0.00032361842220945454   Iteration 12 of 100, tot loss = 4.169466475645701, l1: 9.336913596295442e-05, l2: 0.00032357750145213987   Iteration 13 of 100, tot loss = 4.033544558745164, l1: 9.210009235315598e-05, l2: 0.00031125435466842295   Iteration 14 of 100, tot loss = 4.050835830824716, l1: 9.121033248707786e-05, l2: 0.00031387324166384393   Iteration 15 of 100, tot loss = 4.201937754948934, l1: 9.440924768568947e-05, l2: 0.00032578451888790977   Iteration 16 of 100, tot loss = 4.078753620386124, l1: 9.259361650038045e-05, l2: 0.0003152817371301353   Iteration 17 of 100, tot loss = 4.076263539931354, l1: 9.215532576062662e-05, l2: 0.00031547102159546576   Iteration 18 of 100, tot loss = 4.021702514754401, l1: 9.21702740015462e-05, l2: 0.0003099999699366486   Iteration 19 of 100, tot loss = 4.1051571871105, l1: 9.354621349637837e-05, l2: 0.000316969498128042   Iteration 20 of 100, tot loss = 4.266614758968354, l1: 9.725252020871266e-05, l2: 0.0003294089481642004   Iteration 21 of 100, tot loss = 4.279422567004249, l1: 9.85264778137207e-05, l2: 0.0003294157719383726   Iteration 22 of 100, tot loss = 4.363003481518138, l1: 9.945193001344292e-05, l2: 0.0003368484116931954   Iteration 23 of 100, tot loss = 4.339010891707047, l1: 0.00010003460793877426, l2: 0.0003338664758737887   Iteration 24 of 100, tot loss = 4.303314924240112, l1: 0.00010033263940082786, l2: 0.0003299988481254938   Iteration 25 of 100, tot loss = 4.307320079803467, l1: 9.995198313845322e-05, l2: 0.0003307800192851573   Iteration 26 of 100, tot loss = 4.290674062875601, l1: 9.90665693489763e-05, l2: 0.0003300008314769142   Iteration 27 of 100, tot loss = 4.282174940462466, l1: 9.882950123735807e-05, l2: 0.0003293879875585575   Iteration 28 of 100, tot loss = 4.317232779094151, l1: 9.884354845520907e-05, l2: 0.0003328797236982999   Iteration 29 of 100, tot loss = 4.345582945593472, l1: 0.00010029602332185183, l2: 0.0003342622658237815   Iteration 30 of 100, tot loss = 4.337755982081095, l1: 0.00010009931186990191, l2: 0.00033367628057021646   Iteration 31 of 100, tot loss = 4.307329047110773, l1: 9.979252823658527e-05, l2: 0.000330940370575615   Iteration 32 of 100, tot loss = 4.297638565301895, l1: 9.916603767123888e-05, l2: 0.00033059781253541587   Iteration 33 of 100, tot loss = 4.340841755722508, l1: 9.953922276371017e-05, l2: 0.0003345449460374022   Iteration 34 of 100, tot loss = 4.323398982777315, l1: 9.945206928772249e-05, l2: 0.000332887822878547   Iteration 35 of 100, tot loss = 4.36020679473877, l1: 0.00010005316123299833, l2: 0.00033596751330021234   Iteration 36 of 100, tot loss = 4.310829838116963, l1: 9.891589090532054e-05, l2: 0.000332167087940939   Iteration 37 of 100, tot loss = 4.2955032361520304, l1: 9.874489124568582e-05, l2: 0.00033080542729097744   Iteration 38 of 100, tot loss = 4.281351001639115, l1: 9.828670418324978e-05, l2: 0.0003298483915339028   Iteration 39 of 100, tot loss = 4.358102725102351, l1: 9.967082904693551e-05, l2: 0.0003361394393813008   Iteration 40 of 100, tot loss = 4.3791624188423155, l1: 0.00010031130386778386, l2: 0.0003376049342477927   Iteration 41 of 100, tot loss = 4.376203188082067, l1: 0.00010039530699453658, l2: 0.0003372250084350703   Iteration 42 of 100, tot loss = 4.400635162989299, l1: 0.00010065794343799574, l2: 0.00033940556992699084   Iteration 43 of 100, tot loss = 4.428531469300736, l1: 0.00010028915164374941, l2: 0.000342563993018662   Iteration 44 of 100, tot loss = 4.493894273584539, l1: 0.00010063910502305424, l2: 0.00034875032096434467   Iteration 45 of 100, tot loss = 4.492724482218424, l1: 0.0001005741744948965, l2: 0.00034869827310709904   Iteration 46 of 100, tot loss = 4.4626259337300835, l1: 0.0001000150055599237, l2: 0.0003462475873200911   Iteration 47 of 100, tot loss = 4.498437551741905, l1: 0.00010000455593863661, l2: 0.0003498391990743062   Iteration 48 of 100, tot loss = 4.490064665675163, l1: 0.00010038421214630944, l2: 0.0003486222543263769   Iteration 49 of 100, tot loss = 4.486840175122631, l1: 0.00010065407933529505, l2: 0.0003480299376545246   Iteration 50 of 100, tot loss = 4.437075951099396, l1: 9.979367488995195e-05, l2: 0.00034391391993267463   Iteration 51 of 100, tot loss = 4.444458730080548, l1: 9.98055737570622e-05, l2: 0.00034464029886294156   Iteration 52 of 100, tot loss = 4.448240016515438, l1: 0.0001003026862655623, l2: 0.0003445213148482323   Iteration 53 of 100, tot loss = 4.4407181042545245, l1: 9.95312026511869e-05, l2: 0.0003445406072610497   Iteration 54 of 100, tot loss = 4.418356809351179, l1: 9.921567059926154e-05, l2: 0.00034262000985176266   Iteration 55 of 100, tot loss = 4.409751742536371, l1: 9.961341347661801e-05, l2: 0.00034136176029939884   Iteration 56 of 100, tot loss = 4.416802915079253, l1: 9.975317068113197e-05, l2: 0.0003419271199943198   Iteration 57 of 100, tot loss = 4.38977781304142, l1: 9.880430668106368e-05, l2: 0.00034017347385534985   Iteration 58 of 100, tot loss = 4.374115386913562, l1: 9.853145193433035e-05, l2: 0.00033888008579981095   Iteration 59 of 100, tot loss = 4.407094464463703, l1: 9.902100948423494e-05, l2: 0.0003416884361995163   Iteration 60 of 100, tot loss = 4.41658652027448, l1: 9.88686406344641e-05, l2: 0.0003427900107150587   Iteration 61 of 100, tot loss = 4.428055788649887, l1: 9.945645350123161e-05, l2: 0.00034334912483931565   Iteration 62 of 100, tot loss = 4.424295958011381, l1: 9.96906356088021e-05, l2: 0.00034273895970170177   Iteration 63 of 100, tot loss = 4.388814345238701, l1: 9.90556739178294e-05, l2: 0.00033982576005944304   Iteration 64 of 100, tot loss = 4.3812380861490965, l1: 9.902238639369898e-05, l2: 0.0003391014215594623   Iteration 65 of 100, tot loss = 4.3854498184644255, l1: 9.906074959032524e-05, l2: 0.00033948423179726186   Iteration 66 of 100, tot loss = 4.384390850861867, l1: 9.90980331676617e-05, l2: 0.0003393410518059902   Iteration 67 of 100, tot loss = 4.413227883737479, l1: 9.96855637075991e-05, l2: 0.00034163722424392605   Iteration 68 of 100, tot loss = 4.407841617570204, l1: 9.946367690110278e-05, l2: 0.0003413204839863979   Iteration 69 of 100, tot loss = 4.412060065545898, l1: 9.96064867966039e-05, l2: 0.0003415995190574693   Iteration 70 of 100, tot loss = 4.4710805944034036, l1: 0.00010080175270559266, l2: 0.00034630630689207463   Iteration 71 of 100, tot loss = 4.442695260047913, l1: 0.00010012022350155617, l2: 0.0003441493024862647   Iteration 72 of 100, tot loss = 4.441652594341172, l1: 0.00010010755072976786, l2: 0.00034405770869246527   Iteration 73 of 100, tot loss = 4.431839631028371, l1: 9.957493149495225e-05, l2: 0.00034360903174112463   Iteration 74 of 100, tot loss = 4.4497572038624735, l1: 9.999886198194283e-05, l2: 0.00034497685811476077   Iteration 75 of 100, tot loss = 4.422970674832662, l1: 9.956680737862674e-05, l2: 0.00034273025987204166   Iteration 76 of 100, tot loss = 4.425401435086601, l1: 9.987943902064887e-05, l2: 0.0003426607043748893   Iteration 77 of 100, tot loss = 4.412497215456777, l1: 9.971024923430235e-05, l2: 0.0003415394720678859   Iteration 78 of 100, tot loss = 4.453750199232346, l1: 0.00010029110102876961, l2: 0.0003450839182896277   Iteration 79 of 100, tot loss = 4.443931285339065, l1: 0.00010017441732836946, l2: 0.0003442187103534212   Iteration 80 of 100, tot loss = 4.443622548878193, l1: 0.00010011483814196254, l2: 0.0003442474155235686   Iteration 81 of 100, tot loss = 4.417642821500331, l1: 9.948870766818823e-05, l2: 0.00034227557329426483   Iteration 82 of 100, tot loss = 4.431437287388778, l1: 9.968870831582491e-05, l2: 0.0003434550191031587   Iteration 83 of 100, tot loss = 4.461832688515445, l1: 0.00010022608006123386, l2: 0.00034595718740552934   Iteration 84 of 100, tot loss = 4.479404130152294, l1: 0.0001006759030133253, l2: 0.00034726450879437226   Iteration 85 of 100, tot loss = 4.491353140157812, l1: 0.00010085612593684346, l2: 0.0003482791865655385   Iteration 86 of 100, tot loss = 4.48323683267416, l1: 0.00010081940363729806, l2: 0.0003475042782198681   Iteration 87 of 100, tot loss = 4.481132478549562, l1: 0.00010101008530031761, l2: 0.0003471031608381148   Iteration 88 of 100, tot loss = 4.509384267709472, l1: 0.000101380373135643, l2: 0.00034955805180784824   Iteration 89 of 100, tot loss = 4.508734808878952, l1: 0.00010122286546269034, l2: 0.0003496506136941483   Iteration 90 of 100, tot loss = 4.532125847869449, l1: 0.00010151495823972962, l2: 0.0003516976246222233   Iteration 91 of 100, tot loss = 4.564556015716804, l1: 0.00010207111855769264, l2: 0.000354384481177696   Iteration 92 of 100, tot loss = 4.565405922091526, l1: 0.00010187997775020726, l2: 0.0003546606122931142   Iteration 93 of 100, tot loss = 4.578723301169693, l1: 0.0001020426295512426, l2: 0.0003558296985017957   Iteration 94 of 100, tot loss = 4.55335987501956, l1: 0.00010158976835961593, l2: 0.0003537462170959074   Iteration 95 of 100, tot loss = 4.539882615992897, l1: 0.00010113489100292914, l2: 0.0003528533684163305   Iteration 96 of 100, tot loss = 4.553381871432066, l1: 0.00010133721032919614, l2: 0.00035400097476667725   Iteration 97 of 100, tot loss = 4.579131289855721, l1: 0.00010184355651102865, l2: 0.00035606957035926506   Iteration 98 of 100, tot loss = 4.567015011699832, l1: 0.00010155058088439175, l2: 0.0003551509182624595   Iteration 99 of 100, tot loss = 4.560341171544008, l1: 0.00010118548607811182, l2: 0.0003548486289365989   Iteration 100 of 100, tot loss = 4.547735763788223, l1: 0.00010111620620591566, l2: 0.00035365736795938573
   End of epoch 1400; saving model... 

Epoch 1401 of 2000
   Iteration 1 of 100, tot loss = 5.968507766723633, l1: 0.0001398744498146698, l2: 0.0004569762968458235   Iteration 2 of 100, tot loss = 4.800882935523987, l1: 0.00012128602611483075, l2: 0.00035880225186701864   Iteration 3 of 100, tot loss = 4.6084301471710205, l1: 0.00011311881098663434, l2: 0.0003477241940951596   Iteration 4 of 100, tot loss = 4.472900331020355, l1: 0.00011028391600120813, l2: 0.0003370061094756238   Iteration 5 of 100, tot loss = 5.740970182418823, l1: 0.0001267205778276548, l2: 0.00044737643911503256   Iteration 6 of 100, tot loss = 5.545027057329814, l1: 0.00012736840047485506, l2: 0.0004271343059372157   Iteration 7 of 100, tot loss = 5.079516240528652, l1: 0.00011603413076954894, l2: 0.0003919174944582794   Iteration 8 of 100, tot loss = 5.005694359540939, l1: 0.00011610711635512416, l2: 0.00038446232065325603   Iteration 9 of 100, tot loss = 5.350897868474324, l1: 0.00012114935129324699, l2: 0.0004139404368793799   Iteration 10 of 100, tot loss = 5.607206320762634, l1: 0.0001255292536370689, l2: 0.0004351913754362613   Iteration 11 of 100, tot loss = 5.311501502990723, l1: 0.00012001148868214592, l2: 0.0004111386581578038   Iteration 12 of 100, tot loss = 5.208426992098491, l1: 0.00011746833630847202, l2: 0.0004033743607578799   Iteration 13 of 100, tot loss = 5.224434485802283, l1: 0.00011830693004482712, l2: 0.0004041365173179656   Iteration 14 of 100, tot loss = 5.408859423228672, l1: 0.00012198749781029099, l2: 0.00041889844370806325   Iteration 15 of 100, tot loss = 5.367955303192138, l1: 0.0001198084736339903, l2: 0.000416987055602173   Iteration 16 of 100, tot loss = 5.533774346113205, l1: 0.0001225748467277299, l2: 0.00043080258546979167   Iteration 17 of 100, tot loss = 5.434516191482544, l1: 0.00012034158712862442, l2: 0.0004231100302764817   Iteration 18 of 100, tot loss = 5.378075957298279, l1: 0.0001190349485113984, l2: 0.00041877264447975904   Iteration 19 of 100, tot loss = 5.386927617223639, l1: 0.00011815571154951805, l2: 0.00042053704811750273   Iteration 20 of 100, tot loss = 5.281161451339722, l1: 0.00011646654838841642, l2: 0.00041164959548041226   Iteration 21 of 100, tot loss = 5.195752280099051, l1: 0.00011313177258541276, l2: 0.00040644345322756896   Iteration 22 of 100, tot loss = 5.127429593693126, l1: 0.00011117373022981073, l2: 0.00040156922742343426   Iteration 23 of 100, tot loss = 5.025843444077865, l1: 0.00010951263380854431, l2: 0.00039307170917037064   Iteration 24 of 100, tot loss = 4.928145706653595, l1: 0.0001080292589297945, l2: 0.00038478531071935623   Iteration 25 of 100, tot loss = 4.938042144775391, l1: 0.00010851840837858618, l2: 0.0003852858062600717   Iteration 26 of 100, tot loss = 4.857561854215769, l1: 0.00010699046861890775, l2: 0.00037876571653983916   Iteration 27 of 100, tot loss = 4.894226083049068, l1: 0.0001074556647735234, l2: 0.0003819669428695407   Iteration 28 of 100, tot loss = 4.8277342574937006, l1: 0.00010633941227362291, l2: 0.000376434012910717   Iteration 29 of 100, tot loss = 4.829122321359042, l1: 0.00010665225297622061, l2: 0.0003762599778289361   Iteration 30 of 100, tot loss = 4.82916476726532, l1: 0.00010763052002099963, l2: 0.00037528595566982403   Iteration 31 of 100, tot loss = 4.819800676838044, l1: 0.00010697348766224158, l2: 0.00037500657931706236   Iteration 32 of 100, tot loss = 4.778731346130371, l1: 0.00010643509244800953, l2: 0.00037143804138395353   Iteration 33 of 100, tot loss = 4.751691702640418, l1: 0.00010649169019994919, l2: 0.000368677479634767   Iteration 34 of 100, tot loss = 4.753351393867941, l1: 0.00010682268154649886, l2: 0.0003685124577125808   Iteration 35 of 100, tot loss = 4.765034580230713, l1: 0.00010688306252372318, l2: 0.0003696203959407285   Iteration 36 of 100, tot loss = 4.688803950945537, l1: 0.00010571051825536415, l2: 0.00036316987744713615   Iteration 37 of 100, tot loss = 4.763560398204906, l1: 0.00010673392004006214, l2: 0.00036962212116860256   Iteration 38 of 100, tot loss = 4.699324494914005, l1: 0.00010571491140198257, l2: 0.0003642175395629908   Iteration 39 of 100, tot loss = 4.661655908975845, l1: 0.00010476187031972819, l2: 0.00036140372200558585   Iteration 40 of 100, tot loss = 4.687188988924026, l1: 0.00010556192737567471, l2: 0.00036315697361715137   Iteration 41 of 100, tot loss = 4.709121954150316, l1: 0.0001063747769832134, l2: 0.0003645374203986693   Iteration 42 of 100, tot loss = 4.67549326873961, l1: 0.0001056080434950889, l2: 0.0003619412856919336   Iteration 43 of 100, tot loss = 4.661795721497646, l1: 0.0001053161928254223, l2: 0.00036086338141738157   Iteration 44 of 100, tot loss = 4.640560984611511, l1: 0.00010519221806052056, l2: 0.0003588638825352642   Iteration 45 of 100, tot loss = 4.63703088760376, l1: 0.00010523642558837309, l2: 0.00035846666562267476   Iteration 46 of 100, tot loss = 4.618579931881117, l1: 0.00010411396816261518, l2: 0.00035774402773392427   Iteration 47 of 100, tot loss = 4.607549621703777, l1: 0.00010360572521173534, l2: 0.000357149239441261   Iteration 48 of 100, tot loss = 4.631305659810702, l1: 0.00010402625548522337, l2: 0.00035910431324737146   Iteration 49 of 100, tot loss = 4.627853631973267, l1: 0.00010404862524057758, l2: 0.00035873674122350555   Iteration 50 of 100, tot loss = 4.630444760322571, l1: 0.00010453472757944838, l2: 0.00035850975138600915   Iteration 51 of 100, tot loss = 4.621949527777877, l1: 0.00010444673563481546, l2: 0.0003577482198392425   Iteration 52 of 100, tot loss = 4.608329543700585, l1: 0.00010421019760542549, l2: 0.0003566227589124957   Iteration 53 of 100, tot loss = 4.572980367912437, l1: 0.00010355224490785128, l2: 0.00035374579409726037   Iteration 54 of 100, tot loss = 4.573765737039071, l1: 0.00010330229916790914, l2: 0.00035407427667114125   Iteration 55 of 100, tot loss = 4.575276114723899, l1: 0.0001028059225063771, l2: 0.0003547216911101714   Iteration 56 of 100, tot loss = 4.562251261302403, l1: 0.00010254773094077661, l2: 0.0003536773976000924   Iteration 57 of 100, tot loss = 4.554490365480122, l1: 0.00010235760329809987, l2: 0.0003530914354517958   Iteration 58 of 100, tot loss = 4.550762086079039, l1: 0.0001021863307837991, l2: 0.00035288988021483386   Iteration 59 of 100, tot loss = 4.55413117651212, l1: 0.00010242600707234657, l2: 0.00035298711317394057   Iteration 60 of 100, tot loss = 4.576177167892456, l1: 0.00010297974440618419, l2: 0.00035463797466945836   Iteration 61 of 100, tot loss = 4.55818603468723, l1: 0.00010267170537339493, l2: 0.00035314690049730057   Iteration 62 of 100, tot loss = 4.564747860354762, l1: 0.00010308257966948253, l2: 0.0003533922091235347   Iteration 63 of 100, tot loss = 4.555724643525624, l1: 0.00010331313156740858, l2: 0.00035225933520234234   Iteration 64 of 100, tot loss = 4.585050068795681, l1: 0.00010378380113706953, l2: 0.00035472120794111106   Iteration 65 of 100, tot loss = 4.563454749034001, l1: 0.00010344912477124195, l2: 0.00035289635237019797   Iteration 66 of 100, tot loss = 4.540798974759651, l1: 0.00010316662254891443, l2: 0.00035091327718515515   Iteration 67 of 100, tot loss = 4.549932849940969, l1: 0.00010324477354054159, l2: 0.0003517485135095889   Iteration 68 of 100, tot loss = 4.594780318877277, l1: 0.00010400862073070398, l2: 0.00035546941302634557   Iteration 69 of 100, tot loss = 4.594093364218007, l1: 0.00010425376335067837, l2: 0.00035515557459987485   Iteration 70 of 100, tot loss = 4.570712505068098, l1: 0.00010378250954090618, l2: 0.00035328874239764574   Iteration 71 of 100, tot loss = 4.566879185152725, l1: 0.00010341113969408185, l2: 0.00035327678041296523   Iteration 72 of 100, tot loss = 4.551829665899277, l1: 0.0001032325950897454, l2: 0.00035195037303815805   Iteration 73 of 100, tot loss = 4.55051411994516, l1: 0.00010335694291638465, l2: 0.00035169447033844684   Iteration 74 of 100, tot loss = 4.546443774893477, l1: 0.00010309637779042728, l2: 0.00035154800099081586   Iteration 75 of 100, tot loss = 4.550683158238729, l1: 0.00010293502331478522, l2: 0.0003521332936361432   Iteration 76 of 100, tot loss = 4.546777377003117, l1: 0.0001030970118000895, l2: 0.00035158072684058234   Iteration 77 of 100, tot loss = 4.574781389979573, l1: 0.00010339281997604102, l2: 0.00035408532017140417   Iteration 78 of 100, tot loss = 4.6015677482653885, l1: 0.00010406414413666472, l2: 0.0003560926311482222   Iteration 79 of 100, tot loss = 4.61430644687218, l1: 0.00010398875784181954, l2: 0.00035744188780766686   Iteration 80 of 100, tot loss = 4.597524261474609, l1: 0.0001036291579112003, l2: 0.0003561232690117322   Iteration 81 of 100, tot loss = 4.621055037887008, l1: 0.00010379014493980916, l2: 0.0003583153593353927   Iteration 82 of 100, tot loss = 4.658808324395157, l1: 0.00010441280463099957, l2: 0.0003614680282203708   Iteration 83 of 100, tot loss = 4.672600148672081, l1: 0.00010483069859055174, l2: 0.00036242931691301335   Iteration 84 of 100, tot loss = 4.679142724900019, l1: 0.00010497162223882264, l2: 0.00036294265113870746   Iteration 85 of 100, tot loss = 4.666497710171868, l1: 0.000104805661163375, l2: 0.0003618441107819843   Iteration 86 of 100, tot loss = 4.649502745894498, l1: 0.0001047927258315572, l2: 0.0003601575493988538   Iteration 87 of 100, tot loss = 4.621774854331181, l1: 0.00010436758204649642, l2: 0.00035780990391236396   Iteration 88 of 100, tot loss = 4.622838274999098, l1: 0.000104513191796617, l2: 0.00035777063633511057   Iteration 89 of 100, tot loss = 4.644490263435278, l1: 0.00010480654682294371, l2: 0.0003596424804333325   Iteration 90 of 100, tot loss = 4.626798868179321, l1: 0.00010445313770712043, l2: 0.00035822675029824796   Iteration 91 of 100, tot loss = 4.610051438048646, l1: 0.00010422613263222297, l2: 0.0003567790121011497   Iteration 92 of 100, tot loss = 4.617596087248429, l1: 0.00010413182937317406, l2: 0.00035762777991729547   Iteration 93 of 100, tot loss = 4.6236208587564445, l1: 0.00010439510499778134, l2: 0.0003579669816341872   Iteration 94 of 100, tot loss = 4.632727191803303, l1: 0.00010453229836631625, l2: 0.00035874042155325137   Iteration 95 of 100, tot loss = 4.623108876378913, l1: 0.00010435547783823782, l2: 0.00035795541066602855   Iteration 96 of 100, tot loss = 4.631330293913682, l1: 0.00010445959833305096, l2: 0.00035867343179537176   Iteration 97 of 100, tot loss = 4.627186620358339, l1: 0.00010441338798552079, l2: 0.00035830527485813946   Iteration 98 of 100, tot loss = 4.609037625546358, l1: 0.00010415004889720727, l2: 0.0003567537143161254   Iteration 99 of 100, tot loss = 4.622985866334703, l1: 0.00010433875947703861, l2: 0.00035795982782446754   Iteration 100 of 100, tot loss = 4.60814840555191, l1: 0.0001040711441601161, l2: 0.0003567436970479321
   End of epoch 1401; saving model... 

Epoch 1402 of 2000
   Iteration 1 of 100, tot loss = 5.023403167724609, l1: 9.444252646062523e-05, l2: 0.00040789777995087206   Iteration 2 of 100, tot loss = 3.6847212314605713, l1: 6.930323615961242e-05, l2: 0.00029916888888692483   Iteration 3 of 100, tot loss = 3.748356819152832, l1: 6.985578632641894e-05, l2: 0.0003049798882178341   Iteration 4 of 100, tot loss = 3.872628092765808, l1: 7.486422600777587e-05, l2: 0.0003123985770798754   Iteration 5 of 100, tot loss = 4.087965869903565, l1: 8.229462182498536e-05, l2: 0.00032650195935275407   Iteration 6 of 100, tot loss = 4.212592442830403, l1: 9.008748929772992e-05, l2: 0.0003311717470448154   Iteration 7 of 100, tot loss = 3.9807534217834473, l1: 8.464728931098111e-05, l2: 0.0003134280476453049   Iteration 8 of 100, tot loss = 3.9512369632720947, l1: 8.459219588985434e-05, l2: 0.0003105314935964998   Iteration 9 of 100, tot loss = 3.9709972275627985, l1: 8.549383185001918e-05, l2: 0.00031160588130458363   Iteration 10 of 100, tot loss = 4.030305576324463, l1: 8.960333288996481e-05, l2: 0.00031342721777036784   Iteration 11 of 100, tot loss = 4.210610866546631, l1: 9.428571832937781e-05, l2: 0.00032677536100064487   Iteration 12 of 100, tot loss = 4.13616555929184, l1: 9.150320814417985e-05, l2: 0.00032211333988622454   Iteration 13 of 100, tot loss = 4.1532424963437595, l1: 8.965218036722105e-05, l2: 0.00032567206206015096   Iteration 14 of 100, tot loss = 4.362931438854763, l1: 9.189906060263249e-05, l2: 0.0003443940762995875   Iteration 15 of 100, tot loss = 4.288210185368856, l1: 9.284022235078737e-05, l2: 0.0003359807888045907   Iteration 16 of 100, tot loss = 4.267495676875114, l1: 9.410554594069254e-05, l2: 0.00033264401463384274   Iteration 17 of 100, tot loss = 4.139365322449628, l1: 9.238257916877046e-05, l2: 0.00032155394557562163   Iteration 18 of 100, tot loss = 4.1625783575905695, l1: 9.296436837757938e-05, l2: 0.0003232934597892583   Iteration 19 of 100, tot loss = 4.157842372593127, l1: 9.245798566170331e-05, l2: 0.00032332624342464104   Iteration 20 of 100, tot loss = 4.134117782115936, l1: 9.155774205282796e-05, l2: 0.00032185402742470617   Iteration 21 of 100, tot loss = 4.150972627458119, l1: 9.206614742565545e-05, l2: 0.0003230311080447531   Iteration 22 of 100, tot loss = 4.1145887049761685, l1: 9.122388821031728e-05, l2: 0.000320234975581777   Iteration 23 of 100, tot loss = 4.093786457310552, l1: 9.167954356045179e-05, l2: 0.00031769909685902786   Iteration 24 of 100, tot loss = 4.124435017506282, l1: 9.165405269110731e-05, l2: 0.00032078944362486556   Iteration 25 of 100, tot loss = 4.076543121337891, l1: 9.116090979659929e-05, l2: 0.0003164933971129358   Iteration 26 of 100, tot loss = 4.1509309365199165, l1: 9.235247489414178e-05, l2: 0.0003227406123187393   Iteration 27 of 100, tot loss = 4.2436372792279276, l1: 9.3482194643002e-05, l2: 0.0003308815282286593   Iteration 28 of 100, tot loss = 4.255136660167149, l1: 9.348615588221167e-05, l2: 0.00033202750533486584   Iteration 29 of 100, tot loss = 4.257297630967765, l1: 9.387009651681152e-05, l2: 0.00033185966232598854   Iteration 30 of 100, tot loss = 4.273715511957804, l1: 9.449330464121886e-05, l2: 0.00033287824286768836   Iteration 31 of 100, tot loss = 4.297633294136293, l1: 9.517433077675261e-05, l2: 0.0003345889948128212   Iteration 32 of 100, tot loss = 4.313916578888893, l1: 9.627612257645524e-05, l2: 0.000335115531925112   Iteration 33 of 100, tot loss = 4.245579726768263, l1: 9.44983135427306e-05, l2: 0.00033005965594907826   Iteration 34 of 100, tot loss = 4.214503701995401, l1: 9.349197406503234e-05, l2: 0.00032795839326675325   Iteration 35 of 100, tot loss = 4.180099201202393, l1: 9.283670213855138e-05, l2: 0.00032517321524210273   Iteration 36 of 100, tot loss = 4.186183982425266, l1: 9.258344258948152e-05, l2: 0.00032603495330679126   Iteration 37 of 100, tot loss = 4.202492817028149, l1: 9.30093387118226e-05, l2: 0.0003272399410127184   Iteration 38 of 100, tot loss = 4.195056796073914, l1: 9.244800754746201e-05, l2: 0.0003270576700359877   Iteration 39 of 100, tot loss = 4.238173918846326, l1: 9.299263024914604e-05, l2: 0.0003308247601708923   Iteration 40 of 100, tot loss = 4.345542114973068, l1: 9.40073768106231e-05, l2: 0.0003405468334676698   Iteration 41 of 100, tot loss = 4.338164916852626, l1: 9.388260203683407e-05, l2: 0.0003399338875613289   Iteration 42 of 100, tot loss = 4.331852396329244, l1: 9.406598829151509e-05, l2: 0.00033911924886827666   Iteration 43 of 100, tot loss = 4.3073862652445944, l1: 9.37097487791149e-05, l2: 0.00033702887527016533   Iteration 44 of 100, tot loss = 4.3119179660623725, l1: 9.393188611284131e-05, l2: 0.0003372599081062204   Iteration 45 of 100, tot loss = 4.351637808481852, l1: 9.47177826472196e-05, l2: 0.000340445995486031   Iteration 46 of 100, tot loss = 4.407469500666079, l1: 9.574062966313922e-05, l2: 0.000345006316859761   Iteration 47 of 100, tot loss = 4.412039239355859, l1: 9.599985191635689e-05, l2: 0.0003452040688531037   Iteration 48 of 100, tot loss = 4.377202053864797, l1: 9.564794004290889e-05, l2: 0.00034207226205277647   Iteration 49 of 100, tot loss = 4.376665601924974, l1: 9.612799825548309e-05, l2: 0.0003415385587259709   Iteration 50 of 100, tot loss = 4.341062183380127, l1: 9.557396777381654e-05, l2: 0.00033853224740596486   Iteration 51 of 100, tot loss = 4.373579586253447, l1: 9.595897805126037e-05, l2: 0.00034139897730247134   Iteration 52 of 100, tot loss = 4.394810456496018, l1: 9.608901179420242e-05, l2: 0.00034339203081729537   Iteration 53 of 100, tot loss = 4.408023870216225, l1: 9.688504806550589e-05, l2: 0.00034391733665098346   Iteration 54 of 100, tot loss = 4.3996950034742, l1: 9.668513121141586e-05, l2: 0.0003432843667623173   Iteration 55 of 100, tot loss = 4.381894826889038, l1: 9.652549957867118e-05, l2: 0.00034166398082478817   Iteration 56 of 100, tot loss = 4.384075390441077, l1: 9.629820845345551e-05, l2: 0.000342109328162873   Iteration 57 of 100, tot loss = 4.362223470420168, l1: 9.545683560805118e-05, l2: 0.00034076550933573264   Iteration 58 of 100, tot loss = 4.381537844394815, l1: 9.587650448997135e-05, l2: 0.00034227727782904524   Iteration 59 of 100, tot loss = 4.450447417921939, l1: 9.708686738415084e-05, l2: 0.00034795787310724983   Iteration 60 of 100, tot loss = 4.454196234544118, l1: 9.716059218286924e-05, l2: 0.00034825903009429263   Iteration 61 of 100, tot loss = 4.454029947030739, l1: 9.716954040503297e-05, l2: 0.0003482334525228982   Iteration 62 of 100, tot loss = 4.484403837111689, l1: 9.737144600583105e-05, l2: 0.00035106893624141513   Iteration 63 of 100, tot loss = 4.499860123982505, l1: 9.785151049736813e-05, l2: 0.0003521345000961677   Iteration 64 of 100, tot loss = 4.48566260933876, l1: 9.752266379337016e-05, l2: 0.0003510435951739055   Iteration 65 of 100, tot loss = 4.4692388864663934, l1: 9.744577361673762e-05, l2: 0.00034947811307994506   Iteration 66 of 100, tot loss = 4.431543899304939, l1: 9.690850818129327e-05, l2: 0.0003462458797995086   Iteration 67 of 100, tot loss = 4.432145161415214, l1: 9.725971475161904e-05, l2: 0.0003459547994286517   Iteration 68 of 100, tot loss = 4.435616135597229, l1: 9.72199629412738e-05, l2: 0.000346341648229795   Iteration 69 of 100, tot loss = 4.427853739779929, l1: 9.686442348723084e-05, l2: 0.0003459209479658149   Iteration 70 of 100, tot loss = 4.46170118536268, l1: 9.765188905086169e-05, l2: 0.00034851822731850134   Iteration 71 of 100, tot loss = 4.445773809728488, l1: 9.713726134398545e-05, l2: 0.000347440117276536   Iteration 72 of 100, tot loss = 4.476647714773814, l1: 9.782933127806043e-05, l2: 0.000349835437898744   Iteration 73 of 100, tot loss = 4.530907428427918, l1: 9.869981140706828e-05, l2: 0.00035439092886780884   Iteration 74 of 100, tot loss = 4.540558073971723, l1: 9.876317833274968e-05, l2: 0.00035529262665501873   Iteration 75 of 100, tot loss = 4.553239765167237, l1: 9.921237811795436e-05, l2: 0.0003561115961444254   Iteration 76 of 100, tot loss = 4.569734692573547, l1: 9.877382203022104e-05, l2: 0.00035819964537758527   Iteration 77 of 100, tot loss = 4.582862730150099, l1: 9.911728080009812e-05, l2: 0.00035916899036851944   Iteration 78 of 100, tot loss = 4.554087990369553, l1: 9.85829261387549e-05, l2: 0.0003568258710005559   Iteration 79 of 100, tot loss = 4.603159647953661, l1: 9.961012260703947e-05, l2: 0.00036070584057965846   Iteration 80 of 100, tot loss = 4.5797532349824905, l1: 9.959032227015995e-05, l2: 0.0003583849995266064   Iteration 81 of 100, tot loss = 4.5827282946786765, l1: 9.95549261135216e-05, l2: 0.00035871790178022405   Iteration 82 of 100, tot loss = 4.579640466992448, l1: 9.932387516395155e-05, l2: 0.0003586401703281941   Iteration 83 of 100, tot loss = 4.558068091610828, l1: 9.89405312158128e-05, l2: 0.00035686627681176346   Iteration 84 of 100, tot loss = 4.561818559964498, l1: 9.912974092869609e-05, l2: 0.0003570521138109533   Iteration 85 of 100, tot loss = 4.561785669887767, l1: 9.936047225731753e-05, l2: 0.00035681809360325775   Iteration 86 of 100, tot loss = 4.550914404004119, l1: 9.941206494553372e-05, l2: 0.0003556793743100147   Iteration 87 of 100, tot loss = 4.557924698139059, l1: 9.949374550066343e-05, l2: 0.0003562987234775277   Iteration 88 of 100, tot loss = 4.554807467894121, l1: 9.956357267914096e-05, l2: 0.0003559171729483007   Iteration 89 of 100, tot loss = 4.552897710478708, l1: 9.975340026868978e-05, l2: 0.0003555363696199329   Iteration 90 of 100, tot loss = 4.55556485388014, l1: 9.997725852978571e-05, l2: 0.0003555792257733022   Iteration 91 of 100, tot loss = 4.552514463990599, l1: 9.996923228634961e-05, l2: 0.00035528221345445203   Iteration 92 of 100, tot loss = 4.532903826755026, l1: 9.967305354423488e-05, l2: 0.00035361732838994254   Iteration 93 of 100, tot loss = 4.523694666483069, l1: 9.93171730657704e-05, l2: 0.0003530522928916178   Iteration 94 of 100, tot loss = 4.5103365614059125, l1: 9.888627682038475e-05, l2: 0.0003521473785092004   Iteration 95 of 100, tot loss = 4.544688214753803, l1: 9.944928410971586e-05, l2: 0.0003550195369538606   Iteration 96 of 100, tot loss = 4.549142122268677, l1: 9.933565377195919e-05, l2: 0.00035557855774944375   Iteration 97 of 100, tot loss = 4.5531539572882895, l1: 9.956468566838105e-05, l2: 0.0003557507095428798   Iteration 98 of 100, tot loss = 4.563552369876784, l1: 9.962683042412748e-05, l2: 0.0003567284060112315   Iteration 99 of 100, tot loss = 4.551690465272075, l1: 9.962109746111585e-05, l2: 0.00035554794856285994   Iteration 100 of 100, tot loss = 4.545345175266266, l1: 9.92581221726141e-05, l2: 0.0003552763948391657
   End of epoch 1402; saving model... 

Epoch 1403 of 2000
   Iteration 1 of 100, tot loss = 7.805638790130615, l1: 0.00014790984278079122, l2: 0.0006326540024019778   Iteration 2 of 100, tot loss = 6.65477991104126, l1: 0.000135825110191945, l2: 0.0005296528397593647   Iteration 3 of 100, tot loss = 5.538161118825276, l1: 0.00011542509795011331, l2: 0.00043839098846850294   Iteration 4 of 100, tot loss = 5.133610546588898, l1: 0.00010722563638410065, l2: 0.00040613539749756455   Iteration 5 of 100, tot loss = 5.286771440505982, l1: 0.00010605851566651836, l2: 0.00042261860799044373   Iteration 6 of 100, tot loss = 5.587783535321553, l1: 0.00011292808267171495, l2: 0.0004458502517081797   Iteration 7 of 100, tot loss = 5.519453695842198, l1: 0.00010644752806651272, l2: 0.000445497825109799   Iteration 8 of 100, tot loss = 5.580255299806595, l1: 0.00010864913019759115, l2: 0.0004493763808568474   Iteration 9 of 100, tot loss = 5.641379912694295, l1: 0.00011039423568743384, l2: 0.0004537437376307531   Iteration 10 of 100, tot loss = 5.421731162071228, l1: 0.00010709433990996331, l2: 0.000435078761074692   Iteration 11 of 100, tot loss = 5.224903410131281, l1: 0.00010467820009745827, l2: 0.0004178121267944913   Iteration 12 of 100, tot loss = 5.3164723714192705, l1: 0.00010473153209507775, l2: 0.0004269156900894207   Iteration 13 of 100, tot loss = 5.515007825998159, l1: 0.00010758415961530633, l2: 0.00044391660888392764   Iteration 14 of 100, tot loss = 5.370056544031415, l1: 0.0001053663030948623, l2: 0.0004316393377458943   Iteration 15 of 100, tot loss = 5.302507638931274, l1: 0.00010471549176145345, l2: 0.0004255352580609421   Iteration 16 of 100, tot loss = 5.168891593813896, l1: 0.0001037362490023952, l2: 0.00041315289763588225   Iteration 17 of 100, tot loss = 5.162087033776676, l1: 0.0001046798429075245, l2: 0.00041152884831022034   Iteration 18 of 100, tot loss = 5.154173440403408, l1: 0.00010527740717710306, l2: 0.0004101399269226628   Iteration 19 of 100, tot loss = 5.103113513243826, l1: 0.00010566174942620197, l2: 0.00040464959257453874   Iteration 20 of 100, tot loss = 5.069029867649078, l1: 0.00010635398139129393, l2: 0.00040054899800452405   Iteration 21 of 100, tot loss = 4.945801189967564, l1: 0.00010345898755033323, l2: 0.00039112112357369845   Iteration 22 of 100, tot loss = 4.858032746748491, l1: 0.00010218425466684329, l2: 0.0003836190120306459   Iteration 23 of 100, tot loss = 4.834027290344238, l1: 0.00010227650241374604, l2: 0.0003811262184804629   Iteration 24 of 100, tot loss = 4.788813054561615, l1: 0.00010022562037192984, l2: 0.00037865567780196824   Iteration 25 of 100, tot loss = 4.971238574981689, l1: 0.00010324734816094861, l2: 0.00039387650380376724   Iteration 26 of 100, tot loss = 4.943670016068679, l1: 0.00010283369024714026, l2: 0.0003915333067058012   Iteration 27 of 100, tot loss = 4.971115889372649, l1: 0.00010315103847662815, l2: 0.00039396054678406844   Iteration 28 of 100, tot loss = 4.903982043266296, l1: 0.00010149556913217696, l2: 0.0003889026312598227   Iteration 29 of 100, tot loss = 4.8067831910889725, l1: 9.990335795386084e-05, l2: 0.00038077495741288596   Iteration 30 of 100, tot loss = 4.7504772265752155, l1: 9.988814587510811e-05, l2: 0.0003751595737412572   Iteration 31 of 100, tot loss = 4.771138921860726, l1: 0.00010004616882314065, l2: 0.00037706772150892403   Iteration 32 of 100, tot loss = 4.7449586763978004, l1: 9.89314951311826e-05, l2: 0.00037556437109742546   Iteration 33 of 100, tot loss = 4.6869346156264795, l1: 9.785414383231634e-05, l2: 0.00037083931701173157   Iteration 34 of 100, tot loss = 4.629412321483388, l1: 9.677652611999827e-05, l2: 0.000366164704781327   Iteration 35 of 100, tot loss = 4.6411549091339115, l1: 9.728597859585924e-05, l2: 0.00036682950989675844   Iteration 36 of 100, tot loss = 4.626182973384857, l1: 9.748521673221451e-05, l2: 0.00036513307835169445   Iteration 37 of 100, tot loss = 4.60559952581251, l1: 9.717960347188637e-05, l2: 0.00036338034732506385   Iteration 38 of 100, tot loss = 4.600618832989743, l1: 9.743013594599783e-05, l2: 0.0003626317465157052   Iteration 39 of 100, tot loss = 4.634289014033782, l1: 9.794424747666105e-05, l2: 0.0003654846525303303   Iteration 40 of 100, tot loss = 4.6434626400470735, l1: 9.782408942555776e-05, l2: 0.00036652217422670217   Iteration 41 of 100, tot loss = 4.614911282934794, l1: 9.704215459883349e-05, l2: 0.0003644489735029865   Iteration 42 of 100, tot loss = 4.635403275489807, l1: 9.739248811716347e-05, l2: 0.0003661478393561473   Iteration 43 of 100, tot loss = 4.619703071061955, l1: 9.768040374267933e-05, l2: 0.00036428990296012356   Iteration 44 of 100, tot loss = 4.621046337214383, l1: 9.800695492727259e-05, l2: 0.0003640976782745301   Iteration 45 of 100, tot loss = 4.6706074290805395, l1: 9.845977991871122e-05, l2: 0.0003686009624895329   Iteration 46 of 100, tot loss = 4.66424244383107, l1: 9.853802001326467e-05, l2: 0.00036788622326085993   Iteration 47 of 100, tot loss = 4.662643077525686, l1: 9.92082790075306e-05, l2: 0.0003670560275922787   Iteration 48 of 100, tot loss = 4.6599450806776685, l1: 9.939401282584488e-05, l2: 0.0003666004937864879   Iteration 49 of 100, tot loss = 4.6463508995211855, l1: 9.938026080501968e-05, l2: 0.0003652548276681491   Iteration 50 of 100, tot loss = 4.607634444236755, l1: 9.885581777780317e-05, l2: 0.00036190762504702435   Iteration 51 of 100, tot loss = 4.570227594936595, l1: 9.8404363562426e-05, l2: 0.00035861839441007334   Iteration 52 of 100, tot loss = 4.552980459653414, l1: 9.835325517297651e-05, l2: 0.0003569447897182097   Iteration 53 of 100, tot loss = 4.550100119608753, l1: 9.829356164324431e-05, l2: 0.0003567164489792941   Iteration 54 of 100, tot loss = 4.565092272228664, l1: 9.892228034139541e-05, l2: 0.00035758694624140236   Iteration 55 of 100, tot loss = 4.537184736945412, l1: 9.812788656828079e-05, l2: 0.0003555905864976177   Iteration 56 of 100, tot loss = 4.531494553600039, l1: 9.80217008483513e-05, l2: 0.0003551277537293832   Iteration 57 of 100, tot loss = 4.511302772321199, l1: 9.767912004526966e-05, l2: 0.00035345115624327343   Iteration 58 of 100, tot loss = 4.484733770633566, l1: 9.683324869597684e-05, l2: 0.0003516401271114993   Iteration 59 of 100, tot loss = 4.471231351464482, l1: 9.659915014122755e-05, l2: 0.00035052398348945366   Iteration 60 of 100, tot loss = 4.479071303208669, l1: 9.694391907639025e-05, l2: 0.0003509632100758608   Iteration 61 of 100, tot loss = 4.509090857427628, l1: 9.756680631907047e-05, l2: 0.0003533422783280524   Iteration 62 of 100, tot loss = 4.506237095402133, l1: 9.752800929897105e-05, l2: 0.0003530956992988415   Iteration 63 of 100, tot loss = 4.522879922200763, l1: 9.788662074632677e-05, l2: 0.00035440136973250894   Iteration 64 of 100, tot loss = 4.547907080501318, l1: 9.816199883516674e-05, l2: 0.00035662870755004406   Iteration 65 of 100, tot loss = 4.636093810888437, l1: 9.95438800041134e-05, l2: 0.00036406549871361885   Iteration 66 of 100, tot loss = 4.614574118094011, l1: 9.932921448401457e-05, l2: 0.00036212819483780277   Iteration 67 of 100, tot loss = 4.6354325458184995, l1: 0.00010013242989323282, l2: 0.0003634108219177587   Iteration 68 of 100, tot loss = 4.639653728288763, l1: 0.00010012231937520589, l2: 0.0003638430502820376   Iteration 69 of 100, tot loss = 4.63869459387185, l1: 9.999643362538916e-05, l2: 0.0003638730225814641   Iteration 70 of 100, tot loss = 4.635690672057016, l1: 9.986702685377427e-05, l2: 0.0003637020372220182   Iteration 71 of 100, tot loss = 4.652544058544535, l1: 0.00010011930176070039, l2: 0.00036513510128994745   Iteration 72 of 100, tot loss = 4.669143978092405, l1: 0.00010037330119100968, l2: 0.0003665410937780204   Iteration 73 of 100, tot loss = 4.642018056895635, l1: 9.988323818853967e-05, l2: 0.0003643185647813705   Iteration 74 of 100, tot loss = 4.666961644146894, l1: 0.00010024357156047749, l2: 0.0003664525895539948   Iteration 75 of 100, tot loss = 4.677643775939941, l1: 0.00010024157013200845, l2: 0.00036752280371729287   Iteration 76 of 100, tot loss = 4.676607226070605, l1: 0.00010015001824189982, l2: 0.0003675107005907002   Iteration 77 of 100, tot loss = 4.6875952002290004, l1: 0.00010055443697212893, l2: 0.0003682050788578541   Iteration 78 of 100, tot loss = 4.722115932366787, l1: 0.0001012422210642865, l2: 0.0003709693680717371   Iteration 79 of 100, tot loss = 4.721600097945974, l1: 0.00010131586582880302, l2: 0.0003708441396571012   Iteration 80 of 100, tot loss = 4.700213286280632, l1: 0.00010101812304128544, l2: 0.00036900320119457317   Iteration 81 of 100, tot loss = 4.682488688716182, l1: 0.00010083838001994508, l2: 0.00036741048456917025   Iteration 82 of 100, tot loss = 4.660544378001515, l1: 0.00010052312604895589, l2: 0.0003655313073356477   Iteration 83 of 100, tot loss = 4.682406557611673, l1: 0.00010076729331448877, l2: 0.000367473358223888   Iteration 84 of 100, tot loss = 4.6752155394781205, l1: 0.00010035079009997259, l2: 0.0003671707593076419   Iteration 85 of 100, tot loss = 4.671798616297105, l1: 0.00010045991229537108, l2: 0.00036671994466279796   Iteration 86 of 100, tot loss = 4.662215887114059, l1: 0.00010021642819358317, l2: 0.000366005155821006   Iteration 87 of 100, tot loss = 4.663508289161769, l1: 0.00010022496556710645, l2: 0.00036612585864040527   Iteration 88 of 100, tot loss = 4.654894882982427, l1: 0.00010019448927778285, l2: 0.0003652949943154288   Iteration 89 of 100, tot loss = 4.646452145630054, l1: 0.00010029021490318177, l2: 0.00036435499506102604   Iteration 90 of 100, tot loss = 4.629347032970852, l1: 0.00010029053155449219, l2: 0.00036264416719657475   Iteration 91 of 100, tot loss = 4.620320639767489, l1: 0.00010027424390760384, l2: 0.00036175781557116414   Iteration 92 of 100, tot loss = 4.613716571227364, l1: 0.00010016949873597301, l2: 0.0003612021538846539   Iteration 93 of 100, tot loss = 4.596923100051059, l1: 9.97360283524544e-05, l2: 0.0003599562772652335   Iteration 94 of 100, tot loss = 4.608955905792561, l1: 9.98497290949788e-05, l2: 0.0003610458570232793   Iteration 95 of 100, tot loss = 4.616616891559802, l1: 9.99928967901628e-05, l2: 0.0003616687880007942   Iteration 96 of 100, tot loss = 4.627765009800593, l1: 0.00010022631685539334, l2: 0.00036255017994335503   Iteration 97 of 100, tot loss = 4.627166497338679, l1: 0.00010044026863191171, l2: 0.0003622763767087179   Iteration 98 of 100, tot loss = 4.610806336208266, l1: 9.993514678932783e-05, l2: 0.00036114548248053545   Iteration 99 of 100, tot loss = 4.617432457028014, l1: 0.00010008194107230696, l2: 0.0003616612998714595   Iteration 100 of 100, tot loss = 4.62122481584549, l1: 0.00010023994283983484, l2: 0.00036188253390719184
   End of epoch 1403; saving model... 

Epoch 1404 of 2000
   Iteration 1 of 100, tot loss = 4.499704360961914, l1: 9.75110669969581e-05, l2: 0.0003524593776091933   Iteration 2 of 100, tot loss = 3.8254590034484863, l1: 8.380439976463094e-05, l2: 0.0002987415064126253   Iteration 3 of 100, tot loss = 3.5218403339385986, l1: 8.028689868903409e-05, l2: 0.0002718971421321233   Iteration 4 of 100, tot loss = 4.151191174983978, l1: 8.941732266976032e-05, l2: 0.0003257018106523901   Iteration 5 of 100, tot loss = 4.090966463088989, l1: 8.500281401211396e-05, l2: 0.00032409384730271997   Iteration 6 of 100, tot loss = 3.7285638054211936, l1: 7.799499690008815e-05, l2: 0.0002948613958627296   Iteration 7 of 100, tot loss = 3.5448304074151173, l1: 7.698621889825777e-05, l2: 0.0002774968348343724   Iteration 8 of 100, tot loss = 3.5666512101888657, l1: 7.735256758678588e-05, l2: 0.0002793125659081852   Iteration 9 of 100, tot loss = 3.665339880519443, l1: 8.124595761829469e-05, l2: 0.00028528803927151277   Iteration 10 of 100, tot loss = 3.847940003871918, l1: 8.576042055210564e-05, l2: 0.0002990335851791315   Iteration 11 of 100, tot loss = 3.9636161002245816, l1: 8.994361699230716e-05, l2: 0.000306418000201864   Iteration 12 of 100, tot loss = 4.1586094200611115, l1: 9.279941059503471e-05, l2: 0.00032306153419388767   Iteration 13 of 100, tot loss = 4.121701433108403, l1: 9.252589454543061e-05, l2: 0.0003196442524383131   Iteration 14 of 100, tot loss = 4.095166487353189, l1: 9.150163818308752e-05, l2: 0.0003180150140126768   Iteration 15 of 100, tot loss = 4.303031547864278, l1: 9.461871304665692e-05, l2: 0.0003356844448717311   Iteration 16 of 100, tot loss = 4.293860591948032, l1: 9.43915804327844e-05, l2: 0.00033499448272777954   Iteration 17 of 100, tot loss = 4.384372185258305, l1: 9.683027511688552e-05, l2: 0.0003416069487096084   Iteration 18 of 100, tot loss = 4.481425768799252, l1: 9.765608269339686e-05, l2: 0.00035048649927678827   Iteration 19 of 100, tot loss = 4.462160543391579, l1: 9.675212085743337e-05, l2: 0.00034946393861901015   Iteration 20 of 100, tot loss = 4.558615309000015, l1: 9.822710217122221e-05, l2: 0.00035763443520409055   Iteration 21 of 100, tot loss = 4.512372783252171, l1: 9.685011329046185e-05, l2: 0.0003543871716829017   Iteration 22 of 100, tot loss = 4.564308551224795, l1: 9.857264393014537e-05, l2: 0.0003578582181944512   Iteration 23 of 100, tot loss = 4.584906966789909, l1: 9.789595835330977e-05, l2: 0.0003605947453963935   Iteration 24 of 100, tot loss = 4.572130277752876, l1: 9.750059249806024e-05, l2: 0.0003597124432417331   Iteration 25 of 100, tot loss = 4.5257191514968875, l1: 9.639540265197866e-05, l2: 0.0003561765205813572   Iteration 26 of 100, tot loss = 4.588699162006378, l1: 9.717265339321994e-05, l2: 0.0003616972708886561   Iteration 27 of 100, tot loss = 4.546310561674613, l1: 9.61244595937724e-05, l2: 0.00035850660437804265   Iteration 28 of 100, tot loss = 4.587021516902106, l1: 9.677333725578916e-05, l2: 0.00036192882284272594   Iteration 29 of 100, tot loss = 4.742838954103404, l1: 9.875822597050814e-05, l2: 0.0003755256771471287   Iteration 30 of 100, tot loss = 4.755053675174713, l1: 9.947800851174785e-05, l2: 0.0003760273660494325   Iteration 31 of 100, tot loss = 4.841653066296732, l1: 0.0001005564276232279, l2: 0.0003836088857966505   Iteration 32 of 100, tot loss = 4.842122998088598, l1: 0.00010056068879293889, l2: 0.00038365161753972643   Iteration 33 of 100, tot loss = 4.834464705351627, l1: 0.00010018717027075279, l2: 0.0003832593065453693   Iteration 34 of 100, tot loss = 4.870452498688417, l1: 0.00010135018248333568, l2: 0.0003856950724559069   Iteration 35 of 100, tot loss = 4.817837085042681, l1: 0.00010075481477542781, l2: 0.000381028898742183   Iteration 36 of 100, tot loss = 4.801350086927414, l1: 0.00010041753325317081, l2: 0.0003797174799223689   Iteration 37 of 100, tot loss = 4.8224350020692155, l1: 0.00010131526279429956, l2: 0.00038092824232424736   Iteration 38 of 100, tot loss = 4.8364122008022505, l1: 0.00010119602714104603, l2: 0.0003824451984990829   Iteration 39 of 100, tot loss = 4.818619150381822, l1: 0.00010063624591939748, l2: 0.0003812256748997009   Iteration 40 of 100, tot loss = 4.803994986414909, l1: 0.00010048455797004863, l2: 0.00037991494573361704   Iteration 41 of 100, tot loss = 4.789441222097816, l1: 0.00010062805733646312, l2: 0.00037831606992191024   Iteration 42 of 100, tot loss = 4.739240291572752, l1: 9.971365034289192e-05, l2: 0.00037421038377076564   Iteration 43 of 100, tot loss = 4.780007270879524, l1: 0.00010063711594083607, l2: 0.0003773636167035113   Iteration 44 of 100, tot loss = 4.759194078770551, l1: 9.989663927130592e-05, l2: 0.0003760227740382437   Iteration 45 of 100, tot loss = 4.808695504400466, l1: 0.00010072219536393984, l2: 0.00038014735974785353   Iteration 46 of 100, tot loss = 4.793798516625944, l1: 0.00010082563415420262, l2: 0.0003785542218753582   Iteration 47 of 100, tot loss = 4.7830770852717945, l1: 0.00010096533491803789, l2: 0.00037734237799242617   Iteration 48 of 100, tot loss = 4.849650618930657, l1: 0.00010182240453104896, l2: 0.0003831426614245477   Iteration 49 of 100, tot loss = 4.841764194624765, l1: 0.00010171915932822193, l2: 0.00038245726407657625   Iteration 50 of 100, tot loss = 4.9104926085472105, l1: 0.00010272662875649985, l2: 0.0003883226361358538   Iteration 51 of 100, tot loss = 4.852914634872885, l1: 0.00010158440950452168, l2: 0.000383707057849011   Iteration 52 of 100, tot loss = 4.8747615195237675, l1: 0.000102315686210484, l2: 0.0003851604689351426   Iteration 53 of 100, tot loss = 4.8619415467640135, l1: 0.00010193563056813184, l2: 0.00038425852698361815   Iteration 54 of 100, tot loss = 4.848435677863933, l1: 0.00010174907657966725, l2: 0.00038309449370933215   Iteration 55 of 100, tot loss = 4.836128098314458, l1: 0.00010127422237102027, l2: 0.0003823385902003131   Iteration 56 of 100, tot loss = 4.791668144719941, l1: 0.00010070718592812357, l2: 0.00037845963093526995   Iteration 57 of 100, tot loss = 4.738826030179074, l1: 9.960044483529616e-05, l2: 0.00037428216060027156   Iteration 58 of 100, tot loss = 4.785259902477264, l1: 0.00010031010574964276, l2: 0.0003782158869555509   Iteration 59 of 100, tot loss = 4.7497624724598255, l1: 9.991780363202837e-05, l2: 0.0003750584459709698   Iteration 60 of 100, tot loss = 4.74507769147555, l1: 0.00010029258137365105, l2: 0.00037421518961006464   Iteration 61 of 100, tot loss = 4.765083955936745, l1: 0.00010081818326114921, l2: 0.0003756902137194898   Iteration 62 of 100, tot loss = 4.759751937081737, l1: 0.00010078544689731057, l2: 0.00037518974779466647   Iteration 63 of 100, tot loss = 4.744055564441378, l1: 0.0001008861067280772, l2: 0.00037351945098230825   Iteration 64 of 100, tot loss = 4.731403479352593, l1: 0.00010092052633581261, l2: 0.0003722198227933404   Iteration 65 of 100, tot loss = 4.748053398499122, l1: 0.00010111165393027477, l2: 0.0003736936870084789   Iteration 66 of 100, tot loss = 4.733774140025631, l1: 0.00010080015244690885, l2: 0.00037257726261018297   Iteration 67 of 100, tot loss = 4.739325300971074, l1: 0.00010096089230935479, l2: 0.000372971638510657   Iteration 68 of 100, tot loss = 4.73874042840565, l1: 0.00010131386063542723, l2: 0.00037256018314270903   Iteration 69 of 100, tot loss = 4.752460747525312, l1: 0.00010143987242043005, l2: 0.000373806203572093   Iteration 70 of 100, tot loss = 4.73815609557288, l1: 0.00010133563830874794, l2: 0.00037247997222168906   Iteration 71 of 100, tot loss = 4.71671311788156, l1: 0.00010129079114257121, l2: 0.00037038052154534404   Iteration 72 of 100, tot loss = 4.727661508652899, l1: 0.00010155052389867099, l2: 0.0003712156277389214   Iteration 73 of 100, tot loss = 4.730111198882534, l1: 0.00010163619752920691, l2: 0.00037137492303098617   Iteration 74 of 100, tot loss = 4.751049916486482, l1: 0.00010183743570696852, l2: 0.0003732675564557163   Iteration 75 of 100, tot loss = 4.748214058876037, l1: 0.00010192137478346315, l2: 0.0003729000318950663   Iteration 76 of 100, tot loss = 4.743645446865182, l1: 0.00010204875956045918, l2: 0.0003723157860804349   Iteration 77 of 100, tot loss = 4.744411479343068, l1: 0.0001018509556869704, l2: 0.00037259019324676945   Iteration 78 of 100, tot loss = 4.739008260078919, l1: 0.00010191827548111085, l2: 0.0003719825516992177   Iteration 79 of 100, tot loss = 4.732471325729467, l1: 0.00010140613076030984, l2: 0.00037184100276849505   Iteration 80 of 100, tot loss = 4.717115677893162, l1: 0.00010127410018867522, l2: 0.000370437468518503   Iteration 81 of 100, tot loss = 4.712339020069734, l1: 0.00010132689109283362, l2: 0.0003699070118643619   Iteration 82 of 100, tot loss = 4.694294349449437, l1: 0.00010117989102848408, l2: 0.0003682495447630971   Iteration 83 of 100, tot loss = 4.689478783722383, l1: 0.00010120776861152858, l2: 0.00036774011060920916   Iteration 84 of 100, tot loss = 4.67637512087822, l1: 0.00010084823645142716, l2: 0.0003667892766922402   Iteration 85 of 100, tot loss = 4.685747880094191, l1: 0.00010100952011485146, l2: 0.00036756526884239386   Iteration 86 of 100, tot loss = 4.686880698037702, l1: 0.00010116973520103615, l2: 0.0003675183357107778   Iteration 87 of 100, tot loss = 4.711093703905742, l1: 0.0001016914313535297, l2: 0.0003694179400951527   Iteration 88 of 100, tot loss = 4.70719566534866, l1: 0.00010149852199571218, l2: 0.0003692210458129094   Iteration 89 of 100, tot loss = 4.6875182470578824, l1: 0.0001010655219391318, l2: 0.00036768630402701585   Iteration 90 of 100, tot loss = 4.681436860561371, l1: 0.00010094052708558997, l2: 0.00036720316000153413   Iteration 91 of 100, tot loss = 4.677257908569588, l1: 0.00010097996850023404, l2: 0.000366745823207516   Iteration 92 of 100, tot loss = 4.693282773961192, l1: 0.00010144212015090844, l2: 0.0003678861582517604   Iteration 93 of 100, tot loss = 4.674811244010925, l1: 0.00010109505385644884, l2: 0.00036638607152096765   Iteration 94 of 100, tot loss = 4.681949959156361, l1: 0.00010095026691294364, l2: 0.0003672447300174254   Iteration 95 of 100, tot loss = 4.672327748097872, l1: 0.00010079316481623161, l2: 0.0003664396108912402   Iteration 96 of 100, tot loss = 4.678093521545331, l1: 0.00010085182426185686, l2: 0.0003669575286645947   Iteration 97 of 100, tot loss = 4.658879401757545, l1: 0.00010031155319122319, l2: 0.0003655763875846703   Iteration 98 of 100, tot loss = 4.654222642888828, l1: 0.00010029289385746945, l2: 0.00036512937115229743   Iteration 99 of 100, tot loss = 4.655951965938915, l1: 0.00010052178987105509, l2: 0.0003650734075251026   Iteration 100 of 100, tot loss = 4.635732804536819, l1: 0.00010019202080002287, l2: 0.00036338126054033634
   End of epoch 1404; saving model... 

Epoch 1405 of 2000
   Iteration 1 of 100, tot loss = 5.281562805175781, l1: 0.0001322991884080693, l2: 0.00039585711783729494   Iteration 2 of 100, tot loss = 5.342204570770264, l1: 0.00011337490286678076, l2: 0.00042084555025212467   Iteration 3 of 100, tot loss = 5.2763363520304365, l1: 0.00010868544389571373, l2: 0.000418948174531882   Iteration 4 of 100, tot loss = 5.233471512794495, l1: 0.00010718350677052513, l2: 0.00041616363887442276   Iteration 5 of 100, tot loss = 5.197441673278808, l1: 0.00010770345979835839, l2: 0.00041204070439562204   Iteration 6 of 100, tot loss = 5.244874954223633, l1: 0.00010856884788760605, l2: 0.00041591864040431875   Iteration 7 of 100, tot loss = 5.027077334267752, l1: 0.00010617179317965306, l2: 0.0003965359355788678   Iteration 8 of 100, tot loss = 4.9829670786857605, l1: 0.0001045041444740491, l2: 0.0003937925575883128   Iteration 9 of 100, tot loss = 4.878565788269043, l1: 0.00010454549353906057, l2: 0.0003833110806428724   Iteration 10 of 100, tot loss = 4.973452758789063, l1: 0.0001078990324458573, l2: 0.00038944623956922443   Iteration 11 of 100, tot loss = 4.858816862106323, l1: 0.00010420204820217226, l2: 0.00038167963397096503   Iteration 12 of 100, tot loss = 4.979343791802724, l1: 0.00010587229371594731, l2: 0.00039206208020914346   Iteration 13 of 100, tot loss = 4.81317479793842, l1: 0.0001029151014965744, l2: 0.0003784023731266363   Iteration 14 of 100, tot loss = 4.66084349155426, l1: 0.00010200920015839594, l2: 0.00036407514354712997   Iteration 15 of 100, tot loss = 4.67981546719869, l1: 0.0001024566493773212, l2: 0.0003655248932773247   Iteration 16 of 100, tot loss = 4.776729300618172, l1: 0.00010559580823610304, l2: 0.00037207711739029037   Iteration 17 of 100, tot loss = 4.858881263171925, l1: 0.00010605923050795408, l2: 0.00037982889112112494   Iteration 18 of 100, tot loss = 4.85279577308231, l1: 0.00010625304558844719, l2: 0.0003790265287130347   Iteration 19 of 100, tot loss = 4.934071478090789, l1: 0.0001070004766102341, l2: 0.00038640666829642693   Iteration 20 of 100, tot loss = 5.052313578128815, l1: 0.00010922867113549728, l2: 0.0003960026842833031   Iteration 21 of 100, tot loss = 5.046139206205096, l1: 0.00010901757992715353, l2: 0.0003955963385080741   Iteration 22 of 100, tot loss = 4.986655181104487, l1: 0.00010753901568188502, l2: 0.0003911264998764223   Iteration 23 of 100, tot loss = 4.972066599389781, l1: 0.00010769091528820117, l2: 0.00038951574179404616   Iteration 24 of 100, tot loss = 4.89880308508873, l1: 0.00010637860456578589, l2: 0.0003835017008289772   Iteration 25 of 100, tot loss = 4.922377996444702, l1: 0.00010741599835455418, l2: 0.0003848217969061807   Iteration 26 of 100, tot loss = 4.810771566170913, l1: 0.00010506854120579262, l2: 0.00037600861105602235   Iteration 27 of 100, tot loss = 4.815602876521923, l1: 0.00010456577513617015, l2: 0.00037699450708546294   Iteration 28 of 100, tot loss = 4.689460809741702, l1: 0.00010204117279499769, l2: 0.0003669049032656143   Iteration 29 of 100, tot loss = 4.653669657378361, l1: 0.00010177587958831533, l2: 0.00036359108143470026   Iteration 30 of 100, tot loss = 4.689970608552297, l1: 0.00010293794111930765, l2: 0.00036605911397297555   Iteration 31 of 100, tot loss = 4.732363689330317, l1: 0.00010336746415686644, l2: 0.0003698689007808665   Iteration 32 of 100, tot loss = 4.738291535526514, l1: 0.00010345204532313801, l2: 0.0003703771039909043   Iteration 33 of 100, tot loss = 4.717324029315602, l1: 0.00010314973968144676, l2: 0.00036858265935513896   Iteration 34 of 100, tot loss = 4.659532795934116, l1: 0.00010211488509358948, l2: 0.000363838390673182   Iteration 35 of 100, tot loss = 4.615391973086766, l1: 0.00010133474279427901, l2: 0.0003602044507195907   Iteration 36 of 100, tot loss = 4.629852887656954, l1: 0.00010186430376456378, l2: 0.0003611209815264576   Iteration 37 of 100, tot loss = 4.590262351809321, l1: 0.00010137813994849755, l2: 0.0003576480922748561   Iteration 38 of 100, tot loss = 4.604345688694401, l1: 0.0001015718235545424, l2: 0.00035886274226389726   Iteration 39 of 100, tot loss = 4.576838141832596, l1: 0.00010125510669193971, l2: 0.00035642870464541303   Iteration 40 of 100, tot loss = 4.595009091496467, l1: 0.00010153178645850857, l2: 0.00035796912015939597   Iteration 41 of 100, tot loss = 4.569002788241317, l1: 0.00010174984380705055, l2: 0.00035515043241392095   Iteration 42 of 100, tot loss = 4.612811732859838, l1: 0.00010265772670252426, l2: 0.00035862344382275337   Iteration 43 of 100, tot loss = 4.6204509596491965, l1: 0.0001031475261443966, l2: 0.0003588975675885977   Iteration 44 of 100, tot loss = 4.6333128620277755, l1: 0.00010325110088160727, l2: 0.0003600801828724798   Iteration 45 of 100, tot loss = 4.674216405550639, l1: 0.00010396184661658481, l2: 0.00036345979153540813   Iteration 46 of 100, tot loss = 4.64439819947533, l1: 0.00010365481664295025, l2: 0.00036078500093224096   Iteration 47 of 100, tot loss = 4.618515017184805, l1: 0.00010356547043440824, l2: 0.0003582860294858271   Iteration 48 of 100, tot loss = 4.669287336369355, l1: 0.00010417722781615642, l2: 0.00036275150402313255   Iteration 49 of 100, tot loss = 4.69183007308415, l1: 0.00010505893056242898, l2: 0.00036412407525301894   Iteration 50 of 100, tot loss = 4.7075352025032045, l1: 0.000105245426093461, l2: 0.00036550809221807867   Iteration 51 of 100, tot loss = 4.6885339115180225, l1: 0.00010488339367108055, l2: 0.0003639699956265223   Iteration 52 of 100, tot loss = 4.755240584795292, l1: 0.00010538378536204199, l2: 0.00037014027083597076   Iteration 53 of 100, tot loss = 4.772774973005618, l1: 0.00010585387878203413, l2: 0.0003714236163947169   Iteration 54 of 100, tot loss = 4.773270225083387, l1: 0.0001062563480647643, l2: 0.000371070671934393   Iteration 55 of 100, tot loss = 4.7707047440788966, l1: 0.0001061844278328036, l2: 0.00037088604347611015   Iteration 56 of 100, tot loss = 4.788348553436143, l1: 0.00010635169045209685, l2: 0.0003724831622093916   Iteration 57 of 100, tot loss = 4.799944315040321, l1: 0.00010647076825925819, l2: 0.00037352366091095304   Iteration 58 of 100, tot loss = 4.800459251321596, l1: 0.00010667557154224126, l2: 0.00037337035133407033   Iteration 59 of 100, tot loss = 4.768198532573248, l1: 0.00010560816723628412, l2: 0.0003712116834368178   Iteration 60 of 100, tot loss = 4.761366353432337, l1: 0.00010519039569771849, l2: 0.00037094623694429175   Iteration 61 of 100, tot loss = 4.718814120918024, l1: 0.0001046503569594905, l2: 0.00036723105228299915   Iteration 62 of 100, tot loss = 4.717756038711917, l1: 0.00010488340335394887, l2: 0.00036689219785095644   Iteration 63 of 100, tot loss = 4.698007123810904, l1: 0.00010438865114886698, l2: 0.00036541205883536135   Iteration 64 of 100, tot loss = 4.696605214849114, l1: 0.00010432516512537404, l2: 0.0003653353539903037   Iteration 65 of 100, tot loss = 4.731543557460491, l1: 0.00010511506507800032, l2: 0.0003680392882625501   Iteration 66 of 100, tot loss = 4.730118095874786, l1: 0.00010527527300644468, l2: 0.00036773653405499783   Iteration 67 of 100, tot loss = 4.717625463186805, l1: 0.00010511647779998411, l2: 0.0003666460660456527   Iteration 68 of 100, tot loss = 4.736008225118413, l1: 0.0001054183462924009, l2: 0.00036818247313289353   Iteration 69 of 100, tot loss = 4.707828958829244, l1: 0.00010492581107731287, l2: 0.00036585708195174897   Iteration 70 of 100, tot loss = 4.7158685633114406, l1: 0.000105009698017966, l2: 0.00036657715536421166   Iteration 71 of 100, tot loss = 4.720242851217028, l1: 0.00010523015768906678, l2: 0.0003667941243587796   Iteration 72 of 100, tot loss = 4.695427664452129, l1: 0.00010447339986462289, l2: 0.0003650693635993068   Iteration 73 of 100, tot loss = 4.682823574706299, l1: 0.00010436360429444554, l2: 0.00036391875011310593   Iteration 74 of 100, tot loss = 4.651924946823636, l1: 0.00010373255109526154, l2: 0.00036145994051145643   Iteration 75 of 100, tot loss = 4.684894029299418, l1: 0.0001042672651722872, l2: 0.0003642221342306584   Iteration 76 of 100, tot loss = 4.689523623177879, l1: 0.00010454477997930218, l2: 0.0003644075787681351   Iteration 77 of 100, tot loss = 4.65222623750761, l1: 0.0001037703760783944, l2: 0.0003614522440755111   Iteration 78 of 100, tot loss = 4.6322799523671465, l1: 0.0001031234841297518, l2: 0.0003601045074248806   Iteration 79 of 100, tot loss = 4.616223715528657, l1: 0.00010294763300799223, l2: 0.00035867473509168556   Iteration 80 of 100, tot loss = 4.618255400657654, l1: 0.00010275468307554547, l2: 0.00035907085384678794   Iteration 81 of 100, tot loss = 4.647378791997462, l1: 0.00010302617365708517, l2: 0.0003617117021528912   Iteration 82 of 100, tot loss = 4.6419840440517515, l1: 0.00010302590724278139, l2: 0.000361172493906385   Iteration 83 of 100, tot loss = 4.612958517419287, l1: 0.0001025335839651497, l2: 0.00035876226460656533   Iteration 84 of 100, tot loss = 4.613466001692272, l1: 0.0001026223520793359, l2: 0.0003587242447663032   Iteration 85 of 100, tot loss = 4.615666327756994, l1: 0.00010257522889707402, l2: 0.000358991400742739   Iteration 86 of 100, tot loss = 4.600242032561192, l1: 0.00010235675734631598, l2: 0.0003576674429242239   Iteration 87 of 100, tot loss = 4.612832052954312, l1: 0.0001027804556260576, l2: 0.0003585027466894759   Iteration 88 of 100, tot loss = 4.585829450325533, l1: 0.00010239526855829436, l2: 0.0003561876735396006   Iteration 89 of 100, tot loss = 4.600704672631253, l1: 0.00010245005451201649, l2: 0.00035762040989651355   Iteration 90 of 100, tot loss = 4.621213454670376, l1: 0.000102896476689946, l2: 0.0003592248660020737   Iteration 91 of 100, tot loss = 4.604900307707734, l1: 0.00010251390411432801, l2: 0.0003579761241281221   Iteration 92 of 100, tot loss = 4.607678465221239, l1: 0.0001025039632820509, l2: 0.0003582638809862344   Iteration 93 of 100, tot loss = 4.596612507297147, l1: 0.00010208880485151894, l2: 0.00035757244369315525   Iteration 94 of 100, tot loss = 4.600087837970003, l1: 0.00010214215584942684, l2: 0.00035786662564087817   Iteration 95 of 100, tot loss = 4.594675327602186, l1: 0.00010226079432054815, l2: 0.0003572067362256348   Iteration 96 of 100, tot loss = 4.593949916462104, l1: 0.00010218812019502366, l2: 0.00035720686901186127   Iteration 97 of 100, tot loss = 4.608239776080417, l1: 0.00010272045218482969, l2: 0.00035810352305488025   Iteration 98 of 100, tot loss = 4.5961057993830465, l1: 0.00010249040753144666, l2: 0.00035712017014869775   Iteration 99 of 100, tot loss = 4.597414681405732, l1: 0.00010243355199686402, l2: 0.0003573079137813601   Iteration 100 of 100, tot loss = 4.582647457122802, l1: 0.0001019678064767504, l2: 0.000356296936806757
   End of epoch 1405; saving model... 

Epoch 1406 of 2000
   Iteration 1 of 100, tot loss = 3.6516430377960205, l1: 9.097794827539474e-05, l2: 0.00027418637182563543   Iteration 2 of 100, tot loss = 2.9238088130950928, l1: 7.779026418575086e-05, l2: 0.00021459062554640695   Iteration 3 of 100, tot loss = 3.1210904121398926, l1: 8.260364605424304e-05, l2: 0.00022950540005695075   Iteration 4 of 100, tot loss = 3.5321091413497925, l1: 9.262294770451263e-05, l2: 0.0002605879744805861   Iteration 5 of 100, tot loss = 3.520994234085083, l1: 9.122861374635249e-05, l2: 0.00026087081350851805   Iteration 6 of 100, tot loss = 3.469401240348816, l1: 9.15124607369459e-05, l2: 0.00025542766525177285   Iteration 7 of 100, tot loss = 3.4215304851531982, l1: 9.022509662567504e-05, l2: 0.0002519279535460685   Iteration 8 of 100, tot loss = 3.657256096601486, l1: 9.407052129972726e-05, l2: 0.00027165509163751267   Iteration 9 of 100, tot loss = 3.693395084804959, l1: 9.376832329305923e-05, l2: 0.00027557119028642774   Iteration 10 of 100, tot loss = 3.79953932762146, l1: 9.529328453936615e-05, l2: 0.0002846606512321159   Iteration 11 of 100, tot loss = 3.7836976918307217, l1: 9.538229377622801e-05, l2: 0.0002829874786336652   Iteration 12 of 100, tot loss = 3.7401825984319053, l1: 9.19762066284117e-05, l2: 0.00028204205591464415   Iteration 13 of 100, tot loss = 3.747528021152203, l1: 9.317186777479947e-05, l2: 0.0002815809365039548   Iteration 14 of 100, tot loss = 3.750066348484584, l1: 9.228964751985456e-05, l2: 0.00028271698933427354   Iteration 15 of 100, tot loss = 4.0173468589782715, l1: 9.46390141810601e-05, l2: 0.00030709567363373935   Iteration 16 of 100, tot loss = 4.0523868799209595, l1: 9.476293598709162e-05, l2: 0.0003104757524852175   Iteration 17 of 100, tot loss = 4.112481173347025, l1: 9.454835012815344e-05, l2: 0.00031669976819744883   Iteration 18 of 100, tot loss = 4.178138256072998, l1: 9.429992739266406e-05, l2: 0.00032351389947709523   Iteration 19 of 100, tot loss = 4.131462787327013, l1: 9.242842248855403e-05, l2: 0.00032071785780748253   Iteration 20 of 100, tot loss = 4.141179859638214, l1: 9.425215739611303e-05, l2: 0.0003198658305336721   Iteration 21 of 100, tot loss = 4.173905883516584, l1: 9.287712340925022e-05, l2: 0.00032451346522152777   Iteration 22 of 100, tot loss = 4.186023592948914, l1: 9.348453535163902e-05, l2: 0.00032511782377365637   Iteration 23 of 100, tot loss = 4.181012039599211, l1: 9.35957329562845e-05, l2: 0.0003245054711284035   Iteration 24 of 100, tot loss = 4.109555929899216, l1: 9.252984409613418e-05, l2: 0.0003184257487494809   Iteration 25 of 100, tot loss = 4.0931408500671385, l1: 9.179598171613179e-05, l2: 0.000317518103402108   Iteration 26 of 100, tot loss = 4.048636601521419, l1: 9.031982909408935e-05, l2: 0.0003145438318964667   Iteration 27 of 100, tot loss = 4.027603078771521, l1: 8.985769643374995e-05, l2: 0.0003129026121718602   Iteration 28 of 100, tot loss = 4.005844124725887, l1: 8.950645276075062e-05, l2: 0.00031107796010993686   Iteration 29 of 100, tot loss = 4.010649820853924, l1: 8.952602353207928e-05, l2: 0.00031153895844030996   Iteration 30 of 100, tot loss = 4.066251635551453, l1: 9.000002307099445e-05, l2: 0.00031662514181031535   Iteration 31 of 100, tot loss = 4.103575098899103, l1: 9.162696451605899e-05, l2: 0.0003187305466722577   Iteration 32 of 100, tot loss = 4.112341977655888, l1: 9.221667244219134e-05, l2: 0.0003190175266354345   Iteration 33 of 100, tot loss = 4.195510625839233, l1: 9.376597936903691e-05, l2: 0.0003257850837668009   Iteration 34 of 100, tot loss = 4.187958773444681, l1: 9.372009438212605e-05, l2: 0.0003250757838352857   Iteration 35 of 100, tot loss = 4.220940140315465, l1: 9.409368597386804e-05, l2: 0.00032800032890268737   Iteration 36 of 100, tot loss = 4.20078483555052, l1: 9.36781516429619e-05, l2: 0.00032640033249562193   Iteration 37 of 100, tot loss = 4.186413777841104, l1: 9.342624420045272e-05, l2: 0.0003252151340397226   Iteration 38 of 100, tot loss = 4.194678043064318, l1: 9.318275729846821e-05, l2: 0.00032628504681940143   Iteration 39 of 100, tot loss = 4.150114096128023, l1: 9.251431719092533e-05, l2: 0.0003224970923605351   Iteration 40 of 100, tot loss = 4.174965167045594, l1: 9.342553794340347e-05, l2: 0.0003240709789679386   Iteration 41 of 100, tot loss = 4.111684235130868, l1: 9.217314738368547e-05, l2: 0.00031899527640212556   Iteration 42 of 100, tot loss = 4.08536700407664, l1: 9.177406320218107e-05, l2: 0.0003167626372097792   Iteration 43 of 100, tot loss = 4.097141459930775, l1: 9.2702633907699e-05, l2: 0.00031701151263320733   Iteration 44 of 100, tot loss = 4.123044962232763, l1: 9.33056422912738e-05, l2: 0.00031899885404775637   Iteration 45 of 100, tot loss = 4.177244827482435, l1: 9.408158406990374e-05, l2: 0.0003236428984867719   Iteration 46 of 100, tot loss = 4.1367380670879195, l1: 9.323071706445342e-05, l2: 0.0003204430896680013   Iteration 47 of 100, tot loss = 4.134813699316471, l1: 9.349109894444948e-05, l2: 0.0003199902707723247   Iteration 48 of 100, tot loss = 4.137780105074246, l1: 9.408346727468597e-05, l2: 0.0003196945431227505   Iteration 49 of 100, tot loss = 4.187137316684334, l1: 9.486851677695309e-05, l2: 0.00032384521449673733   Iteration 50 of 100, tot loss = 4.227105593681335, l1: 9.51700044242898e-05, l2: 0.0003275405541353393   Iteration 51 of 100, tot loss = 4.190992079529107, l1: 9.467152060596181e-05, l2: 0.0003244276864707525   Iteration 52 of 100, tot loss = 4.183620622524848, l1: 9.47918185304913e-05, l2: 0.0003235702426116824   Iteration 53 of 100, tot loss = 4.2190271098658725, l1: 9.535467146427968e-05, l2: 0.0003265480378824989   Iteration 54 of 100, tot loss = 4.230788888754668, l1: 9.548789111485584e-05, l2: 0.0003275909962044184   Iteration 55 of 100, tot loss = 4.245256020806052, l1: 9.586248601077717e-05, l2: 0.00032866311468586154   Iteration 56 of 100, tot loss = 4.278848269156048, l1: 9.644638423976306e-05, l2: 0.0003314384415976487   Iteration 57 of 100, tot loss = 4.290479245938752, l1: 9.689852857008463e-05, l2: 0.000332149395513912   Iteration 58 of 100, tot loss = 4.262994778567347, l1: 9.649660951881429e-05, l2: 0.00032980286815817916   Iteration 59 of 100, tot loss = 4.292228686607491, l1: 9.654770363309673e-05, l2: 0.00033267516451907517   Iteration 60 of 100, tot loss = 4.3496860067049665, l1: 9.741998504371925e-05, l2: 0.0003375486146978801   Iteration 61 of 100, tot loss = 4.318639931131582, l1: 9.678521466233431e-05, l2: 0.00033507877748486294   Iteration 62 of 100, tot loss = 4.355290955112826, l1: 9.764361962997129e-05, l2: 0.00033788547525839547   Iteration 63 of 100, tot loss = 4.357544274557204, l1: 9.748609065556795e-05, l2: 0.0003382683359040317   Iteration 64 of 100, tot loss = 4.351263102144003, l1: 9.739642899830869e-05, l2: 0.00033772988069813437   Iteration 65 of 100, tot loss = 4.365879429303683, l1: 9.771886605169409e-05, l2: 0.0003388690761102435   Iteration 66 of 100, tot loss = 4.354180733362834, l1: 9.744397169869802e-05, l2: 0.0003379741007820209   Iteration 67 of 100, tot loss = 4.352745219842712, l1: 9.74892860920509e-05, l2: 0.0003377852348720224   Iteration 68 of 100, tot loss = 4.365345204577727, l1: 9.747394971568342e-05, l2: 0.0003390605697324431   Iteration 69 of 100, tot loss = 4.356361503186434, l1: 9.718204740277977e-05, l2: 0.0003384541018074716   Iteration 70 of 100, tot loss = 4.3458646910531185, l1: 9.708691395644564e-05, l2: 0.0003374995537991968   Iteration 71 of 100, tot loss = 4.322257733680833, l1: 9.67979821082185e-05, l2: 0.00033542778989395234   Iteration 72 of 100, tot loss = 4.312381496032079, l1: 9.700831747573425e-05, l2: 0.000334229830918452   Iteration 73 of 100, tot loss = 4.286459315313052, l1: 9.654058297587342e-05, l2: 0.0003321053473596231   Iteration 74 of 100, tot loss = 4.267882949597126, l1: 9.631641124370445e-05, l2: 0.0003304718825470568   Iteration 75 of 100, tot loss = 4.26704098701477, l1: 9.663258528841348e-05, l2: 0.0003300715122410717   Iteration 76 of 100, tot loss = 4.288948501411237, l1: 9.663608977318386e-05, l2: 0.0003322587589967665   Iteration 77 of 100, tot loss = 4.263232732748056, l1: 9.62298854338096e-05, l2: 0.00033009338637514574   Iteration 78 of 100, tot loss = 4.277519391133235, l1: 9.660265543341386e-05, l2: 0.0003311492825256816   Iteration 79 of 100, tot loss = 4.3110142116305195, l1: 9.711132593168523e-05, l2: 0.0003339900946299121   Iteration 80 of 100, tot loss = 4.329767572879791, l1: 9.763842658685462e-05, l2: 0.00033533833020555904   Iteration 81 of 100, tot loss = 4.335792294255009, l1: 9.80771182196209e-05, l2: 0.0003355021106494579   Iteration 82 of 100, tot loss = 4.334691995527686, l1: 9.775191179736483e-05, l2: 0.00033571728707146374   Iteration 83 of 100, tot loss = 4.369510610419583, l1: 9.817895015767573e-05, l2: 0.0003387721099207705   Iteration 84 of 100, tot loss = 4.364409117471604, l1: 9.836714287389658e-05, l2: 0.0003380737679912099   Iteration 85 of 100, tot loss = 4.361613498014562, l1: 9.798234279835871e-05, l2: 0.00033817900599815936   Iteration 86 of 100, tot loss = 4.3580923024998155, l1: 9.778712330105833e-05, l2: 0.0003380221061761827   Iteration 87 of 100, tot loss = 4.370744760009064, l1: 9.809846772535318e-05, l2: 0.0003389760072579896   Iteration 88 of 100, tot loss = 4.350422068075701, l1: 9.776375302614973e-05, l2: 0.00033727845295123353   Iteration 89 of 100, tot loss = 4.355264797639311, l1: 9.802810067351479e-05, l2: 0.00033749837869466963   Iteration 90 of 100, tot loss = 4.365233055750529, l1: 9.817225723559709e-05, l2: 0.00033835104784682495   Iteration 91 of 100, tot loss = 4.376014615153219, l1: 9.830362918424665e-05, l2: 0.0003392978320774561   Iteration 92 of 100, tot loss = 4.372163342392963, l1: 9.829138138150523e-05, l2: 0.0003389249529351693   Iteration 93 of 100, tot loss = 4.369796968275501, l1: 9.82923135183306e-05, l2: 0.0003386873834652536   Iteration 94 of 100, tot loss = 4.390156416182823, l1: 9.860156395471348e-05, l2: 0.0003404140778466752   Iteration 95 of 100, tot loss = 4.3903401926944134, l1: 9.853259555376625e-05, l2: 0.0003405014240630216   Iteration 96 of 100, tot loss = 4.392999654014905, l1: 9.869056827938039e-05, l2: 0.000340609397502097   Iteration 97 of 100, tot loss = 4.407903450051534, l1: 9.880557180875577e-05, l2: 0.0003419847737850151   Iteration 98 of 100, tot loss = 4.409876161692094, l1: 9.850855790968385e-05, l2: 0.00034247905888169414   Iteration 99 of 100, tot loss = 4.407431467614993, l1: 9.843873134502586e-05, l2: 0.00034230441594969085   Iteration 100 of 100, tot loss = 4.401478857994079, l1: 9.829757855186471e-05, l2: 0.00034185030766821
   End of epoch 1406; saving model... 

Epoch 1407 of 2000
   Iteration 1 of 100, tot loss = 5.196203708648682, l1: 0.00012591257109306753, l2: 0.0003937078290618956   Iteration 2 of 100, tot loss = 5.55282187461853, l1: 0.00013262065476737916, l2: 0.0004226615565130487   Iteration 3 of 100, tot loss = 4.559051911036174, l1: 0.00011328492594960456, l2: 0.0003426202796011542   Iteration 4 of 100, tot loss = 4.586767733097076, l1: 0.00011429523874539882, l2: 0.00034438154762028717   Iteration 5 of 100, tot loss = 4.147916269302368, l1: 0.00010505014652153477, l2: 0.0003097414912190288   Iteration 6 of 100, tot loss = 4.475019176801045, l1: 0.00010980708975694142, l2: 0.0003376948394967864   Iteration 7 of 100, tot loss = 4.451250655310495, l1: 0.00010387703514425084, l2: 0.00034124803745986095   Iteration 8 of 100, tot loss = 4.259472072124481, l1: 9.82138340077654e-05, l2: 0.00032773337807157077   Iteration 9 of 100, tot loss = 4.241037686665853, l1: 9.671076622908004e-05, l2: 0.0003273930083701594   Iteration 10 of 100, tot loss = 4.1516451120376585, l1: 9.536481338727754e-05, l2: 0.00031979970226529985   Iteration 11 of 100, tot loss = 3.99514178796248, l1: 9.32979366387537e-05, l2: 0.0003062162445117296   Iteration 12 of 100, tot loss = 3.826702445745468, l1: 8.864729200771156e-05, l2: 0.00029402295452503796   Iteration 13 of 100, tot loss = 3.998874856875493, l1: 9.052005127439491e-05, l2: 0.0003093674346858349   Iteration 14 of 100, tot loss = 4.197246900626591, l1: 9.168850140538e-05, l2: 0.00032803619043469164   Iteration 15 of 100, tot loss = 4.02798904577891, l1: 8.887903750292026e-05, l2: 0.0003139198689799135   Iteration 16 of 100, tot loss = 4.039565958082676, l1: 9.074993408830778e-05, l2: 0.00031320666221290594   Iteration 17 of 100, tot loss = 4.128058384446537, l1: 9.398015540415038e-05, l2: 0.00031882568402900635   Iteration 18 of 100, tot loss = 4.102601336108314, l1: 9.370396522475251e-05, l2: 0.00031655616880420386   Iteration 19 of 100, tot loss = 4.295333027839661, l1: 9.65413447901435e-05, l2: 0.00033299195791570177   Iteration 20 of 100, tot loss = 4.3221459805965425, l1: 9.638684587116586e-05, l2: 0.0003358277514053043   Iteration 21 of 100, tot loss = 4.415944911184765, l1: 9.688818040775091e-05, l2: 0.00034470630773631415   Iteration 22 of 100, tot loss = 4.455183608965441, l1: 9.79259521872419e-05, l2: 0.0003475924068797295   Iteration 23 of 100, tot loss = 4.355264067649841, l1: 9.681229241086048e-05, l2: 0.00033871411237076086   Iteration 24 of 100, tot loss = 4.34813480079174, l1: 9.610839227510344e-05, l2: 0.00033870508620263234   Iteration 25 of 100, tot loss = 4.335533776283264, l1: 9.68291079334449e-05, l2: 0.0003367242688545957   Iteration 26 of 100, tot loss = 4.308086372338808, l1: 9.664525756446752e-05, l2: 0.0003341633794032252   Iteration 27 of 100, tot loss = 4.243332937911704, l1: 9.604296213149576e-05, l2: 0.00032829033174224334   Iteration 28 of 100, tot loss = 4.278079258544104, l1: 9.751790938545517e-05, l2: 0.00033029001757053526   Iteration 29 of 100, tot loss = 4.34665203505549, l1: 9.902194782124094e-05, l2: 0.0003356432561272884   Iteration 30 of 100, tot loss = 4.442463370164235, l1: 0.00010036651644137843, l2: 0.0003438798201386817   Iteration 31 of 100, tot loss = 4.477522484717831, l1: 0.00010020362793922334, l2: 0.00034754862023081873   Iteration 32 of 100, tot loss = 4.485874358564615, l1: 0.00010081021434871218, l2: 0.000347777222032164   Iteration 33 of 100, tot loss = 4.60324556538553, l1: 0.00010274453068797646, l2: 0.00035758002680571127   Iteration 34 of 100, tot loss = 4.62045348742429, l1: 0.00010282326216196559, l2: 0.00035922208788927975   Iteration 35 of 100, tot loss = 4.620192190579005, l1: 0.00010303962013235183, l2: 0.0003589796006313658   Iteration 36 of 100, tot loss = 4.622366153531605, l1: 0.00010302906725377802, l2: 0.0003592075497888598   Iteration 37 of 100, tot loss = 4.581330470136694, l1: 0.00010242449570796452, l2: 0.0003557085528390834   Iteration 38 of 100, tot loss = 4.585711607807561, l1: 0.00010248207083924698, l2: 0.0003560890916943256   Iteration 39 of 100, tot loss = 4.6274401010611115, l1: 0.00010284883757823338, l2: 0.0003598951742064972   Iteration 40 of 100, tot loss = 4.600313434004784, l1: 0.00010249429496980155, l2: 0.00035753704942180774   Iteration 41 of 100, tot loss = 4.627361469152497, l1: 0.00010294048018006766, l2: 0.00035979566746391356   Iteration 42 of 100, tot loss = 4.605533165591104, l1: 0.00010175840240359928, l2: 0.00035879491492995017   Iteration 43 of 100, tot loss = 4.584413281706876, l1: 0.00010177796824326262, l2: 0.00035666336094267496   Iteration 44 of 100, tot loss = 4.614969510923732, l1: 0.00010251850008816373, l2: 0.00035897845174820924   Iteration 45 of 100, tot loss = 4.661807799339295, l1: 0.00010312955128028989, l2: 0.00036305122905307347   Iteration 46 of 100, tot loss = 4.6170070663742395, l1: 0.00010274538441950901, l2: 0.00035895532283061385   Iteration 47 of 100, tot loss = 4.640538385573854, l1: 0.00010313890333178433, l2: 0.00036091493576408384   Iteration 48 of 100, tot loss = 4.609714853266875, l1: 0.00010260468313087283, l2: 0.0003583668027810442   Iteration 49 of 100, tot loss = 4.6055832760674615, l1: 0.00010252775246044621, l2: 0.0003580305759785507   Iteration 50 of 100, tot loss = 4.613255212306976, l1: 0.00010296173990354873, l2: 0.00035836378170643   Iteration 51 of 100, tot loss = 4.608719542914746, l1: 0.00010246317890728368, l2: 0.0003584087757831987   Iteration 52 of 100, tot loss = 4.597194137481543, l1: 0.00010250682372805805, l2: 0.000357212590573069   Iteration 53 of 100, tot loss = 4.580663858719592, l1: 0.00010172228809645979, l2: 0.0003563440986908972   Iteration 54 of 100, tot loss = 4.618972210972397, l1: 0.00010219078986092764, l2: 0.00035970643188597425   Iteration 55 of 100, tot loss = 4.608775075999174, l1: 0.00010207259467940524, l2: 0.0003588049138091843   Iteration 56 of 100, tot loss = 4.585572385362217, l1: 0.00010187328494534345, l2: 0.00035668395451336565   Iteration 57 of 100, tot loss = 4.56356746481176, l1: 0.00010165850510116128, l2: 0.00035469824210291305   Iteration 58 of 100, tot loss = 4.577817006357785, l1: 0.00010178697398103019, l2: 0.0003559947280305566   Iteration 59 of 100, tot loss = 4.5619194325754195, l1: 0.00010153757373977974, l2: 0.0003546543704973281   Iteration 60 of 100, tot loss = 4.545147055387497, l1: 0.00010127673637422655, l2: 0.0003532379703149976   Iteration 61 of 100, tot loss = 4.565299317484996, l1: 0.0001019223804416738, l2: 0.0003546075520895757   Iteration 62 of 100, tot loss = 4.544302700027343, l1: 0.00010150891742546998, l2: 0.00035292135374132363   Iteration 63 of 100, tot loss = 4.566392669602046, l1: 0.00010174197727575942, l2: 0.00035489729022614597   Iteration 64 of 100, tot loss = 4.556937882676721, l1: 0.00010206654741296006, l2: 0.00035362724133847223   Iteration 65 of 100, tot loss = 4.568300557136536, l1: 0.00010221501439585923, l2: 0.000354615041690592   Iteration 66 of 100, tot loss = 4.536019637729183, l1: 0.00010164719054900519, l2: 0.0003519547734123824   Iteration 67 of 100, tot loss = 4.581852807927487, l1: 0.00010232437874838612, l2: 0.00035586090191135376   Iteration 68 of 100, tot loss = 4.61093528305783, l1: 0.00010259976031246719, l2: 0.0003584937673140838   Iteration 69 of 100, tot loss = 4.62628129772518, l1: 0.00010274956725154787, l2: 0.000359878562132666   Iteration 70 of 100, tot loss = 4.604883798531124, l1: 0.00010263626292206546, l2: 0.00035785211636851143   Iteration 71 of 100, tot loss = 4.6029143383805184, l1: 0.00010295091257356829, l2: 0.0003573405210004175   Iteration 72 of 100, tot loss = 4.5953351110219955, l1: 0.00010274326172697733, l2: 0.0003567902492957526   Iteration 73 of 100, tot loss = 4.603775597598455, l1: 0.00010263668555624414, l2: 0.0003577408742411572   Iteration 74 of 100, tot loss = 4.612252188695444, l1: 0.00010276278334385024, l2: 0.0003584624357392533   Iteration 75 of 100, tot loss = 4.626005419095358, l1: 0.00010278732947578344, l2: 0.0003598132126110916   Iteration 76 of 100, tot loss = 4.624422410601063, l1: 0.0001025498485384895, l2: 0.00035989239270412855   Iteration 77 of 100, tot loss = 4.645806732115807, l1: 0.00010311602539792597, l2: 0.000361464647976863   Iteration 78 of 100, tot loss = 4.675885859208229, l1: 0.00010384047173153167, l2: 0.00036374811409190536   Iteration 79 of 100, tot loss = 4.667603261863129, l1: 0.00010354801666329002, l2: 0.00036321230924576286   Iteration 80 of 100, tot loss = 4.640497414767742, l1: 0.0001029138694775611, l2: 0.000361135871753504   Iteration 81 of 100, tot loss = 4.620011498898636, l1: 0.00010233556464232538, l2: 0.0003596655850192348   Iteration 82 of 100, tot loss = 4.627768695354462, l1: 0.00010229126995155395, l2: 0.00036048559949693567   Iteration 83 of 100, tot loss = 4.642572447478053, l1: 0.00010265834226815805, l2: 0.0003615989024029955   Iteration 84 of 100, tot loss = 4.64402678892726, l1: 0.00010270405146868489, l2: 0.0003616986271351509   Iteration 85 of 100, tot loss = 4.6622268774930165, l1: 0.00010310259544092011, l2: 0.00036312009204480356   Iteration 86 of 100, tot loss = 4.667971703895303, l1: 0.0001031988271025448, l2: 0.00036359834332468506   Iteration 87 of 100, tot loss = 4.690286892584001, l1: 0.00010372031181174244, l2: 0.0003653083773712553   Iteration 88 of 100, tot loss = 4.702676972204989, l1: 0.0001038673492323803, l2: 0.00036640034770905254   Iteration 89 of 100, tot loss = 4.683548028549452, l1: 0.00010376532618159146, l2: 0.00036458947644778266   Iteration 90 of 100, tot loss = 4.688528422514597, l1: 0.00010396569830643583, l2: 0.0003648871439509094   Iteration 91 of 100, tot loss = 4.682845316090426, l1: 0.00010385083462077111, l2: 0.00036443369674168854   Iteration 92 of 100, tot loss = 4.660295552533606, l1: 0.00010343832870959254, l2: 0.00036259122609170964   Iteration 93 of 100, tot loss = 4.681811013529377, l1: 0.00010369208253142724, l2: 0.0003644890181334709   Iteration 94 of 100, tot loss = 4.673572168705311, l1: 0.0001034049144707635, l2: 0.0003639523016085769   Iteration 95 of 100, tot loss = 4.648782299694262, l1: 0.00010296176490220732, l2: 0.000361916464355186   Iteration 96 of 100, tot loss = 4.642584687719743, l1: 0.00010282624797734267, l2: 0.0003614322202641536   Iteration 97 of 100, tot loss = 4.62847883553849, l1: 0.000102387874838164, l2: 0.0003604600084501501   Iteration 98 of 100, tot loss = 4.654698842642259, l1: 0.00010298586007871675, l2: 0.00036248402377205653   Iteration 99 of 100, tot loss = 4.6709386640124855, l1: 0.00010343606775888768, l2: 0.00036365779824061036   Iteration 100 of 100, tot loss = 4.679863241910934, l1: 0.00010349657441111049, l2: 0.00036448974933591673
   End of epoch 1407; saving model... 

Epoch 1408 of 2000
   Iteration 1 of 100, tot loss = 5.0083723068237305, l1: 9.670008876128122e-05, l2: 0.0004041371284984052   Iteration 2 of 100, tot loss = 4.431135058403015, l1: 8.991162030724809e-05, l2: 0.0003532018745318055   Iteration 3 of 100, tot loss = 4.40886648495992, l1: 0.0001021998420280094, l2: 0.00033868679505152005   Iteration 4 of 100, tot loss = 3.994418442249298, l1: 9.87001239991514e-05, l2: 0.00030074171081651   Iteration 5 of 100, tot loss = 3.79085431098938, l1: 9.723589173518121e-05, l2: 0.0002818495326209813   Iteration 6 of 100, tot loss = 4.262699405352275, l1: 0.00010198298696195707, l2: 0.00032428694248665124   Iteration 7 of 100, tot loss = 4.046702929905483, l1: 9.353085676723692e-05, l2: 0.00031113942636043897   Iteration 8 of 100, tot loss = 3.908478260040283, l1: 9.33011820052343e-05, l2: 0.0002975466359202983   Iteration 9 of 100, tot loss = 3.705433342191908, l1: 8.974988506654174e-05, l2: 0.00028079344095507014   Iteration 10 of 100, tot loss = 3.7348020553588865, l1: 9.077699833142106e-05, l2: 0.0002827031988999806   Iteration 11 of 100, tot loss = 3.7155912572687324, l1: 9.032371830967763e-05, l2: 0.000281235397613438   Iteration 12 of 100, tot loss = 3.8072351217269897, l1: 9.216881608153926e-05, l2: 0.00028855468675222556   Iteration 13 of 100, tot loss = 3.730048729823186, l1: 9.042472335680101e-05, l2: 0.0002825801415243544   Iteration 14 of 100, tot loss = 3.764669350215367, l1: 9.124963435169775e-05, l2: 0.0002852172921328539   Iteration 15 of 100, tot loss = 3.8503679275512694, l1: 9.285022218440039e-05, l2: 0.00029218656030328326   Iteration 16 of 100, tot loss = 3.890324890613556, l1: 9.508686184744874e-05, l2: 0.0002939456180683919   Iteration 17 of 100, tot loss = 3.932567035450655, l1: 9.615425005904399e-05, l2: 0.0002971024445929181   Iteration 18 of 100, tot loss = 3.9380230373806424, l1: 9.524498434782597e-05, l2: 0.0002985573123118633   Iteration 19 of 100, tot loss = 3.945544594212582, l1: 9.566667894863425e-05, l2: 0.00029888777230792726   Iteration 20 of 100, tot loss = 4.059273743629456, l1: 9.666605237725889e-05, l2: 0.00030926131512387656   Iteration 21 of 100, tot loss = 4.1257582846142, l1: 9.76489659695376e-05, l2: 0.00031492685680166774   Iteration 22 of 100, tot loss = 4.109793305397034, l1: 9.748499509111174e-05, l2: 0.0003134943300127898   Iteration 23 of 100, tot loss = 4.208802689676699, l1: 9.911988894126135e-05, l2: 0.0003217603731647377   Iteration 24 of 100, tot loss = 4.196403841177623, l1: 9.87143022636398e-05, l2: 0.0003209260751949235   Iteration 25 of 100, tot loss = 4.2200506401062015, l1: 9.920983560732566e-05, l2: 0.0003227952209999785   Iteration 26 of 100, tot loss = 4.316556215286255, l1: 0.00010055091549405076, l2: 0.00033110469890883763   Iteration 27 of 100, tot loss = 4.3860045539008246, l1: 0.00010240855947443008, l2: 0.00033619188912713006   Iteration 28 of 100, tot loss = 4.333292373589107, l1: 0.00010204712621738768, l2: 0.0003312821042657431   Iteration 29 of 100, tot loss = 4.234583867007289, l1: 9.967002702539722e-05, l2: 0.0003237883529750277   Iteration 30 of 100, tot loss = 4.286394321918488, l1: 0.00010081426250205064, l2: 0.00032782516282168215   Iteration 31 of 100, tot loss = 4.237331340389867, l1: 0.00010005718227588542, l2: 0.00032367594535425006   Iteration 32 of 100, tot loss = 4.268366303294897, l1: 0.000100877831073376, l2: 0.0003259587936099706   Iteration 33 of 100, tot loss = 4.258444746335347, l1: 0.00010033804926823711, l2: 0.0003255064196612996   Iteration 34 of 100, tot loss = 4.22933173530242, l1: 9.982066566247821e-05, l2: 0.0003231125017063117   Iteration 35 of 100, tot loss = 4.20413087776729, l1: 9.892383053998596e-05, l2: 0.00032148925134346687   Iteration 36 of 100, tot loss = 4.168290161424213, l1: 9.816263607515591e-05, l2: 0.00031866637416694884   Iteration 37 of 100, tot loss = 4.22634596115834, l1: 9.91907123731122e-05, l2: 0.0003234438786544283   Iteration 38 of 100, tot loss = 4.274609161050696, l1: 0.00010000969077258273, l2: 0.0003274512202647441   Iteration 39 of 100, tot loss = 4.260620456475478, l1: 9.998387451140353e-05, l2: 0.0003260781662762523   Iteration 40 of 100, tot loss = 4.233806034922599, l1: 9.981290295399959e-05, l2: 0.00032356769570469625   Iteration 41 of 100, tot loss = 4.1958233583264235, l1: 9.934875525664765e-05, l2: 0.0003202335762425053   Iteration 42 of 100, tot loss = 4.185899442150479, l1: 9.901696570783056e-05, l2: 0.0003195729741058867   Iteration 43 of 100, tot loss = 4.194747849952343, l1: 9.906842108669712e-05, l2: 0.0003204063590559134   Iteration 44 of 100, tot loss = 4.242266375910152, l1: 9.99267849692842e-05, l2: 0.00032429984788574404   Iteration 45 of 100, tot loss = 4.281461506419712, l1: 0.00010031881166570303, l2: 0.0003278273350184059   Iteration 46 of 100, tot loss = 4.27348611406658, l1: 9.994959359376125e-05, l2: 0.00032739901428652484   Iteration 47 of 100, tot loss = 4.271639527158534, l1: 9.984582380213319e-05, l2: 0.0003273181251604407   Iteration 48 of 100, tot loss = 4.319978880385558, l1: 0.00010051088247564621, l2: 0.0003314870020100595   Iteration 49 of 100, tot loss = 4.289289204441771, l1: 9.995354182082133e-05, l2: 0.00032897537490424265   Iteration 50 of 100, tot loss = 4.291360924243927, l1: 9.999793706811033e-05, l2: 0.00032913815157371574   Iteration 51 of 100, tot loss = 4.287083721628376, l1: 9.992019191735843e-05, l2: 0.0003287881769995461   Iteration 52 of 100, tot loss = 4.258819962923344, l1: 9.936183148686093e-05, l2: 0.00032652016181180865   Iteration 53 of 100, tot loss = 4.2394412261135175, l1: 9.856026639990544e-05, l2: 0.0003253838530893192   Iteration 54 of 100, tot loss = 4.241594122515784, l1: 9.90598056415803e-05, l2: 0.00032509960348963633   Iteration 55 of 100, tot loss = 4.243978606570851, l1: 9.91983717540279e-05, l2: 0.0003251994860792448   Iteration 56 of 100, tot loss = 4.236181512475014, l1: 9.896567358477373e-05, l2: 0.0003246524747737567   Iteration 57 of 100, tot loss = 4.241760368932757, l1: 9.913893512004921e-05, l2: 0.00032503709920555385   Iteration 58 of 100, tot loss = 4.253034499184839, l1: 9.887980120075899e-05, l2: 0.0003264236459879863   Iteration 59 of 100, tot loss = 4.2368870205798395, l1: 9.869036592258992e-05, l2: 0.00032499833370389227   Iteration 60 of 100, tot loss = 4.214635870854059, l1: 9.847986887810597e-05, l2: 0.00032298371576568267   Iteration 61 of 100, tot loss = 4.17931001889901, l1: 9.749713145076205e-05, l2: 0.00032043386812295125   Iteration 62 of 100, tot loss = 4.205391716572546, l1: 9.776476725810133e-05, l2: 0.0003227744024526703   Iteration 63 of 100, tot loss = 4.201854340613834, l1: 9.771641709939724e-05, l2: 0.0003224690150264429   Iteration 64 of 100, tot loss = 4.199856689199805, l1: 9.749742190479083e-05, l2: 0.00032248824538783083   Iteration 65 of 100, tot loss = 4.193083339471084, l1: 9.752930060270815e-05, l2: 0.00032177903172291386   Iteration 66 of 100, tot loss = 4.186653670036431, l1: 9.752648348336207e-05, l2: 0.00032113888179689337   Iteration 67 of 100, tot loss = 4.190219514405549, l1: 9.759527920187563e-05, l2: 0.0003214266704814161   Iteration 68 of 100, tot loss = 4.206597270334468, l1: 9.82602735829834e-05, l2: 0.00032239945176115725   Iteration 69 of 100, tot loss = 4.189641083496205, l1: 9.805974648612471e-05, l2: 0.0003209043602312348   Iteration 70 of 100, tot loss = 4.190482045922961, l1: 9.80369088210864e-05, l2: 0.0003210112939996179   Iteration 71 of 100, tot loss = 4.185134567005534, l1: 9.7700463401721e-05, l2: 0.0003208129915357961   Iteration 72 of 100, tot loss = 4.203728889425595, l1: 9.818267067708398e-05, l2: 0.0003221902165427713   Iteration 73 of 100, tot loss = 4.201711865320598, l1: 9.805904552842847e-05, l2: 0.0003221121391206533   Iteration 74 of 100, tot loss = 4.188972909708281, l1: 9.77346958648382e-05, l2: 0.0003211625932906188   Iteration 75 of 100, tot loss = 4.17526353041331, l1: 9.765143879728081e-05, l2: 0.0003198749122869534   Iteration 76 of 100, tot loss = 4.174625664949417, l1: 9.772722081068328e-05, l2: 0.0003197353439885008   Iteration 77 of 100, tot loss = 4.169726952329858, l1: 9.72151949997181e-05, l2: 0.0003197574985172757   Iteration 78 of 100, tot loss = 4.1607279639977675, l1: 9.726678185054484e-05, l2: 0.0003188060129618045   Iteration 79 of 100, tot loss = 4.154591525657268, l1: 9.73309500215139e-05, l2: 0.00031812820125657097   Iteration 80 of 100, tot loss = 4.168416668474674, l1: 9.742447718963376e-05, l2: 0.0003194171881659713   Iteration 81 of 100, tot loss = 4.193835707358372, l1: 9.743108589814424e-05, l2: 0.00032195248355425   Iteration 82 of 100, tot loss = 4.189086496829987, l1: 9.699974614749208e-05, l2: 0.0003219089019516903   Iteration 83 of 100, tot loss = 4.198286942688815, l1: 9.717113320527778e-05, l2: 0.00032265755954178905   Iteration 84 of 100, tot loss = 4.202911264839626, l1: 9.72309034772306e-05, l2: 0.0003230602216631052   Iteration 85 of 100, tot loss = 4.197663914456087, l1: 9.700560510870727e-05, l2: 0.00032276078512522336   Iteration 86 of 100, tot loss = 4.17885097098905, l1: 9.67029498201026e-05, l2: 0.00032118214593813103   Iteration 87 of 100, tot loss = 4.154419730449545, l1: 9.611052456452383e-05, l2: 0.00031933144707401313   Iteration 88 of 100, tot loss = 4.177143856883049, l1: 9.614475828295161e-05, l2: 0.00032156962590141285   Iteration 89 of 100, tot loss = 4.1563312243879516, l1: 9.588264690004447e-05, l2: 0.0003197504740680589   Iteration 90 of 100, tot loss = 4.159144588311514, l1: 9.572556773491669e-05, l2: 0.00032018888943841577   Iteration 91 of 100, tot loss = 4.181685234164144, l1: 9.634429087592442e-05, l2: 0.0003218242312828846   Iteration 92 of 100, tot loss = 4.198987262404484, l1: 9.657607342492354e-05, l2: 0.00032332265136178847   Iteration 93 of 100, tot loss = 4.209347546741527, l1: 9.669426912961338e-05, l2: 0.0003242404845584514   Iteration 94 of 100, tot loss = 4.229946731252873, l1: 9.713679649761848e-05, l2: 0.00032585787568600513   Iteration 95 of 100, tot loss = 4.226104883143776, l1: 9.696058807189968e-05, l2: 0.00032564989929883984   Iteration 96 of 100, tot loss = 4.235286967207988, l1: 9.676355098993857e-05, l2: 0.0003267651449429347   Iteration 97 of 100, tot loss = 4.262213939243985, l1: 9.73048674458153e-05, l2: 0.0003289165256272512   Iteration 98 of 100, tot loss = 4.2627556920051575, l1: 9.728058192157899e-05, l2: 0.0003289949863570581   Iteration 99 of 100, tot loss = 4.262513378653863, l1: 9.701856723757263e-05, l2: 0.0003292327697507595   Iteration 100 of 100, tot loss = 4.253854414224625, l1: 9.709845613542712e-05, l2: 0.00032828698451339735
   End of epoch 1408; saving model... 

Epoch 1409 of 2000
   Iteration 1 of 100, tot loss = 4.41959810256958, l1: 9.647431579651311e-05, l2: 0.00034548548865132034   Iteration 2 of 100, tot loss = 5.152543544769287, l1: 0.00012080218220944516, l2: 0.0003944521740777418   Iteration 3 of 100, tot loss = 5.530552228291829, l1: 0.0001260273541750697, l2: 0.0004270278635279586   Iteration 4 of 100, tot loss = 4.817309558391571, l1: 0.00011558797632460482, l2: 0.0003661429764179047   Iteration 5 of 100, tot loss = 4.9834812641143795, l1: 0.00011813775054179133, l2: 0.00038021037180442364   Iteration 6 of 100, tot loss = 5.232256929079692, l1: 0.00011884554987773299, l2: 0.00040438013820676133   Iteration 7 of 100, tot loss = 5.175273180007935, l1: 0.00011829141814294937, l2: 0.00039923589585149396   Iteration 8 of 100, tot loss = 5.399839252233505, l1: 0.0001201612221848336, l2: 0.00041982269794971216   Iteration 9 of 100, tot loss = 5.265330235163371, l1: 0.0001190128597146314, l2: 0.0004075201593675754   Iteration 10 of 100, tot loss = 5.410257077217102, l1: 0.00012036390544380992, l2: 0.00042066180176334453   Iteration 11 of 100, tot loss = 5.9374402002854785, l1: 0.0001272360598456792, l2: 0.0004665079613914713   Iteration 12 of 100, tot loss = 5.881000896294911, l1: 0.00012532629140575105, l2: 0.00046277380048801814   Iteration 13 of 100, tot loss = 5.558918604483972, l1: 0.00011874095462889482, l2: 0.00043715090858033643   Iteration 14 of 100, tot loss = 5.476213301931109, l1: 0.00011793078166582356, l2: 0.0004296905535738915   Iteration 15 of 100, tot loss = 5.429087114334107, l1: 0.00011751995819698398, l2: 0.0004253887590796997   Iteration 16 of 100, tot loss = 5.472068056464195, l1: 0.00011951223450523685, l2: 0.00042769457832037006   Iteration 17 of 100, tot loss = 5.31414862240062, l1: 0.0001168945138274199, l2: 0.0004145203548593118   Iteration 18 of 100, tot loss = 5.443925301233928, l1: 0.00011922942293393944, l2: 0.00042516311320165795   Iteration 19 of 100, tot loss = 5.360100833993209, l1: 0.00011835558153076195, l2: 0.00041765450824689314   Iteration 20 of 100, tot loss = 5.394831335544586, l1: 0.00011854364420287311, l2: 0.00042093949450645597   Iteration 21 of 100, tot loss = 5.341862962359474, l1: 0.00011793370330927982, l2: 0.000416252597157533   Iteration 22 of 100, tot loss = 5.332037329673767, l1: 0.00011675908039747314, l2: 0.00041644465818535537   Iteration 23 of 100, tot loss = 5.23805660786836, l1: 0.00011555933926264633, l2: 0.00040824632679172993   Iteration 24 of 100, tot loss = 5.117579023043315, l1: 0.00011294301672630051, l2: 0.00039881489040756907   Iteration 25 of 100, tot loss = 5.113642387390136, l1: 0.00011277911937213503, l2: 0.00039858512580394747   Iteration 26 of 100, tot loss = 5.10730635202848, l1: 0.00011285317822722181, l2: 0.00039787746433061187   Iteration 27 of 100, tot loss = 5.087309413486057, l1: 0.00011295831211976258, l2: 0.0003957726358849969   Iteration 28 of 100, tot loss = 5.054485661642892, l1: 0.00011226195459000467, l2: 0.0003931866179170486   Iteration 29 of 100, tot loss = 4.940629474047957, l1: 0.00010993071040941467, l2: 0.00038413224332355735   Iteration 30 of 100, tot loss = 4.915636722246806, l1: 0.00011018312022012347, l2: 0.00038138055727661896   Iteration 31 of 100, tot loss = 4.978971196759131, l1: 0.00011137023055550432, l2: 0.00038652689357648694   Iteration 32 of 100, tot loss = 5.008905701339245, l1: 0.00011262039151915815, l2: 0.0003882701821567025   Iteration 33 of 100, tot loss = 4.993212259177006, l1: 0.00011211342070689143, l2: 0.0003872078077895849   Iteration 34 of 100, tot loss = 5.003942749079536, l1: 0.0001119511349208872, l2: 0.00038844314301112555   Iteration 35 of 100, tot loss = 4.923962511335101, l1: 0.00011062824861645433, l2: 0.000381768005068547   Iteration 36 of 100, tot loss = 4.951763219303555, l1: 0.00011110458480187744, l2: 0.00038407173997256905   Iteration 37 of 100, tot loss = 4.921199173540683, l1: 0.00011048450078060096, l2: 0.00038163541938883026   Iteration 38 of 100, tot loss = 4.97729019742263, l1: 0.00011085311684598167, l2: 0.0003868759047595392   Iteration 39 of 100, tot loss = 4.9490221952780695, l1: 0.00011011056090072275, l2: 0.0003847916601583935   Iteration 40 of 100, tot loss = 4.971451008319855, l1: 0.00011079511459683999, l2: 0.00038634998782072216   Iteration 41 of 100, tot loss = 4.925031039772964, l1: 0.00011006657391922867, l2: 0.0003824365320303137   Iteration 42 of 100, tot loss = 4.981654025259472, l1: 0.00011131688994022884, l2: 0.0003868485153015215   Iteration 43 of 100, tot loss = 4.97837171443673, l1: 0.00011096503700638666, l2: 0.00038687213693561337   Iteration 44 of 100, tot loss = 4.9956118139353665, l1: 0.0001113844191422686, l2: 0.00038817676481399263   Iteration 45 of 100, tot loss = 4.9737633228302, l1: 0.00011143141843300934, l2: 0.0003859449161811628   Iteration 46 of 100, tot loss = 4.968794392502827, l1: 0.00011122573287929814, l2: 0.0003856537093626052   Iteration 47 of 100, tot loss = 4.9769770591817, l1: 0.00011096574304788869, l2: 0.0003867319656877798   Iteration 48 of 100, tot loss = 4.991158609588941, l1: 0.00011139554544570274, l2: 0.0003877203183340801   Iteration 49 of 100, tot loss = 4.969268356050764, l1: 0.00011082743924130135, l2: 0.0003860993990888439   Iteration 50 of 100, tot loss = 4.98431013584137, l1: 0.00011086955812061205, l2: 0.0003875614583375864   Iteration 51 of 100, tot loss = 5.002311402676153, l1: 0.00011107411267528055, l2: 0.0003891570305830671   Iteration 52 of 100, tot loss = 4.989032676586738, l1: 0.00011088965276520376, l2: 0.00038801361733931117   Iteration 53 of 100, tot loss = 5.004277206816763, l1: 0.00011156104740638391, l2: 0.000388866675664003   Iteration 54 of 100, tot loss = 4.994196958012051, l1: 0.00011156698464118462, l2: 0.00038785271371361214   Iteration 55 of 100, tot loss = 4.9863023714585735, l1: 0.00011151192500785163, l2: 0.000387118314657445   Iteration 56 of 100, tot loss = 5.014431259461811, l1: 0.0001123107159790899, l2: 0.0003891324130173806   Iteration 57 of 100, tot loss = 5.015147999713295, l1: 0.00011269893564116373, l2: 0.0003888158670426428   Iteration 58 of 100, tot loss = 5.021864114136531, l1: 0.0001130362189573573, l2: 0.00038915019555635556   Iteration 59 of 100, tot loss = 5.015342934656951, l1: 0.00011281740283928969, l2: 0.00038871689333572543   Iteration 60 of 100, tot loss = 4.997761527697246, l1: 0.00011270097929809708, l2: 0.000387075175967766   Iteration 61 of 100, tot loss = 4.964194344692543, l1: 0.00011193449740187402, l2: 0.00038448493945847465   Iteration 62 of 100, tot loss = 4.984000305975637, l1: 0.00011204752011126988, l2: 0.00038635251351684755   Iteration 63 of 100, tot loss = 5.003153377109104, l1: 0.00011247641726633505, l2: 0.0003878389231561284   Iteration 64 of 100, tot loss = 4.995833873748779, l1: 0.00011235519889396528, l2: 0.00038722819158465427   Iteration 65 of 100, tot loss = 4.992735635317289, l1: 0.00011214797861892013, l2: 0.00038712558788784706   Iteration 66 of 100, tot loss = 4.966161644820011, l1: 0.00011172111286820534, l2: 0.00038489505470462257   Iteration 67 of 100, tot loss = 4.992211779551719, l1: 0.00011177508637427227, l2: 0.00038744609519741983   Iteration 68 of 100, tot loss = 4.9948315024375916, l1: 0.00011189245293173692, l2: 0.00038759070080499546   Iteration 69 of 100, tot loss = 4.970711224321006, l1: 0.00011111107190684551, l2: 0.00038596005406757087   Iteration 70 of 100, tot loss = 4.960920681272234, l1: 0.00011120648897693692, l2: 0.00038488558244093187   Iteration 71 of 100, tot loss = 4.942136244035103, l1: 0.00011092664002281823, l2: 0.00038328698736248197   Iteration 72 of 100, tot loss = 4.947158253855175, l1: 0.00011054616278569383, l2: 0.00038416966546921886   Iteration 73 of 100, tot loss = 4.9578308438601555, l1: 0.00011064036451686774, l2: 0.0003851427228591877   Iteration 74 of 100, tot loss = 4.951897585714185, l1: 0.0001100442526925739, l2: 0.00038514550849229   Iteration 75 of 100, tot loss = 4.93722310702006, l1: 0.00010957552493588688, l2: 0.00038414678846796353   Iteration 76 of 100, tot loss = 4.949647028195231, l1: 0.00011009210265910951, l2: 0.00038487260309538166   Iteration 77 of 100, tot loss = 4.921406597286076, l1: 0.00010924984669984112, l2: 0.00038289081576649194   Iteration 78 of 100, tot loss = 4.913824594937838, l1: 0.00010907400615113333, l2: 0.00038230845623333246   Iteration 79 of 100, tot loss = 4.903644380690176, l1: 0.00010852183545409124, l2: 0.000381842605759319   Iteration 80 of 100, tot loss = 4.898724395036697, l1: 0.00010863488450922887, l2: 0.0003812375578490901   Iteration 81 of 100, tot loss = 4.914070276566494, l1: 0.00010905280969045298, l2: 0.0003823542208942573   Iteration 82 of 100, tot loss = 4.907484188312438, l1: 0.00010880268497957575, l2: 0.0003819457365685461   Iteration 83 of 100, tot loss = 4.939094003424587, l1: 0.00010922818165202345, l2: 0.00038468122116924004   Iteration 84 of 100, tot loss = 4.97033467179253, l1: 0.00010993169598805252, l2: 0.00038710177354265137   Iteration 85 of 100, tot loss = 4.995855381909538, l1: 0.00011014895974179072, l2: 0.00038943658086389084   Iteration 86 of 100, tot loss = 5.019752264022827, l1: 0.00011032608482881102, l2: 0.0003916491437256661   Iteration 87 of 100, tot loss = 5.004204204712791, l1: 0.00011033577543091907, l2: 0.00039008464721090365   Iteration 88 of 100, tot loss = 4.981085102666508, l1: 0.00010969692725252985, l2: 0.00038841158493596595   Iteration 89 of 100, tot loss = 4.985750522506371, l1: 0.00010981896305698744, l2: 0.000388756091085137   Iteration 90 of 100, tot loss = 4.966239876217312, l1: 0.0001094577789141719, l2: 0.0003871662104049594   Iteration 91 of 100, tot loss = 4.979522657918406, l1: 0.00010951318072729518, l2: 0.0003884390866765138   Iteration 92 of 100, tot loss = 5.018385685008505, l1: 0.00011007650000069786, l2: 0.0003917620704906648   Iteration 93 of 100, tot loss = 5.032404197159634, l1: 0.00011036426467496791, l2: 0.00039287615690745855   Iteration 94 of 100, tot loss = 5.024503804267721, l1: 0.00011014132491873944, l2: 0.0003923090573268982   Iteration 95 of 100, tot loss = 5.028103261244924, l1: 0.0001101562307188655, l2: 0.0003926540971549816   Iteration 96 of 100, tot loss = 5.066822891434033, l1: 0.00011070698364316438, l2: 0.0003959753068253728   Iteration 97 of 100, tot loss = 5.071453079734881, l1: 0.000110802645610523, l2: 0.00039634266379169316   Iteration 98 of 100, tot loss = 5.073327604605227, l1: 0.00011087486159432994, l2: 0.00039645790033949996   Iteration 99 of 100, tot loss = 5.065195743483726, l1: 0.00011079891978303262, l2: 0.00039572065600283404   Iteration 100 of 100, tot loss = 5.041784603595733, l1: 0.00011056229606765556, l2: 0.00039361616596579553
   End of epoch 1409; saving model... 

Epoch 1410 of 2000
   Iteration 1 of 100, tot loss = 4.082054138183594, l1: 0.00010588466102490202, l2: 0.00030232075368985534   Iteration 2 of 100, tot loss = 3.290705442428589, l1: 8.740398698137142e-05, l2: 0.00024166655202861875   Iteration 3 of 100, tot loss = 3.9705848693847656, l1: 9.714917056650545e-05, l2: 0.00029990931701225537   Iteration 4 of 100, tot loss = 4.126836895942688, l1: 9.489831245446112e-05, l2: 0.0003177853868692182   Iteration 5 of 100, tot loss = 3.7944684982299806, l1: 8.817472844384611e-05, l2: 0.00029127212765160947   Iteration 6 of 100, tot loss = 3.5523144404093423, l1: 8.614412945462391e-05, l2: 0.00026908732009663555   Iteration 7 of 100, tot loss = 3.479250431060791, l1: 8.347839840488242e-05, l2: 0.00026444664815374253   Iteration 8 of 100, tot loss = 3.4741737246513367, l1: 8.198283467208967e-05, l2: 0.0002654345425980864   Iteration 9 of 100, tot loss = 3.8680480321248374, l1: 8.920895278505568e-05, l2: 0.0002975958551461291   Iteration 10 of 100, tot loss = 4.08888611793518, l1: 9.239147329935804e-05, l2: 0.0003164971407386474   Iteration 11 of 100, tot loss = 4.296830177307129, l1: 9.674943232146853e-05, l2: 0.0003329335899748416   Iteration 12 of 100, tot loss = 4.128672540187836, l1: 9.293288500581791e-05, l2: 0.00031993437247971695   Iteration 13 of 100, tot loss = 4.25265541443458, l1: 9.558373998375968e-05, l2: 0.0003296818042424722   Iteration 14 of 100, tot loss = 4.30873555796487, l1: 9.411708145177857e-05, l2: 0.0003367564786458388   Iteration 15 of 100, tot loss = 4.411634651819865, l1: 9.569910932138252e-05, l2: 0.00034546435927040875   Iteration 16 of 100, tot loss = 4.388444676995277, l1: 9.546471414978441e-05, l2: 0.0003433797573961783   Iteration 17 of 100, tot loss = 4.547622638590195, l1: 9.776428893982323e-05, l2: 0.00035699797958573876   Iteration 18 of 100, tot loss = 4.739370703697205, l1: 0.00010177051626669709, l2: 0.0003721665578066475   Iteration 19 of 100, tot loss = 4.716467920102571, l1: 0.00010140638321325624, l2: 0.00037024041337549294   Iteration 20 of 100, tot loss = 4.825487601757049, l1: 0.00010248571798001648, l2: 0.0003800630467594601   Iteration 21 of 100, tot loss = 4.7946633497873945, l1: 0.00010173191947701743, l2: 0.0003777344190027742   Iteration 22 of 100, tot loss = 4.772431774572893, l1: 0.00010195205107844562, l2: 0.0003752911288756877   Iteration 23 of 100, tot loss = 4.975813917491747, l1: 0.00010487902545719407, l2: 0.00039270236764265144   Iteration 24 of 100, tot loss = 4.93121353785197, l1: 0.00010429128224132, l2: 0.00038883007315841195   Iteration 25 of 100, tot loss = 4.921528453826904, l1: 0.0001033651850593742, l2: 0.0003887876612134278   Iteration 26 of 100, tot loss = 4.913731575012207, l1: 0.00010381008192220739, l2: 0.00038756307689114834   Iteration 27 of 100, tot loss = 4.846898282015765, l1: 0.00010245251622917649, l2: 0.0003822373131428052   Iteration 28 of 100, tot loss = 4.848593362740108, l1: 0.00010301426781162653, l2: 0.00038184506930909786   Iteration 29 of 100, tot loss = 4.8022670910276215, l1: 0.00010255659222382056, l2: 0.0003776701181275963   Iteration 30 of 100, tot loss = 4.820800161361694, l1: 0.00010264027211330055, l2: 0.00037943974505954734   Iteration 31 of 100, tot loss = 4.7784825140430085, l1: 0.0001017396877409004, l2: 0.00037610856440639304   Iteration 32 of 100, tot loss = 4.717943325638771, l1: 0.00010069855522942817, l2: 0.0003710957780640456   Iteration 33 of 100, tot loss = 4.662228100227587, l1: 0.00010020334494501267, l2: 0.00036601946604522794   Iteration 34 of 100, tot loss = 4.65940462140476, l1: 9.959313052497557e-05, l2: 0.0003663473326014355   Iteration 35 of 100, tot loss = 4.639219897133963, l1: 9.888199887687473e-05, l2: 0.00036503999144770204   Iteration 36 of 100, tot loss = 4.64663479063246, l1: 9.852467105196815e-05, l2: 0.0003661388085068514   Iteration 37 of 100, tot loss = 4.6237790133502035, l1: 9.78121605766245e-05, l2: 0.00036456574122673154   Iteration 38 of 100, tot loss = 4.610587345926385, l1: 9.798210875918525e-05, l2: 0.0003630766265192314   Iteration 39 of 100, tot loss = 4.642555970412034, l1: 9.904950359039224e-05, l2: 0.00036520609417213843   Iteration 40 of 100, tot loss = 4.629243648052215, l1: 9.876328122118138e-05, l2: 0.00036416108414414337   Iteration 41 of 100, tot loss = 4.606982254400486, l1: 9.87860975572669e-05, l2: 0.00036191212853825674